{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2041f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8cc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380ad84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "#from torchnet.meter import AverageValueMeter\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36d0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118577 data points\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/michael/Desktop/projects/ROAR/data/output\")\n",
    "center_depth_dir = data_dir / \"front_depth\"\n",
    "veh_state_dir = data_dir / \"vehicle_state\"\n",
    "\n",
    "center_depth_paths = [p for p in sorted(center_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "veh_state_paths = [p for p in sorted(veh_state_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "print(f\"Found { len(center_depth_paths)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aba12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import depth2colorjet, random_flip, crop_roi, depthToLogDepth\n",
    "import cv2\n",
    "def build_df_from_y(y):\n",
    "    throttles, steerings = [], []\n",
    "    for st_path in y:\n",
    "        array = np.load(st_path)\n",
    "        throttles.append(array[-2])\n",
    "        steerings.append(array[-1])\n",
    "    df = pd.DataFrame(\n",
    "        data={\"throttle\":throttles, \"steering\":steerings}\n",
    "    )\n",
    "    return df\n",
    "\n",
    "class CarDataset(data.Dataset):\n",
    "    def __init__(self, X, y, img_width=64, img_height=64, should_take_roi=False, roi_min_height=30,\n",
    "                 should_use_log_depth=True, should_stack=False, resize=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.img_width=img_width\n",
    "        self.img_height=img_height\n",
    "        self.should_take_roi = should_take_roi\n",
    "        self.roi_min_height = roi_min_height\n",
    "        self.should_use_log_depth=should_use_log_depth\n",
    "        self.should_stack = should_stack\n",
    "        self.resize = resize\n",
    "        self.df = build_df_from_y(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if np.random.rand(1)[0] < 0.7:\n",
    "            rand_index = self.df[abs(self.df[\"steering\"]) > 0.25].sample(1).index[0]\n",
    "            return self.get_item_helper(rand_index)\n",
    "        else:\n",
    "            return self.get_item_helper(index)\n",
    "\n",
    "    def get_item_helper(self, index):\n",
    "        img_path = self.X[index]\n",
    "        veh_state_path = self.y[index]\n",
    "        steering_angle = np.load(veh_state_path)[-1]\n",
    "        img = np.load(img_path)\n",
    "        if self.should_take_roi: # will not enter\n",
    "            img = img[30:self.img_width, :]\n",
    "        if self.should_use_log_depth:\n",
    "            img = depthToLogDepth(img)\n",
    "        if self.resize is not None:\n",
    "            img = cv2.resize(img, (self.resize))\n",
    "        \n",
    "        # generate random noise to the image\n",
    "        img, steering_angle = random_flip(img, steering_angle)\n",
    "        return img, steering_angle\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c2a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdf7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13745446",
   "metadata": {},
   "source": [
    "### Use monte carlo method to understand our distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9768cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_loader(loader, N=100):\n",
    "    i = 0\n",
    "    loader_generator = iter(loader)\n",
    "    steerings = []\n",
    "    while i < N:\n",
    "        img, steering = next(loader_generator)\n",
    "        angles = steering.numpy()\n",
    "        for angle in angles:\n",
    "            steerings.append(angle)\n",
    "        i += 1\n",
    "    df = pd.DataFrame(\n",
    "        data={\"steering\":steerings}\n",
    "    )\n",
    "    df.hist(\"steering\"), df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4195d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f137496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd4fe065",
   "metadata": {},
   "source": [
    "## Split data into Train and valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a07431",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(center_depth_paths, \n",
    "                                                      veh_state_paths, \n",
    "                                                      test_size=0.1, \n",
    "                                                      shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05714d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "car_train_loader = DataLoader(CarDataset(X=X_train, y=y_train), \n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "car_valid_loader = DataLoader(CarDataset(X=X_valid, y=y_valid), \n",
    "                              batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995e8016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_loader(car_train_loader, N=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ed6490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 600, 800]), torch.Size([1]), 106719)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, angles = next(iter(car_train_loader))\n",
    "imgs.shape, angles.shape, len(car_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a39d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbc707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c7536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CarModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4fd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1b3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTensor(data, use_cuda, device=None):\n",
    "    img, target = data\n",
    "#     img, target = torch.from_numpy(img).float(), torch.from_numpy(np.array([target])).float()\n",
    "    img = img.float()\n",
    "    target = target.float()\n",
    "    if use_cuda:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "    return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb4a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1000000\n",
    "\n",
    "def train(epoch, net, train_loader, optimizer, criterion, use_cuda, \n",
    "          device=None, save_dir=Path(\".\"),MAX_BATCH=1000):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        img, steering_angle = toTensor(data, use_cuda, device)\n",
    "        output = net(img) # Tensor([ITEM])\n",
    "        loss = criterion(output, steering_angle)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'    Batch: {batch_idx} --> Loss: {train_loss / (batch_idx+1)}') \n",
    "        \n",
    "        if batch_idx >= MAX_BATCH:\n",
    "            break\n",
    "    \n",
    "\n",
    "def valid(epoch, net, validloader, criterion, use_cuda, device=None, save_dir=Path(\".\"),MAX_BATCH=1000):\n",
    "    global best_loss\n",
    "    net.eval()\n",
    "    valid_loss = 0 \n",
    "    for batch_idx, data in enumerate(validloader):\n",
    "        img, steering_angle = toTensor(data, use_cuda, device)\n",
    "        outputs = net(img)\n",
    "        loss = criterion(outputs, steering_angle)\n",
    "        valid_loss += loss.data.item()\n",
    "        \n",
    "        avg_valid_loss = valid_loss / (batch_idx + 1)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"    Valid Loss: {avg_valid_loss}\" )\n",
    "        \n",
    "        if avg_valid_loss <= best_loss:\n",
    "            best_loss = avg_valid_loss\n",
    "            print(f\"         Saving.... Best epoch: {epoch} -> {avg_valid_loss}\")\n",
    "            torch.save(net.state_dict(),save_dir / \"best_model_state_dict.h5\" )\n",
    "#             torch.save(net, save_dir / \"best_model.h5\")\n",
    "        \n",
    "        if batch_idx >= MAX_BATCH:\n",
    "            break\n",
    "    \n",
    "    torch.save(net.state_dict(),save_dir / \"model_state_dict.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d48db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c2f4512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/ROAR3.8/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "#load the previous net or use new net\n",
    "# net = torch.load(\"/home/michael/Desktop/projects/ROAR/misc/data/best_model.h5\")\n",
    "# net.training = True\n",
    "\n",
    "net = CarModel(batch_size=batch_size, \n",
    "              image_width=800,\n",
    "              image_height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10b89e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f128bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "    Batch: 0 --> Loss: 0.12930475175380707\n",
      "    Batch: 100 --> Loss: 0.1213341714823004\n",
      "    Batch: 200 --> Loss: 0.10911168004131197\n",
      "    Batch: 300 --> Loss: 0.09655893557135078\n",
      "    Batch: 400 --> Loss: 0.08682453820924518\n",
      "    Batch: 500 --> Loss: 0.07987128551911658\n",
      "    Batch: 600 --> Loss: 0.07352255209667478\n",
      "    Batch: 700 --> Loss: 0.0658066236480989\n",
      "    Batch: 800 --> Loss: 0.06006317603200967\n",
      "    Batch: 900 --> Loss: 0.05581457245647643\n",
      "    Batch: 1000 --> Loss: 0.05281737198390654\n",
      "    Batch: 1100 --> Loss: 0.049635106871046324\n",
      "    Batch: 1200 --> Loss: 0.04714078195118114\n",
      "    Batch: 1300 --> Loss: 0.04443407174023898\n",
      "    Batch: 1400 --> Loss: 0.042477292587460566\n",
      "    Batch: 1500 --> Loss: 0.04045581227663706\n",
      "    Batch: 1600 --> Loss: 0.038910089365243906\n",
      "    Batch: 1700 --> Loss: 0.037707500147060105\n",
      "    Batch: 1800 --> Loss: 0.0367383973318749\n",
      "    Batch: 1900 --> Loss: 0.03553477820688362\n",
      "    Batch: 2000 --> Loss: 0.03453198164653629\n",
      "    Batch: 2100 --> Loss: 0.033727048179483674\n",
      "    Batch: 2200 --> Loss: 0.03294935264542463\n",
      "    Batch: 2300 --> Loss: 0.032224847121211216\n",
      "    Batch: 2400 --> Loss: 0.03152395749489946\n",
      "    Batch: 2500 --> Loss: 0.030856740463716614\n",
      "    Batch: 2600 --> Loss: 0.03042090287019559\n",
      "    Batch: 2700 --> Loss: 0.029716875345037107\n",
      "    Batch: 2800 --> Loss: 0.0291530275546276\n",
      "    Batch: 2900 --> Loss: 0.028641804749600284\n",
      "    Batch: 3000 --> Loss: 0.028170853640710737\n",
      "    Batch: 3100 --> Loss: 0.02775268044371545\n",
      "    Batch: 3200 --> Loss: 0.027258900286431624\n",
      "    Batch: 3300 --> Loss: 0.026876790019071752\n",
      "    Batch: 3400 --> Loss: 0.026645757492883355\n",
      "    Batch: 3500 --> Loss: 0.026284776040620853\n",
      "    Batch: 3600 --> Loss: 0.026031966865513838\n",
      "    Batch: 3700 --> Loss: 0.026140141272944787\n",
      "    Batch: 3800 --> Loss: 0.02590500089243906\n",
      "    Batch: 3900 --> Loss: 0.025706283029243333\n",
      "    Batch: 4000 --> Loss: 0.025349369913845454\n",
      "    Batch: 4100 --> Loss: 0.02525242622029936\n",
      "    Batch: 4200 --> Loss: 0.025047694653554362\n",
      "    Batch: 4300 --> Loss: 0.025120347418466304\n",
      "    Batch: 4400 --> Loss: 0.02486069862758775\n",
      "    Batch: 4500 --> Loss: 0.024779073621376945\n",
      "    Batch: 4600 --> Loss: 0.024848017972850616\n",
      "    Batch: 4700 --> Loss: 0.024725364745101456\n",
      "    Batch: 4800 --> Loss: 0.02462507230988105\n",
      "    Batch: 4900 --> Loss: 0.024480849139330706\n",
      "    Batch: 5000 --> Loss: 0.024215968922617563\n",
      "    Batch: 5100 --> Loss: 0.024214760629376383\n",
      "    Batch: 5200 --> Loss: 0.024083819105259488\n",
      "    Batch: 5300 --> Loss: 0.024082025783963804\n",
      "    Batch: 5400 --> Loss: 0.02391930177182208\n",
      "    Batch: 5500 --> Loss: 0.0236925158615116\n",
      "    Batch: 5600 --> Loss: 0.023660705361680762\n",
      "    Batch: 5700 --> Loss: 0.02363980506863715\n",
      "    Batch: 5800 --> Loss: 0.023687002516954266\n",
      "    Batch: 5900 --> Loss: 0.023586672342021962\n",
      "    Batch: 6000 --> Loss: 0.023594833107837776\n",
      "    Batch: 6100 --> Loss: 0.02339340137355189\n",
      "    Batch: 6200 --> Loss: 0.02323324414212766\n",
      "    Batch: 6300 --> Loss: 0.0231037330142624\n",
      "    Batch: 6400 --> Loss: 0.022933805364418837\n",
      "    Batch: 6500 --> Loss: 0.02277864570563441\n",
      "    Batch: 6600 --> Loss: 0.02266846718061386\n",
      "    Batch: 6700 --> Loss: 0.022540132832279583\n",
      "    Batch: 6800 --> Loss: 0.022468015639644647\n",
      "    Batch: 6900 --> Loss: 0.022429385503498885\n",
      "    Batch: 7000 --> Loss: 0.02230714841885291\n",
      "    Batch: 7100 --> Loss: 0.02217102163148015\n",
      "    Batch: 7200 --> Loss: 0.022066013702938394\n",
      "    Batch: 7300 --> Loss: 0.021938071488654797\n",
      "    Batch: 7400 --> Loss: 0.021823821486527433\n",
      "    Batch: 7500 --> Loss: 0.02172947867372359\n",
      "    Batch: 7600 --> Loss: 0.02161883177914751\n",
      "    Batch: 7700 --> Loss: 0.02149646879964626\n",
      "    Batch: 7800 --> Loss: 0.02155579929550948\n",
      "    Batch: 7900 --> Loss: 0.021416684275369115\n",
      "    Batch: 8000 --> Loss: 0.021273715714246418\n",
      "    Batch: 8100 --> Loss: 0.02118384740724935\n",
      "    Batch: 8200 --> Loss: 0.02109626412376453\n",
      "    Batch: 8300 --> Loss: 0.021079678676476823\n",
      "    Batch: 8400 --> Loss: 0.0210825804928965\n",
      "    Batch: 8500 --> Loss: 0.020984747921084575\n",
      "    Batch: 8600 --> Loss: 0.020966728897471143\n",
      "    Batch: 8700 --> Loss: 0.020892256843016103\n",
      "    Batch: 8800 --> Loss: 0.020845070051210883\n",
      "    Batch: 8900 --> Loss: 0.020749812998785096\n",
      "    Batch: 9000 --> Loss: 0.02061791782721472\n",
      "    Batch: 9100 --> Loss: 0.020533283673503128\n",
      "    Batch: 9200 --> Loss: 0.02047333765882187\n",
      "    Batch: 9300 --> Loss: 0.020339202626266577\n",
      "    Batch: 9400 --> Loss: 0.020238957162377662\n",
      "    Batch: 9500 --> Loss: 0.02018926491839735\n",
      "    Batch: 9600 --> Loss: 0.020095502700320855\n",
      "    Batch: 9700 --> Loss: 0.02002076670563855\n",
      "    Batch: 9800 --> Loss: 0.019973687771611345\n",
      "    Batch: 9900 --> Loss: 0.019878630846364646\n",
      "    Batch: 10000 --> Loss: 0.019818187491798415\n",
      "    Batch: 10100 --> Loss: 0.019756071008573375\n",
      "    Batch: 10200 --> Loss: 0.01977492518676836\n",
      "    Batch: 10300 --> Loss: 0.0197955208081799\n",
      "    Batch: 10400 --> Loss: 0.01975431202774231\n",
      "    Batch: 10500 --> Loss: 0.019686296057406998\n",
      "    Batch: 10600 --> Loss: 0.01963451945985105\n",
      "    Batch: 10700 --> Loss: 0.019593937828238456\n",
      "    Batch: 10800 --> Loss: 0.019550089634105167\n",
      "    Batch: 10900 --> Loss: 0.019481162155354413\n",
      "    Batch: 11000 --> Loss: 0.01944373563867615\n",
      "    Batch: 11100 --> Loss: 0.019428084293491882\n",
      "    Batch: 11200 --> Loss: 0.019405859118626637\n",
      "    Batch: 11300 --> Loss: 0.019365859974751005\n",
      "    Batch: 11400 --> Loss: 0.01930034414583515\n",
      "    Batch: 11500 --> Loss: 0.01925919358803763\n",
      "    Batch: 11600 --> Loss: 0.019200608496591526\n",
      "    Batch: 11700 --> Loss: 0.019134139001339433\n",
      "    Batch: 11800 --> Loss: 0.019079008415885335\n",
      "    Batch: 11900 --> Loss: 0.01904259218969353\n",
      "    Batch: 12000 --> Loss: 0.01898582113707612\n",
      "    Batch: 12100 --> Loss: 0.018929176150945932\n",
      "    Batch: 12200 --> Loss: 0.018867298494877078\n",
      "    Batch: 12300 --> Loss: 0.018804352324928265\n",
      "    Batch: 12400 --> Loss: 0.01874220990759854\n",
      "    Batch: 12500 --> Loss: 0.01869537054248834\n",
      "    Batch: 12600 --> Loss: 0.018623174528049416\n",
      "    Batch: 12700 --> Loss: 0.018561959767975138\n",
      "    Batch: 12800 --> Loss: 0.018531892698387474\n",
      "    Batch: 12900 --> Loss: 0.018469728710244902\n",
      "    Batch: 13000 --> Loss: 0.01845053934869141\n",
      "    Batch: 13100 --> Loss: 0.018373253705441146\n",
      "    Batch: 13200 --> Loss: 0.018322834565026796\n",
      "    Batch: 13300 --> Loss: 0.01828150235762851\n",
      "    Batch: 13400 --> Loss: 0.018281428117455215\n",
      "    Batch: 13500 --> Loss: 0.018258775613443735\n",
      "    Batch: 13600 --> Loss: 0.01823019245408395\n",
      "    Batch: 13700 --> Loss: 0.018207466022052653\n",
      "    Batch: 13800 --> Loss: 0.018179046017332813\n",
      "    Batch: 13900 --> Loss: 0.01815541401124779\n",
      "    Batch: 14000 --> Loss: 0.018107032505292373\n",
      "    Batch: 14100 --> Loss: 0.018046106644899678\n",
      "    Batch: 14200 --> Loss: 0.01802392617541453\n",
      "    Batch: 14300 --> Loss: 0.01799099593685059\n",
      "    Batch: 14400 --> Loss: 0.017951688109362453\n",
      "    Batch: 14500 --> Loss: 0.01789247515070169\n",
      "    Batch: 14600 --> Loss: 0.01786010132309007\n",
      "    Batch: 14700 --> Loss: 0.01780592808892776\n",
      "    Batch: 14800 --> Loss: 0.017739446470562338\n",
      "    Batch: 14900 --> Loss: 0.017714419365950373\n",
      "    Batch: 15000 --> Loss: 0.017680308482956028\n",
      "    Batch: 15100 --> Loss: 0.017663001737605814\n",
      "    Batch: 15200 --> Loss: 0.017633084391158082\n",
      "    Batch: 15300 --> Loss: 0.017602245488571967\n",
      "    Batch: 15400 --> Loss: 0.01757838397307203\n",
      "    Batch: 15500 --> Loss: 0.017549674206541178\n",
      "    Batch: 15600 --> Loss: 0.017518743610213054\n",
      "    Batch: 15700 --> Loss: 0.017485349087705597\n",
      "    Batch: 15800 --> Loss: 0.017436844047719578\n",
      "    Batch: 15900 --> Loss: 0.017440400423355616\n",
      "    Batch: 16000 --> Loss: 0.017408666712158436\n",
      "    Batch: 16100 --> Loss: 0.01740754955867782\n",
      "    Batch: 16200 --> Loss: 0.017396461117683434\n",
      "    Batch: 16300 --> Loss: 0.01734804494869537\n",
      "    Batch: 16400 --> Loss: 0.017298609880809404\n",
      "    Batch: 16500 --> Loss: 0.01731152947569266\n",
      "    Batch: 16600 --> Loss: 0.017260460392095403\n",
      "    Batch: 16700 --> Loss: 0.017232877120860093\n",
      "    Batch: 16800 --> Loss: 0.01719563962185939\n",
      "    Batch: 16900 --> Loss: 0.017183473414189042\n",
      "    Batch: 17000 --> Loss: 0.017144831916902897\n",
      "    Batch: 17100 --> Loss: 0.017118278450285563\n",
      "    Batch: 17200 --> Loss: 0.01706151017668448\n",
      "    Batch: 17300 --> Loss: 0.017033877642863494\n",
      "    Batch: 17400 --> Loss: 0.01698484747094877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 17500 --> Loss: 0.016955271979019956\n",
      "    Batch: 17600 --> Loss: 0.016904738931701163\n",
      "    Batch: 17700 --> Loss: 0.016861603012617698\n",
      "    Batch: 17800 --> Loss: 0.01681739524603371\n",
      "    Batch: 17900 --> Loss: 0.01678186889270917\n",
      "    Batch: 18000 --> Loss: 0.016764483523338338\n",
      "    Batch: 18100 --> Loss: 0.016719852506620465\n",
      "    Batch: 18200 --> Loss: 0.016679631920189678\n",
      "    Batch: 18300 --> Loss: 0.01671729823730133\n",
      "    Batch: 18400 --> Loss: 0.0166847839581468\n",
      "    Batch: 18500 --> Loss: 0.016660034509203395\n",
      "    Batch: 18600 --> Loss: 0.01662219317069713\n",
      "    Batch: 18700 --> Loss: 0.016607074210961206\n",
      "    Batch: 18800 --> Loss: 0.01656750855270407\n",
      "    Batch: 18900 --> Loss: 0.016550535027461847\n",
      "    Batch: 19000 --> Loss: 0.016524215912445975\n",
      "    Batch: 19100 --> Loss: 0.016508389247237906\n",
      "    Batch: 19200 --> Loss: 0.01647517511688955\n",
      "    Batch: 19300 --> Loss: 0.016463259322508778\n",
      "    Batch: 19400 --> Loss: 0.01643439241780796\n",
      "    Batch: 19500 --> Loss: 0.01640980286507978\n",
      "    Batch: 19600 --> Loss: 0.016369537886370977\n",
      "    Batch: 19700 --> Loss: 0.01633215709131196\n",
      "    Batch: 19800 --> Loss: 0.016296576118621887\n",
      "    Batch: 19900 --> Loss: 0.01626954934658091\n",
      "    Batch: 20000 --> Loss: 0.01624130228890576\n",
      "    Batch: 20100 --> Loss: 0.016207007890910695\n",
      "    Batch: 20200 --> Loss: 0.016164659588404195\n",
      "    Batch: 20300 --> Loss: 0.01616123098194965\n",
      "    Batch: 20400 --> Loss: 0.016117253265648997\n",
      "    Batch: 20500 --> Loss: 0.016082148889498372\n",
      "    Batch: 20600 --> Loss: 0.016069238773291323\n",
      "    Batch: 20700 --> Loss: 0.016048459956944074\n",
      "    Batch: 20800 --> Loss: 0.01602558066567697\n",
      "    Batch: 20900 --> Loss: 0.01599174907024645\n",
      "    Batch: 21000 --> Loss: 0.015999805968930206\n",
      "    Batch: 21100 --> Loss: 0.015987794633538003\n",
      "    Batch: 21200 --> Loss: 0.01597912881689479\n",
      "    Batch: 21300 --> Loss: 0.015951296543030857\n",
      "    Batch: 21400 --> Loss: 0.015924153878977058\n",
      "    Batch: 21500 --> Loss: 0.015901231351332666\n",
      "    Batch: 21600 --> Loss: 0.015873890996961508\n",
      "    Batch: 21700 --> Loss: 0.015834861797089442\n",
      "    Batch: 21800 --> Loss: 0.015814694479521847\n",
      "    Batch: 21900 --> Loss: 0.01578016384920143\n",
      "    Batch: 22000 --> Loss: 0.01575756468107277\n",
      "    Batch: 22100 --> Loss: 0.015729350589087497\n",
      "    Batch: 22200 --> Loss: 0.015712409899348474\n",
      "    Batch: 22300 --> Loss: 0.015694254507823534\n",
      "    Batch: 22400 --> Loss: 0.015669854297454718\n",
      "    Batch: 22500 --> Loss: 0.015647608462731354\n",
      "    Batch: 22600 --> Loss: 0.01562191692902003\n",
      "    Batch: 22700 --> Loss: 0.015608246237617264\n",
      "    Batch: 22800 --> Loss: 0.015565687902019862\n",
      "    Batch: 22900 --> Loss: 0.015528264922407425\n",
      "    Batch: 23000 --> Loss: 0.015528079540123866\n",
      "    Batch: 23100 --> Loss: 0.015525518579096205\n",
      "    Batch: 23200 --> Loss: 0.015528437359409758\n",
      "    Batch: 23300 --> Loss: 0.01550775500455283\n",
      "    Batch: 23400 --> Loss: 0.01548468665121736\n",
      "    Batch: 23500 --> Loss: 0.015474866744591757\n",
      "    Batch: 23600 --> Loss: 0.015462266692504148\n",
      "    Batch: 23700 --> Loss: 0.015435918649819257\n",
      "    Batch: 23800 --> Loss: 0.015412310931775457\n",
      "    Batch: 23900 --> Loss: 0.015405501786827222\n",
      "    Batch: 24000 --> Loss: 0.015418432378087536\n",
      "    Batch: 24100 --> Loss: 0.01542180870563211\n",
      "    Batch: 24200 --> Loss: 0.015385584247851102\n",
      "    Batch: 24300 --> Loss: 0.015365207051051094\n",
      "    Batch: 24400 --> Loss: 0.015330983939558283\n",
      "    Batch: 24500 --> Loss: 0.015303832318079\n",
      "    Batch: 24600 --> Loss: 0.015293833799606419\n",
      "    Batch: 24700 --> Loss: 0.015263063269021796\n",
      "    Batch: 24800 --> Loss: 0.015275712457330393\n",
      "    Batch: 24900 --> Loss: 0.015252471628293645\n",
      "    Batch: 25000 --> Loss: 0.015227440122546093\n",
      "    Batch: 25100 --> Loss: 0.015226540824634286\n",
      "    Batch: 25200 --> Loss: 0.015198289623565635\n",
      "    Batch: 25300 --> Loss: 0.015160792565792844\n",
      "    Batch: 25400 --> Loss: 0.015121869958012823\n",
      "    Batch: 25500 --> Loss: 0.015096416002724789\n",
      "    Batch: 25600 --> Loss: 0.015066523141832567\n",
      "    Batch: 25700 --> Loss: 0.015046943766299906\n",
      "    Batch: 25800 --> Loss: 0.015047115523185155\n",
      "    Batch: 25900 --> Loss: 0.015030912117472813\n",
      "    Batch: 26000 --> Loss: 0.015023593810169\n",
      "    Batch: 26100 --> Loss: 0.015024568526564046\n",
      "    Batch: 26200 --> Loss: 0.015025722805724638\n",
      "    Batch: 26300 --> Loss: 0.015004581653450755\n",
      "    Batch: 26400 --> Loss: 0.014997463199315558\n",
      "    Batch: 26500 --> Loss: 0.014973062186421651\n",
      "    Batch: 26600 --> Loss: 0.014964732728861936\n",
      "    Valid Loss: 0.002179632196202874\n",
      "         Saving.... Best epoch: 0 -> 0.002179632196202874\n",
      "         Saving.... Best epoch: 0 -> 0.0012367258314043283\n",
      "         Saving.... Best epoch: 0 -> 0.0008582043131658187\n",
      "    Valid Loss: 0.005983398417559122\n",
      "    Valid Loss: 0.007904596721915024\n",
      "    Valid Loss: 0.009721424712916352\n",
      "    Valid Loss: 0.009634419779393352\n",
      "    Valid Loss: 0.009675319591175324\n",
      "    Valid Loss: 0.009841486234906846\n",
      "    Valid Loss: 0.009903322107500388\n",
      "    Valid Loss: 0.009880342602799547\n",
      "    Valid Loss: 0.009447972150905839\n",
      "    Valid Loss: 0.009154527977532365\n",
      "Epoch 1\n",
      "    Batch: 0 --> Loss: 0.000264766626060009\n",
      "    Batch: 100 --> Loss: 0.009383291662578811\n",
      "    Batch: 200 --> Loss: 0.008817961104553481\n",
      "    Batch: 300 --> Loss: 0.011178709327223361\n",
      "    Batch: 400 --> Loss: 0.010995517054113292\n",
      "    Batch: 500 --> Loss: 0.010742699768991667\n",
      "    Batch: 600 --> Loss: 0.01043545775936017\n",
      "    Batch: 700 --> Loss: 0.01016950053195919\n",
      "    Batch: 800 --> Loss: 0.009804907438534693\n",
      "    Batch: 900 --> Loss: 0.009438637417743383\n",
      "    Batch: 1000 --> Loss: 0.009790977218293244\n",
      "    Batch: 1100 --> Loss: 0.009842410236891504\n",
      "    Batch: 1200 --> Loss: 0.009608315997717484\n",
      "    Batch: 1300 --> Loss: 0.0095270433681193\n",
      "    Batch: 1400 --> Loss: 0.009898325409680058\n",
      "    Batch: 1500 --> Loss: 0.009710415409532169\n",
      "    Batch: 1600 --> Loss: 0.010024639458344415\n",
      "    Batch: 1700 --> Loss: 0.0102974909219102\n",
      "    Batch: 1800 --> Loss: 0.01013210767888493\n",
      "    Batch: 1900 --> Loss: 0.009915377601145331\n",
      "    Batch: 2000 --> Loss: 0.00988148048848949\n",
      "    Batch: 2100 --> Loss: 0.009913088513939747\n",
      "    Batch: 2200 --> Loss: 0.009867002443662782\n",
      "    Batch: 2300 --> Loss: 0.00981414331273203\n",
      "    Batch: 2400 --> Loss: 0.009923864611066327\n",
      "    Batch: 2500 --> Loss: 0.009948505483771993\n",
      "    Batch: 2600 --> Loss: 0.009848602369946708\n",
      "    Batch: 2700 --> Loss: 0.009773606798318664\n",
      "    Batch: 2800 --> Loss: 0.009774026125077601\n",
      "    Batch: 2900 --> Loss: 0.00961552254899261\n",
      "    Batch: 3000 --> Loss: 0.009825206840896389\n",
      "    Batch: 3100 --> Loss: 0.009936029001162859\n",
      "    Batch: 3200 --> Loss: 0.009910328116671193\n",
      "    Batch: 3300 --> Loss: 0.009948224399035198\n",
      "    Batch: 3400 --> Loss: 0.010087005956868607\n",
      "    Batch: 3500 --> Loss: 0.010047475350069657\n",
      "    Batch: 3600 --> Loss: 0.010073797212461858\n",
      "    Batch: 3700 --> Loss: 0.010048442210268328\n",
      "    Batch: 3800 --> Loss: 0.010035660283862156\n",
      "    Batch: 3900 --> Loss: 0.009978608296369554\n",
      "    Batch: 4000 --> Loss: 0.01010056758784729\n",
      "    Batch: 4100 --> Loss: 0.01004664180282254\n",
      "    Batch: 4200 --> Loss: 0.009967369604679106\n",
      "    Batch: 4300 --> Loss: 0.010116563925257874\n",
      "    Batch: 4400 --> Loss: 0.010076973381092664\n",
      "    Batch: 4500 --> Loss: 0.010000306523841316\n",
      "    Batch: 4600 --> Loss: 0.010003768399701948\n",
      "    Batch: 4700 --> Loss: 0.009997295351553524\n",
      "    Batch: 4800 --> Loss: 0.009930106265501934\n",
      "    Batch: 4900 --> Loss: 0.009862980229167482\n",
      "    Batch: 5000 --> Loss: 0.009899955137425188\n",
      "    Batch: 5100 --> Loss: 0.009879311592161105\n",
      "    Batch: 5200 --> Loss: 0.00982522460666861\n",
      "    Batch: 5300 --> Loss: 0.009751266961922684\n",
      "    Batch: 5400 --> Loss: 0.009757163473089973\n",
      "    Batch: 5500 --> Loss: 0.009781412525252945\n",
      "    Batch: 5600 --> Loss: 0.009787511999252149\n",
      "    Batch: 5700 --> Loss: 0.009807773304672996\n",
      "    Batch: 5800 --> Loss: 0.009756549611851526\n",
      "    Batch: 5900 --> Loss: 0.009673256201314435\n",
      "    Batch: 6000 --> Loss: 0.009628754477425257\n",
      "    Batch: 6100 --> Loss: 0.009592447273894697\n",
      "    Batch: 6200 --> Loss: 0.00961942630892964\n",
      "    Batch: 6300 --> Loss: 0.009576051967021007\n",
      "    Batch: 6400 --> Loss: 0.009527240873066247\n",
      "    Batch: 6500 --> Loss: 0.009495098182499674\n",
      "    Batch: 6600 --> Loss: 0.009520306931986092\n",
      "    Batch: 6700 --> Loss: 0.009551675192782471\n",
      "    Batch: 6800 --> Loss: 0.009641259331518935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 6900 --> Loss: 0.009721681680389598\n",
      "    Batch: 7000 --> Loss: 0.009652008026840391\n",
      "    Batch: 7100 --> Loss: 0.009626605319128881\n",
      "    Batch: 7200 --> Loss: 0.009653179157010439\n",
      "    Batch: 7300 --> Loss: 0.009619696689262854\n",
      "    Batch: 7400 --> Loss: 0.009597292591444439\n",
      "    Batch: 7500 --> Loss: 0.009563131185914463\n",
      "    Batch: 7600 --> Loss: 0.009563112248414664\n",
      "    Batch: 7700 --> Loss: 0.009557151174079112\n",
      "    Batch: 7800 --> Loss: 0.00955186317207266\n",
      "    Batch: 7900 --> Loss: 0.00952354155615313\n",
      "    Batch: 8000 --> Loss: 0.009548821989909107\n",
      "    Batch: 8100 --> Loss: 0.00954925781661976\n",
      "    Batch: 8200 --> Loss: 0.009547303136863958\n",
      "    Batch: 8300 --> Loss: 0.009526569998745082\n",
      "    Batch: 8400 --> Loss: 0.009505535063097613\n",
      "    Batch: 8500 --> Loss: 0.009474104442929883\n",
      "    Batch: 8600 --> Loss: 0.00946803788708148\n",
      "    Batch: 8700 --> Loss: 0.009531237304439772\n",
      "    Batch: 8800 --> Loss: 0.00950017753368076\n",
      "    Batch: 8900 --> Loss: 0.009477319668986956\n",
      "    Batch: 9000 --> Loss: 0.009474304464262439\n",
      "    Batch: 9100 --> Loss: 0.009468918950581502\n",
      "    Batch: 9200 --> Loss: 0.009459385462954883\n",
      "    Batch: 9300 --> Loss: 0.009470332104785107\n",
      "    Batch: 9400 --> Loss: 0.009472234848168844\n",
      "    Batch: 9500 --> Loss: 0.009500835449938492\n",
      "    Batch: 9600 --> Loss: 0.009459316414112798\n",
      "    Batch: 9700 --> Loss: 0.009425967993155692\n",
      "    Batch: 9800 --> Loss: 0.0093982092419221\n",
      "    Batch: 9900 --> Loss: 0.009387449038039338\n",
      "    Batch: 10000 --> Loss: 0.009408094865363202\n",
      "    Batch: 10100 --> Loss: 0.009430751378965836\n",
      "    Batch: 10200 --> Loss: 0.009448529928654623\n",
      "    Batch: 10300 --> Loss: 0.009431866802623652\n",
      "    Batch: 10400 --> Loss: 0.009418056530544816\n",
      "    Batch: 10500 --> Loss: 0.009463495427652706\n",
      "    Batch: 10600 --> Loss: 0.009454815388399075\n",
      "    Batch: 10700 --> Loss: 0.009428705327176965\n",
      "    Batch: 10800 --> Loss: 0.009431658107715665\n",
      "    Batch: 10900 --> Loss: 0.009405157083480381\n",
      "    Batch: 11000 --> Loss: 0.009376619638453166\n",
      "    Batch: 11100 --> Loss: 0.009378078324707976\n",
      "    Batch: 11200 --> Loss: 0.00935122479806931\n",
      "    Batch: 11300 --> Loss: 0.009422182704156625\n",
      "    Batch: 11400 --> Loss: 0.009396667263052372\n",
      "    Batch: 11500 --> Loss: 0.009365889405715517\n",
      "    Batch: 11600 --> Loss: 0.009353663605627654\n",
      "    Batch: 11700 --> Loss: 0.009351382674897133\n",
      "    Batch: 11800 --> Loss: 0.009337074326689423\n",
      "    Batch: 11900 --> Loss: 0.00934778011134385\n",
      "    Batch: 12000 --> Loss: 0.009322891900440548\n",
      "    Batch: 12100 --> Loss: 0.00929427750180693\n",
      "    Batch: 12200 --> Loss: 0.009312367938026976\n",
      "    Batch: 12300 --> Loss: 0.009305837159256664\n",
      "    Batch: 12400 --> Loss: 0.00926985349574124\n",
      "    Batch: 12500 --> Loss: 0.00927608862010123\n",
      "    Batch: 12600 --> Loss: 0.009249439196898433\n",
      "    Batch: 12700 --> Loss: 0.009237471307891038\n",
      "    Batch: 12800 --> Loss: 0.009223213367921843\n",
      "    Batch: 12900 --> Loss: 0.009234937198295863\n",
      "    Batch: 13000 --> Loss: 0.009201357668393868\n",
      "    Batch: 13100 --> Loss: 0.009194086423306472\n",
      "    Batch: 13200 --> Loss: 0.009171882124811747\n",
      "    Batch: 13300 --> Loss: 0.009196106224607464\n",
      "    Batch: 13400 --> Loss: 0.009177930581025971\n",
      "    Batch: 13500 --> Loss: 0.009163467822438225\n",
      "    Batch: 13600 --> Loss: 0.0091569044040075\n",
      "    Batch: 13700 --> Loss: 0.009142585640396243\n",
      "    Batch: 13800 --> Loss: 0.009135749973773472\n",
      "    Batch: 13900 --> Loss: 0.009118081417697713\n",
      "    Batch: 14000 --> Loss: 0.009105505484747735\n",
      "    Batch: 14100 --> Loss: 0.009066152589728378\n",
      "    Batch: 14200 --> Loss: 0.009071700107557566\n",
      "    Batch: 14300 --> Loss: 0.009091850229914655\n",
      "    Batch: 14400 --> Loss: 0.0090927327061649\n",
      "    Batch: 14500 --> Loss: 0.00908469348932096\n",
      "    Batch: 14600 --> Loss: 0.009088176479174535\n",
      "    Batch: 14700 --> Loss: 0.009066530304490549\n",
      "    Batch: 14800 --> Loss: 0.009072098552546544\n",
      "    Batch: 14900 --> Loss: 0.00905991639501751\n",
      "    Batch: 15000 --> Loss: 0.009058479250645186\n",
      "    Batch: 15100 --> Loss: 0.009052921299176868\n",
      "    Batch: 15200 --> Loss: 0.009064750628797935\n",
      "    Batch: 15300 --> Loss: 0.009052678577863134\n",
      "    Batch: 15400 --> Loss: 0.009058545565085818\n",
      "    Batch: 15500 --> Loss: 0.009024774140453669\n",
      "    Batch: 15600 --> Loss: 0.009008429168315337\n",
      "    Batch: 15700 --> Loss: 0.008995602023421404\n",
      "    Batch: 15800 --> Loss: 0.008981853415500667\n",
      "    Batch: 15900 --> Loss: 0.008963647221841594\n",
      "    Batch: 16000 --> Loss: 0.00897822503916303\n",
      "    Batch: 16100 --> Loss: 0.008961961748574283\n",
      "    Batch: 16200 --> Loss: 0.008949850724368817\n",
      "    Batch: 16300 --> Loss: 0.0089341170137501\n",
      "    Batch: 16400 --> Loss: 0.008943458176582663\n",
      "    Batch: 16500 --> Loss: 0.008987140277875887\n",
      "    Batch: 16600 --> Loss: 0.008984756921911013\n",
      "    Batch: 16700 --> Loss: 0.008973097491689763\n",
      "    Batch: 16800 --> Loss: 0.008957870668854699\n",
      "    Batch: 16900 --> Loss: 0.00895092000769433\n",
      "    Batch: 17000 --> Loss: 0.008960954696527258\n",
      "    Batch: 17100 --> Loss: 0.008949276170287193\n",
      "    Batch: 17200 --> Loss: 0.008930624430836226\n",
      "    Batch: 17300 --> Loss: 0.008917708038856768\n",
      "    Batch: 17400 --> Loss: 0.008954381210263726\n",
      "    Batch: 17500 --> Loss: 0.008950826014803626\n",
      "    Batch: 17600 --> Loss: 0.008925214137934253\n",
      "    Batch: 17700 --> Loss: 0.008902064789241392\n",
      "    Batch: 17800 --> Loss: 0.008945459337155716\n",
      "    Batch: 17900 --> Loss: 0.00893849106775545\n",
      "    Batch: 18000 --> Loss: 0.008962395819103984\n",
      "    Batch: 18100 --> Loss: 0.008962171742657732\n",
      "    Batch: 18200 --> Loss: 0.008958154605640995\n",
      "    Batch: 18300 --> Loss: 0.008967340872626886\n",
      "    Batch: 18400 --> Loss: 0.00896964774741299\n",
      "    Batch: 18500 --> Loss: 0.008950213901049843\n",
      "    Batch: 18600 --> Loss: 0.00895738428730683\n",
      "    Batch: 18700 --> Loss: 0.008933121427674419\n",
      "    Batch: 18800 --> Loss: 0.008928747201428304\n",
      "    Batch: 18900 --> Loss: 0.008912005783429051\n",
      "    Batch: 19000 --> Loss: 0.0089215998204773\n",
      "    Batch: 19100 --> Loss: 0.008948476093456685\n",
      "    Batch: 19200 --> Loss: 0.008948199558286482\n",
      "    Batch: 19300 --> Loss: 0.008934692075431658\n",
      "    Batch: 19400 --> Loss: 0.008936443813102482\n",
      "    Batch: 19500 --> Loss: 0.008927136657540144\n",
      "    Batch: 19600 --> Loss: 0.00891573075877288\n",
      "    Batch: 19700 --> Loss: 0.008920670636329054\n",
      "    Batch: 19800 --> Loss: 0.00891539461090947\n",
      "    Batch: 19900 --> Loss: 0.008897216440650923\n",
      "    Batch: 20000 --> Loss: 0.008896281576736515\n",
      "    Batch: 20100 --> Loss: 0.008907749604550234\n",
      "    Batch: 20200 --> Loss: 0.008894631518131352\n",
      "    Batch: 20300 --> Loss: 0.008905374801572911\n",
      "    Batch: 20400 --> Loss: 0.008898768396840416\n",
      "    Batch: 20500 --> Loss: 0.008906330232316597\n",
      "    Batch: 20600 --> Loss: 0.00889803076363214\n",
      "    Batch: 20700 --> Loss: 0.008879509903728924\n",
      "    Batch: 20800 --> Loss: 0.008860556660476035\n",
      "    Batch: 20900 --> Loss: 0.00884426307526203\n",
      "    Batch: 21000 --> Loss: 0.008831513133450625\n",
      "    Batch: 21100 --> Loss: 0.008836779494472894\n",
      "    Batch: 21200 --> Loss: 0.0088165489904104\n",
      "    Batch: 21300 --> Loss: 0.008831876600758026\n",
      "    Batch: 21400 --> Loss: 0.008817693612044536\n",
      "    Batch: 21500 --> Loss: 0.008804395373158877\n",
      "    Batch: 21600 --> Loss: 0.00882728787089447\n",
      "    Batch: 21700 --> Loss: 0.008860895019476907\n",
      "    Batch: 21800 --> Loss: 0.008852145917058147\n",
      "    Batch: 21900 --> Loss: 0.008843918880592709\n",
      "    Batch: 22000 --> Loss: 0.008855234191132627\n",
      "    Batch: 22100 --> Loss: 0.008851173071262269\n",
      "    Batch: 22200 --> Loss: 0.008831069209529372\n",
      "    Batch: 22300 --> Loss: 0.008848558939728614\n",
      "    Batch: 22400 --> Loss: 0.008841700665220173\n",
      "    Batch: 22500 --> Loss: 0.008827655580610754\n",
      "    Batch: 22600 --> Loss: 0.008818646383616282\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18572/3049274906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     train(epoch, net, car_train_loader, optimizer, criterion, use_cuda, device=device, save_dir=save_dir, \n\u001b[0m\u001b[1;32m      8\u001b[0m           MAX_BATCH=len(car_train_loader)//4)\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcar_valid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_BATCH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18572/2565364393.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, train_loader, optimizer, criterion, use_cuda, device, save_dir, MAX_BATCH)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'    Batch: {batch_idx} --> Loss: {train_loss / (batch_idx+1)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"./data\")\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "if use_cuda:\n",
    "    net.to(device)\n",
    "for epoch in range(0, 10):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train(epoch, net, car_train_loader, optimizer, criterion, use_cuda, device=device, save_dir=save_dir, \n",
    "          MAX_BATCH=len(car_train_loader)//4)\n",
    "    valid(epoch, net, car_valid_loader, criterion, use_cuda, device=device, save_dir=save_dir, MAX_BATCH=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018df332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4b712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a75d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2572979 0.247793510556221\n",
      "-0.24644285 -0.23011928796768188\n",
      "0.27391472 0.3711860775947571\n",
      "0.2573366 0.3710019886493683\n",
      "-0.12147804 -0.07343771308660507\n",
      "0.1489084 0.24445492029190063\n",
      "-0.21758002 -0.2101329267024994\n",
      "-0.14934948 -0.20807412266731262\n",
      "-0.23870525 -0.23123469948768616\n",
      "0.22465569 0.2074546217918396\n",
      "0.047944836 0.031077463179826736\n",
      "-0.13415655 -0.07748732715845108\n",
      "0.21049064 0.2122012823820114\n",
      "0.17843717 0.05034371465444565\n",
      "0.053923912 0.07145029306411743\n",
      "-0.1585224 -0.21463623642921448\n",
      "-0.047052696 0.02933523990213871\n",
      "0.22325256 0.22274674475193024\n",
      "0.22667295 0.20325329899787903\n",
      "0.16792546 0.08929488807916641\n",
      "-0.28596342 -0.31280726194381714\n",
      "-0.1716949 -0.206185981631279\n",
      "-0.2584745 -0.22814881801605225\n",
      "0.1634875 0.21009241044521332\n",
      "0.0223137 0.28685715794563293\n",
      "-0.13322711 0.04804837331175804\n",
      "0.23795572 0.21330009400844574\n",
      "0.20664993 0.20744571089744568\n",
      "-0.22905114 -0.21201227605342865\n",
      "0.19513035 0.11184201389551163\n",
      "-0.21932608 -0.2076878547668457\n",
      "0.22515023 0.40488389134407043\n",
      "0.20617121 0.09835697710514069\n",
      "0.19430137 0.24700482189655304\n",
      "0.18846855 0.25174522399902344\n",
      "-0.20413837 -0.2561732530593872\n",
      "0.21848813 0.22914135456085205\n",
      "-0.07019129 -0.21994927525520325\n",
      "-0.20228058 -0.230763241648674\n",
      "-0.2422978 -0.20755894482135773\n",
      "0.13129652 0.10012871026992798\n",
      "0.21147615 0.23830261826515198\n",
      "0.24666536 0.32884249091148376\n",
      "0.2795755 0.20140232145786285\n",
      "0.17869937 0.2191089391708374\n",
      "-0.2875074 -0.202180415391922\n",
      "-0.07596926 -0.11236567795276642\n",
      "0.23389193 0.21978037059307098\n",
      "-0.21510258 -0.2426493763923645\n",
      "-0.16400403 -0.25929775834083557\n",
      "-0.104980774 -0.22242388129234314\n",
      "-0.23293936 -0.11915450543165207\n",
      "0.077338874 0.005047404207289219\n",
      "0.037287258 0.07459592819213867\n",
      "-0.25769365 -0.2649552822113037\n",
      "-0.28651792 -0.2238268256187439\n",
      "0.1830109 0.2076878547668457\n",
      "0.16762163 0.20130379498004913\n",
      "0.15097144 0.019036458805203438\n",
      "-0.22905114 -0.21201227605342865\n",
      "0.045102336 0.026939837262034416\n",
      "-0.011610985 -0.02431647479534149\n",
      "0.20421952 0.2588503956794739\n",
      "-0.01623571 -0.05586213991045952\n",
      "0.19591334 0.31529471278190613\n",
      "-0.007027641 -0.05306825786828995\n",
      "-0.17048943 -0.09283540397882462\n",
      "0.19206342 0.21736565232276917\n",
      "0.24666536 0.32884249091148376\n",
      "0.3036117 0.38599109649658203\n",
      "0.23190543 0.20888188481330872\n",
      "-0.24524564 -0.2669019103050232\n",
      "-0.07626722 -0.061330754309892654\n",
      "0.26871753 0.39660415053367615\n",
      "0.17578736 0.2305709272623062\n",
      "0.18956599 0.22677946090698242\n",
      "-0.023420379 0.029005035758018494\n",
      "-0.25768706 -0.24628868699073792\n",
      "0.2857551 0.23417195677757263\n",
      "0.2220627 0.20598772168159485\n",
      "-0.04645621 -0.041490133851766586\n",
      "-0.06698918 -0.07619937509298325\n",
      "0.16574565 0.036513056606054306\n",
      "0.02522 -0.026518944650888443\n",
      "0.20084295 0.20084545016288757\n",
      "0.2075716 0.09459178149700165\n",
      "0.24410355 0.2436084896326065\n",
      "-0.01117412 0.2793973386287689\n",
      "-0.15136021 -0.09391532838344574\n",
      "-0.038222447 -0.021364916115999222\n",
      "-0.16320819 -0.17913749814033508\n",
      "0.20464933 0.1822330355644226\n",
      "0.21178374 0.15326237678527832\n",
      "0.27447432 0.3771387040615082\n",
      "-0.20152864 -0.2171725034713745\n",
      "0.049354978 -0.016348375007510185\n",
      "0.22019261 0.2645185887813568\n",
      "0.1830109 0.2076878547668457\n",
      "-0.08298922 -0.04916299879550934\n",
      "-0.13274646 -0.06163966283202171\n"
     ]
    }
   ],
   "source": [
    "valid_iter = iter(car_valid_loader)\n",
    "net.eval()\n",
    "for i in range(100):\n",
    "    img, angle = next(valid_iter)\n",
    "    angle = angle.numpy()[0]\n",
    "    predicted = net(img.float().to(device)).cpu().detach().numpy()[0]\n",
    "    print(predicted, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6bde7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),save_dir / \"model_state_dict.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c11663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
