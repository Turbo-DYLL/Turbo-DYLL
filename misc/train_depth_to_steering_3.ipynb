{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7ae434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "#from torchnet.meter import AverageValueMeter\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from utils import depthToLogDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6d77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118577 data points\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/michael/Desktop/projects/ROAR/data/output\")\n",
    "center_depth_dir = data_dir / \"front_depth\"\n",
    "center_rgb_dir = data_dir / \"front_rgb\"\n",
    "veh_state_dir = data_dir / \"vehicle_state\"\n",
    "\n",
    "center_depth_paths = [p for p in sorted(center_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "center_rgb_paths = [p for p in sorted(center_rgb_dir.glob(\"*.png\", ), key=os.path.getmtime)]\n",
    "veh_state_paths = [p for p in sorted(veh_state_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "assert len(center_depth_paths) == len(center_rgb_paths) == len(veh_state_paths)\n",
    "print(f\"Found { len(center_rgb_paths)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e752a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fc56cfdfe84583b884000551712610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steerings , throttles = [0]*len(center_depth_paths), [0]*len(center_depth_paths)\n",
    "for i in tqdm(range(len(center_depth_paths))):\n",
    "    state = np.load(veh_state_paths[i])\n",
    "    steerings[i] = state[-1]\n",
    "    throttles[i] = state[-2]\n",
    "df = pd.DataFrame({\"center_depth\": center_depth_paths, \"steering\":steerings, \"throttle\": throttles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54d42a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center_depth</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/michael/Desktop/projects/ROAR/data/outpu...</td>\n",
       "      <td>0.053245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/michael/Desktop/projects/ROAR/data/outpu...</td>\n",
       "      <td>0.044596</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/michael/Desktop/projects/ROAR/data/outpu...</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/michael/Desktop/projects/ROAR/data/outpu...</td>\n",
       "      <td>0.035561</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/michael/Desktop/projects/ROAR/data/outpu...</td>\n",
       "      <td>0.035561</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        center_depth  steering  throttle\n",
       "0  /home/michael/Desktop/projects/ROAR/data/outpu...  0.053245       1.0\n",
       "1  /home/michael/Desktop/projects/ROAR/data/outpu...  0.044596       1.0\n",
       "2  /home/michael/Desktop/projects/ROAR/data/outpu...  0.035562       1.0\n",
       "3  /home/michael/Desktop/projects/ROAR/data/outpu...  0.035561       1.0\n",
       "4  /home/michael/Desktop/projects/ROAR/data/outpu...  0.035561       1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a87403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118577.000000</td>\n",
       "      <td>118577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.007236</td>\n",
       "      <td>0.604110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.094685</td>\n",
       "      <td>0.345911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.640327</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.068903</td>\n",
       "      <td>0.466233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.006032</td>\n",
       "      <td>0.588960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.061620</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.733144</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            steering       throttle\n",
       "count  118577.000000  118577.000000\n",
       "mean       -0.007236       0.604110\n",
       "std         0.094685       0.345911\n",
       "min        -0.640327       0.000000\n",
       "25%        -0.068903       0.466233\n",
       "50%        -0.006032       0.588960\n",
       "75%         0.061620       1.000000\n",
       "max         0.733144       1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf4b92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'steering'}>,\n",
       "        <AxesSubplot:title={'center':'throttle'}>]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f960fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, should_use_log_depth=True, sigma = 0.1, thres_prob = 0.7, bin_sz = 0.05):\n",
    "        self.df = df\n",
    "        self.should_use_log_depth = should_use_log_depth\n",
    "        \n",
    "        self.sigma = sigma\n",
    "        self.thres_prob = thres_prob\n",
    "        self.bin_sz = bin_sz\n",
    "    def calc_bin(self, rn):\n",
    "        mult = round(1 / self.bin_sz)\n",
    "        d = rn * mult // 1\n",
    "        return d * self.bin_sz, (d + 1) * self.bin_sz\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        a = np.random.rand()\n",
    "        if a > self.thres_prob:\n",
    "            return self.get_item_helper(index)\n",
    "        else:\n",
    "            rn = np.random.normal(scale = self.sigma)\n",
    "            low, high = self.calc_bin(rn)\n",
    "            subset = self.df[(self.df[\"steering\"] >= low) & (self.df[\"steering\"] <= high)]\n",
    "            # degenerate case :) ignore this\n",
    "            if not len(subset):\n",
    "                return self.get_item_helper(index)\n",
    "            else:\n",
    "                return self.get_item_helper(np.random.choice(subset.index))\n",
    "    \n",
    "    def get_item_helper(self, index):\n",
    "#         path: Path = self.df.iloc[index][\"center_depth\"]\n",
    "        path:Path = self.df[\"center_depth\"][index]\n",
    "        path_str = path.as_posix()\n",
    "        depth_img = np.load(path_str)\n",
    "        if self.should_use_log_depth:\n",
    "            depth_img = depthToLogDepth(depth_img)\n",
    "\n",
    "        steering = self.df[\"steering\"][index]\n",
    "        \n",
    "        return depth_img, steering\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6731ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, \n",
    "                                test_size=0.1, \n",
    "                                shuffle=True)\n",
    "train = train.reset_index() \n",
    "valid = valid.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b57f246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'steering'}>]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.hist([\"steering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fdd50de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'steering'}>]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid.hist([\"steering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae04399",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "car_train_loader = DataLoader(CarDataset(df=train), \n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "car_valid_loader = DataLoader(CarDataset(df=valid), \n",
    "                              batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd318e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f38ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 600, 800]), torch.Size([1]), 106719)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, angles = next(iter(car_train_loader))\n",
    "imgs.shape, angles.shape, len(car_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a24d6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CarModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6492fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTensor(data, use_cuda, device=None):\n",
    "    img, target = data\n",
    "#     img, target = torch.from_numpy(img).float(), torch.from_numpy(np.array([target])).float()\n",
    "    img = img.float()\n",
    "    target = target.float()\n",
    "    if use_cuda:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "    return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "843d3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1000000\n",
    "\n",
    "def train(epoch, net, train_loader, optimizer, criterion, use_cuda, \n",
    "          device=None, save_dir=Path(\".\"),MAX_BATCH=1000):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        img, steering_angle = toTensor(data, use_cuda, device)\n",
    "        output = net(img) # Tensor([ITEM])\n",
    "        loss = criterion(output, steering_angle)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'    Batch: {batch_idx} --> Loss: {train_loss / (batch_idx+1)}') \n",
    "        \n",
    "        if batch_idx >= MAX_BATCH:\n",
    "            break\n",
    "    \n",
    "\n",
    "def valid(epoch, net, validloader, criterion, use_cuda, device=None, save_dir=Path(\".\"),MAX_BATCH=1000):\n",
    "    global best_loss\n",
    "    net.eval()\n",
    "    valid_loss = 0 \n",
    "    for batch_idx, data in enumerate(validloader):\n",
    "        img, steering_angle = toTensor(data, use_cuda, device)\n",
    "        outputs = net(img)\n",
    "        loss = criterion(outputs, steering_angle)\n",
    "        valid_loss += loss.data.item()\n",
    "        \n",
    "        avg_valid_loss = valid_loss / (batch_idx + 1)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"    Valid Loss: {avg_valid_loss}\" )\n",
    "        \n",
    "        if avg_valid_loss <= best_loss:\n",
    "            best_loss = avg_valid_loss\n",
    "            print(f\"         Saving.... Best epoch: {epoch} -> {avg_valid_loss}\")\n",
    "            torch.save(net.state_dict(),save_dir / \"best_model_state_dict.h5\" )\n",
    "#             torch.save(net, save_dir / \"best_model.h5\")\n",
    "        \n",
    "        if batch_idx >= MAX_BATCH:\n",
    "            break\n",
    "    \n",
    "    torch.save(net.state_dict(),save_dir / \"model_state_dict.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e77f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/ROAR3.8/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "#load the previous net or use new net\n",
    "# net = torch.load(\"/home/michael/Desktop/projects/ROAR/misc/data/best_model.h5\")\n",
    "# net.training = True\n",
    "\n",
    "net = CarModel(batch_size=batch_size, \n",
    "              image_width=800,\n",
    "              image_height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b64e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5fbf86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "    Batch: 0 --> Loss: 0.014868524856865406\n",
      "    Batch: 100 --> Loss: 0.037118387965370175\n",
      "    Batch: 200 --> Loss: 0.03207724473569312\n",
      "    Batch: 300 --> Loss: 0.026898221439581104\n",
      "    Batch: 400 --> Loss: 0.022965004514613918\n",
      "    Batch: 500 --> Loss: 0.02027678732027382\n",
      "    Batch: 600 --> Loss: 0.018090973884155304\n",
      "    Batch: 700 --> Loss: 0.01644222891158719\n",
      "    Batch: 800 --> Loss: 0.015089117767316156\n",
      "    Batch: 900 --> Loss: 0.013923087677359601\n",
      "    Batch: 1000 --> Loss: 0.01284694781097009\n",
      "    Batch: 1100 --> Loss: 0.012092366124692768\n",
      "    Batch: 1200 --> Loss: 0.011452500039403745\n",
      "    Batch: 1300 --> Loss: 0.01083808449675266\n",
      "    Batch: 1400 --> Loss: 0.010316325334941185\n",
      "    Batch: 1500 --> Loss: 0.009864319206597489\n",
      "    Batch: 1600 --> Loss: 0.009545704631328167\n",
      "    Batch: 1700 --> Loss: 0.009312734238996857\n",
      "    Batch: 1800 --> Loss: 0.009154030005917432\n",
      "    Batch: 1900 --> Loss: 0.008872691129857252\n",
      "    Batch: 2000 --> Loss: 0.00866341616183306\n",
      "    Batch: 2100 --> Loss: 0.008621280552143246\n",
      "    Batch: 2200 --> Loss: 0.00839385682184359\n",
      "    Batch: 2300 --> Loss: 0.008246703763759121\n",
      "    Batch: 2400 --> Loss: 0.008080817483439154\n",
      "    Batch: 2500 --> Loss: 0.007935766196875917\n",
      "    Batch: 2600 --> Loss: 0.007832457861084448\n",
      "    Batch: 2700 --> Loss: 0.007714504558345727\n",
      "    Batch: 2800 --> Loss: 0.007565315061535949\n",
      "    Batch: 2900 --> Loss: 0.007463555206399549\n",
      "    Batch: 3000 --> Loss: 0.007308828397691236\n",
      "    Batch: 3100 --> Loss: 0.007238530400356291\n",
      "    Batch: 3200 --> Loss: 0.00713767861590383\n",
      "    Batch: 3300 --> Loss: 0.007043980526476148\n",
      "    Batch: 3400 --> Loss: 0.006901738115740606\n",
      "    Batch: 3500 --> Loss: 0.006800319414971329\n",
      "    Batch: 3600 --> Loss: 0.006746101348876341\n",
      "    Batch: 3700 --> Loss: 0.006673478668949796\n",
      "    Batch: 3800 --> Loss: 0.006636963829484472\n",
      "    Batch: 3900 --> Loss: 0.006565587795398783\n",
      "    Batch: 4000 --> Loss: 0.0065066957177040904\n",
      "    Batch: 4100 --> Loss: 0.0064540418367785984\n",
      "    Batch: 4200 --> Loss: 0.0064325189294928775\n",
      "    Batch: 4300 --> Loss: 0.006395624736238403\n",
      "    Batch: 4400 --> Loss: 0.006369205145389674\n",
      "    Batch: 4500 --> Loss: 0.006315602899925093\n",
      "    Batch: 4600 --> Loss: 0.006247208276396919\n",
      "    Batch: 4700 --> Loss: 0.006199010543438536\n",
      "    Batch: 4800 --> Loss: 0.006131722237488286\n",
      "    Batch: 4900 --> Loss: 0.006145591273476596\n",
      "    Batch: 5000 --> Loss: 0.006082690663216504\n",
      "    Batch: 5100 --> Loss: 0.006035514074976424\n",
      "    Batch: 5200 --> Loss: 0.005987845297647281\n",
      "    Batch: 5300 --> Loss: 0.005944777732377543\n",
      "    Batch: 5400 --> Loss: 0.005929712669017036\n",
      "    Batch: 5500 --> Loss: 0.005965628377929885\n",
      "    Batch: 5600 --> Loss: 0.005926647861446968\n",
      "    Batch: 5700 --> Loss: 0.005878029066641824\n",
      "    Batch: 5800 --> Loss: 0.005839345631539592\n",
      "    Batch: 5900 --> Loss: 0.005781424472938492\n",
      "    Batch: 6000 --> Loss: 0.005737855581001703\n",
      "    Batch: 6100 --> Loss: 0.005685018214946489\n",
      "    Batch: 6200 --> Loss: 0.005674683554420112\n",
      "    Batch: 6300 --> Loss: 0.005643514937145793\n",
      "    Batch: 6400 --> Loss: 0.005594005122914135\n",
      "    Batch: 6500 --> Loss: 0.005550636784576529\n",
      "    Batch: 6600 --> Loss: 0.005522641907296338\n",
      "    Batch: 6700 --> Loss: 0.0054753339784651\n",
      "    Batch: 6800 --> Loss: 0.005445808285915737\n",
      "    Batch: 6900 --> Loss: 0.00545995224180147\n",
      "    Batch: 7000 --> Loss: 0.0054129313829482635\n",
      "    Batch: 7100 --> Loss: 0.005380611222521109\n",
      "    Batch: 7200 --> Loss: 0.005345283926786956\n",
      "    Batch: 7300 --> Loss: 0.005308303379318516\n",
      "    Batch: 7400 --> Loss: 0.0053001894807627415\n",
      "    Batch: 7500 --> Loss: 0.005269812239533353\n",
      "    Batch: 7600 --> Loss: 0.005269581096311206\n",
      "    Batch: 7700 --> Loss: 0.005259237645372532\n",
      "    Batch: 7800 --> Loss: 0.005238108282365299\n",
      "    Batch: 7900 --> Loss: 0.005250639679150346\n",
      "    Batch: 8000 --> Loss: 0.005227259892548352\n",
      "    Batch: 8100 --> Loss: 0.0052207950183525646\n",
      "    Batch: 8200 --> Loss: 0.005196215792928275\n",
      "    Batch: 8300 --> Loss: 0.0051635804156383676\n",
      "    Batch: 8400 --> Loss: 0.005127972461265422\n",
      "    Batch: 8500 --> Loss: 0.0051206013424623335\n",
      "    Batch: 8600 --> Loss: 0.005109459689615223\n",
      "    Batch: 8700 --> Loss: 0.005101075551028582\n",
      "    Batch: 8800 --> Loss: 0.005071291452256676\n",
      "    Batch: 8900 --> Loss: 0.005045429324649108\n",
      "    Batch: 9000 --> Loss: 0.005020750727497167\n",
      "    Batch: 9100 --> Loss: 0.005007160623816798\n",
      "    Batch: 9200 --> Loss: 0.0050017523833811315\n",
      "    Batch: 9300 --> Loss: 0.004984007887315291\n",
      "    Batch: 9400 --> Loss: 0.0049533119846407\n",
      "    Batch: 9500 --> Loss: 0.0049285974552241\n",
      "    Batch: 9600 --> Loss: 0.00489937787736985\n",
      "    Batch: 9700 --> Loss: 0.004884565491526025\n",
      "    Batch: 9800 --> Loss: 0.004866638671737498\n",
      "    Batch: 9900 --> Loss: 0.004849181668305659\n",
      "    Batch: 10000 --> Loss: 0.004821940307064797\n",
      "    Batch: 10100 --> Loss: 0.004801943979465503\n",
      "    Batch: 10200 --> Loss: 0.004782737958036049\n",
      "    Batch: 10300 --> Loss: 0.004798324503607298\n",
      "    Batch: 10400 --> Loss: 0.004781976901734582\n",
      "    Batch: 10500 --> Loss: 0.004795135368215006\n",
      "    Batch: 10600 --> Loss: 0.004778850835140349\n",
      "    Batch: 10700 --> Loss: 0.004768673677148337\n",
      "    Batch: 10800 --> Loss: 0.004749745626314095\n",
      "    Batch: 10900 --> Loss: 0.0047264202637027\n",
      "    Batch: 11000 --> Loss: 0.00471214057416018\n",
      "    Batch: 11100 --> Loss: 0.004681568304777316\n",
      "    Batch: 11200 --> Loss: 0.004665214259811357\n",
      "    Batch: 11300 --> Loss: 0.004649134676985054\n",
      "    Batch: 11400 --> Loss: 0.0046295004900868365\n",
      "    Batch: 11500 --> Loss: 0.00461467522703873\n",
      "    Batch: 11600 --> Loss: 0.004604091597313181\n",
      "    Batch: 11700 --> Loss: 0.004592120603030934\n",
      "    Batch: 11800 --> Loss: 0.004587226796039954\n",
      "    Batch: 11900 --> Loss: 0.004580801714209505\n",
      "    Batch: 12000 --> Loss: 0.004567984339321746\n",
      "    Batch: 12100 --> Loss: 0.00455762533778108\n",
      "    Batch: 12200 --> Loss: 0.0045616143231611336\n",
      "    Batch: 12300 --> Loss: 0.004545900031609817\n",
      "    Batch: 12400 --> Loss: 0.004531848012391903\n",
      "    Batch: 12500 --> Loss: 0.004520881474660212\n",
      "    Batch: 12600 --> Loss: 0.004513041851987238\n",
      "    Batch: 12700 --> Loss: 0.004506313345798429\n",
      "    Batch: 12800 --> Loss: 0.00449195171830895\n",
      "    Batch: 12900 --> Loss: 0.0044819885992274655\n",
      "    Batch: 13000 --> Loss: 0.004478840539931835\n",
      "    Batch: 13100 --> Loss: 0.004468251085935052\n",
      "    Batch: 13200 --> Loss: 0.004452562875835158\n",
      "    Batch: 13300 --> Loss: 0.004443506806138829\n",
      "    Batch: 13400 --> Loss: 0.004431001098298937\n",
      "    Batch: 13500 --> Loss: 0.004427620869306678\n",
      "    Batch: 13600 --> Loss: 0.0044123045102163515\n",
      "    Batch: 13700 --> Loss: 0.004400311984649251\n",
      "    Batch: 13800 --> Loss: 0.004384005595038705\n",
      "    Batch: 13900 --> Loss: 0.004369279510268328\n",
      "    Batch: 14000 --> Loss: 0.004350993595733917\n",
      "    Batch: 14100 --> Loss: 0.004343000699995528\n",
      "    Batch: 14200 --> Loss: 0.004335726114785333\n",
      "    Batch: 14300 --> Loss: 0.004329388240141776\n",
      "    Batch: 14400 --> Loss: 0.004314573900122856\n",
      "    Batch: 14500 --> Loss: 0.004321449198568772\n",
      "    Batch: 14600 --> Loss: 0.004312803264568881\n",
      "    Batch: 14700 --> Loss: 0.004303642283536379\n",
      "    Batch: 14800 --> Loss: 0.0042870826477271855\n",
      "    Batch: 14900 --> Loss: 0.0042717482368409086\n",
      "    Batch: 15000 --> Loss: 0.004261485273220453\n",
      "    Batch: 15100 --> Loss: 0.00424705628027975\n",
      "    Batch: 15200 --> Loss: 0.004254290876545337\n",
      "    Batch: 15300 --> Loss: 0.00424018529274559\n",
      "    Batch: 15400 --> Loss: 0.004229421259469251\n",
      "    Batch: 15500 --> Loss: 0.004221382674524407\n",
      "    Batch: 15600 --> Loss: 0.00421152857124561\n",
      "    Batch: 15700 --> Loss: 0.004198400535828448\n",
      "    Batch: 15800 --> Loss: 0.00419640987571911\n",
      "    Batch: 15900 --> Loss: 0.004184431812577554\n",
      "    Batch: 16000 --> Loss: 0.004170683389417178\n",
      "    Batch: 16100 --> Loss: 0.0041682281666083005\n",
      "    Batch: 16200 --> Loss: 0.004153798529574272\n",
      "    Batch: 16300 --> Loss: 0.00414782207720724\n",
      "    Batch: 16400 --> Loss: 0.00413811420946718\n",
      "    Batch: 16500 --> Loss: 0.004130646872268643\n",
      "    Batch: 16600 --> Loss: 0.0041258502269807375\n",
      "    Batch: 16700 --> Loss: 0.0041166438978322215\n",
      "    Batch: 16800 --> Loss: 0.004101645160303862\n",
      "    Batch: 16900 --> Loss: 0.004090302065768669\n",
      "    Batch: 17000 --> Loss: 0.004092129819325141\n",
      "    Batch: 17100 --> Loss: 0.004091354528222695\n",
      "    Batch: 17200 --> Loss: 0.004079739187275977\n",
      "    Batch: 17300 --> Loss: 0.004075699761764295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 17400 --> Loss: 0.004063192477394124\n",
      "    Batch: 17500 --> Loss: 0.004059433692370118\n",
      "    Batch: 17600 --> Loss: 0.004051616706209826\n",
      "    Batch: 17700 --> Loss: 0.004044347513255152\n",
      "    Batch: 17800 --> Loss: 0.004033338205808996\n",
      "    Batch: 17900 --> Loss: 0.004025029215757206\n",
      "    Batch: 18000 --> Loss: 0.0040134528801951085\n",
      "    Batch: 18100 --> Loss: 0.00400165850158412\n",
      "    Batch: 18200 --> Loss: 0.003987773460798886\n",
      "    Batch: 18300 --> Loss: 0.003979798716519079\n",
      "    Batch: 18400 --> Loss: 0.003972630658404715\n",
      "    Batch: 18500 --> Loss: 0.003961074014993103\n",
      "    Batch: 18600 --> Loss: 0.003948019093613001\n",
      "    Batch: 18700 --> Loss: 0.003939528183860203\n",
      "    Batch: 18800 --> Loss: 0.003934708700223811\n",
      "    Batch: 18900 --> Loss: 0.003925685288462468\n",
      "    Batch: 19000 --> Loss: 0.003915091334397446\n",
      "    Batch: 19100 --> Loss: 0.0039053769944449837\n",
      "    Batch: 19200 --> Loss: 0.0039016629909102833\n",
      "    Batch: 19300 --> Loss: 0.0038944226015625483\n",
      "    Batch: 19400 --> Loss: 0.003894309578528773\n",
      "    Batch: 19500 --> Loss: 0.003890070140136669\n",
      "    Batch: 19600 --> Loss: 0.0038809299798425925\n",
      "    Batch: 19700 --> Loss: 0.0038786115784639657\n",
      "    Batch: 19800 --> Loss: 0.003866108372431596\n",
      "    Batch: 19900 --> Loss: 0.0038578391138173423\n",
      "    Batch: 20000 --> Loss: 0.003862055385700557\n",
      "    Batch: 20100 --> Loss: 0.0038525543456792272\n",
      "    Batch: 20200 --> Loss: 0.0038449337252199436\n",
      "    Batch: 20300 --> Loss: 0.0038386422863107782\n",
      "    Batch: 20400 --> Loss: 0.0038337273019341655\n",
      "    Batch: 20500 --> Loss: 0.0038260236037205185\n",
      "    Batch: 20600 --> Loss: 0.0038160403746856095\n",
      "    Batch: 20700 --> Loss: 0.003809132313200897\n",
      "    Batch: 20800 --> Loss: 0.003802255282252789\n",
      "    Batch: 20900 --> Loss: 0.00379133526054426\n",
      "    Batch: 21000 --> Loss: 0.0037824573386203366\n",
      "    Batch: 21100 --> Loss: 0.0037720259972197846\n",
      "    Batch: 21200 --> Loss: 0.0037626871402227707\n",
      "    Batch: 21300 --> Loss: 0.0037508168824077893\n",
      "    Batch: 21400 --> Loss: 0.003741556904183821\n",
      "    Batch: 21500 --> Loss: 0.003730799079355198\n",
      "    Batch: 21600 --> Loss: 0.003720183318617608\n",
      "    Batch: 21700 --> Loss: 0.003711045218652894\n",
      "    Batch: 21800 --> Loss: 0.0037017752357606714\n",
      "    Batch: 21900 --> Loss: 0.003695456435469947\n",
      "    Batch: 22000 --> Loss: 0.003688561420359001\n",
      "    Batch: 22100 --> Loss: 0.0036865835017612504\n",
      "    Batch: 22200 --> Loss: 0.003680522452507331\n",
      "    Batch: 22300 --> Loss: 0.0036777519639732185\n",
      "    Batch: 22400 --> Loss: 0.003669383654420415\n",
      "    Batch: 22500 --> Loss: 0.003661700807708072\n",
      "    Batch: 22600 --> Loss: 0.0036550786862167505\n",
      "    Batch: 22700 --> Loss: 0.003650198157578134\n",
      "    Batch: 22800 --> Loss: 0.003650194270667745\n",
      "    Batch: 22900 --> Loss: 0.003645237862852161\n",
      "    Batch: 23000 --> Loss: 0.003639609594069838\n",
      "    Batch: 23100 --> Loss: 0.003630179373392894\n",
      "    Batch: 23200 --> Loss: 0.003623384169699726\n",
      "    Batch: 23300 --> Loss: 0.0036195931050821235\n",
      "    Batch: 23400 --> Loss: 0.003614230718355108\n",
      "    Batch: 23500 --> Loss: 0.003606208989281115\n",
      "    Batch: 23600 --> Loss: 0.0036001455288550647\n",
      "    Batch: 23700 --> Loss: 0.0035948789322777512\n",
      "    Batch: 23800 --> Loss: 0.003587163503894972\n",
      "    Batch: 23900 --> Loss: 0.0035779677943360466\n",
      "    Batch: 24000 --> Loss: 0.0035669834194996127\n",
      "    Batch: 24100 --> Loss: 0.0035607711141380284\n",
      "    Batch: 24200 --> Loss: 0.0035556611586428422\n",
      "    Batch: 24300 --> Loss: 0.003549032402237102\n",
      "    Batch: 24400 --> Loss: 0.003543643696175939\n",
      "    Batch: 24500 --> Loss: 0.0035415612509086354\n",
      "    Batch: 24600 --> Loss: 0.0035332282193718764\n",
      "    Batch: 24700 --> Loss: 0.0035255506548459332\n",
      "    Batch: 24800 --> Loss: 0.003526754395247183\n",
      "    Batch: 24900 --> Loss: 0.0035249094168374705\n",
      "    Batch: 25000 --> Loss: 0.0035180144189837217\n",
      "    Batch: 25100 --> Loss: 0.003512953414510777\n",
      "    Batch: 25200 --> Loss: 0.0035084671756213737\n",
      "    Batch: 25300 --> Loss: 0.0035044589126276464\n",
      "    Batch: 25400 --> Loss: 0.0035001251967644196\n",
      "    Batch: 25500 --> Loss: 0.0034939788052402495\n",
      "    Batch: 25600 --> Loss: 0.0034968142862045254\n",
      "    Batch: 25700 --> Loss: 0.0034889238982913464\n",
      "    Batch: 25800 --> Loss: 0.003487585523606116\n",
      "    Batch: 25900 --> Loss: 0.0034844280010248884\n",
      "    Batch: 26000 --> Loss: 0.0034839932678398956\n",
      "    Batch: 26100 --> Loss: 0.0034778290304288357\n",
      "    Batch: 26200 --> Loss: 0.003476105157527334\n",
      "    Batch: 26300 --> Loss: 0.0034730004644791686\n",
      "    Batch: 26400 --> Loss: 0.0034673497291033096\n",
      "    Batch: 26500 --> Loss: 0.0034616433657833497\n",
      "    Batch: 26600 --> Loss: 0.0034557112327515283\n",
      "    Valid Loss: 0.0018839057302102447\n",
      "         Saving.... Best epoch: 0 -> 0.0018839057302102447\n",
      "         Saving.... Best epoch: 0 -> 0.0009522105710857431\n",
      "         Saving.... Best epoch: 0 -> 0.0008410935230737474\n",
      "         Saving.... Best epoch: 0 -> 0.000631079141214741\n",
      "         Saving.... Best epoch: 0 -> 0.0006237996982300891\n",
      "         Saving.... Best epoch: 0 -> 0.0006037833044558738\n",
      "         Saving.... Best epoch: 0 -> 0.0005717511255397767\n",
      "         Saving.... Best epoch: 0 -> 0.0005578676979999167\n",
      "         Saving.... Best epoch: 0 -> 0.0005523092128539377\n",
      "         Saving.... Best epoch: 0 -> 0.0005468435628699809\n",
      "         Saving.... Best epoch: 0 -> 0.0005324095392801785\n",
      "         Saving.... Best epoch: 0 -> 0.0005158133888762961\n",
      "         Saving.... Best epoch: 0 -> 0.0005110242088593706\n",
      "         Saving.... Best epoch: 0 -> 0.000496117445239259\n",
      "         Saving.... Best epoch: 0 -> 0.00048446041519680975\n",
      "    Valid Loss: 0.000957150660528018\n",
      "    Valid Loss: 0.0013415125904458057\n",
      "    Valid Loss: 0.0012985139814487335\n",
      "    Valid Loss: 0.00142248954784537\n",
      "    Valid Loss: 0.001512367079989914\n",
      "    Valid Loss: 0.0014652939613865356\n",
      "    Valid Loss: 0.0014878482174244109\n",
      "    Valid Loss: 0.0014840156556193997\n",
      "    Valid Loss: 0.0014690519579078506\n",
      "    Valid Loss: 0.001454749360823426\n",
      "Epoch 1\n",
      "    Batch: 0 --> Loss: 0.0030313425231724977\n",
      "    Batch: 100 --> Loss: 0.0033539692723586465\n",
      "    Batch: 200 --> Loss: 0.00278260761968681\n",
      "    Batch: 300 --> Loss: 0.002462106285692622\n",
      "    Batch: 400 --> Loss: 0.0022885186872225183\n",
      "    Batch: 500 --> Loss: 0.0021199326692810244\n",
      "    Batch: 600 --> Loss: 0.001952873898452675\n",
      "    Batch: 700 --> Loss: 0.0019991268772981156\n",
      "    Batch: 800 --> Loss: 0.0019150399051421141\n",
      "    Batch: 900 --> Loss: 0.0019495098345597571\n",
      "    Batch: 1000 --> Loss: 0.0019545447029834405\n",
      "    Batch: 1100 --> Loss: 0.001961234165720549\n",
      "    Batch: 1200 --> Loss: 0.0019911119936786266\n",
      "    Batch: 1300 --> Loss: 0.0020265250220472845\n",
      "    Batch: 1400 --> Loss: 0.002031294769308124\n",
      "    Batch: 1500 --> Loss: 0.001999995486753203\n",
      "    Batch: 1600 --> Loss: 0.0020036167626418288\n",
      "    Batch: 1700 --> Loss: 0.001991215507762744\n",
      "    Batch: 1800 --> Loss: 0.001955717827558383\n",
      "    Batch: 1900 --> Loss: 0.0019398872608426426\n",
      "    Batch: 2000 --> Loss: 0.0019501192046320866\n",
      "    Batch: 2100 --> Loss: 0.0019459120416469682\n",
      "    Batch: 2200 --> Loss: 0.0019343035367571154\n",
      "    Batch: 2300 --> Loss: 0.0019459133410930553\n",
      "    Batch: 2400 --> Loss: 0.0019103048324332584\n",
      "    Batch: 2500 --> Loss: 0.001941535262285362\n",
      "    Batch: 2600 --> Loss: 0.0019434559839614366\n",
      "    Batch: 2700 --> Loss: 0.0019370887688389185\n",
      "    Batch: 2800 --> Loss: 0.0019930191051681285\n",
      "    Batch: 2900 --> Loss: 0.0020091465029161534\n",
      "    Batch: 3000 --> Loss: 0.0020409827127350123\n",
      "    Batch: 3100 --> Loss: 0.0020440676248526584\n",
      "    Batch: 3200 --> Loss: 0.0020473081622692556\n",
      "    Batch: 3300 --> Loss: 0.0020362385252889765\n",
      "    Batch: 3400 --> Loss: 0.0020206981662690515\n",
      "    Batch: 3500 --> Loss: 0.0020288089093676407\n",
      "    Batch: 3600 --> Loss: 0.0020459171202172203\n",
      "    Batch: 3700 --> Loss: 0.0020300728645123965\n",
      "    Batch: 3800 --> Loss: 0.0020152237313551882\n",
      "    Batch: 3900 --> Loss: 0.0020362083824677604\n",
      "    Batch: 4000 --> Loss: 0.002016161179204508\n",
      "    Batch: 4100 --> Loss: 0.002013895727550506\n",
      "    Batch: 4200 --> Loss: 0.0020013447225648577\n",
      "    Batch: 4300 --> Loss: 0.002000118066015241\n",
      "    Batch: 4400 --> Loss: 0.0019873541330782036\n",
      "    Batch: 4500 --> Loss: 0.002006483246335397\n",
      "    Batch: 4600 --> Loss: 0.002039782304408055\n",
      "    Batch: 4700 --> Loss: 0.0020637028057579206\n",
      "    Batch: 4800 --> Loss: 0.002079335686355164\n",
      "    Batch: 4900 --> Loss: 0.002102440318002007\n",
      "    Batch: 5000 --> Loss: 0.0021004592643202183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 5100 --> Loss: 0.002094227055630997\n",
      "    Batch: 5200 --> Loss: 0.0020924091662036766\n",
      "    Batch: 5300 --> Loss: 0.0021009309726319337\n",
      "    Batch: 5400 --> Loss: 0.0021128773201641005\n",
      "    Batch: 5500 --> Loss: 0.0021432711773610434\n",
      "    Batch: 5600 --> Loss: 0.0021457968282768804\n",
      "    Batch: 5700 --> Loss: 0.002136531827673116\n",
      "    Batch: 5800 --> Loss: 0.0021395792400534983\n",
      "    Batch: 5900 --> Loss: 0.002163821591637117\n",
      "    Batch: 6000 --> Loss: 0.0021461104021561067\n",
      "    Batch: 6100 --> Loss: 0.002128375095395765\n",
      "    Batch: 6200 --> Loss: 0.002145480189137258\n",
      "    Batch: 6300 --> Loss: 0.0021632302127323278\n",
      "    Batch: 6400 --> Loss: 0.002162625072633514\n",
      "    Batch: 6500 --> Loss: 0.00215922607252857\n",
      "    Batch: 6600 --> Loss: 0.002157545761777538\n",
      "    Batch: 6700 --> Loss: 0.002152472451411649\n",
      "    Batch: 6800 --> Loss: 0.0021506753406327975\n",
      "    Batch: 6900 --> Loss: 0.0021539151002436204\n",
      "    Batch: 7000 --> Loss: 0.0021523847924025405\n",
      "    Batch: 7100 --> Loss: 0.002160717253803985\n",
      "    Batch: 7200 --> Loss: 0.0021793548069284946\n",
      "    Batch: 7300 --> Loss: 0.002165063219811411\n",
      "    Batch: 7400 --> Loss: 0.002159278085073254\n",
      "    Batch: 7500 --> Loss: 0.0021616896590863684\n",
      "    Batch: 7600 --> Loss: 0.0021711948103069704\n",
      "    Batch: 7700 --> Loss: 0.0021615972281457383\n",
      "    Batch: 7800 --> Loss: 0.0021797538597838058\n",
      "    Batch: 7900 --> Loss: 0.002178295429705035\n",
      "    Batch: 8000 --> Loss: 0.0021747528828374605\n",
      "    Batch: 8100 --> Loss: 0.002171489841486046\n",
      "    Batch: 8200 --> Loss: 0.0021700032676134073\n",
      "    Batch: 8300 --> Loss: 0.0021658988740852563\n",
      "    Batch: 8400 --> Loss: 0.00216001190403505\n",
      "    Batch: 8500 --> Loss: 0.002150677786919674\n",
      "    Batch: 8600 --> Loss: 0.0021558318248529286\n",
      "    Batch: 8700 --> Loss: 0.0021445586969055007\n",
      "    Batch: 8800 --> Loss: 0.0021399666207993244\n",
      "    Batch: 8900 --> Loss: 0.0021519933771687993\n",
      "    Batch: 9000 --> Loss: 0.0021401075893279073\n",
      "    Batch: 9100 --> Loss: 0.0021330475209779477\n",
      "    Batch: 9200 --> Loss: 0.0021448149277374286\n",
      "    Batch: 9300 --> Loss: 0.002152875650318392\n",
      "    Batch: 9400 --> Loss: 0.002148875958270046\n",
      "    Batch: 9500 --> Loss: 0.0021460208022123874\n",
      "    Batch: 9600 --> Loss: 0.0021457424053308205\n",
      "    Batch: 9700 --> Loss: 0.002148933331762373\n",
      "    Batch: 9800 --> Loss: 0.002156273573057509\n",
      "    Batch: 9900 --> Loss: 0.002149838711210316\n",
      "    Batch: 10000 --> Loss: 0.0021446748900915986\n",
      "    Batch: 10100 --> Loss: 0.0021477025830190857\n",
      "    Batch: 10200 --> Loss: 0.002181797082466557\n",
      "    Batch: 10300 --> Loss: 0.00217807168531405\n",
      "    Batch: 10400 --> Loss: 0.002179564635833996\n",
      "    Batch: 10500 --> Loss: 0.0021743261811960186\n",
      "    Batch: 10600 --> Loss: 0.0021865317012939745\n",
      "    Batch: 10700 --> Loss: 0.002177458488500632\n",
      "    Batch: 10800 --> Loss: 0.0021727509319560624\n",
      "    Batch: 10900 --> Loss: 0.0021670304756455463\n",
      "    Batch: 11000 --> Loss: 0.002169128540352181\n",
      "    Batch: 11100 --> Loss: 0.0021645678802638133\n",
      "    Batch: 11200 --> Loss: 0.002159295540927522\n",
      "    Batch: 11300 --> Loss: 0.0021544573602759157\n",
      "    Batch: 11400 --> Loss: 0.0021483298885509425\n",
      "    Batch: 11500 --> Loss: 0.0021393779097737477\n",
      "    Batch: 11600 --> Loss: 0.00213197280613475\n",
      "    Batch: 11700 --> Loss: 0.0021237395179594097\n",
      "    Batch: 11800 --> Loss: 0.0021239545910088594\n",
      "    Batch: 11900 --> Loss: 0.0021249947705652165\n",
      "    Batch: 12000 --> Loss: 0.0021263017203887796\n",
      "    Batch: 12100 --> Loss: 0.0021222633762001467\n",
      "    Batch: 12200 --> Loss: 0.0021289676267335243\n",
      "    Batch: 12300 --> Loss: 0.0021359562893676323\n",
      "    Batch: 12400 --> Loss: 0.0021317873161313793\n",
      "    Batch: 12500 --> Loss: 0.0021245951819843134\n",
      "    Batch: 12600 --> Loss: 0.0021247589106714376\n",
      "    Batch: 12700 --> Loss: 0.0021207237187007713\n",
      "    Batch: 12800 --> Loss: 0.002113830327971685\n",
      "    Batch: 12900 --> Loss: 0.0021063483112368922\n",
      "    Batch: 13000 --> Loss: 0.0020988085675169997\n",
      "    Batch: 13100 --> Loss: 0.002099755193160148\n",
      "    Batch: 13200 --> Loss: 0.002095112059941261\n",
      "    Batch: 13300 --> Loss: 0.002100586679696823\n",
      "    Batch: 13400 --> Loss: 0.0020982484534700077\n",
      "    Batch: 13500 --> Loss: 0.0020933554838315116\n",
      "    Batch: 13600 --> Loss: 0.002091698817969068\n",
      "    Batch: 13700 --> Loss: 0.002087275585634059\n",
      "    Batch: 13800 --> Loss: 0.002097838386595622\n",
      "    Batch: 13900 --> Loss: 0.0020932963128649784\n",
      "    Batch: 14000 --> Loss: 0.002099702803604043\n",
      "    Batch: 14100 --> Loss: 0.0021056946896447003\n",
      "    Batch: 14200 --> Loss: 0.0021048548883880573\n",
      "    Batch: 14300 --> Loss: 0.002106823512864698\n",
      "    Batch: 14400 --> Loss: 0.002108044306190806\n",
      "    Batch: 14500 --> Loss: 0.0021081827345062003\n",
      "    Batch: 14600 --> Loss: 0.002108712285792676\n",
      "    Batch: 14700 --> Loss: 0.0021028279374781652\n",
      "    Batch: 14800 --> Loss: 0.002099875199916708\n",
      "    Batch: 14900 --> Loss: 0.0021034572041976297\n",
      "    Batch: 15000 --> Loss: 0.0021112410170591304\n",
      "    Batch: 15100 --> Loss: 0.002111683452076517\n",
      "    Batch: 15200 --> Loss: 0.0021072812330291744\n",
      "    Batch: 15300 --> Loss: 0.0021045674623294576\n",
      "    Batch: 15400 --> Loss: 0.002103938569831149\n",
      "    Batch: 15500 --> Loss: 0.0021024200300624307\n",
      "    Batch: 15600 --> Loss: 0.0020989774674403145\n",
      "    Batch: 15700 --> Loss: 0.0020961196156374412\n",
      "    Batch: 15800 --> Loss: 0.0020915079549848766\n",
      "    Batch: 15900 --> Loss: 0.0020906166594176685\n",
      "    Batch: 16000 --> Loss: 0.0020933909173995015\n",
      "    Batch: 16100 --> Loss: 0.0020983410796404996\n",
      "    Batch: 16200 --> Loss: 0.0020974649060215645\n",
      "    Batch: 16300 --> Loss: 0.0020975920713079163\n",
      "    Batch: 16400 --> Loss: 0.0020959697463771155\n",
      "    Batch: 16500 --> Loss: 0.0020947523819286893\n",
      "    Batch: 16600 --> Loss: 0.0020971393849879357\n",
      "    Batch: 16700 --> Loss: 0.0020952359704099243\n",
      "    Batch: 16800 --> Loss: 0.0020902099161503986\n",
      "    Batch: 16900 --> Loss: 0.0020844969510393674\n",
      "    Batch: 17000 --> Loss: 0.002088231949327246\n",
      "    Batch: 17100 --> Loss: 0.0020839862354070267\n",
      "    Batch: 17200 --> Loss: 0.0020858123982697703\n",
      "    Batch: 17300 --> Loss: 0.0020847976308752325\n",
      "    Batch: 17400 --> Loss: 0.002086441680400105\n",
      "    Batch: 17500 --> Loss: 0.0020821793023957395\n",
      "    Batch: 17600 --> Loss: 0.002081039432949303\n",
      "    Batch: 17700 --> Loss: 0.0020771931146078334\n",
      "    Batch: 17800 --> Loss: 0.002073730734098583\n",
      "    Batch: 17900 --> Loss: 0.0020744006056888998\n",
      "    Batch: 18000 --> Loss: 0.0020721122887839443\n",
      "    Batch: 18100 --> Loss: 0.002068570750772072\n",
      "    Batch: 18200 --> Loss: 0.0020659973355806813\n",
      "    Batch: 18300 --> Loss: 0.002060438960621916\n",
      "    Batch: 18400 --> Loss: 0.0020564291961620174\n",
      "    Batch: 18500 --> Loss: 0.0020523813811140234\n",
      "    Batch: 18600 --> Loss: 0.002049399671706617\n",
      "    Batch: 18700 --> Loss: 0.0020538660212903966\n",
      "    Batch: 18800 --> Loss: 0.002057672081211893\n",
      "    Batch: 18900 --> Loss: 0.0020544283411921774\n",
      "    Batch: 19000 --> Loss: 0.002061021688874586\n",
      "    Batch: 19100 --> Loss: 0.0020616305528378892\n",
      "    Batch: 19200 --> Loss: 0.002067929097530478\n",
      "    Batch: 19300 --> Loss: 0.0020687559493693516\n",
      "    Batch: 19400 --> Loss: 0.002066133793422554\n",
      "    Batch: 19500 --> Loss: 0.002061514197757117\n",
      "    Batch: 19600 --> Loss: 0.0020577851034723426\n",
      "    Batch: 19700 --> Loss: 0.0020536318516150786\n",
      "    Batch: 19800 --> Loss: 0.002051292059727726\n",
      "    Batch: 19900 --> Loss: 0.0020489770349308\n",
      "    Batch: 20000 --> Loss: 0.0020478402412834897\n",
      "    Batch: 20100 --> Loss: 0.0020490366274990516\n",
      "    Batch: 20200 --> Loss: 0.0020473354071474147\n",
      "    Batch: 20300 --> Loss: 0.0020475381399000666\n",
      "    Batch: 20400 --> Loss: 0.0020492003755487355\n",
      "    Batch: 20500 --> Loss: 0.0020454110269228043\n",
      "    Batch: 20600 --> Loss: 0.0020426981413527927\n",
      "    Batch: 20700 --> Loss: 0.002043802833486376\n",
      "    Batch: 20800 --> Loss: 0.0020411515930420284\n",
      "    Batch: 20900 --> Loss: 0.0020386911643597828\n",
      "    Batch: 21000 --> Loss: 0.002035750919784395\n",
      "    Batch: 21100 --> Loss: 0.0020395514180101865\n",
      "    Batch: 21200 --> Loss: 0.0020425161875112416\n",
      "    Batch: 21300 --> Loss: 0.0020436149136559695\n",
      "    Batch: 21400 --> Loss: 0.00204120189923531\n",
      "    Batch: 21500 --> Loss: 0.0020469054533773902\n",
      "    Batch: 21600 --> Loss: 0.0020440030697490025\n",
      "    Batch: 21700 --> Loss: 0.0020415195567264886\n",
      "    Batch: 21800 --> Loss: 0.0020421124000068484\n",
      "    Batch: 21900 --> Loss: 0.0020398145178127276\n",
      "    Batch: 22000 --> Loss: 0.002036075571922292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 22100 --> Loss: 0.002032114772656265\n",
      "    Batch: 22200 --> Loss: 0.0020321515426863237\n",
      "    Batch: 22300 --> Loss: 0.0020284649090510174\n",
      "    Batch: 22400 --> Loss: 0.002027654387940406\n",
      "    Batch: 22500 --> Loss: 0.0020275047052004907\n",
      "    Batch: 22600 --> Loss: 0.002028777701769208\n",
      "    Batch: 22700 --> Loss: 0.002029518373538526\n",
      "    Batch: 22800 --> Loss: 0.00203108963929457\n",
      "    Batch: 22900 --> Loss: 0.002029240144161679\n",
      "    Batch: 23000 --> Loss: 0.0020286128773773626\n",
      "    Batch: 23100 --> Loss: 0.0020256412294021044\n",
      "    Batch: 23200 --> Loss: 0.0020238955841726954\n",
      "    Batch: 23300 --> Loss: 0.00202047681437776\n",
      "    Batch: 23400 --> Loss: 0.0020267776756666953\n",
      "    Batch: 23500 --> Loss: 0.0020302294471139203\n",
      "    Batch: 23600 --> Loss: 0.0020279273109853713\n",
      "    Batch: 23700 --> Loss: 0.0020247050307218273\n",
      "    Batch: 23800 --> Loss: 0.0020225172394842473\n",
      "    Batch: 23900 --> Loss: 0.002022354704017242\n",
      "    Batch: 24000 --> Loss: 0.0020207422046224295\n",
      "    Batch: 24100 --> Loss: 0.0020219276258771454\n",
      "    Batch: 24200 --> Loss: 0.0020175174258273555\n",
      "    Batch: 24300 --> Loss: 0.002015287948411682\n",
      "    Batch: 24400 --> Loss: 0.002012508093905799\n",
      "    Batch: 24500 --> Loss: 0.0020108955251529792\n",
      "    Batch: 24600 --> Loss: 0.0020134270356726484\n",
      "    Batch: 24700 --> Loss: 0.0020127846367651216\n",
      "    Batch: 24800 --> Loss: 0.002012015590523109\n",
      "    Batch: 24900 --> Loss: 0.0020105018172898816\n",
      "    Batch: 25000 --> Loss: 0.0020069015690651172\n",
      "    Batch: 25100 --> Loss: 0.0020060110192606606\n",
      "    Batch: 25200 --> Loss: 0.002003202002498331\n",
      "    Batch: 25300 --> Loss: 0.0020052702013915037\n",
      "    Batch: 25400 --> Loss: 0.0020032018321788575\n",
      "    Batch: 25500 --> Loss: 0.0020004941036158336\n",
      "    Batch: 25600 --> Loss: 0.001999239674152907\n",
      "    Batch: 25700 --> Loss: 0.0019966444222573533\n",
      "    Batch: 25800 --> Loss: 0.001998673657536795\n",
      "    Batch: 25900 --> Loss: 0.0020004869757283368\n",
      "    Batch: 26000 --> Loss: 0.0019982256180719896\n",
      "    Batch: 26100 --> Loss: 0.001996159245310052\n",
      "    Batch: 26200 --> Loss: 0.001998931297517411\n",
      "    Batch: 26300 --> Loss: 0.0020005463091965488\n",
      "    Batch: 26400 --> Loss: 0.002004628796226838\n",
      "    Batch: 26500 --> Loss: 0.0020026131860292574\n",
      "    Batch: 26600 --> Loss: 0.0020038309384541363\n",
      "    Valid Loss: 0.0028615230694413185\n",
      "    Valid Loss: 0.0013587801931717652\n",
      "    Valid Loss: 0.001450852487790821\n",
      "    Valid Loss: 0.0014366545869758927\n",
      "    Valid Loss: 0.0014344800821703126\n",
      "    Valid Loss: 0.0012689676325076899\n",
      "    Valid Loss: 0.0013028745808196097\n",
      "    Valid Loss: 0.001312973850914711\n",
      "    Valid Loss: 0.0013075340131732665\n",
      "    Valid Loss: 0.0012559793606848248\n",
      "    Valid Loss: 0.0012784035436342503\n",
      "Epoch 2\n",
      "    Batch: 0 --> Loss: 0.0007190105388872325\n",
      "    Batch: 100 --> Loss: 0.0016634043790233477\n",
      "    Batch: 200 --> Loss: 0.0014470676282059356\n",
      "    Batch: 300 --> Loss: 0.0014110669974256194\n",
      "    Batch: 400 --> Loss: 0.0012876244452765802\n",
      "    Batch: 500 --> Loss: 0.0012213977017824882\n",
      "    Batch: 600 --> Loss: 0.0012736173684231614\n",
      "    Batch: 700 --> Loss: 0.0014354161063604902\n",
      "    Batch: 800 --> Loss: 0.0014578221797120331\n",
      "    Batch: 900 --> Loss: 0.0014683660830707662\n",
      "    Batch: 1000 --> Loss: 0.0014672399202415361\n",
      "    Batch: 1100 --> Loss: 0.0015373480293777765\n",
      "    Batch: 1200 --> Loss: 0.0015981209483617177\n",
      "    Batch: 1300 --> Loss: 0.0015929034729271804\n",
      "    Batch: 1400 --> Loss: 0.0015838298974169879\n",
      "    Batch: 1500 --> Loss: 0.0016078376523135486\n",
      "    Batch: 1600 --> Loss: 0.0016290025016512766\n",
      "    Batch: 1700 --> Loss: 0.0016606479717838072\n",
      "    Batch: 1800 --> Loss: 0.0016553742823952724\n",
      "    Batch: 1900 --> Loss: 0.0016831386131613594\n",
      "    Batch: 2000 --> Loss: 0.0016564126238017094\n",
      "    Batch: 2100 --> Loss: 0.0017580675977895374\n",
      "    Batch: 2200 --> Loss: 0.0017739471202738832\n",
      "    Batch: 2300 --> Loss: 0.0017772275505409541\n",
      "    Batch: 2400 --> Loss: 0.001793794728934002\n",
      "    Batch: 2500 --> Loss: 0.0018020831560609553\n",
      "    Batch: 2600 --> Loss: 0.0017907314773435669\n",
      "    Batch: 2700 --> Loss: 0.0017964185781435794\n",
      "    Batch: 2800 --> Loss: 0.0018128407150298663\n",
      "    Batch: 2900 --> Loss: 0.0018553576721997461\n",
      "    Batch: 3000 --> Loss: 0.001841164744233441\n",
      "    Batch: 3100 --> Loss: 0.0018339922197484618\n",
      "    Batch: 3200 --> Loss: 0.0018110810336678799\n",
      "    Batch: 3300 --> Loss: 0.0018087764897033872\n",
      "    Batch: 3400 --> Loss: 0.001796378886614081\n",
      "    Batch: 3500 --> Loss: 0.0017824463082247838\n",
      "    Batch: 3600 --> Loss: 0.0017669993082555131\n",
      "    Batch: 3700 --> Loss: 0.0017541995026418304\n",
      "    Batch: 3800 --> Loss: 0.0017430358645641251\n",
      "    Batch: 3900 --> Loss: 0.0017336015182250009\n",
      "    Batch: 4000 --> Loss: 0.00172624961859374\n",
      "    Batch: 4100 --> Loss: 0.0017385177052743374\n",
      "    Batch: 4200 --> Loss: 0.0017512066240026676\n",
      "    Batch: 4300 --> Loss: 0.001758462184265142\n",
      "    Batch: 4400 --> Loss: 0.0017491606766375668\n",
      "    Batch: 4500 --> Loss: 0.0017361891633682317\n",
      "    Batch: 4600 --> Loss: 0.001751763481830723\n",
      "    Batch: 4700 --> Loss: 0.001750034157711447\n",
      "    Batch: 4800 --> Loss: 0.0017409233735591965\n",
      "    Batch: 4900 --> Loss: 0.0017494079966848032\n",
      "    Batch: 5000 --> Loss: 0.0017467858240545657\n",
      "    Batch: 5100 --> Loss: 0.0017769806885666144\n",
      "    Batch: 5200 --> Loss: 0.0017707663445607983\n",
      "    Batch: 5300 --> Loss: 0.0017633487687720845\n",
      "    Batch: 5400 --> Loss: 0.0017547479950512158\n",
      "    Batch: 5500 --> Loss: 0.0017440569182273077\n",
      "    Batch: 5600 --> Loss: 0.0017528886846686234\n",
      "    Batch: 5700 --> Loss: 0.001754679277952585\n",
      "    Batch: 5800 --> Loss: 0.0017524215802197211\n",
      "    Batch: 5900 --> Loss: 0.001757736233007301\n",
      "    Batch: 6000 --> Loss: 0.001747498875097276\n",
      "    Batch: 6100 --> Loss: 0.0017486809647989242\n",
      "    Batch: 6200 --> Loss: 0.001741329300106358\n",
      "    Batch: 6300 --> Loss: 0.0017511692430517633\n",
      "    Batch: 6400 --> Loss: 0.001743624972366031\n",
      "    Batch: 6500 --> Loss: 0.00173947862620952\n",
      "    Batch: 6600 --> Loss: 0.0017307204758050037\n",
      "    Batch: 6700 --> Loss: 0.0017235195335643739\n",
      "    Batch: 6800 --> Loss: 0.0017208807217664753\n",
      "    Batch: 6900 --> Loss: 0.001720456569352118\n",
      "    Batch: 7000 --> Loss: 0.00171313905248915\n",
      "    Batch: 7100 --> Loss: 0.001706835914800651\n",
      "    Batch: 7200 --> Loss: 0.0017093856910310895\n",
      "    Batch: 7300 --> Loss: 0.001713242043977011\n",
      "    Batch: 7400 --> Loss: 0.0017198492589928008\n",
      "    Batch: 7500 --> Loss: 0.0017269224512188794\n",
      "    Batch: 7600 --> Loss: 0.0017261671119411868\n",
      "    Batch: 7700 --> Loss: 0.0017242039680322658\n",
      "    Batch: 7800 --> Loss: 0.0017150287583440542\n",
      "    Batch: 7900 --> Loss: 0.0017068964209023911\n",
      "    Batch: 8000 --> Loss: 0.0017154332104051902\n",
      "    Batch: 8100 --> Loss: 0.0017202922632100156\n",
      "    Batch: 8200 --> Loss: 0.0017136609830759849\n",
      "    Batch: 8300 --> Loss: 0.001711922584931624\n",
      "    Batch: 8400 --> Loss: 0.0017091579272029587\n",
      "    Batch: 8500 --> Loss: 0.0017268099490987466\n",
      "    Batch: 8600 --> Loss: 0.0017194990271411949\n",
      "    Batch: 8700 --> Loss: 0.0017228118365339342\n",
      "    Batch: 8800 --> Loss: 0.0017210128281832373\n",
      "    Batch: 8900 --> Loss: 0.0017311075143776806\n",
      "    Batch: 9000 --> Loss: 0.0017369733357138575\n",
      "    Batch: 9100 --> Loss: 0.0017347518938651987\n",
      "    Batch: 9200 --> Loss: 0.0017310362056185313\n",
      "    Batch: 9300 --> Loss: 0.001728121479735101\n",
      "    Batch: 9400 --> Loss: 0.0017210436383354594\n",
      "    Batch: 9500 --> Loss: 0.0017189818760353368\n",
      "    Batch: 9600 --> Loss: 0.001711829760939264\n",
      "    Batch: 9700 --> Loss: 0.0017103584740312415\n",
      "    Batch: 9800 --> Loss: 0.001713371095003608\n",
      "    Batch: 9900 --> Loss: 0.0017088149995419298\n",
      "    Batch: 10000 --> Loss: 0.0017130384856295431\n",
      "    Batch: 10100 --> Loss: 0.0017092548581923179\n",
      "    Batch: 10200 --> Loss: 0.001702231834374808\n",
      "    Batch: 10300 --> Loss: 0.0017049858875605897\n",
      "    Batch: 10400 --> Loss: 0.0017131523889498818\n",
      "    Batch: 10500 --> Loss: 0.0017265342914656052\n",
      "    Batch: 10600 --> Loss: 0.0017267051804869796\n",
      "    Batch: 10700 --> Loss: 0.0017301109292047689\n",
      "    Batch: 10800 --> Loss: 0.0017265277548970484\n",
      "    Batch: 10900 --> Loss: 0.0017246927574348049\n",
      "    Batch: 11000 --> Loss: 0.0017254570750982726\n",
      "    Batch: 11100 --> Loss: 0.0017203337169858352\n",
      "    Batch: 11200 --> Loss: 0.0017208115025694653\n",
      "    Batch: 11300 --> Loss: 0.001716129232997475\n",
      "    Batch: 11400 --> Loss: 0.0017140433421970703\n",
      "    Batch: 11500 --> Loss: 0.001711294880335796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 11600 --> Loss: 0.0017113108724461747\n",
      "    Batch: 11700 --> Loss: 0.0017110327363134344\n",
      "    Batch: 11800 --> Loss: 0.0017080140933929463\n",
      "    Batch: 11900 --> Loss: 0.0017056843800515118\n",
      "    Batch: 12000 --> Loss: 0.0017043499069843877\n",
      "    Batch: 12100 --> Loss: 0.0017027173631580404\n",
      "    Batch: 12200 --> Loss: 0.0017013390618736115\n",
      "    Batch: 12300 --> Loss: 0.0017088127293480649\n",
      "    Batch: 12400 --> Loss: 0.001704917636676713\n",
      "    Batch: 12500 --> Loss: 0.0017018472864452812\n",
      "    Batch: 12600 --> Loss: 0.0017099954804678178\n",
      "    Batch: 12700 --> Loss: 0.0017067777183547978\n",
      "    Batch: 12800 --> Loss: 0.0017053652936207732\n",
      "    Batch: 12900 --> Loss: 0.0017043389586705354\n",
      "    Batch: 13000 --> Loss: 0.0017012269465125972\n",
      "    Batch: 13100 --> Loss: 0.0016999073641354521\n",
      "    Batch: 13200 --> Loss: 0.0016970829192964612\n",
      "    Batch: 13300 --> Loss: 0.0016928790280415866\n",
      "    Batch: 13400 --> Loss: 0.0016920258351483394\n",
      "    Batch: 13500 --> Loss: 0.001689013151309062\n",
      "    Batch: 13600 --> Loss: 0.0016912798495427057\n",
      "    Batch: 13700 --> Loss: 0.0016916244087878405\n",
      "    Batch: 13800 --> Loss: 0.0016896496496974682\n",
      "    Batch: 13900 --> Loss: 0.0016868963794692445\n",
      "    Batch: 14000 --> Loss: 0.0016837679274457232\n",
      "    Batch: 14100 --> Loss: 0.001688129838644995\n",
      "    Batch: 14200 --> Loss: 0.0016953468917349586\n",
      "    Batch: 14300 --> Loss: 0.0016913226031306076\n",
      "    Batch: 14400 --> Loss: 0.001689191732966307\n",
      "    Batch: 14500 --> Loss: 0.001689428646047161\n",
      "    Batch: 14600 --> Loss: 0.0016861137605864035\n",
      "    Batch: 14700 --> Loss: 0.00168539889215092\n",
      "    Batch: 14800 --> Loss: 0.0016864833297026908\n",
      "    Batch: 14900 --> Loss: 0.0016880806127232413\n",
      "    Batch: 15000 --> Loss: 0.0016880830513041486\n",
      "    Batch: 15100 --> Loss: 0.00168810946011187\n",
      "    Batch: 15200 --> Loss: 0.0016882108728372201\n",
      "    Batch: 15300 --> Loss: 0.0016861356402633394\n",
      "    Batch: 15400 --> Loss: 0.0016841937803434414\n",
      "    Batch: 15500 --> Loss: 0.0016842932336599034\n",
      "    Batch: 15600 --> Loss: 0.0016825821654121397\n",
      "    Batch: 15700 --> Loss: 0.0016822386138168654\n",
      "    Batch: 15800 --> Loss: 0.0016789888189888346\n",
      "    Batch: 15900 --> Loss: 0.0016828723516142238\n",
      "    Batch: 16000 --> Loss: 0.0016824543668539909\n",
      "    Batch: 16100 --> Loss: 0.001686577696497186\n",
      "    Batch: 16200 --> Loss: 0.0016844477833433647\n",
      "    Batch: 16300 --> Loss: 0.0016829660221232947\n",
      "    Batch: 16400 --> Loss: 0.0016815739504948415\n",
      "    Batch: 16500 --> Loss: 0.0016816959057820353\n",
      "    Batch: 16600 --> Loss: 0.001679164251545216\n",
      "    Batch: 16700 --> Loss: 0.0016811973418679334\n",
      "    Batch: 16800 --> Loss: 0.0016793340584065656\n",
      "    Batch: 16900 --> Loss: 0.0016806972116329697\n",
      "    Batch: 17000 --> Loss: 0.0016788166015200918\n",
      "    Batch: 17100 --> Loss: 0.001681733049480631\n",
      "    Batch: 17200 --> Loss: 0.0016894960719217541\n",
      "    Batch: 17300 --> Loss: 0.0016888531435370624\n",
      "    Batch: 17400 --> Loss: 0.0016887874327010876\n",
      "    Batch: 17500 --> Loss: 0.0016852774105657023\n",
      "    Batch: 17600 --> Loss: 0.001683778951616369\n",
      "    Batch: 17700 --> Loss: 0.0016846980508945836\n",
      "    Batch: 17800 --> Loss: 0.0016814764634980113\n",
      "    Batch: 17900 --> Loss: 0.0016807311011793093\n",
      "    Batch: 18000 --> Loss: 0.0016772834253052577\n",
      "    Batch: 18100 --> Loss: 0.0016759802853782198\n",
      "    Batch: 18200 --> Loss: 0.0016733593657258317\n",
      "    Batch: 18300 --> Loss: 0.0016769462024761755\n",
      "    Batch: 18400 --> Loss: 0.0016806735087182693\n",
      "    Batch: 18500 --> Loss: 0.0016792447854696371\n",
      "    Batch: 18600 --> Loss: 0.0016775011060556845\n",
      "    Batch: 18700 --> Loss: 0.0016792519300916828\n",
      "    Batch: 18800 --> Loss: 0.0016955088605404814\n",
      "    Batch: 18900 --> Loss: 0.0016952918465648315\n",
      "    Batch: 19000 --> Loss: 0.0016985336096479637\n",
      "    Batch: 19100 --> Loss: 0.0016982625781577837\n",
      "    Batch: 19200 --> Loss: 0.0017008667061364968\n",
      "    Batch: 19300 --> Loss: 0.0016976298673208534\n",
      "    Batch: 19400 --> Loss: 0.0016983586174986714\n",
      "    Batch: 19500 --> Loss: 0.0016967244603466637\n",
      "    Batch: 19600 --> Loss: 0.0017009572144901122\n",
      "    Batch: 19700 --> Loss: 0.0016978934271818593\n",
      "    Batch: 19800 --> Loss: 0.0016947276096725406\n",
      "    Batch: 19900 --> Loss: 0.0016949038768798927\n",
      "    Batch: 20000 --> Loss: 0.0016943801088705084\n",
      "    Batch: 20100 --> Loss: 0.0016966995517358543\n",
      "    Batch: 20200 --> Loss: 0.0016960110330279013\n",
      "    Batch: 20300 --> Loss: 0.001696700320063866\n",
      "    Batch: 20400 --> Loss: 0.0017015885076290594\n",
      "    Batch: 20500 --> Loss: 0.0017028265609343378\n",
      "    Batch: 20600 --> Loss: 0.001705905185100733\n",
      "    Batch: 20700 --> Loss: 0.0017049692250001047\n",
      "    Batch: 20800 --> Loss: 0.0017081677965241403\n",
      "    Batch: 20900 --> Loss: 0.0017046749782115634\n",
      "    Batch: 21000 --> Loss: 0.0017054561352112332\n",
      "    Batch: 21100 --> Loss: 0.001703860920486806\n",
      "    Batch: 21200 --> Loss: 0.001702010968910837\n",
      "    Batch: 21300 --> Loss: 0.001700151186315695\n",
      "    Batch: 21400 --> Loss: 0.001699575593325622\n",
      "    Batch: 21500 --> Loss: 0.0017004497280651487\n",
      "    Batch: 21600 --> Loss: 0.0016992694028124576\n",
      "    Batch: 21700 --> Loss: 0.0016983978622554547\n",
      "    Batch: 21800 --> Loss: 0.0017037003777676935\n",
      "    Batch: 21900 --> Loss: 0.0017037065697957988\n",
      "    Batch: 22000 --> Loss: 0.0017005196472791237\n",
      "    Batch: 22100 --> Loss: 0.0017018352580692238\n",
      "    Batch: 22200 --> Loss: 0.001702694924110134\n",
      "    Batch: 22300 --> Loss: 0.0017043359137796376\n",
      "    Batch: 22400 --> Loss: 0.001702641872251399\n",
      "    Batch: 22500 --> Loss: 0.0017040421880584547\n",
      "    Batch: 22600 --> Loss: 0.0017056141562527265\n",
      "    Batch: 22700 --> Loss: 0.0017050048019349436\n",
      "    Batch: 22800 --> Loss: 0.0017038198884429317\n",
      "    Batch: 22900 --> Loss: 0.0017038051975473041\n",
      "    Batch: 23000 --> Loss: 0.0017024267895450573\n",
      "    Batch: 23100 --> Loss: 0.0017011028154928095\n",
      "    Batch: 23200 --> Loss: 0.0017014904022896577\n",
      "    Batch: 23300 --> Loss: 0.0016988781910915512\n",
      "    Batch: 23400 --> Loss: 0.0017009755474533693\n",
      "    Batch: 23500 --> Loss: 0.001701878479339824\n",
      "    Batch: 23600 --> Loss: 0.0017016470824990514\n",
      "    Batch: 23700 --> Loss: 0.001699307988678442\n",
      "    Batch: 23800 --> Loss: 0.001697374420355616\n",
      "    Batch: 23900 --> Loss: 0.001695048927321207\n",
      "    Batch: 24000 --> Loss: 0.0016936128972139457\n",
      "    Batch: 24100 --> Loss: 0.0016940142485350415\n",
      "    Batch: 24200 --> Loss: 0.0016955236602391028\n",
      "    Batch: 24300 --> Loss: 0.0016932468194907494\n",
      "    Batch: 24400 --> Loss: 0.00169191553304792\n",
      "    Batch: 24500 --> Loss: 0.0016985874570053614\n",
      "    Batch: 24600 --> Loss: 0.0016980909430816128\n",
      "    Batch: 24700 --> Loss: 0.0016990825861082567\n",
      "    Batch: 24800 --> Loss: 0.0016996336449119319\n",
      "    Batch: 24900 --> Loss: 0.0016963817321126206\n",
      "    Batch: 25000 --> Loss: 0.0016956784214327025\n",
      "    Batch: 25100 --> Loss: 0.001698493715681593\n",
      "    Batch: 25200 --> Loss: 0.00169931411410606\n",
      "    Batch: 25300 --> Loss: 0.0016992815861965086\n",
      "    Batch: 25400 --> Loss: 0.0016970632197434202\n",
      "    Batch: 25500 --> Loss: 0.0016946691005363236\n",
      "    Batch: 25600 --> Loss: 0.0016940575600544868\n",
      "    Batch: 25700 --> Loss: 0.001692129589728571\n",
      "    Batch: 25800 --> Loss: 0.0016954980065468335\n",
      "    Batch: 25900 --> Loss: 0.0016930978021229464\n",
      "    Batch: 26000 --> Loss: 0.0016925051481997326\n",
      "    Batch: 26100 --> Loss: 0.001692040055672184\n",
      "    Batch: 26200 --> Loss: 0.0016922908718294142\n",
      "    Batch: 26300 --> Loss: 0.0016935721379851526\n",
      "    Batch: 26400 --> Loss: 0.0016932046148302679\n",
      "    Batch: 26500 --> Loss: 0.0016952667282290298\n",
      "    Batch: 26600 --> Loss: 0.001692639220442245\n",
      "    Valid Loss: 0.00011131994688184932\n",
      "         Saving.... Best epoch: 2 -> 0.00011131994688184932\n",
      "    Valid Loss: 0.0016495379171486224\n",
      "    Valid Loss: 0.0011438635673970056\n",
      "    Valid Loss: 0.0010864857414191258\n",
      "    Valid Loss: 0.0011202974019398858\n",
      "    Valid Loss: 0.001189960072771955\n",
      "    Valid Loss: 0.0011993177798685574\n",
      "    Valid Loss: 0.0012614793286434838\n",
      "    Valid Loss: 0.001388965839936169\n",
      "    Valid Loss: 0.001342437057800115\n",
      "    Valid Loss: 0.0013072907877921855\n",
      "Epoch 3\n",
      "    Batch: 0 --> Loss: 5.966273874946637e-07\n",
      "    Batch: 100 --> Loss: 0.001048234950963594\n",
      "    Batch: 200 --> Loss: 0.0009581905715678574\n",
      "    Batch: 300 --> Loss: 0.0011806002970680418\n",
      "    Batch: 400 --> Loss: 0.0016171496419021907\n",
      "    Batch: 500 --> Loss: 0.001689494656194283\n",
      "    Batch: 600 --> Loss: 0.0015634930344116348\n",
      "    Batch: 700 --> Loss: 0.0014724525259768692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 800 --> Loss: 0.0014711534786916868\n",
      "    Batch: 900 --> Loss: 0.0014692673675883517\n",
      "    Batch: 1000 --> Loss: 0.001419715754140716\n",
      "    Batch: 1100 --> Loss: 0.0013812836214983934\n",
      "    Batch: 1200 --> Loss: 0.0013775218150964639\n",
      "    Batch: 1300 --> Loss: 0.0014369937389791564\n",
      "    Batch: 1400 --> Loss: 0.0015629547437248073\n",
      "    Batch: 1500 --> Loss: 0.0017083389963350967\n",
      "    Batch: 1600 --> Loss: 0.001716494621376469\n",
      "    Batch: 1700 --> Loss: 0.0016978882091637523\n",
      "    Batch: 1800 --> Loss: 0.001657376666489428\n",
      "    Batch: 1900 --> Loss: 0.0016609239441913838\n",
      "    Batch: 2000 --> Loss: 0.0016167925882532724\n",
      "    Batch: 2100 --> Loss: 0.0015843751362493953\n",
      "    Batch: 2200 --> Loss: 0.0015892260898055302\n",
      "    Batch: 2300 --> Loss: 0.001577999283326145\n",
      "    Batch: 2400 --> Loss: 0.0015970671533765749\n",
      "    Batch: 2500 --> Loss: 0.001594085360765338\n",
      "    Batch: 2600 --> Loss: 0.0016289644974107605\n",
      "    Batch: 2700 --> Loss: 0.0015961800483018731\n",
      "    Batch: 2800 --> Loss: 0.0015678711937449387\n",
      "    Batch: 2900 --> Loss: 0.001552256132014142\n",
      "    Batch: 3000 --> Loss: 0.0015390742917256509\n",
      "    Batch: 3100 --> Loss: 0.0015394232297768796\n",
      "    Batch: 3200 --> Loss: 0.00153042751534355\n",
      "    Batch: 3300 --> Loss: 0.0015311660715116558\n",
      "    Batch: 3400 --> Loss: 0.0015194064169198336\n",
      "    Batch: 3500 --> Loss: 0.0015285021366947212\n",
      "    Batch: 3600 --> Loss: 0.0015565167516034695\n",
      "    Batch: 3700 --> Loss: 0.001551513053879203\n",
      "    Batch: 3800 --> Loss: 0.0015436904073719265\n",
      "    Batch: 3900 --> Loss: 0.0015414568309672875\n",
      "    Batch: 4000 --> Loss: 0.0015301337552702454\n",
      "    Batch: 4100 --> Loss: 0.0015212813064509666\n",
      "    Batch: 4200 --> Loss: 0.0015153332232209142\n",
      "    Batch: 4300 --> Loss: 0.001525442501534255\n",
      "    Batch: 4400 --> Loss: 0.0015276072627650912\n",
      "    Batch: 4500 --> Loss: 0.0015244108385839986\n",
      "    Batch: 4600 --> Loss: 0.001525024563113198\n",
      "    Batch: 4700 --> Loss: 0.0015344169569110277\n",
      "    Batch: 4800 --> Loss: 0.0015430084787026012\n",
      "    Batch: 4900 --> Loss: 0.0015406210371302915\n",
      "    Batch: 5000 --> Loss: 0.001563974644268902\n",
      "    Batch: 5100 --> Loss: 0.001558174775719883\n",
      "    Batch: 5200 --> Loss: 0.0015572820397473251\n",
      "    Batch: 5300 --> Loss: 0.0015465607067091352\n",
      "    Batch: 5400 --> Loss: 0.0015510022673849158\n",
      "    Batch: 5500 --> Loss: 0.0015517251236764988\n",
      "    Batch: 5600 --> Loss: 0.0015506866364323152\n",
      "    Batch: 5700 --> Loss: 0.001539655494101326\n",
      "    Batch: 5800 --> Loss: 0.0015388682882767947\n",
      "    Batch: 5900 --> Loss: 0.0015486034639490473\n",
      "    Batch: 6000 --> Loss: 0.0015348876462210946\n",
      "    Batch: 6100 --> Loss: 0.0015276272078000473\n",
      "    Batch: 6200 --> Loss: 0.0015291722225366563\n",
      "    Batch: 6300 --> Loss: 0.001534771881272138\n",
      "    Batch: 6400 --> Loss: 0.0015284385902513723\n",
      "    Batch: 6500 --> Loss: 0.001535015984268769\n",
      "    Batch: 6600 --> Loss: 0.0015311138661664623\n",
      "    Batch: 6700 --> Loss: 0.001568458748997087\n",
      "    Batch: 6800 --> Loss: 0.0015772449674663809\n",
      "    Batch: 6900 --> Loss: 0.0015753040499103313\n",
      "    Batch: 7000 --> Loss: 0.0015721523106117105\n",
      "    Batch: 7100 --> Loss: 0.0015739618090611807\n",
      "    Batch: 7200 --> Loss: 0.0015707021000105825\n",
      "    Batch: 7300 --> Loss: 0.0015606679487283632\n",
      "    Batch: 7400 --> Loss: 0.0015639023318319409\n",
      "    Batch: 7500 --> Loss: 0.0015611209726330872\n",
      "    Batch: 7600 --> Loss: 0.001571052645806296\n",
      "    Batch: 7700 --> Loss: 0.0015635399104842141\n",
      "    Batch: 7800 --> Loss: 0.001557321120590248\n",
      "    Batch: 7900 --> Loss: 0.0015520403821830122\n",
      "    Batch: 8000 --> Loss: 0.0015475418877237527\n",
      "    Batch: 8100 --> Loss: 0.0015496704291566956\n",
      "    Batch: 8200 --> Loss: 0.001551952515734873\n",
      "    Batch: 8300 --> Loss: 0.001548419696196745\n",
      "    Batch: 8400 --> Loss: 0.001543758816000935\n",
      "    Batch: 8500 --> Loss: 0.0015471067462262671\n",
      "    Batch: 8600 --> Loss: 0.0015491411600792417\n",
      "    Batch: 8700 --> Loss: 0.001552695734811987\n",
      "    Batch: 8800 --> Loss: 0.001555416697461987\n",
      "    Batch: 8900 --> Loss: 0.0015802401776864373\n",
      "    Batch: 9000 --> Loss: 0.0015843589553662565\n",
      "    Batch: 9100 --> Loss: 0.0015791266115590889\n",
      "    Batch: 9200 --> Loss: 0.0016010256399508471\n",
      "    Batch: 9300 --> Loss: 0.0016009226547718939\n",
      "    Batch: 9400 --> Loss: 0.0015970349892280742\n",
      "    Batch: 9500 --> Loss: 0.0016028996872505428\n",
      "    Batch: 9600 --> Loss: 0.0016005597583948407\n",
      "    Batch: 9700 --> Loss: 0.0016023178444423594\n",
      "    Batch: 9800 --> Loss: 0.001595229526727558\n",
      "    Batch: 9900 --> Loss: 0.0015907980157944528\n",
      "    Batch: 10000 --> Loss: 0.0015844538760578912\n",
      "    Batch: 10100 --> Loss: 0.001596322081032286\n",
      "    Batch: 10200 --> Loss: 0.0016020000701987339\n",
      "    Batch: 10300 --> Loss: 0.001597649618865547\n",
      "    Batch: 10400 --> Loss: 0.0015986735501411311\n",
      "    Batch: 10500 --> Loss: 0.0015926262928441554\n",
      "    Batch: 10600 --> Loss: 0.001589092842116854\n",
      "    Batch: 10700 --> Loss: 0.0015870002747342044\n",
      "    Batch: 10800 --> Loss: 0.001585212402331541\n",
      "    Batch: 10900 --> Loss: 0.001580630563958828\n",
      "    Batch: 11000 --> Loss: 0.0015891398290799866\n",
      "    Batch: 11100 --> Loss: 0.0015902896447237242\n",
      "    Batch: 11200 --> Loss: 0.0015831294362409934\n",
      "    Batch: 11300 --> Loss: 0.0015811976200188031\n",
      "    Batch: 11400 --> Loss: 0.001585232537730293\n",
      "    Batch: 11500 --> Loss: 0.0015839846634634295\n",
      "    Batch: 11600 --> Loss: 0.0015788797601345936\n",
      "    Batch: 11700 --> Loss: 0.001573845217654638\n",
      "    Batch: 11800 --> Loss: 0.0015697089271927418\n",
      "    Batch: 11900 --> Loss: 0.001570135270191968\n",
      "    Batch: 12000 --> Loss: 0.0015676036142223812\n",
      "    Batch: 12100 --> Loss: 0.0015613322475453546\n",
      "    Batch: 12200 --> Loss: 0.001560464502323434\n",
      "    Batch: 12300 --> Loss: 0.0015702964907637791\n",
      "    Batch: 12400 --> Loss: 0.0015730620984663545\n",
      "    Batch: 12500 --> Loss: 0.001568201280120944\n",
      "    Batch: 12600 --> Loss: 0.0015711014843745347\n",
      "    Batch: 12700 --> Loss: 0.0015743413411873674\n",
      "    Batch: 12800 --> Loss: 0.001573374843842574\n",
      "    Batch: 12900 --> Loss: 0.0015723611977034484\n",
      "    Batch: 13000 --> Loss: 0.0015738692084201567\n",
      "    Batch: 13100 --> Loss: 0.0015683379966847824\n",
      "    Batch: 13200 --> Loss: 0.0015676784497689323\n",
      "    Batch: 13300 --> Loss: 0.0015734429346829818\n",
      "    Batch: 13400 --> Loss: 0.0015687667418090676\n",
      "    Batch: 13500 --> Loss: 0.0015664876119806685\n",
      "    Batch: 13600 --> Loss: 0.001569915837325491\n",
      "    Batch: 13700 --> Loss: 0.0015724288640961601\n",
      "    Batch: 13800 --> Loss: 0.001569751351307919\n",
      "    Batch: 13900 --> Loss: 0.0015757411789811696\n",
      "    Batch: 14000 --> Loss: 0.0015732081987731733\n",
      "    Batch: 14100 --> Loss: 0.0015724440167595318\n",
      "    Batch: 14200 --> Loss: 0.0015713730623430964\n",
      "    Batch: 14300 --> Loss: 0.001569093288996461\n",
      "    Batch: 14400 --> Loss: 0.001565054453355388\n",
      "    Batch: 14500 --> Loss: 0.0015691506328989484\n",
      "    Batch: 14600 --> Loss: 0.001568348657346385\n",
      "    Batch: 14700 --> Loss: 0.001569038994061889\n",
      "    Batch: 14800 --> Loss: 0.0015731040841743826\n",
      "    Batch: 14900 --> Loss: 0.0015827222832967688\n",
      "    Batch: 15000 --> Loss: 0.001579620778495693\n",
      "    Batch: 15100 --> Loss: 0.0015765814179931395\n",
      "    Batch: 15200 --> Loss: 0.0015789136526175146\n",
      "    Batch: 15300 --> Loss: 0.0015745874414131824\n",
      "    Batch: 15400 --> Loss: 0.0015739984452628926\n",
      "    Batch: 15500 --> Loss: 0.0015726905556314077\n",
      "    Batch: 15600 --> Loss: 0.0015724467409387178\n",
      "    Batch: 15700 --> Loss: 0.001571574031908662\n",
      "    Batch: 15800 --> Loss: 0.0015691255293333976\n",
      "    Batch: 15900 --> Loss: 0.001568795812880051\n",
      "    Batch: 16000 --> Loss: 0.0015710137214955196\n",
      "    Batch: 16100 --> Loss: 0.001569193877206371\n",
      "    Batch: 16200 --> Loss: 0.001573039586363738\n",
      "    Batch: 16300 --> Loss: 0.0015690981148545622\n",
      "    Batch: 16400 --> Loss: 0.0015750396314071161\n",
      "    Batch: 16500 --> Loss: 0.0015768209499052133\n",
      "    Batch: 16600 --> Loss: 0.0015817109033118887\n",
      "    Batch: 16700 --> Loss: 0.0015778575839452702\n",
      "    Batch: 16800 --> Loss: 0.0015771996244129296\n",
      "    Batch: 16900 --> Loss: 0.0015771753342090074\n",
      "    Batch: 17000 --> Loss: 0.0015812283054491255\n",
      "    Batch: 17100 --> Loss: 0.0015783864798438554\n",
      "    Batch: 17200 --> Loss: 0.001575143191805487\n",
      "    Batch: 17300 --> Loss: 0.0015715077919045259\n",
      "    Batch: 17400 --> Loss: 0.001572490558735757\n",
      "    Batch: 17500 --> Loss: 0.0015752502901839487\n",
      "    Batch: 17600 --> Loss: 0.0015783403617263326\n",
      "    Batch: 17700 --> Loss: 0.0015734104657111466\n",
      "    Batch: 17800 --> Loss: 0.001570382329938621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 17900 --> Loss: 0.001567234644404538\n",
      "    Batch: 18000 --> Loss: 0.0015652172191572469\n",
      "    Batch: 18100 --> Loss: 0.0015659362250209198\n",
      "    Batch: 18200 --> Loss: 0.001564303659938316\n",
      "    Batch: 18300 --> Loss: 0.0015608395964125446\n",
      "    Batch: 18400 --> Loss: 0.0015731962325125544\n",
      "    Batch: 18500 --> Loss: 0.001573395158345566\n",
      "    Batch: 18600 --> Loss: 0.001572820488134044\n",
      "    Batch: 18700 --> Loss: 0.0015738479605375685\n",
      "    Batch: 18800 --> Loss: 0.0015747743834098461\n",
      "    Batch: 18900 --> Loss: 0.0015725617015910803\n",
      "    Batch: 19000 --> Loss: 0.0015695805247280736\n",
      "    Batch: 19100 --> Loss: 0.0015698463354929446\n",
      "    Batch: 19200 --> Loss: 0.0015829271933722972\n",
      "    Batch: 19300 --> Loss: 0.001585870377838282\n",
      "    Batch: 19400 --> Loss: 0.0015851073791146738\n",
      "    Batch: 19500 --> Loss: 0.001585811735531421\n",
      "    Batch: 19600 --> Loss: 0.0015830984771585583\n",
      "    Batch: 19700 --> Loss: 0.001579458050222937\n",
      "    Batch: 19800 --> Loss: 0.0015755245274619329\n",
      "    Batch: 19900 --> Loss: 0.0015744691184340098\n",
      "    Batch: 20000 --> Loss: 0.0015723412839686174\n",
      "    Batch: 20100 --> Loss: 0.0015729439200449719\n",
      "    Batch: 20200 --> Loss: 0.0015746720325512222\n",
      "    Batch: 20300 --> Loss: 0.0015733518147117607\n",
      "    Batch: 20400 --> Loss: 0.0015730495414586665\n",
      "    Batch: 20500 --> Loss: 0.0015703246582231447\n",
      "    Batch: 20600 --> Loss: 0.0015669244054865334\n",
      "    Batch: 20700 --> Loss: 0.0015737892456541755\n",
      "    Batch: 20800 --> Loss: 0.0015747542387790391\n",
      "    Batch: 20900 --> Loss: 0.0015761743057539962\n",
      "    Batch: 21000 --> Loss: 0.0015783625711545215\n",
      "    Batch: 21100 --> Loss: 0.0015772073819906873\n",
      "    Batch: 21200 --> Loss: 0.0015783912959871024\n",
      "    Batch: 21300 --> Loss: 0.0015774276543293474\n",
      "    Batch: 21400 --> Loss: 0.0015793569171216664\n",
      "    Batch: 21500 --> Loss: 0.0015801127978162966\n",
      "    Batch: 21600 --> Loss: 0.001585741747864603\n",
      "    Batch: 21700 --> Loss: 0.0015827414850551362\n",
      "    Batch: 21800 --> Loss: 0.0015799590243273012\n",
      "    Batch: 21900 --> Loss: 0.001577244698282146\n",
      "    Batch: 22000 --> Loss: 0.0015776912980204054\n",
      "    Batch: 22100 --> Loss: 0.0015806150018117565\n",
      "    Batch: 22200 --> Loss: 0.0015774688703118758\n",
      "    Batch: 22300 --> Loss: 0.0015772056061719598\n",
      "    Batch: 22400 --> Loss: 0.001574431457691346\n",
      "    Batch: 22500 --> Loss: 0.0015715588157351249\n",
      "    Batch: 22600 --> Loss: 0.0015685208551125518\n",
      "    Batch: 22700 --> Loss: 0.001568550880106349\n",
      "    Batch: 22800 --> Loss: 0.0015698602401141625\n",
      "    Batch: 22900 --> Loss: 0.0015732847837066992\n",
      "    Batch: 23000 --> Loss: 0.0015733229422050723\n",
      "    Batch: 23100 --> Loss: 0.001571126097558476\n",
      "    Batch: 23200 --> Loss: 0.0015760993194955982\n",
      "    Batch: 23300 --> Loss: 0.0015776037786779551\n",
      "    Batch: 23400 --> Loss: 0.0015787212252012634\n",
      "    Batch: 23500 --> Loss: 0.0015767425908973962\n",
      "    Batch: 23600 --> Loss: 0.0015781230640636089\n",
      "    Batch: 23700 --> Loss: 0.001575507151323742\n",
      "    Batch: 23800 --> Loss: 0.0015742231495231135\n",
      "    Batch: 23900 --> Loss: 0.0015767916740096232\n",
      "    Batch: 24000 --> Loss: 0.0015790982556707716\n",
      "    Batch: 24100 --> Loss: 0.0015812521346042006\n",
      "    Batch: 24200 --> Loss: 0.0015822108515711594\n",
      "    Batch: 24300 --> Loss: 0.0015798885502738057\n",
      "    Batch: 24400 --> Loss: 0.0015776265672767646\n",
      "    Batch: 24500 --> Loss: 0.0015772284121398285\n",
      "    Batch: 24600 --> Loss: 0.001576399574150798\n",
      "    Batch: 24700 --> Loss: 0.0015770878694341976\n",
      "    Batch: 24800 --> Loss: 0.0015762038261918775\n",
      "    Batch: 24900 --> Loss: 0.0015745693040303322\n",
      "    Batch: 25000 --> Loss: 0.0015791943071609517\n",
      "    Batch: 25100 --> Loss: 0.0015780972842125275\n",
      "    Batch: 25200 --> Loss: 0.0015759452840455631\n",
      "    Batch: 25300 --> Loss: 0.0015745935992426165\n",
      "    Batch: 25400 --> Loss: 0.0015761914045641214\n",
      "    Batch: 25500 --> Loss: 0.0015752223574567578\n",
      "    Batch: 25600 --> Loss: 0.0015752944965210932\n",
      "    Batch: 25700 --> Loss: 0.0015744586128078357\n",
      "    Batch: 25800 --> Loss: 0.0015733750683306399\n",
      "    Batch: 25900 --> Loss: 0.0015699903072878974\n",
      "    Batch: 26000 --> Loss: 0.0015679882314742191\n",
      "    Batch: 26100 --> Loss: 0.0015673534137532604\n",
      "    Batch: 26200 --> Loss: 0.0015664425834732594\n",
      "    Batch: 26300 --> Loss: 0.0015641512045131898\n",
      "    Batch: 26400 --> Loss: 0.0015650088233897827\n",
      "    Batch: 26500 --> Loss: 0.0015678390564353721\n",
      "    Batch: 26600 --> Loss: 0.0015660000441096984\n",
      "    Valid Loss: 2.213269726780709e-05\n",
      "         Saving.... Best epoch: 3 -> 2.213269726780709e-05\n",
      "    Valid Loss: 0.0010553741776716254\n",
      "    Valid Loss: 0.001156947662426317\n",
      "    Valid Loss: 0.0012238508577695957\n",
      "    Valid Loss: 0.0011893436558423176\n",
      "    Valid Loss: 0.0011560343866772174\n",
      "    Valid Loss: 0.0011717487278869356\n",
      "    Valid Loss: 0.0011250685550682684\n",
      "    Valid Loss: 0.0011226059357405992\n",
      "    Valid Loss: 0.0010868051578231154\n",
      "    Valid Loss: 0.0010981048298767317\n",
      "Epoch 4\n",
      "    Batch: 0 --> Loss: 0.0001536743511678651\n",
      "    Batch: 100 --> Loss: 0.0007508391031697701\n",
      "    Batch: 200 --> Loss: 0.0009624791556639644\n",
      "    Batch: 300 --> Loss: 0.0009152744283984398\n",
      "    Batch: 400 --> Loss: 0.0009235970256112606\n",
      "    Batch: 500 --> Loss: 0.0010942509388480516\n",
      "    Batch: 600 --> Loss: 0.0010826927399728678\n",
      "    Batch: 700 --> Loss: 0.00127701182112905\n",
      "    Batch: 800 --> Loss: 0.0012201071150053872\n",
      "    Batch: 900 --> Loss: 0.0012094023541859079\n",
      "    Batch: 1000 --> Loss: 0.0012006002799325136\n",
      "    Batch: 1100 --> Loss: 0.001295029598893313\n",
      "    Batch: 1200 --> Loss: 0.0013728503640446893\n",
      "    Batch: 1300 --> Loss: 0.0013635199272095462\n",
      "    Batch: 1400 --> Loss: 0.0013524160375993854\n",
      "    Batch: 1500 --> Loss: 0.0013470797020040451\n",
      "    Batch: 1600 --> Loss: 0.0013388430122365821\n",
      "    Batch: 1700 --> Loss: 0.001327479596756885\n",
      "    Batch: 1800 --> Loss: 0.0013216456553412678\n",
      "    Batch: 1900 --> Loss: 0.0013389851606331379\n",
      "    Batch: 2000 --> Loss: 0.0013233811098275676\n",
      "    Batch: 2100 --> Loss: 0.0013186000093529454\n",
      "    Batch: 2200 --> Loss: 0.0013164727352786793\n",
      "    Batch: 2300 --> Loss: 0.0014078612663230555\n",
      "    Batch: 2400 --> Loss: 0.001409783507198943\n",
      "    Batch: 2500 --> Loss: 0.001390268172758746\n",
      "    Batch: 2600 --> Loss: 0.001386137928718139\n",
      "    Batch: 2700 --> Loss: 0.0013660024802670402\n",
      "    Batch: 2800 --> Loss: 0.0013798428940085673\n",
      "    Batch: 2900 --> Loss: 0.0013677818206478778\n",
      "    Batch: 3000 --> Loss: 0.001368698005040878\n",
      "    Batch: 3100 --> Loss: 0.00136599776279663\n",
      "    Batch: 3200 --> Loss: 0.0013526792192725169\n",
      "    Batch: 3300 --> Loss: 0.0013572422771986833\n",
      "    Batch: 3400 --> Loss: 0.0013491796397084963\n",
      "    Batch: 3500 --> Loss: 0.0013515005602393307\n",
      "    Batch: 3600 --> Loss: 0.0013412214551297413\n",
      "    Batch: 3700 --> Loss: 0.00133748696042488\n",
      "    Batch: 3800 --> Loss: 0.0013327168148362404\n",
      "    Batch: 3900 --> Loss: 0.0013335013785883611\n",
      "    Batch: 4000 --> Loss: 0.0013634200300642657\n",
      "    Batch: 4100 --> Loss: 0.0013618987935478718\n",
      "    Batch: 4200 --> Loss: 0.001346637192479138\n",
      "    Batch: 4300 --> Loss: 0.001369496271746782\n",
      "    Batch: 4400 --> Loss: 0.0013787724914613875\n",
      "    Batch: 4500 --> Loss: 0.0013768288189289816\n",
      "    Batch: 4600 --> Loss: 0.0013733990350810192\n",
      "    Batch: 4700 --> Loss: 0.001371374980221394\n",
      "    Batch: 4800 --> Loss: 0.0014099865760389767\n",
      "    Batch: 4900 --> Loss: 0.0014085412833280799\n",
      "    Batch: 5000 --> Loss: 0.001403763454747646\n",
      "    Batch: 5100 --> Loss: 0.001405156712183734\n",
      "    Batch: 5200 --> Loss: 0.0014043724441021254\n",
      "    Batch: 5300 --> Loss: 0.0013966179853420236\n",
      "    Batch: 5400 --> Loss: 0.00139734507335847\n",
      "    Batch: 5500 --> Loss: 0.001394731703325981\n",
      "    Batch: 5600 --> Loss: 0.0013893782332327661\n",
      "    Batch: 5700 --> Loss: 0.0014020706138980967\n",
      "    Batch: 5800 --> Loss: 0.0014223061912175168\n",
      "    Batch: 5900 --> Loss: 0.001418614094163168\n",
      "    Batch: 6000 --> Loss: 0.0014264830897497735\n",
      "    Batch: 6100 --> Loss: 0.0014398243609669542\n",
      "    Batch: 6200 --> Loss: 0.0014472173302937184\n",
      "    Batch: 6300 --> Loss: 0.0014534661127862934\n",
      "    Batch: 6400 --> Loss: 0.0014548114704135595\n",
      "    Batch: 6500 --> Loss: 0.0014745906188912397\n",
      "    Batch: 6600 --> Loss: 0.0014705869611845392\n",
      "    Batch: 6700 --> Loss: 0.0014665292434120716\n",
      "    Batch: 6800 --> Loss: 0.0014546219567822822\n",
      "    Batch: 6900 --> Loss: 0.0014469492519536974\n",
      "    Batch: 7000 --> Loss: 0.001451099829669966\n",
      "    Batch: 7100 --> Loss: 0.0014480565622526473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 7200 --> Loss: 0.0014397913454484595\n",
      "    Batch: 7300 --> Loss: 0.0014434966787783589\n",
      "    Batch: 7400 --> Loss: 0.0014380452330744057\n",
      "    Batch: 7500 --> Loss: 0.0014425300553738798\n",
      "    Batch: 7600 --> Loss: 0.0014416867548330595\n",
      "    Batch: 7700 --> Loss: 0.00145072182425429\n",
      "    Batch: 7800 --> Loss: 0.0014477784087184266\n",
      "    Batch: 7900 --> Loss: 0.0014801393230766803\n",
      "    Batch: 8000 --> Loss: 0.0014959085919595502\n",
      "    Batch: 8100 --> Loss: 0.0014980137518719868\n",
      "    Batch: 8200 --> Loss: 0.0015006507964106823\n",
      "    Batch: 8300 --> Loss: 0.0014961630688123853\n",
      "    Batch: 8400 --> Loss: 0.0014907265230881773\n",
      "    Batch: 8500 --> Loss: 0.0014897746888365506\n",
      "    Batch: 8600 --> Loss: 0.0014826822001913484\n",
      "    Batch: 8700 --> Loss: 0.0014792238825851903\n",
      "    Batch: 8800 --> Loss: 0.0014830833950989848\n",
      "    Batch: 8900 --> Loss: 0.0014788535037829921\n",
      "    Batch: 9000 --> Loss: 0.001480790977071447\n",
      "    Batch: 9100 --> Loss: 0.0014750329590163388\n",
      "    Batch: 9200 --> Loss: 0.0014847110330729475\n",
      "    Batch: 9300 --> Loss: 0.001482300590135147\n",
      "    Batch: 9400 --> Loss: 0.0014786373706347947\n",
      "    Batch: 9500 --> Loss: 0.0014752893694028179\n",
      "    Batch: 9600 --> Loss: 0.0014727542600877883\n",
      "    Batch: 9700 --> Loss: 0.0014699596179502102\n",
      "    Batch: 9800 --> Loss: 0.0014664213183184548\n",
      "    Batch: 9900 --> Loss: 0.0014666441567820104\n",
      "    Batch: 10000 --> Loss: 0.0014771601956138367\n",
      "    Batch: 10100 --> Loss: 0.0014739245229652427\n",
      "    Batch: 10200 --> Loss: 0.0014730114423056387\n",
      "    Batch: 10300 --> Loss: 0.001475982547851914\n",
      "    Batch: 10400 --> Loss: 0.0014779458911908894\n",
      "    Batch: 10500 --> Loss: 0.0014819293566291014\n",
      "    Batch: 10600 --> Loss: 0.0014792093943109081\n",
      "    Batch: 10700 --> Loss: 0.0014741999974585086\n",
      "    Batch: 10800 --> Loss: 0.0014702695370385447\n",
      "    Batch: 10900 --> Loss: 0.0014805139882008978\n",
      "    Batch: 11000 --> Loss: 0.0014763127618866454\n",
      "    Batch: 11100 --> Loss: 0.0014752652257333837\n",
      "    Batch: 11200 --> Loss: 0.0014730153420554727\n",
      "    Batch: 11300 --> Loss: 0.00147722381738866\n",
      "    Batch: 11400 --> Loss: 0.0014748782634191931\n",
      "    Batch: 11500 --> Loss: 0.0014766312396711568\n",
      "    Batch: 11600 --> Loss: 0.0014814089310073125\n",
      "    Batch: 11700 --> Loss: 0.0014905636739906505\n",
      "    Batch: 11800 --> Loss: 0.0014907998242117129\n",
      "    Batch: 11900 --> Loss: 0.0014910399639712574\n",
      "    Batch: 12000 --> Loss: 0.0014920075893886946\n",
      "    Batch: 12100 --> Loss: 0.0014863292200846276\n",
      "    Batch: 12200 --> Loss: 0.0014865418078764468\n",
      "    Batch: 12300 --> Loss: 0.0014846341846669127\n",
      "    Batch: 12400 --> Loss: 0.0014818227475716964\n",
      "    Batch: 12500 --> Loss: 0.0014817554346889784\n",
      "    Batch: 12600 --> Loss: 0.0014805457330595636\n",
      "    Batch: 12700 --> Loss: 0.0014804090099844638\n",
      "    Batch: 12800 --> Loss: 0.001476951457361569\n",
      "    Batch: 12900 --> Loss: 0.001480907590811811\n",
      "    Batch: 13000 --> Loss: 0.001478676942227438\n",
      "    Batch: 13100 --> Loss: 0.001481007510624551\n",
      "    Batch: 13200 --> Loss: 0.001481334153497216\n",
      "    Batch: 13300 --> Loss: 0.0014861973624870346\n",
      "    Batch: 13400 --> Loss: 0.0014825075756112066\n",
      "    Batch: 13500 --> Loss: 0.0014819619530708928\n",
      "    Batch: 13600 --> Loss: 0.0014781775399018665\n",
      "    Batch: 13700 --> Loss: 0.0014748283700568505\n",
      "    Batch: 13800 --> Loss: 0.0014754349512326652\n",
      "    Batch: 13900 --> Loss: 0.0014790175477175556\n",
      "    Batch: 14000 --> Loss: 0.0014750566778108606\n",
      "    Batch: 14100 --> Loss: 0.001471878761128551\n",
      "    Batch: 14200 --> Loss: 0.0014720938982205045\n",
      "    Batch: 14300 --> Loss: 0.0014711052174442483\n",
      "    Batch: 14400 --> Loss: 0.0014724326108832373\n",
      "    Batch: 14500 --> Loss: 0.0014757817634327359\n",
      "    Batch: 14600 --> Loss: 0.0014769636282429223\n",
      "    Batch: 14700 --> Loss: 0.0014788978173913316\n",
      "    Batch: 14800 --> Loss: 0.001480730214302591\n",
      "    Batch: 14900 --> Loss: 0.0014781810445762596\n",
      "    Batch: 15000 --> Loss: 0.0014737620184210526\n",
      "    Batch: 15100 --> Loss: 0.0014712628036685008\n",
      "    Batch: 15200 --> Loss: 0.001470899074160934\n",
      "    Batch: 15300 --> Loss: 0.0014685052086326658\n",
      "    Batch: 15400 --> Loss: 0.0014649700948635219\n",
      "    Batch: 15500 --> Loss: 0.001468362666044936\n",
      "    Batch: 15600 --> Loss: 0.0014662493569629676\n",
      "    Batch: 15700 --> Loss: 0.001471501312055588\n",
      "    Batch: 15800 --> Loss: 0.0014746714452329055\n",
      "    Batch: 15900 --> Loss: 0.0014725353670624007\n",
      "    Batch: 16000 --> Loss: 0.001477161005310659\n",
      "    Batch: 16100 --> Loss: 0.0014779411133401588\n",
      "    Batch: 16200 --> Loss: 0.0014763424594790406\n",
      "    Batch: 16300 --> Loss: 0.0014739425623295597\n",
      "    Batch: 16400 --> Loss: 0.0014706084721376144\n",
      "    Batch: 16500 --> Loss: 0.0014678088108153959\n",
      "    Batch: 16600 --> Loss: 0.0014677049452173083\n",
      "    Batch: 16700 --> Loss: 0.0014664440205107874\n",
      "    Batch: 16800 --> Loss: 0.001467508535749447\n",
      "    Batch: 16900 --> Loss: 0.001474085509098485\n",
      "    Batch: 17000 --> Loss: 0.001474021207896722\n",
      "    Batch: 17100 --> Loss: 0.0014704824185184401\n",
      "    Batch: 17200 --> Loss: 0.0014735579529450784\n",
      "    Batch: 17300 --> Loss: 0.0014750144120712222\n",
      "    Batch: 17400 --> Loss: 0.0014823056146401495\n",
      "    Batch: 17500 --> Loss: 0.0014835785114758505\n",
      "    Batch: 17600 --> Loss: 0.0014889785690643954\n",
      "    Batch: 17700 --> Loss: 0.0014867560822990913\n",
      "    Batch: 17800 --> Loss: 0.001485500442072841\n",
      "    Batch: 17900 --> Loss: 0.0014844439143757934\n",
      "    Batch: 18000 --> Loss: 0.0014822741368047247\n",
      "    Batch: 18100 --> Loss: 0.001483759240160235\n",
      "    Batch: 18200 --> Loss: 0.0014817321378078235\n",
      "    Batch: 18300 --> Loss: 0.0014791688877799256\n",
      "    Batch: 18400 --> Loss: 0.0014796448369516783\n",
      "    Batch: 18500 --> Loss: 0.0014765365881147952\n",
      "    Batch: 18600 --> Loss: 0.001474146689054142\n",
      "    Batch: 18700 --> Loss: 0.0014765607658449944\n",
      "    Batch: 18800 --> Loss: 0.0014765519521087688\n",
      "    Batch: 18900 --> Loss: 0.0014758264560394464\n",
      "    Batch: 19000 --> Loss: 0.0014775957060438108\n",
      "    Batch: 19100 --> Loss: 0.0014828230001505142\n",
      "    Batch: 19200 --> Loss: 0.0014824333982097582\n",
      "    Batch: 19300 --> Loss: 0.00148615124812196\n",
      "    Batch: 19400 --> Loss: 0.0014851560861908857\n",
      "    Batch: 19500 --> Loss: 0.001482829057419214\n",
      "    Batch: 19600 --> Loss: 0.001480686936498436\n",
      "    Batch: 19700 --> Loss: 0.0014786283311035017\n",
      "    Batch: 19800 --> Loss: 0.0014824866638329963\n",
      "    Batch: 19900 --> Loss: 0.0014836306127422418\n",
      "    Batch: 20000 --> Loss: 0.0014839598408224026\n",
      "    Batch: 20100 --> Loss: 0.0014817105928516005\n",
      "    Batch: 20200 --> Loss: 0.0014829926546997784\n",
      "    Batch: 20300 --> Loss: 0.0014810596227442675\n",
      "    Batch: 20400 --> Loss: 0.0014820165412152593\n",
      "    Batch: 20500 --> Loss: 0.0014823424339753001\n",
      "    Batch: 20600 --> Loss: 0.001485195677299339\n",
      "    Batch: 20700 --> Loss: 0.0014839717189111974\n",
      "    Batch: 20800 --> Loss: 0.0014847878342904236\n",
      "    Batch: 20900 --> Loss: 0.0014860608876406415\n",
      "    Batch: 21000 --> Loss: 0.0014858743162937445\n",
      "    Batch: 21100 --> Loss: 0.001485594670242788\n",
      "    Batch: 21200 --> Loss: 0.0014829978427949152\n",
      "    Batch: 21300 --> Loss: 0.0014861001237312752\n",
      "    Batch: 21400 --> Loss: 0.00148892691220627\n",
      "    Batch: 21500 --> Loss: 0.0014867696169821476\n",
      "    Batch: 21600 --> Loss: 0.001487111319019416\n",
      "    Batch: 21700 --> Loss: 0.0014851557549455559\n",
      "    Batch: 21800 --> Loss: 0.0014818743729950639\n",
      "    Batch: 21900 --> Loss: 0.0014802189763097758\n",
      "    Batch: 22000 --> Loss: 0.001481130376844872\n",
      "    Batch: 22100 --> Loss: 0.0014818906872114087\n",
      "    Batch: 22200 --> Loss: 0.001481710135115923\n",
      "    Batch: 22300 --> Loss: 0.0014806537777725694\n",
      "    Batch: 22400 --> Loss: 0.001478530693108004\n",
      "    Batch: 22500 --> Loss: 0.0014786167889346996\n",
      "    Batch: 22600 --> Loss: 0.0014792242297374982\n",
      "    Batch: 22700 --> Loss: 0.0014797410149474596\n",
      "    Batch: 22800 --> Loss: 0.0014817493642318427\n",
      "    Batch: 22900 --> Loss: 0.001483953643054621\n",
      "    Batch: 23000 --> Loss: 0.0014806207917632433\n",
      "    Batch: 23100 --> Loss: 0.0014805160058620223\n",
      "    Batch: 23200 --> Loss: 0.0014879100274591732\n",
      "    Batch: 23300 --> Loss: 0.0014873687853098857\n",
      "    Batch: 23400 --> Loss: 0.001487447373406904\n",
      "    Batch: 23500 --> Loss: 0.001495059771536066\n",
      "    Batch: 23600 --> Loss: 0.0014960752741337419\n",
      "    Batch: 23700 --> Loss: 0.0014951510943399388\n",
      "    Batch: 23800 --> Loss: 0.001497166260175121\n",
      "    Batch: 23900 --> Loss: 0.0014967236887876868\n",
      "    Batch: 24000 --> Loss: 0.0014960037200192854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 24100 --> Loss: 0.0014960523893061022\n",
      "    Batch: 24200 --> Loss: 0.0014953746669767584\n",
      "    Batch: 24300 --> Loss: 0.0014963050402945596\n",
      "    Batch: 24400 --> Loss: 0.0014958135701882698\n",
      "    Batch: 24500 --> Loss: 0.0014961284585372813\n",
      "    Batch: 24600 --> Loss: 0.0014944408218299512\n",
      "    Batch: 24700 --> Loss: 0.0014915132228899117\n",
      "    Batch: 24800 --> Loss: 0.0014904543935344474\n",
      "    Batch: 24900 --> Loss: 0.0014960213189622177\n",
      "    Batch: 25000 --> Loss: 0.001497046823266784\n",
      "    Batch: 25100 --> Loss: 0.0014949325767099095\n",
      "    Batch: 25200 --> Loss: 0.0014938184390832642\n",
      "    Batch: 25300 --> Loss: 0.0014914830838053171\n",
      "    Batch: 25400 --> Loss: 0.0014917547685806044\n",
      "    Batch: 25500 --> Loss: 0.0014930243213561757\n",
      "    Batch: 25600 --> Loss: 0.0014951716391903766\n",
      "    Batch: 25700 --> Loss: 0.0014976697797640587\n",
      "    Batch: 25800 --> Loss: 0.00150006772176332\n",
      "    Batch: 25900 --> Loss: 0.001499106403991372\n",
      "    Batch: 26000 --> Loss: 0.0014979416445180876\n",
      "    Batch: 26100 --> Loss: 0.0014971118107906202\n",
      "    Batch: 26200 --> Loss: 0.001495916120770467\n",
      "    Batch: 26300 --> Loss: 0.0014951695820256353\n",
      "    Batch: 26400 --> Loss: 0.001492339983370955\n",
      "    Batch: 26500 --> Loss: 0.0014901076261193027\n",
      "    Batch: 26600 --> Loss: 0.0014884740322897525\n",
      "    Valid Loss: 0.0001221474667545408\n",
      "    Valid Loss: 0.0009504967504557595\n",
      "    Valid Loss: 0.0012816023792771186\n",
      "    Valid Loss: 0.0012523196715323827\n",
      "    Valid Loss: 0.0016174540204468284\n",
      "    Valid Loss: 0.001506207590215832\n",
      "    Valid Loss: 0.0015448584687621314\n",
      "    Valid Loss: 0.0014709411922076568\n",
      "    Valid Loss: 0.0013766633522910485\n",
      "    Valid Loss: 0.0013808369588178525\n",
      "    Valid Loss: 0.001343943941705851\n",
      "Epoch 5\n",
      "    Batch: 0 --> Loss: 8.982462895801291e-05\n",
      "    Batch: 100 --> Loss: 0.0009722784772567695\n",
      "    Batch: 200 --> Loss: 0.0010206384044907008\n",
      "    Batch: 300 --> Loss: 0.0011776284346954042\n",
      "    Batch: 400 --> Loss: 0.0012919151468179946\n",
      "    Batch: 500 --> Loss: 0.001251840072930117\n",
      "    Batch: 600 --> Loss: 0.0012746977148197125\n",
      "    Batch: 700 --> Loss: 0.0013714336062344652\n",
      "    Batch: 800 --> Loss: 0.0013532748089460248\n",
      "    Batch: 900 --> Loss: 0.0013568947945557489\n",
      "    Batch: 1000 --> Loss: 0.0013710043367138683\n",
      "    Batch: 1100 --> Loss: 0.0013527393991325301\n",
      "    Batch: 1200 --> Loss: 0.0013403497060032396\n",
      "    Batch: 1300 --> Loss: 0.0012929830961430995\n",
      "    Batch: 1400 --> Loss: 0.0013320349630457139\n",
      "    Batch: 1500 --> Loss: 0.0013921859127721723\n",
      "    Batch: 1600 --> Loss: 0.001350501578604783\n",
      "    Batch: 1700 --> Loss: 0.0013631960987425362\n",
      "    Batch: 1800 --> Loss: 0.0013675628492507655\n",
      "    Batch: 1900 --> Loss: 0.001350619741318541\n",
      "    Batch: 2000 --> Loss: 0.0014105915838807763\n",
      "    Batch: 2100 --> Loss: 0.0013966587751303302\n",
      "    Batch: 2200 --> Loss: 0.0013989371433406935\n",
      "    Batch: 2300 --> Loss: 0.0013731909428949874\n",
      "    Batch: 2400 --> Loss: 0.001396610041458935\n",
      "    Batch: 2500 --> Loss: 0.001407141673135632\n",
      "    Batch: 2600 --> Loss: 0.0014155225540623618\n",
      "    Batch: 2700 --> Loss: 0.0014198624101948924\n",
      "    Batch: 2800 --> Loss: 0.00141057623276622\n",
      "    Batch: 2900 --> Loss: 0.0014045756266451346\n",
      "    Batch: 3000 --> Loss: 0.0014109745143067614\n",
      "    Batch: 3100 --> Loss: 0.0014046714443625363\n",
      "    Batch: 3200 --> Loss: 0.0013926181932942068\n",
      "    Batch: 3300 --> Loss: 0.0013862925077360077\n",
      "    Batch: 3400 --> Loss: 0.0013795603360775436\n",
      "    Batch: 3500 --> Loss: 0.0013914558476510303\n",
      "    Batch: 3600 --> Loss: 0.0013890962203488867\n",
      "    Batch: 3700 --> Loss: 0.0013851185417806371\n",
      "    Batch: 3800 --> Loss: 0.0013749931070556913\n",
      "    Batch: 3900 --> Loss: 0.001368134110963471\n",
      "    Batch: 4000 --> Loss: 0.0013788928531677353\n",
      "    Batch: 4100 --> Loss: 0.001394944071202124\n",
      "    Batch: 4200 --> Loss: 0.0013926609580862163\n",
      "    Batch: 4300 --> Loss: 0.0014065909909347432\n",
      "    Batch: 4400 --> Loss: 0.0013970614814687889\n",
      "    Batch: 4500 --> Loss: 0.0013970228635048604\n",
      "    Batch: 4600 --> Loss: 0.0013993983466143185\n",
      "    Batch: 4700 --> Loss: 0.0014007691433152569\n",
      "    Batch: 4800 --> Loss: 0.0013964747214872384\n",
      "    Batch: 4900 --> Loss: 0.001384972527019624\n",
      "    Batch: 5000 --> Loss: 0.0014112099281287573\n",
      "    Batch: 5100 --> Loss: 0.0014132856883996405\n",
      "    Batch: 5200 --> Loss: 0.0014148824248354421\n",
      "    Batch: 5300 --> Loss: 0.0014124109442070847\n",
      "    Batch: 5400 --> Loss: 0.0014407567094364636\n",
      "    Batch: 5500 --> Loss: 0.0014360677983417172\n",
      "    Batch: 5600 --> Loss: 0.001435867484407382\n",
      "    Batch: 5700 --> Loss: 0.0014493868890646216\n",
      "    Batch: 5800 --> Loss: 0.001456474628794679\n",
      "    Batch: 5900 --> Loss: 0.001450067277860709\n",
      "    Batch: 6000 --> Loss: 0.0014400479800335945\n",
      "    Batch: 6100 --> Loss: 0.001435242470248229\n",
      "    Batch: 6200 --> Loss: 0.0014588458815896208\n",
      "    Batch: 6300 --> Loss: 0.0014601742170710166\n",
      "    Batch: 6400 --> Loss: 0.0014747455077347208\n",
      "    Batch: 6500 --> Loss: 0.0014787875520253665\n",
      "    Batch: 6600 --> Loss: 0.0014845803031862915\n",
      "    Batch: 6700 --> Loss: 0.0014836872167678348\n",
      "    Batch: 6800 --> Loss: 0.0014959033700699506\n",
      "    Batch: 6900 --> Loss: 0.0015048661973830456\n",
      "    Batch: 7000 --> Loss: 0.0015090396667614846\n",
      "    Batch: 7100 --> Loss: 0.0015089454855215108\n",
      "    Batch: 7200 --> Loss: 0.0015072848036837866\n",
      "    Batch: 7300 --> Loss: 0.0015013495415295763\n",
      "    Batch: 7400 --> Loss: 0.0014937538378571097\n",
      "    Batch: 7500 --> Loss: 0.00149299190690342\n",
      "    Batch: 7600 --> Loss: 0.0014936786755168914\n",
      "    Batch: 7700 --> Loss: 0.0014875384986282327\n",
      "    Batch: 7800 --> Loss: 0.0014856470377964406\n",
      "    Batch: 7900 --> Loss: 0.0014956416167197734\n",
      "    Batch: 8000 --> Loss: 0.0014927871282940258\n",
      "    Batch: 8100 --> Loss: 0.0014892782677433931\n",
      "    Batch: 8200 --> Loss: 0.0015237949819064637\n",
      "    Batch: 8300 --> Loss: 0.0015287914322313842\n",
      "    Batch: 8400 --> Loss: 0.0015187482325491894\n",
      "    Batch: 8500 --> Loss: 0.0015234745296708757\n",
      "    Batch: 8600 --> Loss: 0.0015246274984310046\n",
      "    Batch: 8700 --> Loss: 0.0015278986106412715\n",
      "    Batch: 8800 --> Loss: 0.001526158266374645\n",
      "    Batch: 8900 --> Loss: 0.0015257412832116713\n",
      "    Batch: 9000 --> Loss: 0.0015202984924332947\n",
      "    Batch: 9100 --> Loss: 0.0015197029238613107\n",
      "    Batch: 9200 --> Loss: 0.0015156458794679478\n",
      "    Batch: 9300 --> Loss: 0.0015141395949207265\n",
      "    Batch: 9400 --> Loss: 0.0015097251100605214\n",
      "    Batch: 9500 --> Loss: 0.0015079737477931986\n",
      "    Batch: 9600 --> Loss: 0.0015089231862511856\n",
      "    Batch: 9700 --> Loss: 0.0015083402704086143\n",
      "    Batch: 9800 --> Loss: 0.0015051885113742086\n",
      "    Batch: 9900 --> Loss: 0.0014987847378017303\n",
      "    Batch: 10000 --> Loss: 0.001501664439857863\n",
      "    Batch: 10100 --> Loss: 0.001506103977660095\n",
      "    Batch: 10200 --> Loss: 0.0015051872235978157\n",
      "    Batch: 10300 --> Loss: 0.0015009149764970882\n",
      "    Batch: 10400 --> Loss: 0.0015095617809643936\n",
      "    Batch: 10500 --> Loss: 0.0015038914401177207\n",
      "    Batch: 10600 --> Loss: 0.0014996583656109897\n",
      "    Batch: 10700 --> Loss: 0.0014951140539006442\n",
      "    Batch: 10800 --> Loss: 0.0014873449828320814\n",
      "    Batch: 10900 --> Loss: 0.0014856121659828326\n",
      "    Batch: 11000 --> Loss: 0.001483256611928542\n",
      "    Batch: 11100 --> Loss: 0.001485852998124592\n",
      "    Batch: 11200 --> Loss: 0.0014897066623651162\n",
      "    Batch: 11300 --> Loss: 0.0014954650657990041\n",
      "    Batch: 11400 --> Loss: 0.0014939924851716006\n",
      "    Batch: 11500 --> Loss: 0.0014910878453401367\n",
      "    Batch: 11600 --> Loss: 0.00149150691908442\n",
      "    Batch: 11700 --> Loss: 0.001496453936955845\n",
      "    Batch: 11800 --> Loss: 0.0015087778562068467\n",
      "    Batch: 11900 --> Loss: 0.0015102988304642048\n",
      "    Batch: 12000 --> Loss: 0.001506011524912053\n",
      "    Batch: 12100 --> Loss: 0.0015060769000416455\n",
      "    Batch: 12200 --> Loss: 0.001510103825687354\n",
      "    Batch: 12300 --> Loss: 0.0015118411148482807\n",
      "    Batch: 12400 --> Loss: 0.0015105736256620964\n",
      "    Batch: 12500 --> Loss: 0.001510191738457912\n",
      "    Batch: 12600 --> Loss: 0.0015062690130323078\n",
      "    Batch: 12700 --> Loss: 0.0015028285708283645\n",
      "    Batch: 12800 --> Loss: 0.0015040435316655942\n",
      "    Batch: 12900 --> Loss: 0.0015002819795513485\n",
      "    Batch: 13000 --> Loss: 0.0014963107694390655\n",
      "    Batch: 13100 --> Loss: 0.0014912604852650973\n",
      "    Batch: 13200 --> Loss: 0.0014973094647352625\n",
      "    Batch: 13300 --> Loss: 0.0014964527658930101\n",
      "    Batch: 13400 --> Loss: 0.001492775141381914\n",
      "    Batch: 13500 --> Loss: 0.0014926551940553697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 13600 --> Loss: 0.0014908155895943243\n",
      "    Batch: 13700 --> Loss: 0.0014863510722302887\n",
      "    Batch: 13800 --> Loss: 0.0014817928800838606\n",
      "    Batch: 13900 --> Loss: 0.0014810598163052912\n",
      "    Batch: 14000 --> Loss: 0.00147637103099973\n",
      "    Batch: 14100 --> Loss: 0.0014779090264018333\n",
      "    Batch: 14200 --> Loss: 0.0014867561352593127\n",
      "    Batch: 14300 --> Loss: 0.0014834070957240432\n",
      "    Batch: 14400 --> Loss: 0.0014837874505417641\n",
      "    Batch: 14500 --> Loss: 0.001490361870830628\n",
      "    Batch: 14600 --> Loss: 0.0015064976806057988\n",
      "    Batch: 14700 --> Loss: 0.001506785516928536\n",
      "    Batch: 14800 --> Loss: 0.0015047784093353484\n",
      "    Batch: 14900 --> Loss: 0.001512762702579655\n",
      "    Batch: 15000 --> Loss: 0.0015115559389289992\n",
      "    Batch: 15100 --> Loss: 0.0015119924861975939\n",
      "    Batch: 15200 --> Loss: 0.0015123976364928592\n",
      "    Batch: 15300 --> Loss: 0.00150853309165857\n",
      "    Batch: 15400 --> Loss: 0.0015064174991451702\n",
      "    Batch: 15500 --> Loss: 0.0015031603953236687\n",
      "    Batch: 15600 --> Loss: 0.0015043274792605776\n",
      "    Batch: 15700 --> Loss: 0.0015014047609077396\n",
      "    Batch: 15800 --> Loss: 0.0014978701377949546\n",
      "    Batch: 15900 --> Loss: 0.001492839049859148\n",
      "    Batch: 16000 --> Loss: 0.0014897532210848478\n",
      "    Batch: 16100 --> Loss: 0.0014883041131227197\n",
      "    Batch: 16200 --> Loss: 0.0014859271506391845\n",
      "    Batch: 16300 --> Loss: 0.0014829728105091443\n",
      "    Batch: 16400 --> Loss: 0.0014816878547518175\n",
      "    Batch: 16500 --> Loss: 0.0014803563944032563\n",
      "    Batch: 16600 --> Loss: 0.0014791675050274632\n",
      "    Batch: 16700 --> Loss: 0.0014886688778305418\n",
      "    Batch: 16800 --> Loss: 0.001486849313759396\n",
      "    Batch: 16900 --> Loss: 0.0014842091082263877\n",
      "    Batch: 17000 --> Loss: 0.001485066063987983\n",
      "    Batch: 17100 --> Loss: 0.0014833896263278363\n",
      "    Batch: 17200 --> Loss: 0.001480593587845278\n",
      "    Batch: 17300 --> Loss: 0.0014962667836577756\n",
      "    Batch: 17400 --> Loss: 0.0014957921341449156\n",
      "    Batch: 17500 --> Loss: 0.0014925524494241744\n",
      "    Batch: 17600 --> Loss: 0.0014905673449353567\n",
      "    Batch: 17700 --> Loss: 0.001485679236978154\n",
      "    Batch: 17800 --> Loss: 0.0014821039688071008\n",
      "    Batch: 17900 --> Loss: 0.001480391261539198\n",
      "    Batch: 18000 --> Loss: 0.0014776163767479306\n",
      "    Batch: 18100 --> Loss: 0.0014762668913950426\n",
      "    Batch: 18200 --> Loss: 0.001483675402774397\n",
      "    Batch: 18300 --> Loss: 0.0014821363884712964\n",
      "    Batch: 18400 --> Loss: 0.0014807126087347487\n",
      "    Batch: 18500 --> Loss: 0.0014815592280840303\n",
      "    Batch: 18600 --> Loss: 0.0014824442683906574\n",
      "    Batch: 18700 --> Loss: 0.0014829833033847895\n",
      "    Batch: 18800 --> Loss: 0.0014943315244695264\n",
      "    Batch: 18900 --> Loss: 0.0014941633287292582\n",
      "    Batch: 19000 --> Loss: 0.0014928233766063068\n",
      "    Batch: 19100 --> Loss: 0.0014917823551756377\n",
      "    Batch: 19200 --> Loss: 0.001492104141596346\n",
      "    Batch: 19300 --> Loss: 0.0014961368857494466\n",
      "    Batch: 19400 --> Loss: 0.0014955783551946216\n",
      "    Batch: 19500 --> Loss: 0.0014926424364482156\n",
      "    Batch: 19600 --> Loss: 0.0014937650947311695\n",
      "    Batch: 19700 --> Loss: 0.001496597797475106\n",
      "    Batch: 19800 --> Loss: 0.0014978063852443347\n",
      "    Batch: 19900 --> Loss: 0.001494323112502996\n",
      "    Batch: 20000 --> Loss: 0.0014995457361221775\n",
      "    Batch: 20100 --> Loss: 0.001499575620439051\n",
      "    Batch: 20200 --> Loss: 0.0014983364872350564\n",
      "    Batch: 20300 --> Loss: 0.0015034464278360763\n",
      "    Batch: 20400 --> Loss: 0.0015050670436493245\n",
      "    Batch: 20500 --> Loss: 0.0015053625970423738\n",
      "    Batch: 20600 --> Loss: 0.0015044601279965266\n",
      "    Batch: 20700 --> Loss: 0.001503048690348514\n",
      "    Batch: 20800 --> Loss: 0.0015036175710481884\n",
      "    Batch: 20900 --> Loss: 0.0015019356484774608\n",
      "    Batch: 21000 --> Loss: 0.0014995415884618772\n",
      "    Batch: 21100 --> Loss: 0.001498262307555052\n",
      "    Batch: 21200 --> Loss: 0.0014973141239082574\n",
      "    Batch: 21300 --> Loss: 0.0014961499810722725\n",
      "    Batch: 21400 --> Loss: 0.0014941921360074517\n",
      "    Batch: 21500 --> Loss: 0.0014911057888431748\n",
      "    Batch: 21600 --> Loss: 0.0014907190236132378\n",
      "    Batch: 21700 --> Loss: 0.0014900924917705905\n",
      "    Batch: 21800 --> Loss: 0.0014914343458654266\n",
      "    Batch: 21900 --> Loss: 0.0014900137385219123\n",
      "    Batch: 22000 --> Loss: 0.0014909623019098322\n",
      "    Batch: 22100 --> Loss: 0.0014925115420682687\n",
      "    Batch: 22200 --> Loss: 0.0014915000594181319\n",
      "    Batch: 22300 --> Loss: 0.0014887623233385446\n",
      "    Batch: 22400 --> Loss: 0.0014900872269284836\n",
      "    Batch: 22500 --> Loss: 0.0014910993949343134\n",
      "    Batch: 22600 --> Loss: 0.00149246726977899\n",
      "    Batch: 22700 --> Loss: 0.0014906876159280513\n",
      "    Batch: 22800 --> Loss: 0.0014916879394238924\n",
      "    Batch: 22900 --> Loss: 0.0014903610861248806\n",
      "    Batch: 23000 --> Loss: 0.0014887211438014737\n",
      "    Batch: 23100 --> Loss: 0.0014865116973960296\n",
      "    Batch: 23200 --> Loss: 0.0014863960680296308\n",
      "    Batch: 23300 --> Loss: 0.0014840656377316866\n",
      "    Batch: 23400 --> Loss: 0.0014833746961125194\n",
      "    Batch: 23500 --> Loss: 0.0014846255947253955\n",
      "    Batch: 23600 --> Loss: 0.0014826369588862645\n",
      "    Batch: 23700 --> Loss: 0.0014821765905363156\n",
      "    Batch: 23800 --> Loss: 0.0014794094668303364\n",
      "    Batch: 23900 --> Loss: 0.001480586090974851\n",
      "    Batch: 24000 --> Loss: 0.0014784881564226379\n",
      "    Batch: 24100 --> Loss: 0.0014787548670466533\n",
      "    Batch: 24200 --> Loss: 0.0014772755345774829\n",
      "    Batch: 24300 --> Loss: 0.001475881185591969\n",
      "    Batch: 24400 --> Loss: 0.0014749304292363981\n",
      "    Batch: 24500 --> Loss: 0.0014712880990760079\n",
      "    Batch: 24600 --> Loss: 0.0014717214245215326\n",
      "    Batch: 24700 --> Loss: 0.0014755655470349207\n",
      "    Batch: 24800 --> Loss: 0.0014789969224183266\n",
      "    Batch: 24900 --> Loss: 0.001477763334961872\n",
      "    Batch: 25000 --> Loss: 0.0014777977585477664\n",
      "    Batch: 25100 --> Loss: 0.0014786269572457753\n",
      "    Batch: 25200 --> Loss: 0.0014814770284779408\n",
      "    Batch: 25300 --> Loss: 0.0014836537165071279\n",
      "    Batch: 25400 --> Loss: 0.0014842372438507872\n",
      "    Batch: 25500 --> Loss: 0.001486095700121165\n",
      "    Batch: 25600 --> Loss: 0.0014852267516598803\n",
      "    Batch: 25700 --> Loss: 0.0014838199141878478\n",
      "    Batch: 25800 --> Loss: 0.0014817365223588172\n",
      "    Batch: 25900 --> Loss: 0.0014810194146283589\n",
      "    Batch: 26000 --> Loss: 0.0014791177542907686\n",
      "    Batch: 26100 --> Loss: 0.0014788186340117407\n",
      "    Batch: 26200 --> Loss: 0.0014759799083784828\n",
      "    Batch: 26300 --> Loss: 0.0014743185581582029\n",
      "    Batch: 26400 --> Loss: 0.0014746003690425116\n",
      "    Batch: 26500 --> Loss: 0.0014768261027556244\n",
      "    Batch: 26600 --> Loss: 0.0014750574864808013\n",
      "    Valid Loss: 3.7221987440716475e-05\n",
      "    Valid Loss: 0.0007446946400579132\n",
      "    Valid Loss: 0.0007408574654869927\n",
      "    Valid Loss: 0.0008558093793233875\n",
      "    Valid Loss: 0.001012636817571878\n",
      "    Valid Loss: 0.0010105603707164699\n",
      "    Valid Loss: 0.0010299658584025026\n",
      "    Valid Loss: 0.0010201953343583576\n",
      "    Valid Loss: 0.0010349257305101822\n",
      "    Valid Loss: 0.0010458652279560984\n",
      "    Valid Loss: 0.0010498260107308882\n",
      "Epoch 6\n",
      "    Batch: 0 --> Loss: 0.00015457606059499085\n",
      "    Batch: 100 --> Loss: 0.001491108809528502\n",
      "    Batch: 200 --> Loss: 0.0013039555816883814\n",
      "    Batch: 300 --> Loss: 0.0011497026612481641\n",
      "    Batch: 400 --> Loss: 0.0010263254654079258\n",
      "    Batch: 500 --> Loss: 0.0009597779698593124\n",
      "    Batch: 600 --> Loss: 0.0010408707203527934\n",
      "    Batch: 700 --> Loss: 0.0010584113613595894\n",
      "    Batch: 800 --> Loss: 0.0011012030370665198\n",
      "    Batch: 900 --> Loss: 0.0011678263095867786\n",
      "    Batch: 1000 --> Loss: 0.0011681960846956442\n",
      "    Batch: 1100 --> Loss: 0.0011645086440644282\n",
      "    Batch: 1200 --> Loss: 0.0012481534042394836\n",
      "    Batch: 1300 --> Loss: 0.0012175660022493472\n",
      "    Batch: 1400 --> Loss: 0.0012290348749139563\n",
      "    Batch: 1500 --> Loss: 0.001232986824801172\n",
      "    Batch: 1600 --> Loss: 0.001255095096875669\n",
      "    Batch: 1700 --> Loss: 0.0013284361366557964\n",
      "    Batch: 1800 --> Loss: 0.001340444070253097\n",
      "    Batch: 1900 --> Loss: 0.0013395611962052389\n",
      "    Batch: 2000 --> Loss: 0.0013390670114711617\n",
      "    Batch: 2100 --> Loss: 0.0013706027368415783\n",
      "    Batch: 2200 --> Loss: 0.0013889943324435095\n",
      "    Batch: 2300 --> Loss: 0.0014795166698894824\n",
      "    Batch: 2400 --> Loss: 0.0014462936376228354\n",
      "    Batch: 2500 --> Loss: 0.0014142702091874417\n",
      "    Batch: 2600 --> Loss: 0.001398771674998824\n",
      "    Batch: 2700 --> Loss: 0.0013839221682950956\n",
      "    Batch: 2800 --> Loss: 0.00138868614632258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 2900 --> Loss: 0.0013738684942077087\n",
      "    Batch: 3000 --> Loss: 0.0013705876425265506\n",
      "    Batch: 3100 --> Loss: 0.0013729445663505455\n",
      "    Batch: 3200 --> Loss: 0.0013582912540909284\n",
      "    Batch: 3300 --> Loss: 0.0013553036287210804\n",
      "    Batch: 3400 --> Loss: 0.00134316650984929\n",
      "    Batch: 3500 --> Loss: 0.0013417072551125822\n",
      "    Batch: 3600 --> Loss: 0.00133077072933804\n",
      "    Batch: 3700 --> Loss: 0.0013201872529458245\n",
      "    Batch: 3800 --> Loss: 0.0013191684149746411\n",
      "    Batch: 3900 --> Loss: 0.0013068976969516665\n",
      "    Batch: 4000 --> Loss: 0.0012944830696731498\n",
      "    Batch: 4100 --> Loss: 0.0012850289470775194\n",
      "    Batch: 4200 --> Loss: 0.0012903160369414403\n",
      "    Batch: 4300 --> Loss: 0.0012917517430287985\n",
      "    Batch: 4400 --> Loss: 0.0012984368045918335\n",
      "    Batch: 4500 --> Loss: 0.0012884887314572933\n",
      "    Batch: 4600 --> Loss: 0.0012918325600871023\n",
      "    Batch: 4700 --> Loss: 0.0012983087602260943\n",
      "    Batch: 4800 --> Loss: 0.0012931215391848623\n",
      "    Batch: 4900 --> Loss: 0.0012909559715187217\n",
      "    Batch: 5000 --> Loss: 0.0012847033235686613\n",
      "    Batch: 5100 --> Loss: 0.0012892615703227717\n",
      "    Batch: 5200 --> Loss: 0.001292972203307974\n",
      "    Batch: 5300 --> Loss: 0.0012938526136162518\n",
      "    Batch: 5400 --> Loss: 0.0012964545466079092\n",
      "    Batch: 5500 --> Loss: 0.0012963244035178174\n",
      "    Batch: 5600 --> Loss: 0.0012984568793355023\n",
      "    Batch: 5700 --> Loss: 0.001314572453400548\n",
      "    Batch: 5800 --> Loss: 0.0013198152096739386\n",
      "    Batch: 5900 --> Loss: 0.0013300543916415603\n",
      "    Batch: 6000 --> Loss: 0.0013316974356629675\n",
      "    Batch: 6100 --> Loss: 0.0013298613393990316\n",
      "    Batch: 6200 --> Loss: 0.001330314109209347\n",
      "    Batch: 6300 --> Loss: 0.00132663670807633\n",
      "    Batch: 6400 --> Loss: 0.0013273351978444672\n",
      "    Batch: 6500 --> Loss: 0.0013250451895265947\n",
      "    Batch: 6600 --> Loss: 0.00131420865001499\n",
      "    Batch: 6700 --> Loss: 0.0013634217868500275\n",
      "    Batch: 6800 --> Loss: 0.0013739512924137967\n",
      "    Batch: 6900 --> Loss: 0.001374327807437473\n",
      "    Batch: 7000 --> Loss: 0.0013700013844225134\n",
      "    Batch: 7100 --> Loss: 0.0013701953865391183\n",
      "    Batch: 7200 --> Loss: 0.0013705484348919602\n",
      "    Batch: 7300 --> Loss: 0.0013684307092730842\n",
      "    Batch: 7400 --> Loss: 0.0013686688752285437\n",
      "    Batch: 7500 --> Loss: 0.0013761349841511716\n",
      "    Batch: 7600 --> Loss: 0.0013690556129155429\n",
      "    Batch: 7700 --> Loss: 0.0013645887763709735\n",
      "    Batch: 7800 --> Loss: 0.0013619317937616077\n",
      "    Batch: 7900 --> Loss: 0.0013798527746942632\n",
      "    Batch: 8000 --> Loss: 0.0013815372344034057\n",
      "    Batch: 8100 --> Loss: 0.001382859768345678\n",
      "    Batch: 8200 --> Loss: 0.0013805411935372952\n",
      "    Batch: 8300 --> Loss: 0.001382306747788877\n",
      "    Batch: 8400 --> Loss: 0.0013799010396236492\n",
      "    Batch: 8500 --> Loss: 0.0013740347771645936\n",
      "    Batch: 8600 --> Loss: 0.0013728733402518008\n",
      "    Batch: 8700 --> Loss: 0.0013669814425059871\n",
      "    Batch: 8800 --> Loss: 0.0013645738643732986\n",
      "    Batch: 8900 --> Loss: 0.001369220785912345\n",
      "    Batch: 9000 --> Loss: 0.0013686744381043443\n",
      "    Batch: 9100 --> Loss: 0.001375694792432576\n",
      "    Batch: 9200 --> Loss: 0.0013742170538195643\n",
      "    Batch: 9300 --> Loss: 0.0013751700329728094\n",
      "    Batch: 9400 --> Loss: 0.0013695094267148523\n",
      "    Batch: 9500 --> Loss: 0.0013632999242594302\n",
      "    Batch: 9600 --> Loss: 0.001359987546837134\n",
      "    Batch: 9700 --> Loss: 0.0013700368723474556\n",
      "    Batch: 9800 --> Loss: 0.001368712702605831\n",
      "    Batch: 9900 --> Loss: 0.0013617356347829256\n",
      "    Batch: 10000 --> Loss: 0.0013704778365218782\n",
      "    Batch: 10100 --> Loss: 0.0013718831524064075\n",
      "    Batch: 10200 --> Loss: 0.0013892359438496854\n",
      "    Batch: 10300 --> Loss: 0.0013908415635695514\n",
      "    Batch: 10400 --> Loss: 0.0013870020760053145\n",
      "    Batch: 10500 --> Loss: 0.0013916630646477611\n",
      "    Batch: 10600 --> Loss: 0.0013979211968396226\n",
      "    Batch: 10700 --> Loss: 0.0014016055377566007\n",
      "    Batch: 10800 --> Loss: 0.00139878281361596\n",
      "    Batch: 10900 --> Loss: 0.0013979609290407364\n",
      "    Batch: 11000 --> Loss: 0.0013991893785894793\n",
      "    Batch: 11100 --> Loss: 0.0014013047618675863\n",
      "    Batch: 11200 --> Loss: 0.0014071683271164464\n",
      "    Batch: 11300 --> Loss: 0.0014081630495774281\n",
      "    Batch: 11400 --> Loss: 0.0014038794042566388\n",
      "    Batch: 11500 --> Loss: 0.0013992339546555035\n",
      "    Batch: 11600 --> Loss: 0.0013953169521187206\n",
      "    Batch: 11700 --> Loss: 0.001391472496124593\n",
      "    Batch: 11800 --> Loss: 0.0013875364011158196\n",
      "    Batch: 11900 --> Loss: 0.001383972713525685\n",
      "    Batch: 12000 --> Loss: 0.0013876369008499372\n",
      "    Batch: 12100 --> Loss: 0.001389714029707819\n",
      "    Batch: 12200 --> Loss: 0.0013894161009223214\n",
      "    Batch: 12300 --> Loss: 0.0013846974527633694\n",
      "    Batch: 12400 --> Loss: 0.001386791805617951\n",
      "    Batch: 12500 --> Loss: 0.0013804044326114053\n",
      "    Batch: 12600 --> Loss: 0.0013829062170124943\n",
      "    Batch: 12700 --> Loss: 0.001382113362344704\n",
      "    Batch: 12800 --> Loss: 0.0013778662655566912\n",
      "    Batch: 12900 --> Loss: 0.0013739945617934786\n",
      "    Batch: 13000 --> Loss: 0.0013712683725961344\n",
      "    Batch: 13100 --> Loss: 0.0013767016176489438\n",
      "    Batch: 13200 --> Loss: 0.0013757141838937235\n",
      "    Batch: 13300 --> Loss: 0.001376620229761106\n",
      "    Batch: 13400 --> Loss: 0.0013873605499289542\n",
      "    Batch: 13500 --> Loss: 0.0013824981303887363\n",
      "    Batch: 13600 --> Loss: 0.0013800265751133683\n",
      "    Batch: 13700 --> Loss: 0.0013750207884103021\n",
      "    Batch: 13800 --> Loss: 0.0013745935061449754\n",
      "    Batch: 13900 --> Loss: 0.0013725713164099871\n",
      "    Batch: 14000 --> Loss: 0.0013713617792975794\n",
      "    Batch: 14100 --> Loss: 0.0013681498989091513\n",
      "    Batch: 14200 --> Loss: 0.001372802663320396\n",
      "    Batch: 14300 --> Loss: 0.0013735751013164875\n",
      "    Batch: 14400 --> Loss: 0.0013752853206710843\n",
      "    Batch: 14500 --> Loss: 0.0013720296235897932\n",
      "    Batch: 14600 --> Loss: 0.001369294183531332\n",
      "    Batch: 14700 --> Loss: 0.0013659266148095823\n",
      "    Batch: 14800 --> Loss: 0.0013617015379794044\n",
      "    Batch: 14900 --> Loss: 0.0013604074573526444\n",
      "    Batch: 15000 --> Loss: 0.0013600554892553931\n",
      "    Batch: 15100 --> Loss: 0.001360485577469093\n",
      "    Batch: 15200 --> Loss: 0.0013568765610367745\n",
      "    Batch: 15300 --> Loss: 0.0013572034029640284\n",
      "    Batch: 15400 --> Loss: 0.0013540235946928989\n",
      "    Batch: 15500 --> Loss: 0.0013513531143772683\n",
      "    Batch: 15600 --> Loss: 0.0013519678437301423\n",
      "    Batch: 15700 --> Loss: 0.0013557437357847256\n",
      "    Batch: 15800 --> Loss: 0.0013544159958578908\n",
      "    Batch: 15900 --> Loss: 0.0013516003154445222\n",
      "    Batch: 16000 --> Loss: 0.0013490406862092918\n",
      "    Batch: 16100 --> Loss: 0.0013463267927606413\n",
      "    Batch: 16200 --> Loss: 0.0013438937904276934\n",
      "    Batch: 16300 --> Loss: 0.0013411342719442652\n",
      "    Batch: 16400 --> Loss: 0.0013394878929805839\n",
      "    Batch: 16500 --> Loss: 0.0013377790812953302\n",
      "    Batch: 16600 --> Loss: 0.0013375890456571735\n",
      "    Batch: 16700 --> Loss: 0.0013339508859303013\n",
      "    Batch: 16800 --> Loss: 0.0013320441351104624\n",
      "    Batch: 16900 --> Loss: 0.0013292186189902658\n",
      "    Batch: 17000 --> Loss: 0.0013305888898984214\n",
      "    Batch: 17100 --> Loss: 0.001332854554240148\n",
      "    Batch: 17200 --> Loss: 0.0013325411885913917\n",
      "    Batch: 17300 --> Loss: 0.0013354715787921747\n",
      "    Batch: 17400 --> Loss: 0.0013341940319234768\n",
      "    Batch: 17500 --> Loss: 0.001331623477885737\n",
      "    Batch: 17600 --> Loss: 0.0013321866919212\n",
      "    Batch: 17700 --> Loss: 0.0013292087721744825\n",
      "    Batch: 17800 --> Loss: 0.001326189385928252\n",
      "    Batch: 17900 --> Loss: 0.0013243417941731245\n",
      "    Batch: 18000 --> Loss: 0.0013248375648876023\n",
      "    Batch: 18100 --> Loss: 0.0013289463085567623\n",
      "    Batch: 18200 --> Loss: 0.0013285232647672878\n",
      "    Batch: 18300 --> Loss: 0.0013273383865019153\n",
      "    Batch: 18400 --> Loss: 0.0013267458156245365\n",
      "    Batch: 18500 --> Loss: 0.001326731744959691\n",
      "    Batch: 18600 --> Loss: 0.0013252403793032877\n",
      "    Batch: 18700 --> Loss: 0.0013236919466284318\n",
      "    Batch: 18800 --> Loss: 0.0013232313819542705\n",
      "    Batch: 18900 --> Loss: 0.0013280961147174144\n",
      "    Batch: 19000 --> Loss: 0.0013281576978861622\n",
      "    Batch: 19100 --> Loss: 0.0013279181422135044\n",
      "    Batch: 19200 --> Loss: 0.00132720293767824\n",
      "    Batch: 19300 --> Loss: 0.0013290204191685495\n",
      "    Batch: 19400 --> Loss: 0.0013265612277853296\n",
      "    Batch: 19500 --> Loss: 0.0013318641478271584\n",
      "    Batch: 19600 --> Loss: 0.0013308995474209048\n",
      "    Batch: 19700 --> Loss: 0.0013322260503563508\n",
      "    Batch: 19800 --> Loss: 0.001335381025396151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 19900 --> Loss: 0.001339034752414447\n",
      "    Batch: 20000 --> Loss: 0.001337922350964244\n",
      "    Batch: 20100 --> Loss: 0.0013359865700562376\n",
      "    Batch: 20200 --> Loss: 0.0013333089568639662\n",
      "    Batch: 20300 --> Loss: 0.0013316631927090036\n",
      "    Batch: 20400 --> Loss: 0.0013302629433836733\n",
      "    Batch: 20500 --> Loss: 0.001329621735349745\n",
      "    Batch: 20600 --> Loss: 0.001328977619988652\n",
      "    Batch: 20700 --> Loss: 0.0013277701791985187\n",
      "    Batch: 20800 --> Loss: 0.0013261054473089\n",
      "    Batch: 20900 --> Loss: 0.001324593463081166\n",
      "    Batch: 21000 --> Loss: 0.001321756822206199\n",
      "    Batch: 21100 --> Loss: 0.0013200134722942294\n",
      "    Batch: 21200 --> Loss: 0.0013189541034223059\n",
      "    Batch: 21300 --> Loss: 0.0013167351546963663\n",
      "    Batch: 21400 --> Loss: 0.0013156821451542828\n",
      "    Batch: 21500 --> Loss: 0.0013140213281666352\n",
      "    Batch: 21600 --> Loss: 0.0013114086416295167\n",
      "    Batch: 21700 --> Loss: 0.0013124613893131824\n",
      "    Batch: 21800 --> Loss: 0.0013109353994743464\n",
      "    Batch: 21900 --> Loss: 0.0013092833384695845\n",
      "    Batch: 22000 --> Loss: 0.0013079132698846324\n",
      "    Batch: 22100 --> Loss: 0.0013064727381617936\n",
      "    Batch: 22200 --> Loss: 0.001312589732460087\n",
      "    Batch: 22300 --> Loss: 0.0013114196364398306\n",
      "    Batch: 22400 --> Loss: 0.0013098003509612814\n",
      "    Batch: 22500 --> Loss: 0.001308286891294521\n",
      "    Batch: 22600 --> Loss: 0.0013080716549552201\n",
      "    Batch: 22700 --> Loss: 0.001305601152566556\n",
      "    Batch: 22800 --> Loss: 0.0013049715827558068\n",
      "    Batch: 22900 --> Loss: 0.0013054965328072557\n",
      "    Batch: 23000 --> Loss: 0.0013047641707018178\n",
      "    Batch: 23100 --> Loss: 0.0013028136948221585\n",
      "    Batch: 23200 --> Loss: 0.0013014232917143867\n",
      "    Batch: 23300 --> Loss: 0.0013029281533570088\n",
      "    Batch: 23400 --> Loss: 0.0013018044254417332\n",
      "    Batch: 23500 --> Loss: 0.001300661000131669\n",
      "    Batch: 23600 --> Loss: 0.001299784910805458\n",
      "    Batch: 23700 --> Loss: 0.0012989766277791198\n",
      "    Batch: 23800 --> Loss: 0.0012983425787185538\n",
      "    Batch: 23900 --> Loss: 0.001299374175857638\n",
      "    Batch: 24000 --> Loss: 0.0012985666422851495\n",
      "    Batch: 24100 --> Loss: 0.0012979225542733104\n",
      "    Batch: 24200 --> Loss: 0.0012971691822303943\n",
      "    Batch: 24300 --> Loss: 0.00131535269390264\n",
      "    Batch: 24400 --> Loss: 0.0013176215647282214\n",
      "    Batch: 24500 --> Loss: 0.0013151879674102814\n",
      "    Batch: 24600 --> Loss: 0.001315415494601111\n",
      "    Batch: 24700 --> Loss: 0.0013134146089026134\n",
      "    Batch: 24800 --> Loss: 0.0013152404250218457\n",
      "    Batch: 24900 --> Loss: 0.0013147482529498075\n",
      "    Batch: 25000 --> Loss: 0.0013174273492310173\n",
      "    Batch: 25100 --> Loss: 0.001314932068223532\n",
      "    Batch: 25200 --> Loss: 0.0013165373020800869\n",
      "    Batch: 25300 --> Loss: 0.0013144401251823062\n",
      "    Batch: 25400 --> Loss: 0.0013170528807452736\n",
      "    Batch: 25500 --> Loss: 0.0013170854213156288\n",
      "    Batch: 25600 --> Loss: 0.0013152269260376939\n",
      "    Batch: 25700 --> Loss: 0.0013163331142488758\n",
      "    Batch: 25800 --> Loss: 0.0013221939785291923\n",
      "    Batch: 25900 --> Loss: 0.0013210940550810381\n",
      "    Batch: 26000 --> Loss: 0.0013211479907237933\n",
      "    Batch: 26100 --> Loss: 0.0013199889576185674\n",
      "    Batch: 26200 --> Loss: 0.001320713659255568\n",
      "    Batch: 26300 --> Loss: 0.0013207587448985803\n",
      "    Batch: 26400 --> Loss: 0.0013189551259691584\n",
      "    Batch: 26500 --> Loss: 0.0013184518146728283\n",
      "    Batch: 26600 --> Loss: 0.0013175800172101275\n",
      "    Valid Loss: 0.0031546037644147873\n",
      "    Valid Loss: 0.0012563462399464089\n",
      "    Valid Loss: 0.002063207776493337\n",
      "    Valid Loss: 0.001877342529117571\n",
      "    Valid Loss: 0.0016303654687901442\n",
      "    Valid Loss: 0.0014835657668197313\n",
      "    Valid Loss: 0.0014396335609564031\n",
      "    Valid Loss: 0.001413345940036499\n",
      "    Valid Loss: 0.0013449500854261916\n",
      "    Valid Loss: 0.0013139379636638753\n",
      "    Valid Loss: 0.0013047862432826985\n",
      "Epoch 7\n",
      "    Batch: 0 --> Loss: 8.797314876574092e-06\n",
      "    Batch: 100 --> Loss: 0.0012260043635859215\n",
      "    Batch: 200 --> Loss: 0.001407795777652521\n",
      "    Batch: 300 --> Loss: 0.0017372786953230664\n",
      "    Batch: 400 --> Loss: 0.0014876224499482602\n",
      "    Batch: 500 --> Loss: 0.0014448147334698903\n",
      "    Batch: 600 --> Loss: 0.001434005696099714\n",
      "    Batch: 700 --> Loss: 0.0013476714342181267\n",
      "    Batch: 800 --> Loss: 0.0013516054715145744\n",
      "    Batch: 900 --> Loss: 0.0013291018592421736\n",
      "    Batch: 1000 --> Loss: 0.001260038555057315\n",
      "    Batch: 1100 --> Loss: 0.0014048788013465885\n",
      "    Batch: 1200 --> Loss: 0.0013522979735080096\n",
      "    Batch: 1300 --> Loss: 0.0013492482665994694\n",
      "    Batch: 1400 --> Loss: 0.0013688570816119373\n",
      "    Batch: 1500 --> Loss: 0.001351863375371256\n",
      "    Batch: 1600 --> Loss: 0.0013650553606797049\n",
      "    Batch: 1700 --> Loss: 0.0013579481925292116\n",
      "    Batch: 1800 --> Loss: 0.00133778765138462\n",
      "    Batch: 1900 --> Loss: 0.0013342003071011297\n",
      "    Batch: 2000 --> Loss: 0.0013088446677994678\n",
      "    Batch: 2100 --> Loss: 0.0013003839092557198\n",
      "    Batch: 2200 --> Loss: 0.0012970349709532192\n",
      "    Batch: 2300 --> Loss: 0.0013494384877432107\n",
      "    Batch: 2400 --> Loss: 0.001362642985997906\n",
      "    Batch: 2500 --> Loss: 0.0013489691049770162\n",
      "    Batch: 2600 --> Loss: 0.0013451269233570732\n",
      "    Batch: 2700 --> Loss: 0.0013249613290180651\n",
      "    Batch: 2800 --> Loss: 0.00130743986743015\n",
      "    Batch: 2900 --> Loss: 0.00132596005174033\n",
      "    Batch: 3000 --> Loss: 0.0013512163290322335\n",
      "    Batch: 3100 --> Loss: 0.0013518929305318369\n",
      "    Batch: 3200 --> Loss: 0.0013567535279755166\n",
      "    Batch: 3300 --> Loss: 0.0013484456180612708\n",
      "    Batch: 3400 --> Loss: 0.0013649452967368961\n",
      "    Batch: 3500 --> Loss: 0.0013940031084440922\n",
      "    Batch: 3600 --> Loss: 0.0013768667736325335\n",
      "    Batch: 3700 --> Loss: 0.0013570264300026282\n",
      "    Batch: 3800 --> Loss: 0.001354697857691638\n",
      "    Batch: 3900 --> Loss: 0.0013399819783022602\n",
      "    Batch: 4000 --> Loss: 0.0013569712383649619\n",
      "    Batch: 4100 --> Loss: 0.0013475568682575062\n",
      "    Batch: 4200 --> Loss: 0.0013364840891297344\n",
      "    Batch: 4300 --> Loss: 0.0013346495164181017\n",
      "    Batch: 4400 --> Loss: 0.0013350186737679896\n",
      "    Batch: 4500 --> Loss: 0.0013242890302044366\n",
      "    Batch: 4600 --> Loss: 0.0013226644544433311\n",
      "    Batch: 4700 --> Loss: 0.0013101364715055294\n",
      "    Batch: 4800 --> Loss: 0.0013306812210397998\n",
      "    Batch: 4900 --> Loss: 0.0013323719421265417\n",
      "    Batch: 5000 --> Loss: 0.0013401714694911476\n",
      "    Batch: 5100 --> Loss: 0.0013383232177335545\n",
      "    Batch: 5200 --> Loss: 0.0013308199163384472\n",
      "    Batch: 5300 --> Loss: 0.001321023431182168\n",
      "    Batch: 5400 --> Loss: 0.0013313727593526534\n",
      "    Batch: 5500 --> Loss: 0.0013321936931112112\n",
      "    Batch: 5600 --> Loss: 0.001322503105989098\n",
      "    Batch: 5700 --> Loss: 0.0013303212261114616\n",
      "    Batch: 5800 --> Loss: 0.001331797433862824\n",
      "    Batch: 5900 --> Loss: 0.0013306299606264066\n",
      "    Batch: 6000 --> Loss: 0.0013327346962182319\n",
      "    Batch: 6100 --> Loss: 0.0013263010246783154\n",
      "    Batch: 6200 --> Loss: 0.0013230156708617235\n",
      "    Batch: 6300 --> Loss: 0.0013213133671112721\n",
      "    Batch: 6400 --> Loss: 0.001317405464946652\n",
      "    Batch: 6500 --> Loss: 0.0013118242180353258\n",
      "    Batch: 6600 --> Loss: 0.0013083572440818567\n",
      "    Batch: 6700 --> Loss: 0.0013094654844511353\n",
      "    Batch: 6800 --> Loss: 0.0013137651469549451\n",
      "    Batch: 6900 --> Loss: 0.0013080619329637686\n",
      "    Batch: 7000 --> Loss: 0.0013122801083185556\n",
      "    Batch: 7100 --> Loss: 0.0013161892040995723\n",
      "    Batch: 7200 --> Loss: 0.0013113228873700286\n",
      "    Batch: 7300 --> Loss: 0.0013163820374353218\n",
      "    Batch: 7400 --> Loss: 0.0013184937693876727\n",
      "    Batch: 7500 --> Loss: 0.0013169876584061498\n",
      "    Batch: 7600 --> Loss: 0.0013161643419578838\n",
      "    Batch: 7700 --> Loss: 0.0013146285973534904\n",
      "    Batch: 7800 --> Loss: 0.0013205252750753775\n",
      "    Batch: 7900 --> Loss: 0.0013133021962267472\n",
      "    Batch: 8000 --> Loss: 0.0013091843498560882\n",
      "    Batch: 8100 --> Loss: 0.0013066649528881574\n",
      "    Batch: 8200 --> Loss: 0.0013218272526192118\n",
      "    Batch: 8300 --> Loss: 0.0013207978663100453\n",
      "    Batch: 8400 --> Loss: 0.0013210656281463257\n",
      "    Batch: 8500 --> Loss: 0.0013307837783253494\n",
      "    Batch: 8600 --> Loss: 0.0013261771711670616\n",
      "    Batch: 8700 --> Loss: 0.001320486984951761\n",
      "    Batch: 8800 --> Loss: 0.0013168946604437293\n",
      "    Batch: 8900 --> Loss: 0.001312825708810107\n",
      "    Batch: 9000 --> Loss: 0.0013123933414077441\n",
      "    Batch: 9100 --> Loss: 0.0013088571700656895\n",
      "    Batch: 9200 --> Loss: 0.0013097634981534836\n",
      "    Batch: 9300 --> Loss: 0.0013087420568328853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 9400 --> Loss: 0.0013045194434225678\n",
      "    Batch: 9500 --> Loss: 0.0013033235857774002\n",
      "    Batch: 9600 --> Loss: 0.0013103356869156092\n",
      "    Batch: 9700 --> Loss: 0.0013144843731809944\n",
      "    Batch: 9800 --> Loss: 0.0013281916540405525\n",
      "    Batch: 9900 --> Loss: 0.0013278472495374953\n",
      "    Batch: 10000 --> Loss: 0.0013241411503351849\n",
      "    Batch: 10100 --> Loss: 0.0013323378244992636\n",
      "    Batch: 10200 --> Loss: 0.001340096539233728\n",
      "    Batch: 10300 --> Loss: 0.0013417215978122157\n",
      "    Batch: 10400 --> Loss: 0.0013381216547907837\n",
      "    Batch: 10500 --> Loss: 0.0013362764136107578\n",
      "    Batch: 10600 --> Loss: 0.0013310545401469392\n",
      "    Batch: 10700 --> Loss: 0.001328015892301901\n",
      "    Batch: 10800 --> Loss: 0.0013250378051410747\n",
      "    Batch: 10900 --> Loss: 0.0013258970067847211\n",
      "    Batch: 11000 --> Loss: 0.0013238970790660019\n",
      "    Batch: 11100 --> Loss: 0.0013217549574513921\n",
      "    Batch: 11200 --> Loss: 0.0013196757287612614\n",
      "    Batch: 11300 --> Loss: 0.0013155154139376831\n",
      "    Batch: 11400 --> Loss: 0.0013123236394375752\n",
      "    Batch: 11500 --> Loss: 0.0013084467971650336\n",
      "    Batch: 11600 --> Loss: 0.0013076033761230776\n",
      "    Batch: 11700 --> Loss: 0.0013046673407048068\n",
      "    Batch: 11800 --> Loss: 0.0013063366298768705\n",
      "    Batch: 11900 --> Loss: 0.0013119626802632982\n",
      "    Batch: 12000 --> Loss: 0.0013112696572969067\n",
      "    Batch: 12100 --> Loss: 0.001307454537494249\n",
      "    Batch: 12200 --> Loss: 0.0013062173263624678\n",
      "    Batch: 12300 --> Loss: 0.001316526580786157\n",
      "    Batch: 12400 --> Loss: 0.0013118297559140126\n",
      "    Batch: 12500 --> Loss: 0.001312271815555686\n",
      "    Batch: 12600 --> Loss: 0.001310104736530731\n",
      "    Batch: 12700 --> Loss: 0.001312021357506325\n",
      "    Batch: 12800 --> Loss: 0.0013087464158391818\n",
      "    Batch: 12900 --> Loss: 0.0013059328189074183\n",
      "    Batch: 13000 --> Loss: 0.001305230751162048\n",
      "    Batch: 13100 --> Loss: 0.0013142543777006997\n",
      "    Batch: 13200 --> Loss: 0.0013137279060895129\n",
      "    Batch: 13300 --> Loss: 0.0013129997348181347\n",
      "    Batch: 13400 --> Loss: 0.0013081891191747707\n",
      "    Batch: 13500 --> Loss: 0.0013042867195477157\n",
      "    Batch: 13600 --> Loss: 0.0013055434510010663\n",
      "    Batch: 13700 --> Loss: 0.0013103327311989905\n",
      "    Batch: 13800 --> Loss: 0.0013078654599362927\n",
      "    Batch: 13900 --> Loss: 0.0013056017838701307\n",
      "    Batch: 14000 --> Loss: 0.001303656775129482\n",
      "    Batch: 14100 --> Loss: 0.0013016257207408189\n",
      "    Batch: 14200 --> Loss: 0.0013020439966472154\n",
      "    Batch: 14300 --> Loss: 0.0012982687716223942\n",
      "    Batch: 14400 --> Loss: 0.001294260926580172\n",
      "    Batch: 14500 --> Loss: 0.0012937546095690131\n",
      "    Batch: 14600 --> Loss: 0.001296951660495477\n",
      "    Batch: 14700 --> Loss: 0.001295448430054988\n",
      "    Batch: 14800 --> Loss: 0.0012971985631699187\n",
      "    Batch: 14900 --> Loss: 0.001302060643733731\n",
      "    Batch: 15000 --> Loss: 0.0013060797845682447\n",
      "    Batch: 15100 --> Loss: 0.0013060295899532865\n",
      "    Batch: 15200 --> Loss: 0.0013053112081087381\n",
      "    Batch: 15300 --> Loss: 0.0013116895837822745\n",
      "    Batch: 15400 --> Loss: 0.0013207500364658\n",
      "    Batch: 15500 --> Loss: 0.0013338592973825751\n",
      "    Batch: 15600 --> Loss: 0.001332860415237488\n",
      "    Batch: 15700 --> Loss: 0.0013303196775238735\n",
      "    Batch: 15800 --> Loss: 0.001329122695306576\n",
      "    Batch: 15900 --> Loss: 0.0013249642826897706\n",
      "    Batch: 16000 --> Loss: 0.0013306453119470486\n",
      "    Batch: 16100 --> Loss: 0.001328949921326157\n",
      "    Batch: 16200 --> Loss: 0.0013280417363421627\n",
      "    Batch: 16300 --> Loss: 0.0013247439524582002\n",
      "    Batch: 16400 --> Loss: 0.001323621552190608\n",
      "    Batch: 16500 --> Loss: 0.0013260209811039598\n",
      "    Batch: 16600 --> Loss: 0.0013255741289003326\n",
      "    Batch: 16700 --> Loss: 0.0013266688115374612\n",
      "    Batch: 16800 --> Loss: 0.0013291462766054532\n",
      "    Batch: 16900 --> Loss: 0.001325760710941265\n",
      "    Batch: 17000 --> Loss: 0.0013280354265586749\n",
      "    Batch: 17100 --> Loss: 0.001328144089477356\n",
      "    Batch: 17200 --> Loss: 0.001327239029735607\n",
      "    Batch: 17300 --> Loss: 0.0013273157141430862\n",
      "    Batch: 17400 --> Loss: 0.0013265857005042175\n",
      "    Batch: 17500 --> Loss: 0.0013283501339556732\n",
      "    Batch: 17600 --> Loss: 0.0013291574273003063\n",
      "    Batch: 17700 --> Loss: 0.0013279845745432972\n",
      "    Batch: 17800 --> Loss: 0.0013304913984335974\n",
      "    Batch: 17900 --> Loss: 0.0013306938960321957\n",
      "    Batch: 18000 --> Loss: 0.0013322337438526146\n",
      "    Batch: 18100 --> Loss: 0.001332172134920508\n",
      "    Batch: 18200 --> Loss: 0.0013403003826753762\n",
      "    Batch: 18300 --> Loss: 0.0013385218809978448\n",
      "    Batch: 18400 --> Loss: 0.0013418309508510962\n",
      "    Batch: 18500 --> Loss: 0.0013385070268403807\n",
      "    Batch: 18600 --> Loss: 0.001349799198577614\n",
      "    Batch: 18700 --> Loss: 0.0013480676990554235\n",
      "    Batch: 18800 --> Loss: 0.001348396751630836\n",
      "    Batch: 18900 --> Loss: 0.0013457010788457996\n",
      "    Batch: 19000 --> Loss: 0.0013478285837277256\n",
      "    Batch: 19100 --> Loss: 0.0013456548268137157\n",
      "    Batch: 19200 --> Loss: 0.0013494792514118436\n",
      "    Batch: 19300 --> Loss: 0.0013472061212973685\n",
      "    Batch: 19400 --> Loss: 0.0013449041217403522\n",
      "    Batch: 19500 --> Loss: 0.0013428823545262693\n",
      "    Batch: 19600 --> Loss: 0.0013415245712398301\n",
      "    Batch: 19700 --> Loss: 0.0013413254565827815\n",
      "    Batch: 19800 --> Loss: 0.0013380699463171563\n",
      "    Batch: 19900 --> Loss: 0.0013359502146992738\n",
      "    Batch: 20000 --> Loss: 0.001343490778624948\n",
      "    Batch: 20100 --> Loss: 0.001342549633975889\n",
      "    Batch: 20200 --> Loss: 0.0013474904927761834\n",
      "    Batch: 20300 --> Loss: 0.0013472026075610224\n",
      "    Batch: 20400 --> Loss: 0.0013461424534971017\n",
      "    Batch: 20500 --> Loss: 0.001351114067206554\n",
      "    Batch: 20600 --> Loss: 0.0013492517535064813\n",
      "    Batch: 20700 --> Loss: 0.0013479310898827036\n",
      "    Batch: 20800 --> Loss: 0.0013491583607721147\n",
      "    Batch: 20900 --> Loss: 0.0013522618432056561\n",
      "    Batch: 21000 --> Loss: 0.0013499247857892659\n",
      "    Batch: 21100 --> Loss: 0.0013504933747438344\n",
      "    Batch: 21200 --> Loss: 0.001349095037398581\n",
      "    Batch: 21300 --> Loss: 0.0013546676900957242\n",
      "    Batch: 21400 --> Loss: 0.0013544072639885298\n",
      "    Batch: 21500 --> Loss: 0.0013533231613231669\n",
      "    Batch: 21600 --> Loss: 0.0013549101610345572\n",
      "    Batch: 21700 --> Loss: 0.0013527874733236227\n",
      "    Batch: 21800 --> Loss: 0.0013522460085436297\n",
      "    Batch: 21900 --> Loss: 0.0013516618733182806\n",
      "    Batch: 22000 --> Loss: 0.0013615956771781553\n",
      "    Batch: 22100 --> Loss: 0.0013612226107660036\n",
      "    Batch: 22200 --> Loss: 0.001364780851579773\n",
      "    Batch: 22300 --> Loss: 0.001363618147112703\n",
      "    Batch: 22400 --> Loss: 0.001361355975316692\n",
      "    Batch: 22500 --> Loss: 0.0013629170684428566\n",
      "    Batch: 22600 --> Loss: 0.0013651026191912993\n",
      "    Batch: 22700 --> Loss: 0.0013688876064773257\n",
      "    Batch: 22800 --> Loss: 0.001367791039342976\n",
      "    Batch: 22900 --> Loss: 0.0013681982953272478\n",
      "    Batch: 23000 --> Loss: 0.001366216289445716\n",
      "    Batch: 23100 --> Loss: 0.001363996988134461\n",
      "    Batch: 23200 --> Loss: 0.001362592925651734\n",
      "    Batch: 23300 --> Loss: 0.0013602641798769017\n",
      "    Batch: 23400 --> Loss: 0.0013608845803366357\n",
      "    Batch: 23500 --> Loss: 0.0013597668492334018\n",
      "    Batch: 23600 --> Loss: 0.0013583120463496014\n",
      "    Batch: 23700 --> Loss: 0.0013576950313927791\n",
      "    Batch: 23800 --> Loss: 0.0013559767118842998\n",
      "    Batch: 23900 --> Loss: 0.0013564553552556093\n",
      "    Batch: 24000 --> Loss: 0.001355048393312084\n",
      "    Batch: 24100 --> Loss: 0.0013567590494506056\n",
      "    Batch: 24200 --> Loss: 0.00135462958976424\n",
      "    Batch: 24300 --> Loss: 0.0013525125763573192\n",
      "    Batch: 24400 --> Loss: 0.0013552936164652769\n",
      "    Batch: 24500 --> Loss: 0.0013543200625650024\n",
      "    Batch: 24600 --> Loss: 0.0013524079045343941\n",
      "    Batch: 24700 --> Loss: 0.001351117068304309\n",
      "    Batch: 24800 --> Loss: 0.0013582574635307976\n",
      "    Batch: 24900 --> Loss: 0.0013557042890836974\n",
      "    Batch: 25000 --> Loss: 0.0013535883338672385\n",
      "    Batch: 25100 --> Loss: 0.0013535594708611479\n",
      "    Batch: 25200 --> Loss: 0.0013536199295788869\n",
      "    Batch: 25300 --> Loss: 0.0013512876041484\n",
      "    Batch: 25400 --> Loss: 0.0013485920540416518\n",
      "    Batch: 25500 --> Loss: 0.0013501282148390165\n",
      "    Batch: 25600 --> Loss: 0.0013507280296166304\n",
      "    Batch: 25700 --> Loss: 0.0013499333265100868\n",
      "    Batch: 25800 --> Loss: 0.001350137824903149\n",
      "    Batch: 25900 --> Loss: 0.001350031159121407\n",
      "    Batch: 26000 --> Loss: 0.0013511067032334428\n",
      "    Batch: 26100 --> Loss: 0.001350734749799344\n",
      "    Batch: 26200 --> Loss: 0.0013498746305721536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 26300 --> Loss: 0.0013497356644668896\n",
      "    Batch: 26400 --> Loss: 0.001349266429810431\n",
      "    Batch: 26500 --> Loss: 0.0013483821660530805\n",
      "    Batch: 26600 --> Loss: 0.001345947065150621\n",
      "    Valid Loss: 9.926814527716488e-06\n",
      "         Saving.... Best epoch: 7 -> 9.926814527716488e-06\n",
      "    Valid Loss: 0.0011056160786843447\n",
      "    Valid Loss: 0.0011559847873265784\n",
      "    Valid Loss: 0.001220055002132364\n",
      "    Valid Loss: 0.0013706424175542633\n",
      "    Valid Loss: 0.001346613454636462\n",
      "    Valid Loss: 0.0014283347998663117\n",
      "    Valid Loss: 0.00132251021437916\n",
      "    Valid Loss: 0.0012386907303553348\n",
      "    Valid Loss: 0.0012414004706584492\n",
      "    Valid Loss: 0.00121107847858223\n",
      "Epoch 8\n",
      "    Batch: 0 --> Loss: 1.6842965123942122e-05\n",
      "    Batch: 100 --> Loss: 0.0009771013714561657\n",
      "    Batch: 200 --> Loss: 0.0012312616616141348\n",
      "    Batch: 300 --> Loss: 0.0012874185528620537\n",
      "    Batch: 400 --> Loss: 0.0012435524863328748\n",
      "    Batch: 500 --> Loss: 0.0012284147283051327\n",
      "    Batch: 600 --> Loss: 0.0011806435823472012\n",
      "    Batch: 700 --> Loss: 0.0011447060072789419\n",
      "    Batch: 800 --> Loss: 0.0011015704068864434\n",
      "    Batch: 900 --> Loss: 0.0011283063596836196\n",
      "    Batch: 1000 --> Loss: 0.0011518176016084407\n",
      "    Batch: 1100 --> Loss: 0.001252814093552687\n",
      "    Batch: 1200 --> Loss: 0.0012500039080832095\n",
      "    Batch: 1300 --> Loss: 0.0012558306342736678\n",
      "    Batch: 1400 --> Loss: 0.0012497278588223368\n",
      "    Batch: 1500 --> Loss: 0.0012316995160002709\n",
      "    Batch: 1600 --> Loss: 0.0012262121804197874\n",
      "    Batch: 1700 --> Loss: 0.0012007014836648486\n",
      "    Batch: 1800 --> Loss: 0.0012140577530911174\n",
      "    Batch: 1900 --> Loss: 0.0012364734558335557\n",
      "    Batch: 2000 --> Loss: 0.0012747739945901128\n",
      "    Batch: 2100 --> Loss: 0.0012481859639247124\n",
      "    Batch: 2200 --> Loss: 0.001237761751254482\n",
      "    Batch: 2300 --> Loss: 0.0012223822071322642\n",
      "    Batch: 2400 --> Loss: 0.0012109934403879681\n",
      "    Batch: 2500 --> Loss: 0.001197548857110686\n",
      "    Batch: 2600 --> Loss: 0.0011861500032855751\n",
      "    Batch: 2700 --> Loss: 0.0011810100162818196\n",
      "    Batch: 2800 --> Loss: 0.0011941989640336607\n",
      "    Batch: 2900 --> Loss: 0.001197563031438814\n",
      "    Batch: 3000 --> Loss: 0.0012065705086066506\n",
      "    Batch: 3100 --> Loss: 0.0012101340715172455\n",
      "    Batch: 3200 --> Loss: 0.0012035600839920176\n",
      "    Batch: 3300 --> Loss: 0.001198520623864848\n",
      "    Batch: 3400 --> Loss: 0.0011970011672131188\n",
      "    Batch: 3500 --> Loss: 0.001201295969739709\n",
      "    Batch: 3600 --> Loss: 0.0012106600454199958\n",
      "    Batch: 3700 --> Loss: 0.0012110050579835897\n",
      "    Batch: 3800 --> Loss: 0.0012210553881554461\n",
      "    Batch: 3900 --> Loss: 0.001236986482313706\n",
      "    Batch: 4000 --> Loss: 0.0012302868186388157\n",
      "    Batch: 4100 --> Loss: 0.0012360799638243764\n",
      "    Batch: 4200 --> Loss: 0.0012276766164567741\n",
      "    Batch: 4300 --> Loss: 0.0012235704729626564\n",
      "    Batch: 4400 --> Loss: 0.001212176158510711\n",
      "    Batch: 4500 --> Loss: 0.0012186656923406207\n",
      "    Batch: 4600 --> Loss: 0.0012149459129847545\n",
      "    Batch: 4700 --> Loss: 0.0012157063618033772\n",
      "    Batch: 4800 --> Loss: 0.0012146438300602391\n",
      "    Batch: 4900 --> Loss: 0.0012123439273338896\n",
      "    Batch: 5000 --> Loss: 0.0011989875079624378\n",
      "    Batch: 5100 --> Loss: 0.0012009645944503816\n",
      "    Batch: 5200 --> Loss: 0.001199126916748681\n",
      "    Batch: 5300 --> Loss: 0.0012038668810474083\n",
      "    Batch: 5400 --> Loss: 0.0012133802308079715\n",
      "    Batch: 5500 --> Loss: 0.0012205860007867423\n",
      "    Batch: 5600 --> Loss: 0.00122057077245443\n",
      "    Batch: 5700 --> Loss: 0.001224955033925893\n",
      "    Batch: 5800 --> Loss: 0.00122825961742528\n",
      "    Batch: 5900 --> Loss: 0.0012309312359540666\n",
      "    Batch: 6000 --> Loss: 0.0012307846344405658\n",
      "    Batch: 6100 --> Loss: 0.001228154161042668\n",
      "    Batch: 6200 --> Loss: 0.0012312832007027896\n",
      "    Batch: 6300 --> Loss: 0.0012365435766453737\n",
      "    Batch: 6400 --> Loss: 0.0012323264734296136\n",
      "    Batch: 6500 --> Loss: 0.0012265724219681536\n",
      "    Batch: 6600 --> Loss: 0.001217479377325629\n",
      "    Batch: 6700 --> Loss: 0.0012162379673191187\n",
      "    Batch: 6800 --> Loss: 0.001222891905275315\n",
      "    Batch: 6900 --> Loss: 0.0012267022586190438\n",
      "    Batch: 7000 --> Loss: 0.001219993187125246\n",
      "    Batch: 7100 --> Loss: 0.0012200192271731778\n",
      "    Batch: 7200 --> Loss: 0.001232881819133737\n",
      "    Batch: 7300 --> Loss: 0.0012284578722155232\n",
      "    Batch: 7400 --> Loss: 0.0012337868793901598\n",
      "    Batch: 7500 --> Loss: 0.0012323423415097007\n",
      "    Batch: 7600 --> Loss: 0.0012269506666053823\n",
      "    Batch: 7700 --> Loss: 0.0012301606729042233\n",
      "    Batch: 7800 --> Loss: 0.0012325139440501787\n",
      "    Batch: 7900 --> Loss: 0.0012244560114294096\n",
      "    Batch: 8000 --> Loss: 0.0012242944241259608\n",
      "    Batch: 8100 --> Loss: 0.0012306268299314453\n",
      "    Batch: 8200 --> Loss: 0.001234029324959858\n",
      "    Batch: 8300 --> Loss: 0.0012328172034478545\n",
      "    Batch: 8400 --> Loss: 0.0012336651865999075\n",
      "    Batch: 8500 --> Loss: 0.0012363131243272177\n",
      "    Batch: 8600 --> Loss: 0.0012336689301121896\n",
      "    Batch: 8700 --> Loss: 0.0012331143093371024\n",
      "    Batch: 8800 --> Loss: 0.001233746047286556\n",
      "    Batch: 8900 --> Loss: 0.0012307707056267842\n",
      "    Batch: 9000 --> Loss: 0.0012333115429135266\n",
      "    Batch: 9100 --> Loss: 0.0012320532644481198\n",
      "    Batch: 9200 --> Loss: 0.0012294960667310407\n",
      "    Batch: 9300 --> Loss: 0.0012271846287996923\n",
      "    Batch: 9400 --> Loss: 0.0012303789676824774\n",
      "    Batch: 9500 --> Loss: 0.0012298158002960918\n",
      "    Batch: 9600 --> Loss: 0.0012299829175419095\n",
      "    Batch: 9700 --> Loss: 0.001230769110879756\n",
      "    Batch: 9800 --> Loss: 0.0012266530059304945\n",
      "    Batch: 9900 --> Loss: 0.0012264912033074426\n",
      "    Batch: 10000 --> Loss: 0.001221833786693511\n",
      "    Batch: 10100 --> Loss: 0.0012233412797670023\n",
      "    Batch: 10200 --> Loss: 0.0012228710428572366\n",
      "    Batch: 10300 --> Loss: 0.0012252620068721925\n",
      "    Batch: 10400 --> Loss: 0.0012300868985472726\n",
      "    Batch: 10500 --> Loss: 0.0012323190639378482\n",
      "    Batch: 10600 --> Loss: 0.0012286544665668986\n",
      "    Batch: 10700 --> Loss: 0.0012240571230123433\n",
      "    Batch: 10800 --> Loss: 0.0012237089284810123\n",
      "    Batch: 10900 --> Loss: 0.00122665087258235\n",
      "    Batch: 11000 --> Loss: 0.0012366791975079057\n",
      "    Batch: 11100 --> Loss: 0.0012399194274888923\n",
      "    Batch: 11200 --> Loss: 0.0012421126297994588\n",
      "    Batch: 11300 --> Loss: 0.0012398118073323893\n",
      "    Batch: 11400 --> Loss: 0.0012382934814025343\n",
      "    Batch: 11500 --> Loss: 0.0012349286338533896\n",
      "    Batch: 11600 --> Loss: 0.0012308708914495954\n",
      "    Batch: 11700 --> Loss: 0.001231655447280457\n",
      "    Batch: 11800 --> Loss: 0.001232244375725212\n",
      "    Batch: 11900 --> Loss: 0.0012336183664027032\n",
      "    Batch: 12000 --> Loss: 0.0012342610971249422\n",
      "    Batch: 12100 --> Loss: 0.0012328685673065657\n",
      "    Batch: 12200 --> Loss: 0.001235090564767225\n",
      "    Batch: 12300 --> Loss: 0.0012366953715058418\n",
      "    Batch: 12400 --> Loss: 0.0012483825515080104\n",
      "    Batch: 12500 --> Loss: 0.0012469553860678542\n",
      "    Batch: 12600 --> Loss: 0.0012440481722866556\n",
      "    Batch: 12700 --> Loss: 0.0012485786929409717\n",
      "    Batch: 12800 --> Loss: 0.0012454637232874215\n",
      "    Batch: 12900 --> Loss: 0.00125106648596881\n",
      "    Batch: 13000 --> Loss: 0.00124949510433838\n",
      "    Batch: 13100 --> Loss: 0.0012550599561788312\n",
      "    Batch: 13200 --> Loss: 0.0012559407774586635\n",
      "    Batch: 13300 --> Loss: 0.0012517732160691353\n",
      "    Batch: 13400 --> Loss: 0.001248365492521524\n",
      "    Batch: 13500 --> Loss: 0.0012455167300313252\n",
      "    Batch: 13600 --> Loss: 0.0012618662282488796\n",
      "    Batch: 13700 --> Loss: 0.0012615235180158822\n",
      "    Batch: 13800 --> Loss: 0.0012571181799366292\n",
      "    Batch: 13900 --> Loss: 0.001258125164351125\n",
      "    Batch: 14000 --> Loss: 0.0012587427516261235\n",
      "    Batch: 14100 --> Loss: 0.001255922701817375\n",
      "    Batch: 14200 --> Loss: 0.0012571355720185544\n",
      "    Batch: 14300 --> Loss: 0.0012575407571181663\n",
      "    Batch: 14400 --> Loss: 0.0012567792854183578\n",
      "    Batch: 14500 --> Loss: 0.0012535873999468925\n",
      "    Batch: 14600 --> Loss: 0.0012623337653642566\n",
      "    Batch: 14700 --> Loss: 0.0012677648268970016\n",
      "    Batch: 14800 --> Loss: 0.0012653894877335096\n",
      "    Batch: 14900 --> Loss: 0.001261132239475962\n",
      "    Batch: 15000 --> Loss: 0.001258850858243213\n",
      "    Batch: 15100 --> Loss: 0.0012596261502834897\n",
      "    Batch: 15200 --> Loss: 0.0012725056441215087\n",
      "    Batch: 15300 --> Loss: 0.0012702683519084407\n",
      "    Batch: 15400 --> Loss: 0.001270508212434234\n",
      "    Batch: 15500 --> Loss: 0.0012686555855885447\n",
      "    Batch: 15600 --> Loss: 0.0012689045304148212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 15700 --> Loss: 0.0012692399272459071\n",
      "    Batch: 15800 --> Loss: 0.0012676235845910655\n",
      "    Batch: 15900 --> Loss: 0.0012642155910053193\n",
      "    Batch: 16000 --> Loss: 0.001265615983771232\n",
      "    Batch: 16100 --> Loss: 0.0012629262163657052\n",
      "    Batch: 16200 --> Loss: 0.0012604190383442558\n",
      "    Batch: 16300 --> Loss: 0.0012598028621380432\n",
      "    Batch: 16400 --> Loss: 0.001259419304387819\n",
      "    Batch: 16500 --> Loss: 0.0012582042564619213\n",
      "    Batch: 16600 --> Loss: 0.0012546761190093507\n",
      "    Batch: 16700 --> Loss: 0.001255278165043036\n",
      "    Batch: 16800 --> Loss: 0.001256549677325866\n",
      "    Batch: 16900 --> Loss: 0.0012652531106546324\n",
      "    Batch: 17000 --> Loss: 0.001262065236948635\n",
      "    Batch: 17100 --> Loss: 0.0012613241799268044\n",
      "    Batch: 17200 --> Loss: 0.0012622267325065582\n",
      "    Batch: 17300 --> Loss: 0.0012591521739046567\n",
      "    Batch: 17400 --> Loss: 0.0012632396160326574\n",
      "    Batch: 17500 --> Loss: 0.001265632351055623\n",
      "    Batch: 17600 --> Loss: 0.0012659257472991952\n",
      "    Batch: 17700 --> Loss: 0.0012654271744011857\n",
      "    Batch: 17800 --> Loss: 0.001265295358960088\n",
      "    Batch: 17900 --> Loss: 0.00126192746753564\n",
      "    Batch: 18000 --> Loss: 0.0012612034815923658\n",
      "    Batch: 18100 --> Loss: 0.0012624134874043025\n",
      "    Batch: 18200 --> Loss: 0.0012620664587979769\n",
      "    Batch: 18300 --> Loss: 0.0012688233537147793\n",
      "    Batch: 18400 --> Loss: 0.0012668500931082218\n",
      "    Batch: 18500 --> Loss: 0.0012651812433573614\n",
      "    Batch: 18600 --> Loss: 0.0012633086796630796\n",
      "    Batch: 18700 --> Loss: 0.001266369435419878\n",
      "    Batch: 18800 --> Loss: 0.001263162037983036\n",
      "    Batch: 18900 --> Loss: 0.001265914758393788\n",
      "    Batch: 19000 --> Loss: 0.0012676594124091227\n",
      "    Batch: 19100 --> Loss: 0.0012682911088677902\n",
      "    Batch: 19200 --> Loss: 0.001267571476732508\n",
      "    Batch: 19300 --> Loss: 0.0012657660254706319\n",
      "    Batch: 19400 --> Loss: 0.0012653177817067026\n",
      "    Batch: 19500 --> Loss: 0.0012629900964721533\n",
      "    Batch: 19600 --> Loss: 0.0012605972298196583\n",
      "    Batch: 19700 --> Loss: 0.0012625275575750257\n",
      "    Batch: 19800 --> Loss: 0.0012621198894180972\n",
      "    Batch: 19900 --> Loss: 0.0012617783188221996\n",
      "    Batch: 20000 --> Loss: 0.0012600722637630842\n",
      "    Batch: 20100 --> Loss: 0.0012586704824066254\n",
      "    Batch: 20200 --> Loss: 0.0012597156628215203\n",
      "    Batch: 20300 --> Loss: 0.0012572077837729762\n",
      "    Batch: 20400 --> Loss: 0.0012559045421347461\n",
      "    Batch: 20500 --> Loss: 0.0012595073746467663\n",
      "    Batch: 20600 --> Loss: 0.0012603491873749545\n",
      "    Batch: 20700 --> Loss: 0.0012636122881010505\n",
      "    Batch: 20800 --> Loss: 0.0012628163195137793\n",
      "    Batch: 20900 --> Loss: 0.0012634617185801095\n",
      "    Batch: 21000 --> Loss: 0.0012616821846366286\n",
      "    Batch: 21100 --> Loss: 0.0012591697608824622\n",
      "    Batch: 21200 --> Loss: 0.0012567245303250335\n",
      "    Batch: 21300 --> Loss: 0.0012566677618146243\n",
      "    Batch: 21400 --> Loss: 0.0012557785153048523\n",
      "    Batch: 21500 --> Loss: 0.0012570755599198378\n",
      "    Batch: 21600 --> Loss: 0.0012547190562225044\n",
      "    Batch: 21700 --> Loss: 0.001253033708572239\n",
      "    Batch: 21800 --> Loss: 0.0012530669343099284\n",
      "    Batch: 21900 --> Loss: 0.0012525455425970076\n",
      "    Batch: 22000 --> Loss: 0.00125445189501872\n",
      "    Batch: 22100 --> Loss: 0.0012534743284547353\n",
      "    Batch: 22200 --> Loss: 0.0012612366320010819\n",
      "    Batch: 22300 --> Loss: 0.0012612583228936544\n",
      "    Batch: 22400 --> Loss: 0.0012605312632177458\n",
      "    Batch: 22500 --> Loss: 0.001261554706582305\n",
      "    Batch: 22600 --> Loss: 0.0012612754424695788\n",
      "    Batch: 22700 --> Loss: 0.0012634013225853603\n",
      "    Batch: 22800 --> Loss: 0.0012623023225685525\n",
      "    Batch: 22900 --> Loss: 0.0012629527772310986\n",
      "    Batch: 23000 --> Loss: 0.0012668130817475793\n",
      "    Batch: 23100 --> Loss: 0.0012662017475873243\n",
      "    Batch: 23200 --> Loss: 0.0012636290180879932\n",
      "    Batch: 23300 --> Loss: 0.0012617554453500889\n",
      "    Batch: 23400 --> Loss: 0.0012662484685323347\n",
      "    Batch: 23500 --> Loss: 0.0012656329802023802\n",
      "    Batch: 23600 --> Loss: 0.0012636756592238846\n",
      "    Batch: 23700 --> Loss: 0.0012635896760950331\n",
      "    Batch: 23800 --> Loss: 0.0012652913593607445\n",
      "    Batch: 23900 --> Loss: 0.0012661314947588938\n",
      "    Batch: 24000 --> Loss: 0.0012643214772849226\n",
      "    Batch: 24100 --> Loss: 0.0012646654335642452\n",
      "    Batch: 24200 --> Loss: 0.0012625206636990378\n",
      "    Batch: 24300 --> Loss: 0.0012639950886188845\n",
      "    Batch: 24400 --> Loss: 0.0012628312394640164\n",
      "    Batch: 24500 --> Loss: 0.0012611073537636332\n",
      "    Batch: 24600 --> Loss: 0.0012594711811117474\n",
      "    Batch: 24700 --> Loss: 0.0012596767234373597\n",
      "    Batch: 24800 --> Loss: 0.0012602690113390068\n",
      "    Batch: 24900 --> Loss: 0.001258471490852502\n",
      "    Batch: 25000 --> Loss: 0.0012585302715724883\n",
      "    Batch: 25100 --> Loss: 0.0012593243079945453\n",
      "    Batch: 25200 --> Loss: 0.0012599558758372867\n",
      "    Batch: 25300 --> Loss: 0.001260225594482898\n",
      "    Batch: 25400 --> Loss: 0.0012599329594794283\n",
      "    Batch: 25500 --> Loss: 0.0012590185564903458\n",
      "    Batch: 25600 --> Loss: 0.001257181620886841\n",
      "    Batch: 25700 --> Loss: 0.0012551940275908704\n",
      "    Batch: 25800 --> Loss: 0.0012530456198153505\n",
      "    Batch: 25900 --> Loss: 0.0012517670644294247\n",
      "    Batch: 26000 --> Loss: 0.0012515082599108066\n",
      "    Batch: 26100 --> Loss: 0.0012504921371602673\n",
      "    Batch: 26200 --> Loss: 0.0012501190620584754\n",
      "    Batch: 26300 --> Loss: 0.0012500640974380888\n",
      "    Batch: 26400 --> Loss: 0.0012496427708970374\n",
      "    Batch: 26500 --> Loss: 0.0012510970304245369\n",
      "    Batch: 26600 --> Loss: 0.0012486907644243075\n",
      "    Valid Loss: 0.000967073836363852\n",
      "    Valid Loss: 0.001319114679689045\n",
      "    Valid Loss: 0.0009436304155527798\n",
      "    Valid Loss: 0.0008525001269750509\n",
      "    Valid Loss: 0.000797273829593786\n",
      "    Valid Loss: 0.0007412174207294566\n",
      "    Valid Loss: 0.0007563557209388898\n",
      "    Valid Loss: 0.0008048560069558883\n",
      "    Valid Loss: 0.0007825344878040744\n",
      "    Valid Loss: 0.0007854459182068268\n",
      "    Valid Loss: 0.0008017973597565144\n",
      "Epoch 9\n",
      "    Batch: 0 --> Loss: 0.013074785470962524\n",
      "    Batch: 100 --> Loss: 0.0014198210628994913\n",
      "    Batch: 200 --> Loss: 0.0012523585763157576\n",
      "    Batch: 300 --> Loss: 0.0011535367739358197\n",
      "    Batch: 400 --> Loss: 0.0011985748418673274\n",
      "    Batch: 500 --> Loss: 0.0012138223959104442\n",
      "    Batch: 600 --> Loss: 0.0012034352350084954\n",
      "    Batch: 700 --> Loss: 0.0011595366086163156\n",
      "    Batch: 800 --> Loss: 0.0011687388834593913\n",
      "    Batch: 900 --> Loss: 0.0011674732889509756\n",
      "    Batch: 1000 --> Loss: 0.0011546843379456899\n",
      "    Batch: 1100 --> Loss: 0.0011992774138135233\n",
      "    Batch: 1200 --> Loss: 0.0011778163479665583\n",
      "    Batch: 1300 --> Loss: 0.0011733824406165644\n",
      "    Batch: 1400 --> Loss: 0.0012273269755167084\n",
      "    Batch: 1500 --> Loss: 0.0012329152894067512\n",
      "    Batch: 1600 --> Loss: 0.0012412315075954982\n",
      "    Batch: 1700 --> Loss: 0.0012510903257436303\n",
      "    Batch: 1800 --> Loss: 0.0012648559084814477\n",
      "    Batch: 1900 --> Loss: 0.0012632411890805318\n",
      "    Batch: 2000 --> Loss: 0.001244585090505261\n",
      "    Batch: 2100 --> Loss: 0.0012505989774064794\n",
      "    Batch: 2200 --> Loss: 0.0012321774778523815\n",
      "    Batch: 2300 --> Loss: 0.001234114346029172\n",
      "    Batch: 2400 --> Loss: 0.0012214349898789752\n",
      "    Batch: 2500 --> Loss: 0.0012093148327364264\n",
      "    Batch: 2600 --> Loss: 0.0012259526108847408\n",
      "    Batch: 2700 --> Loss: 0.0012091230758854278\n",
      "    Batch: 2800 --> Loss: 0.0012205279206361534\n",
      "    Batch: 2900 --> Loss: 0.0012257526138494857\n",
      "    Batch: 3000 --> Loss: 0.0012442517148810478\n",
      "    Batch: 3100 --> Loss: 0.0012707697567374684\n",
      "    Batch: 3200 --> Loss: 0.0012770151671366132\n",
      "    Batch: 3300 --> Loss: 0.001281765278469688\n",
      "    Batch: 3400 --> Loss: 0.0012983931553155804\n",
      "    Batch: 3500 --> Loss: 0.0013023013015711073\n",
      "    Batch: 3600 --> Loss: 0.0013200763007241386\n",
      "    Batch: 3700 --> Loss: 0.0013089636622011433\n",
      "    Batch: 3800 --> Loss: 0.001316301647462584\n",
      "    Batch: 3900 --> Loss: 0.0013079258541639656\n",
      "    Batch: 4000 --> Loss: 0.0012909932997681492\n",
      "    Batch: 4100 --> Loss: 0.001284936203158954\n",
      "    Batch: 4200 --> Loss: 0.0012913201998930434\n",
      "    Batch: 4300 --> Loss: 0.0013144718301444306\n",
      "    Batch: 4400 --> Loss: 0.0013234656604767534\n",
      "    Batch: 4500 --> Loss: 0.0013146765261483478\n",
      "    Batch: 4600 --> Loss: 0.0013034024424324279\n",
      "    Batch: 4700 --> Loss: 0.0012918187715815707\n",
      "    Batch: 4800 --> Loss: 0.0012871664448379744\n",
      "    Batch: 4900 --> Loss: 0.001291166264406445\n",
      "    Batch: 5000 --> Loss: 0.0012997242124100851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 5100 --> Loss: 0.0013108235865979673\n",
      "    Batch: 5200 --> Loss: 0.0013136934446929547\n",
      "    Batch: 5300 --> Loss: 0.00130477188935486\n",
      "    Batch: 5400 --> Loss: 0.001316003798910225\n",
      "    Batch: 5500 --> Loss: 0.0013623623695891748\n",
      "    Batch: 5600 --> Loss: 0.0013540696009014157\n",
      "    Batch: 5700 --> Loss: 0.0013466657364704288\n",
      "    Batch: 5800 --> Loss: 0.0013474748825874517\n",
      "    Batch: 5900 --> Loss: 0.0013784116056989573\n",
      "    Batch: 6000 --> Loss: 0.0013762853117147496\n",
      "    Batch: 6100 --> Loss: 0.0013714799614763511\n",
      "    Batch: 6200 --> Loss: 0.0013573213807758495\n",
      "    Batch: 6300 --> Loss: 0.001350736105657182\n",
      "    Batch: 6400 --> Loss: 0.0013471125669244517\n",
      "    Batch: 6500 --> Loss: 0.0013429242126005193\n",
      "    Batch: 6600 --> Loss: 0.0013436070271254036\n",
      "    Batch: 6700 --> Loss: 0.0013454153939563454\n",
      "    Batch: 6800 --> Loss: 0.0013413648902271982\n",
      "    Batch: 6900 --> Loss: 0.0013400387687769488\n",
      "    Batch: 7000 --> Loss: 0.0013370901731692582\n",
      "    Batch: 7100 --> Loss: 0.0013458126807097289\n",
      "    Batch: 7200 --> Loss: 0.0013581905472230293\n",
      "    Batch: 7300 --> Loss: 0.0013550140216772853\n",
      "    Batch: 7400 --> Loss: 0.0013666216320933795\n",
      "    Batch: 7500 --> Loss: 0.0013649237503298734\n",
      "    Batch: 7600 --> Loss: 0.0013566793133182016\n",
      "    Batch: 7700 --> Loss: 0.0013682992390464522\n",
      "    Batch: 7800 --> Loss: 0.0013648632082542706\n",
      "    Batch: 7900 --> Loss: 0.001363137903162344\n",
      "    Batch: 8000 --> Loss: 0.0013724569757971377\n",
      "    Batch: 8100 --> Loss: 0.001369137456112008\n",
      "    Batch: 8200 --> Loss: 0.0013744097787487316\n",
      "    Batch: 8300 --> Loss: 0.001369532889070532\n",
      "    Batch: 8400 --> Loss: 0.001373356190184579\n",
      "    Batch: 8500 --> Loss: 0.0013692447922066364\n",
      "    Batch: 8600 --> Loss: 0.0013607477074114205\n",
      "    Batch: 8700 --> Loss: 0.0013767088774084052\n",
      "    Batch: 8800 --> Loss: 0.0013720924968745442\n",
      "    Batch: 8900 --> Loss: 0.0013669213422480117\n",
      "    Batch: 9000 --> Loss: 0.0013634156780873562\n",
      "    Batch: 9100 --> Loss: 0.0013646038894945787\n",
      "    Batch: 9200 --> Loss: 0.0013700452625516211\n",
      "    Batch: 9300 --> Loss: 0.0013679691838844328\n",
      "    Batch: 9400 --> Loss: 0.001366581347194067\n",
      "    Batch: 9500 --> Loss: 0.00136896096663569\n",
      "    Batch: 9600 --> Loss: 0.0013685728813171273\n",
      "    Batch: 9700 --> Loss: 0.001365151852095224\n",
      "    Batch: 9800 --> Loss: 0.001359700827451364\n",
      "    Batch: 9900 --> Loss: 0.001354199888251629\n",
      "    Batch: 10000 --> Loss: 0.0013528796232360528\n",
      "    Batch: 10100 --> Loss: 0.0013486859966782005\n",
      "    Batch: 10200 --> Loss: 0.0013457932228780913\n",
      "    Batch: 10300 --> Loss: 0.001354980656789952\n",
      "    Batch: 10400 --> Loss: 0.0013564291596310256\n",
      "    Batch: 10500 --> Loss: 0.0013681704187374156\n",
      "    Batch: 10600 --> Loss: 0.0013617890368137562\n",
      "    Batch: 10700 --> Loss: 0.0013583918701392317\n",
      "    Batch: 10800 --> Loss: 0.0013551084286947662\n",
      "    Batch: 10900 --> Loss: 0.0013604004128909594\n",
      "    Batch: 11000 --> Loss: 0.0013671216570447862\n",
      "    Batch: 11100 --> Loss: 0.0013668804537276661\n",
      "    Batch: 11200 --> Loss: 0.0013607077937125018\n",
      "    Batch: 11300 --> Loss: 0.0013638256930222769\n",
      "    Batch: 11400 --> Loss: 0.0013730143303022023\n",
      "    Batch: 11500 --> Loss: 0.0013712064325814802\n",
      "    Batch: 11600 --> Loss: 0.0013785193121578527\n",
      "    Batch: 11700 --> Loss: 0.0013764173427412859\n",
      "    Batch: 11800 --> Loss: 0.0013740131690572898\n",
      "    Batch: 11900 --> Loss: 0.0013697424123042836\n",
      "    Batch: 12000 --> Loss: 0.0013686378513601113\n",
      "    Batch: 12100 --> Loss: 0.001364830742437533\n",
      "    Batch: 12200 --> Loss: 0.0013649948808223673\n",
      "    Batch: 12300 --> Loss: 0.001361555947571588\n",
      "    Batch: 12400 --> Loss: 0.0013580984412609658\n",
      "    Batch: 12500 --> Loss: 0.001360501742057133\n",
      "    Batch: 12600 --> Loss: 0.001357534589237831\n",
      "    Batch: 12700 --> Loss: 0.0013537790031051549\n",
      "    Batch: 12800 --> Loss: 0.001353232844509129\n",
      "    Batch: 12900 --> Loss: 0.0013543507271990343\n",
      "    Batch: 13000 --> Loss: 0.0013546792540442007\n",
      "    Batch: 13100 --> Loss: 0.001349330057215259\n",
      "    Batch: 13200 --> Loss: 0.0013497049468896454\n",
      "    Batch: 13300 --> Loss: 0.0013574249693232886\n",
      "    Batch: 13400 --> Loss: 0.0013586161303262136\n",
      "    Batch: 13500 --> Loss: 0.0013628887401173147\n",
      "    Batch: 13600 --> Loss: 0.0013590484893969829\n",
      "    Batch: 13700 --> Loss: 0.001356741365748888\n",
      "    Batch: 13800 --> Loss: 0.0013617678304745535\n",
      "    Batch: 13900 --> Loss: 0.0013592969656566454\n",
      "    Batch: 14000 --> Loss: 0.001360056322424748\n",
      "    Batch: 14100 --> Loss: 0.0013564954256325334\n",
      "    Batch: 14200 --> Loss: 0.0013544762365309269\n",
      "    Batch: 14300 --> Loss: 0.0013494348592320135\n",
      "    Batch: 14400 --> Loss: 0.0013520117245166031\n",
      "    Batch: 14500 --> Loss: 0.0013505812811432673\n",
      "    Batch: 14600 --> Loss: 0.0013486676465318639\n",
      "    Batch: 14700 --> Loss: 0.0013440166940964246\n",
      "    Batch: 14800 --> Loss: 0.0013414340030941756\n",
      "    Batch: 14900 --> Loss: 0.0013364692926338638\n",
      "    Batch: 15000 --> Loss: 0.0013386545720538118\n",
      "    Batch: 15100 --> Loss: 0.0013368074759045466\n",
      "    Batch: 15200 --> Loss: 0.0013324518691760932\n",
      "    Batch: 15300 --> Loss: 0.0013288268690363205\n",
      "    Batch: 15400 --> Loss: 0.0013319268538476328\n",
      "    Batch: 15500 --> Loss: 0.001333682931019138\n",
      "    Batch: 15600 --> Loss: 0.0013309293909690013\n",
      "    Batch: 15700 --> Loss: 0.0013281605978057917\n",
      "    Batch: 15800 --> Loss: 0.0013242675565499171\n",
      "    Batch: 15900 --> Loss: 0.0013244136983262904\n",
      "    Batch: 16000 --> Loss: 0.0013251722923535516\n",
      "    Batch: 16100 --> Loss: 0.0013274564737414583\n",
      "    Batch: 16200 --> Loss: 0.001333025392412983\n",
      "    Batch: 16300 --> Loss: 0.0013336348002007396\n",
      "    Batch: 16400 --> Loss: 0.0013297202417903717\n",
      "    Batch: 16500 --> Loss: 0.0013297818756906085\n",
      "    Batch: 16600 --> Loss: 0.0013329556025957\n",
      "    Batch: 16700 --> Loss: 0.0013295743863795818\n",
      "    Batch: 16800 --> Loss: 0.001327412428199409\n",
      "    Batch: 16900 --> Loss: 0.0013259837492734991\n",
      "    Batch: 17000 --> Loss: 0.0013225841387088978\n",
      "    Batch: 17100 --> Loss: 0.0013195368841312587\n",
      "    Batch: 17200 --> Loss: 0.0013160914350142322\n",
      "    Batch: 17300 --> Loss: 0.0013150282787255812\n",
      "    Batch: 17400 --> Loss: 0.0013245426492498374\n",
      "    Batch: 17500 --> Loss: 0.001322350064607892\n",
      "    Batch: 17600 --> Loss: 0.001319760195219639\n",
      "    Batch: 17700 --> Loss: 0.0013196129979381166\n",
      "    Batch: 17800 --> Loss: 0.0013197848874942299\n",
      "    Batch: 17900 --> Loss: 0.0013207808507231637\n",
      "    Batch: 18000 --> Loss: 0.0013192438847942785\n",
      "    Batch: 18100 --> Loss: 0.0013177954243550208\n",
      "    Batch: 18200 --> Loss: 0.0013176079085607882\n",
      "    Batch: 18300 --> Loss: 0.00131600700187276\n",
      "    Batch: 18400 --> Loss: 0.0013150239508892606\n",
      "    Batch: 18500 --> Loss: 0.001315212676379229\n",
      "    Batch: 18600 --> Loss: 0.001313997897170243\n",
      "    Batch: 18700 --> Loss: 0.0013111564893633413\n",
      "    Batch: 18800 --> Loss: 0.0013129422390807015\n",
      "    Batch: 18900 --> Loss: 0.001309545437995243\n",
      "    Batch: 19000 --> Loss: 0.001306179137456672\n",
      "    Batch: 19100 --> Loss: 0.0013056777049257695\n",
      "    Batch: 19200 --> Loss: 0.0013038885402813468\n",
      "    Batch: 19300 --> Loss: 0.0013009438128706128\n",
      "    Batch: 19400 --> Loss: 0.0013006766997524447\n",
      "    Batch: 19500 --> Loss: 0.001299860366358503\n",
      "    Batch: 19600 --> Loss: 0.0012964389087617414\n",
      "    Batch: 19700 --> Loss: 0.0013029355494948824\n",
      "    Batch: 19800 --> Loss: 0.001303022061856265\n",
      "    Batch: 19900 --> Loss: 0.0013028138157039012\n",
      "    Batch: 20000 --> Loss: 0.001303710301389841\n",
      "    Batch: 20100 --> Loss: 0.0013037189921360098\n",
      "    Batch: 20200 --> Loss: 0.0013019337980392973\n",
      "    Batch: 20300 --> Loss: 0.0013024595115580509\n",
      "    Batch: 20400 --> Loss: 0.0012999015902561302\n",
      "    Batch: 20500 --> Loss: 0.0012985099330319735\n",
      "    Batch: 20600 --> Loss: 0.0012962920332541795\n",
      "    Batch: 20700 --> Loss: 0.0012936028898019249\n",
      "    Batch: 20800 --> Loss: 0.0012928176820992466\n",
      "    Batch: 20900 --> Loss: 0.0012949846662203043\n",
      "    Batch: 21000 --> Loss: 0.0012938622746291374\n",
      "    Batch: 21100 --> Loss: 0.0012919544456657449\n",
      "    Batch: 21200 --> Loss: 0.0012902024518993314\n",
      "    Batch: 21300 --> Loss: 0.001289864886006161\n",
      "    Batch: 21400 --> Loss: 0.0012884903976224113\n",
      "    Batch: 21500 --> Loss: 0.0012912028242391344\n",
      "    Batch: 21600 --> Loss: 0.0012893211119855853\n",
      "    Batch: 21700 --> Loss: 0.00130261352956638\n",
      "    Batch: 21800 --> Loss: 0.0013022410138123829\n",
      "    Batch: 21900 --> Loss: 0.0013040874075226428\n",
      "    Batch: 22000 --> Loss: 0.0013051696148080193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 22100 --> Loss: 0.0013027557914750245\n",
      "    Batch: 22200 --> Loss: 0.0013027859412259504\n",
      "    Batch: 22300 --> Loss: 0.0013007194008305735\n",
      "    Batch: 22400 --> Loss: 0.0013014367755914504\n",
      "    Batch: 22500 --> Loss: 0.001299701710785822\n",
      "    Batch: 22600 --> Loss: 0.0012970540226980192\n",
      "    Batch: 22700 --> Loss: 0.0012954933125475395\n",
      "    Batch: 22800 --> Loss: 0.0012932250931726981\n",
      "    Batch: 22900 --> Loss: 0.0012917726746953264\n",
      "    Batch: 23000 --> Loss: 0.0012919282018449588\n",
      "    Batch: 23100 --> Loss: 0.001292314390550176\n",
      "    Batch: 23200 --> Loss: 0.0012928203144340349\n",
      "    Batch: 23300 --> Loss: 0.0012936822245756035\n",
      "    Batch: 23400 --> Loss: 0.001296043116606716\n",
      "    Batch: 23500 --> Loss: 0.0012972772293882403\n",
      "    Batch: 23600 --> Loss: 0.0012958308550349304\n",
      "    Batch: 23700 --> Loss: 0.0012950376237233733\n",
      "    Batch: 23800 --> Loss: 0.0012926144235098328\n",
      "    Batch: 23900 --> Loss: 0.001289868756903895\n",
      "    Batch: 24000 --> Loss: 0.0012891414090565408\n",
      "    Batch: 24100 --> Loss: 0.0012881935250461107\n",
      "    Batch: 24200 --> Loss: 0.001286624298759979\n",
      "    Batch: 24300 --> Loss: 0.001285458779835973\n",
      "    Batch: 24400 --> Loss: 0.0012829658762050441\n",
      "    Batch: 24500 --> Loss: 0.0012815584658757814\n",
      "    Batch: 24600 --> Loss: 0.0012817878990295478\n",
      "    Batch: 24700 --> Loss: 0.0012805828721777364\n",
      "    Batch: 24800 --> Loss: 0.0012783264794860492\n",
      "    Batch: 24900 --> Loss: 0.00127792951816681\n",
      "    Batch: 25000 --> Loss: 0.0012769545979583175\n",
      "    Batch: 25100 --> Loss: 0.0012771024533263229\n",
      "    Batch: 25200 --> Loss: 0.0012775061766580648\n",
      "    Batch: 25300 --> Loss: 0.0012768421856387364\n",
      "    Batch: 25400 --> Loss: 0.001275661371398204\n",
      "    Batch: 25500 --> Loss: 0.0012740067405625647\n",
      "    Batch: 25600 --> Loss: 0.0012730707678075882\n",
      "    Batch: 25700 --> Loss: 0.0012731098540327513\n",
      "    Batch: 25800 --> Loss: 0.001273940535822144\n",
      "    Batch: 25900 --> Loss: 0.0012737963760788618\n",
      "    Batch: 26000 --> Loss: 0.001274183899831882\n",
      "    Batch: 26100 --> Loss: 0.0012739301700632373\n",
      "    Batch: 26200 --> Loss: 0.001272767523261996\n",
      "    Batch: 26300 --> Loss: 0.001271840372977645\n",
      "    Batch: 26400 --> Loss: 0.001273138784585496\n",
      "    Batch: 26500 --> Loss: 0.0012749117006890494\n",
      "    Batch: 26600 --> Loss: 0.0012792276366352282\n",
      "    Valid Loss: 0.00010528213897487149\n",
      "    Valid Loss: 0.0013532319617946603\n",
      "    Valid Loss: 0.001040692931095569\n",
      "    Valid Loss: 0.0009171186051432229\n",
      "    Valid Loss: 0.0008540178290324106\n",
      "    Valid Loss: 0.0011394049383895697\n",
      "    Valid Loss: 0.001066518905292177\n",
      "    Valid Loss: 0.0010190511284647685\n",
      "    Valid Loss: 0.000984556142865696\n",
      "    Valid Loss: 0.0009443333105597107\n",
      "    Valid Loss: 0.0009379431659920331\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"./data\")\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "if use_cuda:\n",
    "    net.to(device)\n",
    "for epoch in range(0, 10):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train(epoch, net, car_train_loader, optimizer, criterion, use_cuda, device=device, save_dir=save_dir, \n",
    "          MAX_BATCH=len(car_train_loader)//4)\n",
    "    valid(epoch, net, car_valid_loader, criterion, use_cuda, device=device, save_dir=save_dir, MAX_BATCH=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2de42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "b    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
