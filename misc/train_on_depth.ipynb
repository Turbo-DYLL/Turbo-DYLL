{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train on Depth\n",
    "reference from \n",
    "\n",
    "https://github.com/asap-report/carla/blob/racetrack/PythonClient/racetrack/train_on_depth.py\n",
    "https://github.com/ManajitPal/DeepLearningForSelfDrivingCars/blob/master/self_driving_car.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if data exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13689 data points\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/michael/Desktop/projects/ROAR/data\")\n",
    "# left_depth_dir = data_dir / \"left\"\n",
    "center_depth_dir = data_dir / \"center\"\n",
    "# right_depth_dir = data_dir / \"right\"\n",
    "veh_state_dir = data_dir / \"state\"\n",
    "\n",
    "# assert left_depth_dir.exists(), \"left_depth_dir does not exist\"\n",
    "# assert right_depth_dir.exists(), \"left_depth_dir does not exist\"\n",
    "assert data_dir.exists(), \"data dir does not exist\"\n",
    "assert center_depth_dir.exists(), \"left_depth_dir does not exist\"\n",
    "assert veh_state_dir.exists(), \"veh_state_dir does not exist\"\n",
    "\n",
    "# right_depth_paths = [p for p in sorted(right_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "# left_depth_paths = [p for p in sorted(left_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "center_depth_paths = [p for p in sorted(center_depth_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "veh_state_paths = [p for p in sorted(veh_state_dir.glob(\"*.npy\", ), key=os.path.getmtime)]\n",
    "\n",
    "assert len(center_depth_paths) == len(veh_state_paths),  \"not the same size of X and y\"\n",
    "# assert len(left_depth_paths) == len(center_depth_paths)== len(right_depth_paths)==len(veh_state_paths), \"not the same size of X and y\"\n",
    "print(f\"Found { len(center_depth_paths)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'throttle'}>,\n",
       "        <AxesSubplot:title={'center':'steering'}>]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "throttles, steerings = [], []\n",
    "for st_path in veh_state_paths:\n",
    "    array = np.load(st_path)\n",
    "    throttles.append(array[-2])\n",
    "    steerings.append(array[-1])\n",
    "df = pd.DataFrame(\n",
    "    data={\"throttle\":throttles, \"steering\":steerings}\n",
    ")\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Do data augmentation\n",
    "\n",
    "Let's only train on steering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, samples, left_depth_dir, center_depth_dir, right_depth_dir, state_dir, transform=None):\n",
    "\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.left_depth_dir: Path = left_depth_dir\n",
    "        self.right_depth_dir: Path = right_depth_dir\n",
    "        self.center_depth_dir: Path = center_depth_dir\n",
    "        self.state_dir: Path = state_dir\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      \n",
    "        sample_path = self.samples[index]\n",
    "        left_depth_path = self.left_depth_dir / sample_path\n",
    "        center_depth_path = self.center_depth_dir / sample_path\n",
    "        right_depth_path = self.right_depth_dir / sample_path\n",
    "        veh_state_path = self.state_dir / sample_path\n",
    "        \n",
    "        left_depth = np.load(left_depth_path)\n",
    "        right_depth = np.load(left_depth_path)\n",
    "        center_depth = np.load(right_depth_path)\n",
    "        veh_state = np.load(veh_state_path)\n",
    "        \n",
    "        steering_angle_center = veh_state[-1]\n",
    "        steering_angle_left = steering_angle_center + 0.2\n",
    "        steering_angle_right = steering_angle_center - 0.2\n",
    "        return (center_depth, steering_angle_center), \\\n",
    "               (left_depth, steering_angle_left), \\\n",
    "               (right_depth, steering_angle_right)\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}\n",
    "samples = [Path(p).name for p in veh_state_paths]\n",
    "samples = samples[:len(samples) - len(samples)%params['batch_size'] ]\n",
    "dataset = Dataset(samples=samples, \n",
    "                  left_depth_dir = left_depth_dir, \n",
    "                  center_depth_dir=center_depth_dir, \n",
    "                  right_depth_dir=right_depth_dir,\n",
    "                  state_dir = veh_state_dir)\n",
    "data_generator = data.DataLoader(dataset, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Step6: Check the device and define function to move tensors to that device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print('device is: ', device)\n",
    "\n",
    "def toDevice(datas, device):\n",
    "    imgs, angles = datas\n",
    "    return imgs.float().to(device), angles.float().to(device)\n",
    "def to_tensor(datas, device):\n",
    "    imgs, angles = datas\n",
    "    return imgs.float().to(device), angles.float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NvidiaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NvidiaModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 24, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(24, 36, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(36, 48, 5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(48, 64, 3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=404736, out_features=50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=50, out_features=10),\n",
    "            nn.Linear(in_features=10, out_features=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):  \n",
    "        input = input.view(input.size(0), 1, 600, 800)\n",
    "        output = self.conv_layers(input)\n",
    "        output = torch.flatten(output)\n",
    "        output = self.linear_layers(output)\n",
    "        return output\n",
    "model = NvidiaModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/ROAR3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 0 -> loss: 3.3395306151360273\n",
      "    At iteration 100 -> loss: 0.20645973961288994\n",
      "    At iteration 200 -> loss: 0.16451815150996546\n",
      "    At iteration 300 -> loss: 0.1465253990256189\n",
      "    At iteration 400 -> loss: 0.13595118645109236\n",
      "    At iteration 500 -> loss: 0.12904846639479645\n",
      "    At iteration 600 -> loss: 0.12515891534442267\n",
      "    At iteration 700 -> loss: 0.12143148365568525\n",
      "    At iteration 800 -> loss: 0.11837439401472861\n",
      "    At iteration 900 -> loss: 0.11641298376155068\n",
      "    At iteration 1000 -> loss: 0.11437199004777973\n",
      "    At iteration 1100 -> loss: 0.11258104675857758\n",
      "    At iteration 1200 -> loss: 0.11159678168689577\n",
      "    At iteration 1300 -> loss: 0.11046241951947361\n",
      "    At iteration 1400 -> loss: 0.1095210861216561\n",
      "    At iteration 1500 -> loss: 0.10842516395194274\n",
      "    At iteration 1600 -> loss: 0.10792438424600839\n",
      "    At iteration 1700 -> loss: 0.10721109253450045\n",
      "    At iteration 1800 -> loss: 0.10648686085190216\n",
      "    At iteration 1900 -> loss: 0.10589057645159101\n",
      "    At iteration 2000 -> loss: 0.10528964039916625\n",
      "    At iteration 2100 -> loss: 0.10488794964409248\n",
      "    At iteration 2200 -> loss: 0.10443165982005953\n",
      "    At iteration 2300 -> loss: 0.10428120575332547\n",
      "    At iteration 2400 -> loss: 0.10394516835677518\n",
      "    At iteration 2500 -> loss: 0.10371478810368935\n",
      "    At iteration 2600 -> loss: 0.10336992270624093\n",
      "    At iteration 2700 -> loss: 0.10302105457090707\n",
      "    At iteration 2800 -> loss: 0.10287360438298906\n",
      "    At iteration 2900 -> loss: 0.1025767583340025\n",
      "    At iteration 3000 -> loss: 0.10232399752860562\n",
      "    At iteration 3100 -> loss: 0.10206312240255824\n",
      "    At iteration 3200 -> loss: 0.1018471270968057\n",
      "    At iteration 3300 -> loss: 0.10177083455719473\n",
      "    At iteration 3400 -> loss: 0.10154054294527518\n",
      "    At iteration 3500 -> loss: 0.10134860783234426\n",
      "    At iteration 3600 -> loss: 0.10118909398158064\n",
      "    At iteration 3700 -> loss: 0.10288713146998749\n",
      "    At iteration 3800 -> loss: 0.1025841931467301\n",
      "    At iteration 3900 -> loss: 0.10255915281216553\n",
      "    At iteration 4000 -> loss: 0.10234367565876257\n",
      "    At iteration 4100 -> loss: 0.10217577446635091\n",
      "    At iteration 4200 -> loss: 0.10190299068126493\n",
      "    At iteration 4300 -> loss: 0.1017008385311254\n",
      "    At iteration 4400 -> loss: 0.1015158250749313\n",
      "    At iteration 4500 -> loss: 0.10130803453331379\n",
      "    At iteration 4600 -> loss: 0.10113523039476154\n",
      "    At iteration 4700 -> loss: 0.10091587737248547\n",
      "    At iteration 4800 -> loss: 0.10075923018731862\n",
      "    At iteration 4900 -> loss: 0.10061520654871421\n",
      "    At iteration 5000 -> loss: 0.1004487742128755\n",
      "    At iteration 5100 -> loss: 0.10034525747322998\n",
      "    At iteration 5200 -> loss: 0.10026962735434014\n",
      "    At iteration 5300 -> loss: 0.1001941619251887\n",
      "    At iteration 5400 -> loss: 0.10002740333739392\n",
      "    At iteration 5500 -> loss: 0.09997547482927552\n",
      "    At iteration 5600 -> loss: 0.09986293825292328\n",
      "    At iteration 5700 -> loss: 0.09974347749308267\n",
      "    At iteration 5800 -> loss: 0.09977361495167376\n",
      "    At iteration 5900 -> loss: 0.09966061657399466\n",
      "    At iteration 6000 -> loss: 0.09967601963299957\n",
      "    At iteration 6100 -> loss: 0.09974933459545916\n",
      "    At iteration 6200 -> loss: 0.09964934799232246\n",
      "    At iteration 6300 -> loss: 0.09955001776744461\n",
      "    At iteration 6400 -> loss: 0.09947095005200757\n",
      "    At iteration 6500 -> loss: 0.09935743080068822\n",
      "    At iteration 6600 -> loss: 0.09932879964760448\n",
      "    At iteration 6700 -> loss: 0.0992381497891963\n",
      "    At iteration 6800 -> loss: 0.09912479201325267\n",
      "    At iteration 6900 -> loss: 0.09907496939517466\n",
      "    At iteration 7000 -> loss: 0.0989811946879397\n",
      "    At iteration 7100 -> loss: 0.0988762796315808\n",
      "    At iteration 7200 -> loss: 0.09884249601856464\n",
      "    At iteration 7300 -> loss: 0.09878139477395127\n",
      "    At iteration 7400 -> loss: 0.09869314599187849\n",
      "    At iteration 7500 -> loss: 0.09862944478884826\n",
      "    At iteration 7600 -> loss: 0.09852814029245627\n",
      "    At iteration 7700 -> loss: 0.09844131192127814\n",
      "    At iteration 7800 -> loss: 0.09835705996431905\n",
      "    At iteration 7900 -> loss: 0.09829885827131153\n",
      "    At iteration 8000 -> loss: 0.09822065820579845\n",
      "    At iteration 8100 -> loss: 0.09816770619751106\n",
      "    At iteration 8200 -> loss: 0.0981116332861094\n",
      "    At iteration 8300 -> loss: 0.09815716861244098\n",
      "    At iteration 8400 -> loss: 0.09812080822961936\n",
      "    At iteration 8500 -> loss: 0.09808429847887065\n",
      "    At iteration 8600 -> loss: 0.0980408945350071\n",
      "    At iteration 8700 -> loss: 0.09796776655239346\n",
      "    At iteration 8800 -> loss: 0.09789533746804362\n",
      "    At iteration 8900 -> loss: 0.09846640034763546\n",
      "    At iteration 9000 -> loss: 0.09849393003867804\n",
      "    At iteration 9100 -> loss: 0.09842672071574762\n",
      "    At iteration 9200 -> loss: 0.09835475998886302\n",
      "    At iteration 9300 -> loss: 0.09833911727502104\n",
      "    At iteration 9400 -> loss: 0.09829647193840296\n",
      "    At iteration 9500 -> loss: 0.0984129061035142\n",
      "    At iteration 9600 -> loss: 0.09833037771835049\n",
      "    At iteration 9700 -> loss: 0.09827908780974212\n",
      "    At iteration 9800 -> loss: 0.0981920036311832\n",
      "    At iteration 9900 -> loss: 0.0981113092429827\n",
      "    At iteration 10000 -> loss: 0.09809604333463806\n",
      "    At iteration 10100 -> loss: 0.09806415889927246\n",
      "    At iteration 10200 -> loss: 0.0980555390853315\n",
      "    At iteration 10300 -> loss: 0.09800521744990016\n",
      "    At iteration 10400 -> loss: 0.09797436246825822\n",
      "    At iteration 10500 -> loss: 0.09791838756781936\n",
      "    At iteration 10600 -> loss: 0.09789458162391218\n",
      "    At iteration 10700 -> loss: 0.09798850599735982\n",
      "    At iteration 10800 -> loss: 0.09793545183327064\n",
      "    At iteration 10900 -> loss: 0.09787854160465564\n",
      "    At iteration 11000 -> loss: 0.098128521054748\n",
      "    At iteration 11100 -> loss: 0.09818788764691075\n",
      "    At iteration 11200 -> loss: 0.09821859786466562\n",
      "    At iteration 11300 -> loss: 0.09815961448403945\n",
      "    At iteration 11400 -> loss: 0.09810898877395166\n",
      "    At iteration 11500 -> loss: 0.09808182403558831\n",
      "    At iteration 11600 -> loss: 0.09805267717085152\n",
      "    At iteration 11700 -> loss: 0.09806455593566557\n",
      "    At iteration 11800 -> loss: 0.09807286049826518\n",
      "    At iteration 11900 -> loss: 0.09802801957761244\n",
      "    At iteration 12000 -> loss: 0.09799224872574758\n",
      "    At iteration 12100 -> loss: 0.09796842836133296\n",
      "    At iteration 12200 -> loss: 0.0979576142429191\n",
      "    At iteration 12300 -> loss: 0.09791373269629453\n",
      "    At iteration 12400 -> loss: 0.09787476196862228\n",
      "    At iteration 12500 -> loss: 0.0978229703478943\n",
      "    At iteration 12600 -> loss: 0.09778586968251438\n",
      "    At iteration 12700 -> loss: 0.09774955713410956\n",
      "    At iteration 12800 -> loss: 0.09770390088733898\n",
      "    At iteration 12900 -> loss: 0.09765425327247625\n",
      "    At iteration 13000 -> loss: 0.09759798442387779\n",
      "    At iteration 13100 -> loss: 0.09757413499078621\n",
      "    At iteration 13200 -> loss: 0.0975395634581122\n",
      "    At iteration 13300 -> loss: 0.09754886554218407\n",
      "    At iteration 13400 -> loss: 0.097532290075655\n",
      "    At iteration 13500 -> loss: 0.09752626366126177\n",
      "    At iteration 13600 -> loss: 0.09752035992825149\n",
      "Staring Epoch 1\n",
      "    At iteration 0 -> loss: 0.08232188253896311\n",
      "    At iteration 100 -> loss: 0.09487034243654512\n",
      "    At iteration 200 -> loss: 0.09597263310507141\n",
      "    At iteration 300 -> loss: 0.09909538729345321\n",
      "    At iteration 400 -> loss: 0.09759790129404938\n",
      "    At iteration 500 -> loss: 0.09686042778812771\n",
      "    At iteration 600 -> loss: 0.09688735461539716\n",
      "    At iteration 700 -> loss: 0.09620374349400121\n",
      "    At iteration 800 -> loss: 0.09566180814534979\n",
      "    At iteration 900 -> loss: 0.09514772654442169\n",
      "    At iteration 1000 -> loss: 0.09460457837739482\n",
      "    At iteration 1100 -> loss: 0.09436613765141934\n",
      "    At iteration 1200 -> loss: 0.09423313192979155\n",
      "    At iteration 1300 -> loss: 0.09431734422144486\n",
      "    At iteration 1400 -> loss: 0.09420532803794611\n",
      "    At iteration 1500 -> loss: 0.09400532067254956\n",
      "    At iteration 1600 -> loss: 0.09407102932090554\n",
      "    At iteration 1700 -> loss: 0.09389877870262946\n",
      "    At iteration 1800 -> loss: 0.09378239170619086\n",
      "    At iteration 1900 -> loss: 0.09375831257386201\n",
      "    At iteration 2000 -> loss: 0.09370608825175923\n",
      "    At iteration 2100 -> loss: 0.09369893029532983\n",
      "    At iteration 2200 -> loss: 0.0936117860516758\n",
      "    At iteration 2300 -> loss: 0.09357408187126232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2400 -> loss: 0.09392399911423528\n",
      "    At iteration 2500 -> loss: 0.09376671847736022\n",
      "    At iteration 2600 -> loss: 0.09410813245150204\n",
      "    At iteration 2700 -> loss: 0.09401467173256274\n",
      "    At iteration 2800 -> loss: 0.09399595232712159\n",
      "    At iteration 2900 -> loss: 0.09382300466112778\n",
      "    At iteration 3000 -> loss: 0.09374380605195547\n",
      "    At iteration 3100 -> loss: 0.0937572089831344\n",
      "    At iteration 3200 -> loss: 0.09369351208440219\n",
      "    At iteration 3300 -> loss: 0.09378143068968173\n",
      "    At iteration 3400 -> loss: 0.09383291884102316\n",
      "    At iteration 3500 -> loss: 0.09385317022448258\n",
      "    At iteration 3600 -> loss: 0.09379020863863655\n",
      "    At iteration 3700 -> loss: 0.09393392049081212\n",
      "    At iteration 3800 -> loss: 0.09395677685702399\n",
      "    At iteration 3900 -> loss: 0.09391636260409716\n",
      "    At iteration 4000 -> loss: 0.09403237777026488\n",
      "    At iteration 4100 -> loss: 0.09414576381790393\n",
      "    At iteration 4200 -> loss: 0.09406856255528423\n",
      "    At iteration 4300 -> loss: 0.09399824474366268\n",
      "    At iteration 4400 -> loss: 0.09394961410886299\n",
      "    At iteration 4500 -> loss: 0.09396336657924352\n",
      "    At iteration 4600 -> loss: 0.0938714182293313\n",
      "    At iteration 4700 -> loss: 0.09388271838283645\n",
      "    At iteration 4800 -> loss: 0.09395436992306958\n",
      "    At iteration 4900 -> loss: 0.09393508803682724\n",
      "    At iteration 5000 -> loss: 0.09387791448279638\n",
      "    At iteration 5100 -> loss: 0.0939754636124888\n",
      "    At iteration 5200 -> loss: 0.09392460305930904\n",
      "    At iteration 5300 -> loss: 0.09387637754357893\n",
      "    At iteration 5400 -> loss: 0.09382050797914233\n",
      "    At iteration 5500 -> loss: 0.09382792321510679\n",
      "    At iteration 5600 -> loss: 0.09377832542423005\n",
      "    At iteration 5700 -> loss: 0.09376531381802425\n",
      "    At iteration 5800 -> loss: 0.09371087430635638\n",
      "    At iteration 5900 -> loss: 0.09371830472052875\n",
      "    At iteration 6000 -> loss: 0.09379190591589787\n",
      "    At iteration 6100 -> loss: 0.09380939480826425\n",
      "    At iteration 6200 -> loss: 0.09381979588224267\n",
      "    At iteration 6300 -> loss: 0.09379994562991473\n",
      "    At iteration 6400 -> loss: 0.09377100164267652\n",
      "    At iteration 6500 -> loss: 0.09376746473933813\n",
      "    At iteration 6600 -> loss: 0.09372548599788155\n",
      "    At iteration 6700 -> loss: 0.0937444102818732\n",
      "    At iteration 6800 -> loss: 0.09376697724446943\n",
      "    At iteration 6900 -> loss: 0.09369527018327789\n",
      "    At iteration 7000 -> loss: 0.09367732455161237\n",
      "    At iteration 7100 -> loss: 0.09378826204449738\n",
      "    At iteration 7200 -> loss: 0.09386535756578133\n",
      "    At iteration 7300 -> loss: 0.09382684991508766\n",
      "    At iteration 7400 -> loss: 0.09378466961445574\n",
      "    At iteration 7500 -> loss: 0.09384325414683559\n",
      "    At iteration 7600 -> loss: 0.0938290141227981\n",
      "    At iteration 7700 -> loss: 0.09378299611582304\n",
      "    At iteration 7800 -> loss: 0.09380489984690284\n",
      "    At iteration 7900 -> loss: 0.09379813686537036\n",
      "    At iteration 8000 -> loss: 0.09378750774572317\n",
      "    At iteration 8100 -> loss: 0.09384314041064251\n",
      "    At iteration 8200 -> loss: 0.09382346514027481\n",
      "    At iteration 8300 -> loss: 0.09380027709689481\n",
      "    At iteration 8400 -> loss: 0.09381093425716888\n",
      "    At iteration 8500 -> loss: 0.09384433960242326\n",
      "    At iteration 8600 -> loss: 0.0938433944620683\n",
      "    At iteration 8700 -> loss: 0.09382651759129375\n",
      "    At iteration 8800 -> loss: 0.0937842859437787\n",
      "    At iteration 8900 -> loss: 0.09377898395522664\n",
      "    At iteration 9000 -> loss: 0.0937791140882835\n",
      "    At iteration 9100 -> loss: 0.09373689859833764\n",
      "    At iteration 9200 -> loss: 0.09376303497791431\n",
      "    At iteration 9300 -> loss: 0.09381610691640394\n",
      "    At iteration 9400 -> loss: 0.09377853399410377\n",
      "    At iteration 9500 -> loss: 0.09384318951197723\n",
      "    At iteration 9600 -> loss: 0.09416237683238514\n",
      "    At iteration 9700 -> loss: 0.09414376249473026\n",
      "    At iteration 9800 -> loss: 0.09410327048521425\n",
      "    At iteration 9900 -> loss: 0.09408174347946903\n",
      "    At iteration 10000 -> loss: 0.09411376150971816\n",
      "    At iteration 10100 -> loss: 0.09416474646985286\n",
      "    At iteration 10200 -> loss: 0.09430089218200863\n",
      "    At iteration 10300 -> loss: 0.09425991811877497\n",
      "    At iteration 10400 -> loss: 0.0942316125316792\n",
      "    At iteration 10500 -> loss: 0.09423923553855923\n",
      "    At iteration 10600 -> loss: 0.09422388669835983\n",
      "    At iteration 10700 -> loss: 0.0942844480562531\n",
      "    At iteration 10800 -> loss: 0.09423918603961706\n",
      "    At iteration 10900 -> loss: 0.09421368927712255\n",
      "    At iteration 11000 -> loss: 0.09417480787711456\n",
      "    At iteration 11100 -> loss: 0.09416254623781165\n",
      "    At iteration 11200 -> loss: 0.09416550681627234\n",
      "    At iteration 11300 -> loss: 0.09417146081822808\n",
      "    At iteration 11400 -> loss: 0.09414577694370184\n",
      "    At iteration 11500 -> loss: 0.09411108581028194\n",
      "    At iteration 11600 -> loss: 0.09410773102982296\n",
      "    At iteration 11700 -> loss: 0.0940831557395914\n",
      "    At iteration 11800 -> loss: 0.09412258520599694\n",
      "    At iteration 11900 -> loss: 0.09407732461169924\n",
      "    At iteration 12000 -> loss: 0.09407574176231037\n",
      "    At iteration 12100 -> loss: 0.09407109096377152\n",
      "    At iteration 12200 -> loss: 0.09404071837985543\n",
      "    At iteration 12300 -> loss: 0.09407636811115119\n",
      "    At iteration 12400 -> loss: 0.09407237483800673\n",
      "    At iteration 12500 -> loss: 0.09406738853075931\n",
      "    At iteration 12600 -> loss: 0.09405270227734697\n",
      "    At iteration 12700 -> loss: 0.09404464795209634\n",
      "    At iteration 12800 -> loss: 0.09402768569902546\n",
      "    At iteration 12900 -> loss: 0.09400116445581803\n",
      "    At iteration 13000 -> loss: 0.09402306342018459\n",
      "    At iteration 13100 -> loss: 0.09400088927878096\n",
      "    At iteration 13200 -> loss: 0.09400875646965226\n",
      "    At iteration 13300 -> loss: 0.0939945702789276\n",
      "    At iteration 13400 -> loss: 0.09400478549442323\n",
      "    At iteration 13500 -> loss: 0.0940334044833924\n",
      "    At iteration 13600 -> loss: 0.0940669900198537\n",
      "Staring Epoch 2\n",
      "    At iteration 0 -> loss: 0.09658492635935545\n",
      "    At iteration 100 -> loss: 0.09449098607103017\n",
      "    At iteration 200 -> loss: 0.09293379522678916\n",
      "    At iteration 300 -> loss: 0.09213228658691171\n",
      "    At iteration 400 -> loss: 0.09189558244462456\n",
      "    At iteration 500 -> loss: 0.0919517936678775\n",
      "    At iteration 600 -> loss: 0.09333641871344975\n",
      "    At iteration 700 -> loss: 0.09309331281842997\n",
      "    At iteration 800 -> loss: 0.0929191473887068\n",
      "    At iteration 900 -> loss: 0.09348517753350088\n",
      "    At iteration 1000 -> loss: 0.09370977425918055\n",
      "    At iteration 1100 -> loss: 0.09340439645626293\n",
      "    At iteration 1200 -> loss: 0.09364414493919594\n",
      "    At iteration 1300 -> loss: 0.09359486824145094\n",
      "    At iteration 1400 -> loss: 0.09365891910499705\n",
      "    At iteration 1500 -> loss: 0.09412502601764824\n",
      "    At iteration 1600 -> loss: 0.09383332772974685\n",
      "    At iteration 1700 -> loss: 0.09374499673442428\n",
      "    At iteration 1800 -> loss: 0.09368583117421725\n",
      "    At iteration 1900 -> loss: 0.09420528746588769\n",
      "    At iteration 2000 -> loss: 0.09398900723449476\n",
      "    At iteration 2100 -> loss: 0.09399422834642678\n",
      "    At iteration 2200 -> loss: 0.09408084827796932\n",
      "    At iteration 2300 -> loss: 0.09433621587116119\n",
      "    At iteration 2400 -> loss: 0.0942171413616676\n",
      "    At iteration 2500 -> loss: 0.09404093576307924\n",
      "    At iteration 2600 -> loss: 0.09408033593457223\n",
      "    At iteration 2700 -> loss: 0.09395362404177056\n",
      "    At iteration 2800 -> loss: 0.09391286634350038\n",
      "    At iteration 2900 -> loss: 0.09386514138473535\n",
      "    At iteration 3000 -> loss: 0.09397344384023892\n",
      "    At iteration 3100 -> loss: 0.09400331093109866\n",
      "    At iteration 3200 -> loss: 0.09393061377200597\n",
      "    At iteration 3300 -> loss: 0.09392256386686482\n",
      "    At iteration 3400 -> loss: 0.09394987897315937\n",
      "    At iteration 3500 -> loss: 0.09396356657459784\n",
      "    At iteration 3600 -> loss: 0.09387827453343839\n",
      "    At iteration 3700 -> loss: 0.09384823711542395\n",
      "    At iteration 3800 -> loss: 0.09413320019971456\n",
      "    At iteration 3900 -> loss: 0.09429687795129546\n",
      "    At iteration 4000 -> loss: 0.09419484302709907\n",
      "    At iteration 4100 -> loss: 0.09414483949073162\n",
      "    At iteration 4200 -> loss: 0.09408079830332552\n",
      "    At iteration 4300 -> loss: 0.09400814959696134\n",
      "    At iteration 4400 -> loss: 0.09405512669577398\n",
      "    At iteration 4500 -> loss: 0.09397396126067015\n",
      "    At iteration 4600 -> loss: 0.09403392309839348\n",
      "    At iteration 4700 -> loss: 0.0940385889011663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4800 -> loss: 0.09401194625010635\n",
      "    At iteration 4900 -> loss: 0.09394602939075251\n",
      "    At iteration 5000 -> loss: 0.09389216159266241\n",
      "    At iteration 5100 -> loss: 0.09383237737135802\n",
      "    At iteration 5200 -> loss: 0.09439931164496711\n",
      "    At iteration 5300 -> loss: 0.09433437981035003\n",
      "    At iteration 5400 -> loss: 0.0943803238498625\n",
      "    At iteration 5500 -> loss: 0.0943501925631405\n",
      "    At iteration 5600 -> loss: 0.09427373373952552\n",
      "    At iteration 5700 -> loss: 0.09423240193488405\n",
      "    At iteration 5800 -> loss: 0.09417116355873682\n",
      "    At iteration 5900 -> loss: 0.0941014419612463\n",
      "    At iteration 6000 -> loss: 0.09410050553730599\n",
      "    At iteration 6100 -> loss: 0.0941606481117776\n",
      "    At iteration 6200 -> loss: 0.09415677457513441\n",
      "    At iteration 6300 -> loss: 0.0941241311442257\n",
      "    At iteration 6400 -> loss: 0.09407283317698999\n",
      "    At iteration 6500 -> loss: 0.09411710195517221\n",
      "    At iteration 6600 -> loss: 0.09405295970536978\n",
      "    At iteration 6700 -> loss: 0.09406660023909044\n",
      "    At iteration 6800 -> loss: 0.09402515102037516\n",
      "    At iteration 6900 -> loss: 0.09403608058190051\n",
      "    At iteration 7000 -> loss: 0.09416303100044655\n",
      "    At iteration 7100 -> loss: 0.09413756913599264\n",
      "    At iteration 7200 -> loss: 0.09409023179999405\n",
      "    At iteration 7300 -> loss: 0.09404300378379206\n",
      "    At iteration 7400 -> loss: 0.09407008468053492\n",
      "    At iteration 7500 -> loss: 0.09406410741860641\n",
      "    At iteration 7600 -> loss: 0.09406771585985096\n",
      "    At iteration 7700 -> loss: 0.09415364767373663\n",
      "    At iteration 7800 -> loss: 0.09413767164042365\n",
      "    At iteration 7900 -> loss: 0.09410818390410798\n",
      "    At iteration 8000 -> loss: 0.09409060145169933\n",
      "    At iteration 8100 -> loss: 0.0940556420183305\n",
      "    At iteration 8200 -> loss: 0.09403689310487949\n",
      "    At iteration 8300 -> loss: 0.09399610974325187\n",
      "    At iteration 8400 -> loss: 0.0939920040580877\n",
      "    At iteration 8500 -> loss: 0.09399682606921159\n",
      "    At iteration 8600 -> loss: 0.09395157070749939\n",
      "    At iteration 8700 -> loss: 0.09401334117504652\n",
      "    At iteration 8800 -> loss: 0.0940778781266885\n",
      "    At iteration 8900 -> loss: 0.09404278308935424\n",
      "    At iteration 9000 -> loss: 0.0940574078169582\n",
      "    At iteration 9100 -> loss: 0.09403155549896304\n",
      "    At iteration 9200 -> loss: 0.09401579235646101\n",
      "    At iteration 9300 -> loss: 0.09397739914937829\n",
      "    At iteration 9400 -> loss: 0.09395043171372064\n",
      "    At iteration 9500 -> loss: 0.0939547465421056\n",
      "    At iteration 9600 -> loss: 0.09393554271857243\n",
      "    At iteration 9700 -> loss: 0.09396729801869165\n",
      "    At iteration 9800 -> loss: 0.09400419202305565\n",
      "    At iteration 9900 -> loss: 0.09400002207054602\n",
      "    At iteration 10000 -> loss: 0.09395838286843382\n",
      "    At iteration 10100 -> loss: 0.09393084183954205\n",
      "    At iteration 10200 -> loss: 0.09390731840043814\n",
      "    At iteration 10300 -> loss: 0.09390440295674615\n",
      "    At iteration 10400 -> loss: 0.0938763214479097\n",
      "    At iteration 10500 -> loss: 0.09385139809565268\n",
      "    At iteration 10600 -> loss: 0.0938169984222011\n",
      "    At iteration 10700 -> loss: 0.0938006796911004\n",
      "    At iteration 10800 -> loss: 0.09378523314521633\n",
      "    At iteration 10900 -> loss: 0.0937975033994432\n",
      "    At iteration 11000 -> loss: 0.09376799706567038\n",
      "    At iteration 11100 -> loss: 0.09395697835647988\n",
      "    At iteration 11200 -> loss: 0.09399095725679545\n",
      "    At iteration 11300 -> loss: 0.09396975368401109\n",
      "    At iteration 11400 -> loss: 0.09399057100170058\n",
      "    At iteration 11500 -> loss: 0.09396919632949671\n",
      "    At iteration 11600 -> loss: 0.09393395627799905\n",
      "    At iteration 11700 -> loss: 0.09390989888709146\n",
      "    At iteration 11800 -> loss: 0.09391063103790584\n",
      "    At iteration 11900 -> loss: 0.09393652713452248\n",
      "    At iteration 12000 -> loss: 0.0939307910351477\n",
      "    At iteration 12100 -> loss: 0.09391421698623573\n",
      "    At iteration 12200 -> loss: 0.09392353830565527\n",
      "    At iteration 12300 -> loss: 0.09391282803671182\n",
      "    At iteration 12400 -> loss: 0.09389754369086573\n",
      "    At iteration 12500 -> loss: 0.09391108129278768\n",
      "    At iteration 12600 -> loss: 0.09389201240063799\n",
      "    At iteration 12700 -> loss: 0.09389685404792561\n",
      "    At iteration 12800 -> loss: 0.09387043376425835\n",
      "    At iteration 12900 -> loss: 0.09386557569624887\n",
      "    At iteration 13000 -> loss: 0.09384115286442864\n",
      "    At iteration 13100 -> loss: 0.09381096507394819\n",
      "    At iteration 13200 -> loss: 0.09378586728001942\n",
      "    At iteration 13300 -> loss: 0.09377748928100307\n",
      "    At iteration 13400 -> loss: 0.09376929853380099\n",
      "    At iteration 13500 -> loss: 0.0937536137338732\n",
      "    At iteration 13600 -> loss: 0.09374724152658916\n",
      "Staring Epoch 3\n",
      "    At iteration 0 -> loss: 0.08077044926176313\n",
      "    At iteration 100 -> loss: 0.09389905682288409\n",
      "    At iteration 200 -> loss: 0.09230052447824513\n",
      "    At iteration 300 -> loss: 0.09243679965464911\n",
      "    At iteration 400 -> loss: 0.09235389015662589\n",
      "    At iteration 500 -> loss: 0.09165737055738035\n",
      "    At iteration 600 -> loss: 0.09200964693744379\n",
      "    At iteration 700 -> loss: 0.09174019071176402\n",
      "    At iteration 800 -> loss: 0.09190640436931016\n",
      "    At iteration 900 -> loss: 0.09231048546781401\n",
      "    At iteration 1000 -> loss: 0.09223069146498915\n",
      "    At iteration 1100 -> loss: 0.09208943124214838\n",
      "    At iteration 1200 -> loss: 0.09210911025737882\n",
      "    At iteration 1300 -> loss: 0.0921970654266526\n",
      "    At iteration 1400 -> loss: 0.09226956508796501\n",
      "    At iteration 1500 -> loss: 0.09242133773671743\n",
      "    At iteration 1600 -> loss: 0.09227194370245732\n",
      "    At iteration 1700 -> loss: 0.09241617139783766\n",
      "    At iteration 1800 -> loss: 0.09225719242196113\n",
      "    At iteration 1900 -> loss: 0.09287367966168272\n",
      "    At iteration 2000 -> loss: 0.0927860139317006\n",
      "    At iteration 2100 -> loss: 0.09272972390592966\n",
      "    At iteration 2200 -> loss: 0.09268485820512186\n",
      "    At iteration 2300 -> loss: 0.09266552048725332\n",
      "    At iteration 2400 -> loss: 0.092625993971067\n",
      "    At iteration 2500 -> loss: 0.09263411394861572\n",
      "    At iteration 2600 -> loss: 0.09265513301306619\n",
      "    At iteration 2700 -> loss: 0.09273603440482346\n",
      "    At iteration 2800 -> loss: 0.09288150278072346\n",
      "    At iteration 2900 -> loss: 0.09281150864628694\n",
      "    At iteration 3000 -> loss: 0.09272050227769864\n",
      "    At iteration 3100 -> loss: 0.09301511864265993\n",
      "    At iteration 3200 -> loss: 0.0930051449722683\n",
      "    At iteration 3300 -> loss: 0.09314065535609937\n",
      "    At iteration 3400 -> loss: 0.09402738609748154\n",
      "    At iteration 3500 -> loss: 0.09427824320970458\n",
      "    At iteration 3600 -> loss: 0.09419799842720164\n",
      "    At iteration 3700 -> loss: 0.09409949996916535\n",
      "    At iteration 3800 -> loss: 0.0942883938616637\n",
      "    At iteration 3900 -> loss: 0.09432412633970401\n",
      "    At iteration 4000 -> loss: 0.09424309331659372\n",
      "    At iteration 4100 -> loss: 0.0942767629537379\n",
      "    At iteration 4200 -> loss: 0.09419619887158584\n",
      "    At iteration 4300 -> loss: 0.09411005542681476\n",
      "    At iteration 4400 -> loss: 0.09402450891528417\n",
      "    At iteration 4500 -> loss: 0.09401527369598246\n",
      "    At iteration 4600 -> loss: 0.09398139358611518\n",
      "    At iteration 4700 -> loss: 0.09411640572270628\n",
      "    At iteration 4800 -> loss: 0.09408908455870062\n",
      "    At iteration 4900 -> loss: 0.09402899732937273\n",
      "    At iteration 5000 -> loss: 0.09402453338842677\n",
      "    At iteration 5100 -> loss: 0.09399362051660948\n",
      "    At iteration 5200 -> loss: 0.09391611250855543\n",
      "    At iteration 5300 -> loss: 0.09383970293050554\n",
      "    At iteration 5400 -> loss: 0.09376958202327118\n",
      "    At iteration 5500 -> loss: 0.0937092878304074\n",
      "    At iteration 5600 -> loss: 0.09370863352308774\n",
      "    At iteration 5700 -> loss: 0.09372947350448733\n",
      "    At iteration 5800 -> loss: 0.093693717346814\n",
      "    At iteration 5900 -> loss: 0.09373074571736298\n",
      "    At iteration 6000 -> loss: 0.0936795326354374\n",
      "    At iteration 6100 -> loss: 0.09366584013798386\n",
      "    At iteration 6200 -> loss: 0.09358566949318209\n",
      "    At iteration 6300 -> loss: 0.09352637053680907\n",
      "    At iteration 6400 -> loss: 0.0935355066257435\n",
      "    At iteration 6500 -> loss: 0.09353261892396161\n",
      "    At iteration 6600 -> loss: 0.09362535669137391\n",
      "    At iteration 6700 -> loss: 0.09361696390950155\n",
      "    At iteration 6800 -> loss: 0.09366413480960668\n",
      "    At iteration 6900 -> loss: 0.09363346928143962\n",
      "    At iteration 7000 -> loss: 0.09358139961361493\n",
      "    At iteration 7100 -> loss: 0.09367496429784627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7200 -> loss: 0.09371123158813657\n",
      "    At iteration 7300 -> loss: 0.09370196250910544\n",
      "    At iteration 7400 -> loss: 0.0936935633051126\n",
      "    At iteration 7500 -> loss: 0.09365544317966232\n",
      "    At iteration 7600 -> loss: 0.0936892503210347\n",
      "    At iteration 7700 -> loss: 0.09368052318126036\n",
      "    At iteration 7800 -> loss: 0.09366496011247477\n",
      "    At iteration 7900 -> loss: 0.0936545923074178\n",
      "    At iteration 8000 -> loss: 0.09362837180985764\n",
      "    At iteration 8100 -> loss: 0.09362989658437375\n",
      "    At iteration 8200 -> loss: 0.09360126691528467\n",
      "    At iteration 8300 -> loss: 0.09358157825833482\n",
      "    At iteration 8400 -> loss: 0.09357765360601573\n",
      "    At iteration 8500 -> loss: 0.09357116712061941\n",
      "    At iteration 8600 -> loss: 0.0935465390429126\n",
      "    At iteration 8700 -> loss: 0.0935631253708343\n",
      "    At iteration 8800 -> loss: 0.09354827675958309\n",
      "    At iteration 8900 -> loss: 0.09352017126361097\n",
      "    At iteration 9000 -> loss: 0.09364314522111906\n",
      "    At iteration 9100 -> loss: 0.09362137506673228\n",
      "    At iteration 9200 -> loss: 0.09357637627314444\n",
      "    At iteration 9300 -> loss: 0.09359871758246163\n",
      "    At iteration 9400 -> loss: 0.09364922709870678\n",
      "    At iteration 9500 -> loss: 0.09362163943693602\n",
      "    At iteration 9600 -> loss: 0.09360425664966623\n",
      "    At iteration 9700 -> loss: 0.09360269851823705\n",
      "    At iteration 9800 -> loss: 0.09355160410974284\n",
      "    At iteration 9900 -> loss: 0.09352654588812967\n",
      "    At iteration 10000 -> loss: 0.09350494602302949\n",
      "    At iteration 10100 -> loss: 0.09348120796196929\n",
      "    At iteration 10200 -> loss: 0.09346180178346275\n",
      "    At iteration 10300 -> loss: 0.09342544717515829\n",
      "    At iteration 10400 -> loss: 0.09341604980016538\n",
      "    At iteration 10500 -> loss: 0.09347165203064706\n",
      "    At iteration 10600 -> loss: 0.09346778555079042\n",
      "    At iteration 10700 -> loss: 0.09343143738214982\n",
      "    At iteration 10800 -> loss: 0.09342400452244161\n",
      "    At iteration 10900 -> loss: 0.09350540330964066\n",
      "    At iteration 11000 -> loss: 0.09355496619519443\n",
      "    At iteration 11100 -> loss: 0.09359648968807043\n",
      "    At iteration 11200 -> loss: 0.09358996712740035\n",
      "    At iteration 11300 -> loss: 0.09360113644381277\n",
      "    At iteration 11400 -> loss: 0.09357728788807218\n",
      "    At iteration 11500 -> loss: 0.0935638527068081\n",
      "    At iteration 11600 -> loss: 0.09356287556019448\n",
      "    At iteration 11700 -> loss: 0.09355708866141037\n",
      "    At iteration 11800 -> loss: 0.09358829662315553\n",
      "    At iteration 11900 -> loss: 0.09362502110427932\n",
      "    At iteration 12000 -> loss: 0.09365734726503118\n",
      "    At iteration 12100 -> loss: 0.09364308935435928\n",
      "    At iteration 12200 -> loss: 0.09361978095828664\n",
      "    At iteration 12300 -> loss: 0.09358832630336267\n",
      "    At iteration 12400 -> loss: 0.09356029325964321\n",
      "    At iteration 12500 -> loss: 0.09352712731076261\n",
      "    At iteration 12600 -> loss: 0.09352612385812256\n",
      "    At iteration 12700 -> loss: 0.09350408807676926\n",
      "    At iteration 12800 -> loss: 0.09349719154148542\n",
      "    At iteration 12900 -> loss: 0.09347600256536676\n",
      "    At iteration 13000 -> loss: 0.09351255395756083\n",
      "    At iteration 13100 -> loss: 0.09348911044041483\n",
      "    At iteration 13200 -> loss: 0.09354954914388096\n",
      "    At iteration 13300 -> loss: 0.09352215084936265\n",
      "    At iteration 13400 -> loss: 0.09351553163060868\n",
      "    At iteration 13500 -> loss: 0.09350471276420282\n",
      "    At iteration 13600 -> loss: 0.09355363680059577\n",
      "Staring Epoch 4\n",
      "    At iteration 0 -> loss: 0.08276087627746165\n",
      "    At iteration 100 -> loss: 0.10270754087400352\n",
      "    At iteration 200 -> loss: 0.09699536711382045\n",
      "    At iteration 300 -> loss: 0.09577443955645853\n",
      "    At iteration 400 -> loss: 0.09692332358981252\n",
      "    At iteration 500 -> loss: 0.096141212305248\n",
      "    At iteration 600 -> loss: 0.09634688044227797\n",
      "    At iteration 700 -> loss: 0.09550357947432851\n",
      "    At iteration 800 -> loss: 0.09484390012817742\n",
      "    At iteration 900 -> loss: 0.0950197223842344\n",
      "    At iteration 1000 -> loss: 0.0948309321325785\n",
      "    At iteration 1100 -> loss: 0.0944588287772258\n",
      "    At iteration 1200 -> loss: 0.09435596630120219\n",
      "    At iteration 1300 -> loss: 0.09403302100658119\n",
      "    At iteration 1400 -> loss: 0.09397299099765263\n",
      "    At iteration 1500 -> loss: 0.09380680903614821\n",
      "    At iteration 1600 -> loss: 0.09349448849726798\n",
      "    At iteration 1700 -> loss: 0.09348153082429381\n",
      "    At iteration 1800 -> loss: 0.09337101698325825\n",
      "    At iteration 1900 -> loss: 0.09322207795044223\n",
      "    At iteration 2000 -> loss: 0.09312880297533911\n",
      "    At iteration 2100 -> loss: 0.09329916837600911\n",
      "    At iteration 2200 -> loss: 0.09320071980428168\n",
      "    At iteration 2300 -> loss: 0.0932567604567188\n",
      "    At iteration 2400 -> loss: 0.09324855670917978\n",
      "    At iteration 2500 -> loss: 0.09318803809471377\n",
      "    At iteration 2600 -> loss: 0.09310851574556239\n",
      "    At iteration 2700 -> loss: 0.0930474878005137\n",
      "    At iteration 2800 -> loss: 0.0929552081103657\n",
      "    At iteration 2900 -> loss: 0.09289298663730708\n",
      "    At iteration 3000 -> loss: 0.0928201927333563\n",
      "    At iteration 3100 -> loss: 0.09299395399153834\n",
      "    At iteration 3200 -> loss: 0.09294962387893867\n",
      "    At iteration 3300 -> loss: 0.09314077274917022\n",
      "    At iteration 3400 -> loss: 0.09313860191856409\n",
      "    At iteration 3500 -> loss: 0.09309608709794026\n",
      "    At iteration 3600 -> loss: 0.09311948740152894\n",
      "    At iteration 3700 -> loss: 0.09305092575255657\n",
      "    At iteration 3800 -> loss: 0.09382387261347469\n",
      "    At iteration 3900 -> loss: 0.0937501095178561\n",
      "    At iteration 4000 -> loss: 0.09368533522182643\n",
      "    At iteration 4100 -> loss: 0.09360707211946297\n",
      "    At iteration 4200 -> loss: 0.09352576037669026\n",
      "    At iteration 4300 -> loss: 0.09349881650135725\n",
      "    At iteration 4400 -> loss: 0.09344155333224878\n",
      "    At iteration 4500 -> loss: 0.09345061714695738\n",
      "    At iteration 4600 -> loss: 0.09342774686719249\n",
      "    At iteration 4700 -> loss: 0.0933987451097856\n",
      "    At iteration 4800 -> loss: 0.09356685937372929\n",
      "    At iteration 4900 -> loss: 0.09357081554245554\n",
      "    At iteration 5000 -> loss: 0.09360489751595667\n",
      "    At iteration 5100 -> loss: 0.0935792236570919\n",
      "    At iteration 5200 -> loss: 0.09364018675822157\n",
      "    At iteration 5300 -> loss: 0.09357097845335728\n",
      "    At iteration 5400 -> loss: 0.09360405369751601\n",
      "    At iteration 5500 -> loss: 0.09357276918998884\n",
      "    At iteration 5600 -> loss: 0.09349679433622261\n",
      "    At iteration 5700 -> loss: 0.0934856926975414\n",
      "    At iteration 5800 -> loss: 0.09348736693626902\n",
      "    At iteration 5900 -> loss: 0.0934750429884216\n",
      "    At iteration 6000 -> loss: 0.09348998900836823\n",
      "    At iteration 6100 -> loss: 0.09347019238531555\n",
      "    At iteration 6200 -> loss: 0.09347736038761026\n",
      "    At iteration 6300 -> loss: 0.09345265206896826\n",
      "    At iteration 6400 -> loss: 0.09344434530494088\n",
      "    At iteration 6500 -> loss: 0.09337974062603119\n",
      "    At iteration 6600 -> loss: 0.09340269414052257\n",
      "    At iteration 6700 -> loss: 0.09335377769006663\n",
      "    At iteration 6800 -> loss: 0.09335138584861301\n",
      "    At iteration 6900 -> loss: 0.09337665925147058\n",
      "    At iteration 7000 -> loss: 0.09335462969038108\n",
      "    At iteration 7100 -> loss: 0.09330257277156515\n",
      "    At iteration 7200 -> loss: 0.0932844253154249\n",
      "    At iteration 7300 -> loss: 0.09329594919793179\n",
      "    At iteration 7400 -> loss: 0.09328773549007546\n",
      "    At iteration 7500 -> loss: 0.09326132885839826\n",
      "    At iteration 7600 -> loss: 0.0932358750667722\n",
      "    At iteration 7700 -> loss: 0.09325571732307551\n",
      "    At iteration 7800 -> loss: 0.09324249497099872\n",
      "    At iteration 7900 -> loss: 0.09323884637256519\n",
      "    At iteration 8000 -> loss: 0.09323216922288091\n",
      "    At iteration 8100 -> loss: 0.09323971649267332\n",
      "    At iteration 8200 -> loss: 0.09323040226824116\n",
      "    At iteration 8300 -> loss: 0.09335054419770816\n",
      "    At iteration 8400 -> loss: 0.09334929861661902\n",
      "    At iteration 8500 -> loss: 0.09330404660697636\n",
      "    At iteration 8600 -> loss: 0.09331367383289445\n",
      "    At iteration 8700 -> loss: 0.09326584226754916\n",
      "    At iteration 8800 -> loss: 0.09327755664653334\n",
      "    At iteration 8900 -> loss: 0.09324369134405087\n",
      "    At iteration 9000 -> loss: 0.09331610865864762\n",
      "    At iteration 9100 -> loss: 0.09335424898155527\n",
      "    At iteration 9200 -> loss: 0.09331414378494668\n",
      "    At iteration 9300 -> loss: 0.09330524677614017\n",
      "    At iteration 9400 -> loss: 0.09331283117725679\n",
      "    At iteration 9500 -> loss: 0.09329939415676498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9600 -> loss: 0.09333613624400545\n",
      "    At iteration 9700 -> loss: 0.0933194009512025\n",
      "    At iteration 9800 -> loss: 0.09341233375392165\n",
      "    At iteration 9900 -> loss: 0.09342677368762722\n",
      "    At iteration 10000 -> loss: 0.09342078560579135\n",
      "    At iteration 10100 -> loss: 0.09340312182555123\n",
      "    At iteration 10200 -> loss: 0.09338090977032752\n",
      "    At iteration 10300 -> loss: 0.09335139973602932\n",
      "    At iteration 10400 -> loss: 0.09333599313306479\n",
      "    At iteration 10500 -> loss: 0.09337434458717933\n",
      "    At iteration 10600 -> loss: 0.09336153613010634\n",
      "    At iteration 10700 -> loss: 0.09333771213169452\n",
      "    At iteration 10800 -> loss: 0.09335134216611711\n",
      "    At iteration 10900 -> loss: 0.09337228178975117\n",
      "    At iteration 11000 -> loss: 0.0933450418273812\n",
      "    At iteration 11100 -> loss: 0.09332998821750595\n",
      "    At iteration 11200 -> loss: 0.0933347318772354\n",
      "    At iteration 11300 -> loss: 0.0933143116991248\n",
      "    At iteration 11400 -> loss: 0.09329614636067127\n",
      "    At iteration 11500 -> loss: 0.09335740333107431\n",
      "    At iteration 11600 -> loss: 0.09339625016848353\n",
      "    At iteration 11700 -> loss: 0.09339123143032563\n",
      "    At iteration 11800 -> loss: 0.09340424919891806\n",
      "    At iteration 11900 -> loss: 0.09345099018630773\n",
      "    At iteration 12000 -> loss: 0.09345814967909583\n",
      "    At iteration 12100 -> loss: 0.09344767490953676\n",
      "    At iteration 12200 -> loss: 0.09346384702246473\n",
      "    At iteration 12300 -> loss: 0.09345919156720307\n",
      "    At iteration 12400 -> loss: 0.09343218569554784\n",
      "    At iteration 12500 -> loss: 0.09344554433938553\n",
      "    At iteration 12600 -> loss: 0.09346457339210915\n",
      "    At iteration 12700 -> loss: 0.09344075416186431\n",
      "    At iteration 12800 -> loss: 0.0934341189815995\n",
      "    At iteration 12900 -> loss: 0.09341534995020305\n",
      "    At iteration 13000 -> loss: 0.09349114328385018\n",
      "    At iteration 13100 -> loss: 0.09347784866589608\n",
      "    At iteration 13200 -> loss: 0.0934743432112742\n",
      "    At iteration 13300 -> loss: 0.09347473356357187\n",
      "    At iteration 13400 -> loss: 0.09345339108312527\n",
      "    At iteration 13500 -> loss: 0.09343575263059585\n",
      "    At iteration 13600 -> loss: 0.0934551650372969\n",
      "Staring Epoch 5\n",
      "    At iteration 0 -> loss: 0.10466322768479586\n",
      "    At iteration 100 -> loss: 0.08955626841817312\n",
      "    At iteration 200 -> loss: 0.09179209886498209\n",
      "    At iteration 300 -> loss: 0.09311136492087106\n",
      "    At iteration 400 -> loss: 0.09219412745892541\n",
      "    At iteration 500 -> loss: 0.09335265564351063\n",
      "    At iteration 600 -> loss: 0.09352318717634035\n",
      "    At iteration 700 -> loss: 0.09325329367568158\n",
      "    At iteration 800 -> loss: 0.09304943344163139\n",
      "    At iteration 900 -> loss: 0.09293157371902239\n",
      "    At iteration 1000 -> loss: 0.09273584408967432\n",
      "    At iteration 1100 -> loss: 0.09296638426985789\n",
      "    At iteration 1200 -> loss: 0.0929182228629859\n",
      "    At iteration 1300 -> loss: 0.09302541017704645\n",
      "    At iteration 1400 -> loss: 0.09335251355447344\n",
      "    At iteration 1500 -> loss: 0.0932146847145706\n",
      "    At iteration 1600 -> loss: 0.09341989201169465\n",
      "    At iteration 1700 -> loss: 0.09344239079393565\n",
      "    At iteration 1800 -> loss: 0.09321073600500439\n",
      "    At iteration 1900 -> loss: 0.09309120267734654\n",
      "    At iteration 2000 -> loss: 0.09310407704339604\n",
      "    At iteration 2100 -> loss: 0.09320228362520018\n",
      "    At iteration 2200 -> loss: 0.09314594097819669\n",
      "    At iteration 2300 -> loss: 0.09309727755972882\n",
      "    At iteration 2400 -> loss: 0.09295801473166068\n",
      "    At iteration 2500 -> loss: 0.09280759618362056\n",
      "    At iteration 2600 -> loss: 0.09283613199850833\n",
      "    At iteration 2700 -> loss: 0.09285126310480377\n",
      "    At iteration 2800 -> loss: 0.0928595130037885\n",
      "    At iteration 2900 -> loss: 0.09303288004537488\n",
      "    At iteration 3000 -> loss: 0.09298768470451593\n",
      "    At iteration 3100 -> loss: 0.09293249694733258\n",
      "    At iteration 3200 -> loss: 0.0928736235545547\n",
      "    At iteration 3300 -> loss: 0.0928731120617654\n",
      "    At iteration 3400 -> loss: 0.0928666171902804\n",
      "    At iteration 3500 -> loss: 0.09285795951931279\n",
      "    At iteration 3600 -> loss: 0.09296121405566424\n",
      "    At iteration 3700 -> loss: 0.09299639974810568\n",
      "    At iteration 3800 -> loss: 0.09289143536041304\n",
      "    At iteration 3900 -> loss: 0.09282182493665593\n",
      "    At iteration 4000 -> loss: 0.09274375749973221\n",
      "    At iteration 4100 -> loss: 0.09276721343838558\n",
      "    At iteration 4200 -> loss: 0.09272783926501771\n",
      "    At iteration 4300 -> loss: 0.09266002858931711\n",
      "    At iteration 4400 -> loss: 0.0927500539279675\n",
      "    At iteration 4500 -> loss: 0.0927686087511469\n",
      "    At iteration 4600 -> loss: 0.09273442974036418\n",
      "    At iteration 4700 -> loss: 0.09268365138052877\n",
      "    At iteration 4800 -> loss: 0.09272945791531062\n",
      "    At iteration 4900 -> loss: 0.09271694429497562\n",
      "    At iteration 5000 -> loss: 0.09273040378758761\n",
      "    At iteration 5100 -> loss: 0.09270862670538572\n",
      "    At iteration 5200 -> loss: 0.09263453895730216\n",
      "    At iteration 5300 -> loss: 0.09273805202674029\n",
      "    At iteration 5400 -> loss: 0.09285702302466858\n",
      "    At iteration 5500 -> loss: 0.09282799678258191\n",
      "    At iteration 5600 -> loss: 0.09280667698944602\n",
      "    At iteration 5700 -> loss: 0.09277480007541782\n",
      "    At iteration 5800 -> loss: 0.09278310423108094\n",
      "    At iteration 5900 -> loss: 0.09271688256849109\n",
      "    At iteration 6000 -> loss: 0.09269118638162142\n",
      "    At iteration 6100 -> loss: 0.09271371264911152\n",
      "    At iteration 6200 -> loss: 0.09269733404593843\n",
      "    At iteration 6300 -> loss: 0.09266824577903256\n",
      "    At iteration 6400 -> loss: 0.09265360038091326\n",
      "    At iteration 6500 -> loss: 0.09269817183043734\n",
      "    At iteration 6600 -> loss: 0.09268217897331679\n",
      "    At iteration 6700 -> loss: 0.09263485462108537\n",
      "    At iteration 6800 -> loss: 0.09261315840465557\n",
      "    At iteration 6900 -> loss: 0.09256639233664436\n",
      "    At iteration 7000 -> loss: 0.09253773966417064\n",
      "    At iteration 7100 -> loss: 0.09257615740857535\n",
      "    At iteration 7200 -> loss: 0.09262849044214151\n",
      "    At iteration 7300 -> loss: 0.09261739316743987\n",
      "    At iteration 7400 -> loss: 0.09258992556730877\n",
      "    At iteration 7500 -> loss: 0.09257071383140694\n",
      "    At iteration 7600 -> loss: 0.0926073091934356\n",
      "    At iteration 7700 -> loss: 0.09274027964593613\n",
      "    At iteration 7800 -> loss: 0.09271639090341127\n",
      "    At iteration 7900 -> loss: 0.09267284788805225\n",
      "    At iteration 8000 -> loss: 0.09264354576510386\n",
      "    At iteration 8100 -> loss: 0.09262689122876414\n",
      "    At iteration 8200 -> loss: 0.0926809560202933\n",
      "    At iteration 8300 -> loss: 0.09273815774313711\n",
      "    At iteration 8400 -> loss: 0.09276118960418041\n",
      "    At iteration 8500 -> loss: 0.09281791500792066\n",
      "    At iteration 8600 -> loss: 0.09282358713826892\n",
      "    At iteration 8700 -> loss: 0.09281353079119091\n",
      "    At iteration 8800 -> loss: 0.09277838848305721\n",
      "    At iteration 8900 -> loss: 0.09310561422231965\n",
      "    At iteration 9000 -> loss: 0.09305194760585203\n",
      "    At iteration 9100 -> loss: 0.09306338093649982\n",
      "    At iteration 9200 -> loss: 0.0931401583802436\n",
      "    At iteration 9300 -> loss: 0.09310394566861288\n",
      "    At iteration 9400 -> loss: 0.09314238291096005\n",
      "    At iteration 9500 -> loss: 0.09309983722986824\n",
      "    At iteration 9600 -> loss: 0.09309146611571494\n",
      "    At iteration 9700 -> loss: 0.09308482411563025\n",
      "    At iteration 9800 -> loss: 0.09309333959963602\n",
      "    At iteration 9900 -> loss: 0.09306397526958449\n",
      "    At iteration 10000 -> loss: 0.0930455714701927\n",
      "    At iteration 10100 -> loss: 0.09305951995773945\n",
      "    At iteration 10200 -> loss: 0.09304350065094065\n",
      "    At iteration 10300 -> loss: 0.09311066838701208\n",
      "    At iteration 10400 -> loss: 0.09310146605726584\n",
      "    At iteration 10500 -> loss: 0.09310849656348355\n",
      "    At iteration 10600 -> loss: 0.09313765392027645\n",
      "    At iteration 10700 -> loss: 0.09319922343075608\n",
      "    At iteration 10800 -> loss: 0.0931895829943184\n",
      "    At iteration 10900 -> loss: 0.0932502389747785\n",
      "    At iteration 11000 -> loss: 0.09322039352466695\n",
      "    At iteration 11100 -> loss: 0.09335288838367993\n",
      "    At iteration 11200 -> loss: 0.0933995781925861\n",
      "    At iteration 11300 -> loss: 0.09339520203962172\n",
      "    At iteration 11400 -> loss: 0.09337006236373374\n",
      "    At iteration 11500 -> loss: 0.09335241688324795\n",
      "    At iteration 11600 -> loss: 0.09332486195656974\n",
      "    At iteration 11700 -> loss: 0.09329532560145545\n",
      "    At iteration 11800 -> loss: 0.09328898314912955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11900 -> loss: 0.09327033215753566\n",
      "    At iteration 12000 -> loss: 0.09330776755962765\n",
      "    At iteration 12100 -> loss: 0.09331787031560923\n",
      "    At iteration 12200 -> loss: 0.09328010297373845\n",
      "    At iteration 12300 -> loss: 0.0932642724480857\n",
      "    At iteration 12400 -> loss: 0.09322379665699063\n",
      "    At iteration 12500 -> loss: 0.09321200948168012\n",
      "    At iteration 12600 -> loss: 0.09322824995413904\n",
      "    At iteration 12700 -> loss: 0.09324992581650371\n",
      "    At iteration 12800 -> loss: 0.09322116610065141\n",
      "    At iteration 12900 -> loss: 0.09328141254237779\n",
      "    At iteration 13000 -> loss: 0.09328401526658352\n",
      "    At iteration 13100 -> loss: 0.09327217646487934\n",
      "    At iteration 13200 -> loss: 0.09327924039216634\n",
      "    At iteration 13300 -> loss: 0.09327741132647537\n",
      "    At iteration 13400 -> loss: 0.0933012574150635\n",
      "    At iteration 13500 -> loss: 0.09327355853531016\n",
      "    At iteration 13600 -> loss: 0.09329171426715431\n",
      "Staring Epoch 6\n",
      "    At iteration 0 -> loss: 0.10213861707597971\n",
      "    At iteration 100 -> loss: 0.09052233409250103\n",
      "    At iteration 200 -> loss: 0.09026154032856605\n",
      "    At iteration 300 -> loss: 0.09020058695304631\n",
      "    At iteration 400 -> loss: 0.09057947765712886\n",
      "    At iteration 500 -> loss: 0.0906270667459066\n",
      "    At iteration 600 -> loss: 0.09071836700365886\n",
      "    At iteration 700 -> loss: 0.09060043214072167\n",
      "    At iteration 800 -> loss: 0.09099291284108373\n",
      "    At iteration 900 -> loss: 0.09076195929877867\n",
      "    At iteration 1000 -> loss: 0.090874639026104\n",
      "    At iteration 1100 -> loss: 0.09107879099938009\n",
      "    At iteration 1200 -> loss: 0.09089880571406038\n",
      "    At iteration 1300 -> loss: 0.0911294046536193\n",
      "    At iteration 1400 -> loss: 0.09181641368442499\n",
      "    At iteration 1500 -> loss: 0.09185158202855855\n",
      "    At iteration 1600 -> loss: 0.09186202957308708\n",
      "    At iteration 1700 -> loss: 0.0917534058839895\n",
      "    At iteration 1800 -> loss: 0.09167779157118969\n",
      "    At iteration 1900 -> loss: 0.09168378032770552\n",
      "    At iteration 2000 -> loss: 0.09162649823186006\n",
      "    At iteration 2100 -> loss: 0.09159703632571485\n",
      "    At iteration 2200 -> loss: 0.09164390751794428\n",
      "    At iteration 2300 -> loss: 0.09151389771401926\n",
      "    At iteration 2400 -> loss: 0.09162569222681732\n",
      "    At iteration 2500 -> loss: 0.09158722062384812\n",
      "    At iteration 2600 -> loss: 0.09161712424130809\n",
      "    At iteration 2700 -> loss: 0.09168719980024768\n",
      "    At iteration 2800 -> loss: 0.09174365329073146\n",
      "    At iteration 2900 -> loss: 0.0916518209874977\n",
      "    At iteration 3000 -> loss: 0.09168542270806837\n",
      "    At iteration 3100 -> loss: 0.09171229389894282\n",
      "    At iteration 3200 -> loss: 0.09185202551196428\n",
      "    At iteration 3300 -> loss: 0.09193506045490164\n",
      "    At iteration 3400 -> loss: 0.09207626185404881\n",
      "    At iteration 3500 -> loss: 0.09209932741697331\n",
      "    At iteration 3600 -> loss: 0.09217717539786512\n",
      "    At iteration 3700 -> loss: 0.09213867640314541\n",
      "    At iteration 3800 -> loss: 0.09217066168470735\n",
      "    At iteration 3900 -> loss: 0.0922095905065513\n",
      "    At iteration 4000 -> loss: 0.09213302608351066\n",
      "    At iteration 4100 -> loss: 0.09208556988405052\n",
      "    At iteration 4200 -> loss: 0.09206736836731377\n",
      "    At iteration 4300 -> loss: 0.0920634671721781\n",
      "    At iteration 4400 -> loss: 0.09209307458715756\n",
      "    At iteration 4500 -> loss: 0.09205165086244821\n",
      "    At iteration 4600 -> loss: 0.09209221372804834\n",
      "    At iteration 4700 -> loss: 0.0921145909384792\n",
      "    At iteration 4800 -> loss: 0.09223148772952344\n",
      "    At iteration 4900 -> loss: 0.09222221678926526\n",
      "    At iteration 5000 -> loss: 0.09221823216174067\n",
      "    At iteration 5100 -> loss: 0.09225312560220358\n",
      "    At iteration 5200 -> loss: 0.09223262453924475\n",
      "    At iteration 5300 -> loss: 0.09221446207464183\n",
      "    At iteration 5400 -> loss: 0.09216521732944456\n",
      "    At iteration 5500 -> loss: 0.09213100851869777\n",
      "    At iteration 5600 -> loss: 0.0921809999146094\n",
      "    At iteration 5700 -> loss: 0.09214933895652778\n",
      "    At iteration 5800 -> loss: 0.09212148804702354\n",
      "    At iteration 5900 -> loss: 0.09208068315060779\n",
      "    At iteration 6000 -> loss: 0.09222170448571491\n",
      "    At iteration 6100 -> loss: 0.09219829507459489\n",
      "    At iteration 6200 -> loss: 0.09215907472445052\n",
      "    At iteration 6300 -> loss: 0.09220284541378518\n",
      "    At iteration 6400 -> loss: 0.09217038084560333\n",
      "    At iteration 6500 -> loss: 0.09214795753726712\n",
      "    At iteration 6600 -> loss: 0.09236638267951817\n",
      "    At iteration 6700 -> loss: 0.09232202874638623\n",
      "    At iteration 6800 -> loss: 0.09287010788637733\n",
      "    At iteration 6900 -> loss: 0.0928287377974233\n",
      "    At iteration 7000 -> loss: 0.0929721705363358\n",
      "    At iteration 7100 -> loss: 0.0929552338056124\n",
      "    At iteration 7200 -> loss: 0.0930049350579214\n",
      "    At iteration 7300 -> loss: 0.0930159843756157\n",
      "    At iteration 7400 -> loss: 0.0930061653367139\n",
      "    At iteration 7500 -> loss: 0.09306072643433262\n",
      "    At iteration 7600 -> loss: 0.09303780702555502\n",
      "    At iteration 7700 -> loss: 0.09299811308289238\n",
      "    At iteration 7800 -> loss: 0.0929918270748258\n",
      "    At iteration 7900 -> loss: 0.09299663053865763\n",
      "    At iteration 8000 -> loss: 0.09298773215952473\n",
      "    At iteration 8100 -> loss: 0.0929843854613638\n",
      "    At iteration 8200 -> loss: 0.09303315269474499\n",
      "    At iteration 8300 -> loss: 0.09300948956751656\n",
      "    At iteration 8400 -> loss: 0.09307614950052019\n",
      "    At iteration 8500 -> loss: 0.09306921638772847\n",
      "    At iteration 8600 -> loss: 0.09304343676882902\n",
      "    At iteration 8700 -> loss: 0.09301202691022234\n",
      "    At iteration 8800 -> loss: 0.09296473091436634\n",
      "    At iteration 8900 -> loss: 0.09293942933661972\n",
      "    At iteration 9000 -> loss: 0.09305122505201233\n",
      "    At iteration 9100 -> loss: 0.09307061168046991\n",
      "    At iteration 9200 -> loss: 0.09304636729241446\n",
      "    At iteration 9300 -> loss: 0.09303658157244889\n",
      "    At iteration 9400 -> loss: 0.09303526560519669\n",
      "    At iteration 9500 -> loss: 0.0930603066254112\n",
      "    At iteration 9600 -> loss: 0.09303480110174853\n",
      "    At iteration 9700 -> loss: 0.0930397805873449\n",
      "    At iteration 9800 -> loss: 0.09306588902075183\n",
      "    At iteration 9900 -> loss: 0.09305899816893873\n",
      "    At iteration 10000 -> loss: 0.09306002732891161\n",
      "    At iteration 10100 -> loss: 0.0930852822711633\n",
      "    At iteration 10200 -> loss: 0.09306515866137356\n",
      "    At iteration 10300 -> loss: 0.09309785723166743\n",
      "    At iteration 10400 -> loss: 0.0930906859817013\n",
      "    At iteration 10500 -> loss: 0.09307550124400954\n",
      "    At iteration 10600 -> loss: 0.09311393541752916\n",
      "    At iteration 10700 -> loss: 0.09309222664390505\n",
      "    At iteration 10800 -> loss: 0.0930860272026601\n",
      "    At iteration 10900 -> loss: 0.09309544579923419\n",
      "    At iteration 11000 -> loss: 0.09307624824417002\n",
      "    At iteration 11100 -> loss: 0.09311020369627605\n",
      "    At iteration 11200 -> loss: 0.0931145533337473\n",
      "    At iteration 11300 -> loss: 0.09308467597812234\n",
      "    At iteration 11400 -> loss: 0.09306676482664011\n",
      "    At iteration 11500 -> loss: 0.09306535718065703\n",
      "    At iteration 11600 -> loss: 0.09303976934499932\n",
      "    At iteration 11700 -> loss: 0.09304923460401536\n",
      "    At iteration 11800 -> loss: 0.09303955156506957\n",
      "    At iteration 11900 -> loss: 0.09308365916619842\n",
      "    At iteration 12000 -> loss: 0.09305257963679248\n",
      "    At iteration 12100 -> loss: 0.09313285846249944\n",
      "    At iteration 12200 -> loss: 0.09312045882338565\n",
      "    At iteration 12300 -> loss: 0.09313663952345139\n",
      "    At iteration 12400 -> loss: 0.0931243098841989\n",
      "    At iteration 12500 -> loss: 0.09313724754707747\n",
      "    At iteration 12600 -> loss: 0.09315594598418242\n",
      "    At iteration 12700 -> loss: 0.09312911286429389\n",
      "    At iteration 12800 -> loss: 0.09318939185628058\n",
      "    At iteration 12900 -> loss: 0.09318436555930702\n",
      "    At iteration 13000 -> loss: 0.09318709533930411\n",
      "    At iteration 13100 -> loss: 0.09323706584309953\n",
      "    At iteration 13200 -> loss: 0.09321874233296755\n",
      "    At iteration 13300 -> loss: 0.09323006646859763\n",
      "    At iteration 13400 -> loss: 0.0932312998233284\n",
      "    At iteration 13500 -> loss: 0.09325773741801578\n",
      "    At iteration 13600 -> loss: 0.09331370927314411\n",
      "Staring Epoch 7\n",
      "    At iteration 0 -> loss: 0.10179658001288772\n",
      "    At iteration 100 -> loss: 0.09499323901781237\n",
      "    At iteration 200 -> loss: 0.09853046104277825\n",
      "    At iteration 300 -> loss: 0.09608449654833934\n",
      "    At iteration 400 -> loss: 0.09539499994710644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 500 -> loss: 0.0951585819418901\n",
      "    At iteration 600 -> loss: 0.0944792602635033\n",
      "    At iteration 700 -> loss: 0.09440664520807288\n",
      "    At iteration 800 -> loss: 0.09435363032741977\n",
      "    At iteration 900 -> loss: 0.09411226378910696\n",
      "    At iteration 1000 -> loss: 0.09358844341355806\n",
      "    At iteration 1100 -> loss: 0.09329486473351241\n",
      "    At iteration 1200 -> loss: 0.0931288763764498\n",
      "    At iteration 1300 -> loss: 0.09333570652600608\n",
      "    At iteration 1400 -> loss: 0.0930974447830149\n",
      "    At iteration 1500 -> loss: 0.09304370127034121\n",
      "    At iteration 1600 -> loss: 0.09332398759236608\n",
      "    At iteration 1700 -> loss: 0.0930876793461414\n",
      "    At iteration 1800 -> loss: 0.09301621693079251\n",
      "    At iteration 1900 -> loss: 0.09289887493493088\n",
      "    At iteration 2000 -> loss: 0.09294831248972822\n",
      "    At iteration 2100 -> loss: 0.09298427865840835\n",
      "    At iteration 2200 -> loss: 0.09310033223988298\n",
      "    At iteration 2300 -> loss: 0.09296808867361096\n",
      "    At iteration 2400 -> loss: 0.0928095719808494\n",
      "    At iteration 2500 -> loss: 0.09278610173039233\n",
      "    At iteration 2600 -> loss: 0.09273991738029506\n",
      "    At iteration 2700 -> loss: 0.09265724612062068\n",
      "    At iteration 2800 -> loss: 0.09265704919537626\n",
      "    At iteration 2900 -> loss: 0.0925782019811077\n",
      "    At iteration 3000 -> loss: 0.09264962406558025\n",
      "    At iteration 3100 -> loss: 0.09260941969551455\n",
      "    At iteration 3200 -> loss: 0.09253334003774324\n",
      "    At iteration 3300 -> loss: 0.09266762236876731\n",
      "    At iteration 3400 -> loss: 0.09259650648850501\n",
      "    At iteration 3500 -> loss: 0.09269748691482325\n",
      "    At iteration 3600 -> loss: 0.09264332983617703\n",
      "    At iteration 3700 -> loss: 0.09266300057986379\n",
      "    At iteration 3800 -> loss: 0.09263644242862092\n",
      "    At iteration 3900 -> loss: 0.09295753816237762\n",
      "    At iteration 4000 -> loss: 0.0930916355187092\n",
      "    At iteration 4100 -> loss: 0.09316150342911249\n",
      "    At iteration 4200 -> loss: 0.09311559702725301\n",
      "    At iteration 4300 -> loss: 0.09316411595286772\n",
      "    At iteration 4400 -> loss: 0.09314296930626188\n",
      "    At iteration 4500 -> loss: 0.09308024575858533\n",
      "    At iteration 4600 -> loss: 0.09325013621038489\n",
      "    At iteration 4700 -> loss: 0.09327127201006105\n",
      "    At iteration 4800 -> loss: 0.09327780211510955\n",
      "    At iteration 4900 -> loss: 0.09322045449731267\n",
      "    At iteration 5000 -> loss: 0.09334526103947667\n",
      "    At iteration 5100 -> loss: 0.09333843894736522\n",
      "    At iteration 5200 -> loss: 0.09391897453551681\n",
      "    At iteration 5300 -> loss: 0.09385989799471814\n",
      "    At iteration 5400 -> loss: 0.0937999709923209\n",
      "    At iteration 5500 -> loss: 0.09372205599513998\n",
      "    At iteration 5600 -> loss: 0.09370332317832146\n",
      "    At iteration 5700 -> loss: 0.09364312997180056\n",
      "    At iteration 5800 -> loss: 0.09363142581906564\n",
      "    At iteration 5900 -> loss: 0.09363162690783738\n",
      "    At iteration 6000 -> loss: 0.09368735634830125\n",
      "    At iteration 6100 -> loss: 0.0938274322209827\n",
      "    At iteration 6200 -> loss: 0.09377732189463535\n",
      "    At iteration 6300 -> loss: 0.0937073749954136\n",
      "    At iteration 6400 -> loss: 0.09365086615318356\n",
      "    At iteration 6500 -> loss: 0.09361639831517947\n",
      "    At iteration 6600 -> loss: 0.09359734927950568\n",
      "    At iteration 6700 -> loss: 0.09353637961841353\n",
      "    At iteration 6800 -> loss: 0.09351240584075199\n",
      "    At iteration 6900 -> loss: 0.09345430512943825\n",
      "    At iteration 7000 -> loss: 0.09342657442893235\n",
      "    At iteration 7100 -> loss: 0.09337142483910732\n",
      "    At iteration 7200 -> loss: 0.09334887246754055\n",
      "    At iteration 7300 -> loss: 0.09334291448775228\n",
      "    At iteration 7400 -> loss: 0.09338753988978718\n",
      "    At iteration 7500 -> loss: 0.09340826062578053\n",
      "    At iteration 7600 -> loss: 0.09336996701053722\n",
      "    At iteration 7700 -> loss: 0.09334565382755365\n",
      "    At iteration 7800 -> loss: 0.09332236290916152\n",
      "    At iteration 7900 -> loss: 0.09328082137112154\n",
      "    At iteration 8000 -> loss: 0.09327678838170406\n",
      "    At iteration 8100 -> loss: 0.09324238692850767\n",
      "    At iteration 8200 -> loss: 0.09324655516399817\n",
      "    At iteration 8300 -> loss: 0.09321863194479925\n",
      "    At iteration 8400 -> loss: 0.09330036394958773\n",
      "    At iteration 8500 -> loss: 0.09326165698072449\n",
      "    At iteration 8600 -> loss: 0.09333451230331866\n",
      "    At iteration 8700 -> loss: 0.09332510436438124\n",
      "    At iteration 8800 -> loss: 0.09330771730214253\n",
      "    At iteration 8900 -> loss: 0.09327193983922437\n",
      "    At iteration 9000 -> loss: 0.09326167141289271\n",
      "    At iteration 9100 -> loss: 0.09325954492404469\n",
      "    At iteration 9200 -> loss: 0.0932992132545785\n",
      "    At iteration 9300 -> loss: 0.09327142990578673\n",
      "    At iteration 9400 -> loss: 0.09325215955811314\n",
      "    At iteration 9500 -> loss: 0.09334428915032238\n",
      "    At iteration 9600 -> loss: 0.09330955050852859\n",
      "    At iteration 9700 -> loss: 0.09335074415358491\n",
      "    At iteration 9800 -> loss: 0.09331636685535785\n",
      "    At iteration 9900 -> loss: 0.09331736862842036\n",
      "    At iteration 10000 -> loss: 0.09332373576127354\n",
      "    At iteration 10100 -> loss: 0.09328914665980863\n",
      "    At iteration 10200 -> loss: 0.09327638137472136\n",
      "    At iteration 10300 -> loss: 0.0932911152739784\n",
      "    At iteration 10400 -> loss: 0.09331236361293728\n",
      "    At iteration 10500 -> loss: 0.09330779815064076\n",
      "    At iteration 10600 -> loss: 0.09336121005044454\n",
      "    At iteration 10700 -> loss: 0.09335308239157353\n",
      "    At iteration 10800 -> loss: 0.09333020226050781\n",
      "    At iteration 10900 -> loss: 0.09331183835326529\n",
      "    At iteration 11000 -> loss: 0.09330473140449713\n",
      "    At iteration 11100 -> loss: 0.09331978040449047\n",
      "    At iteration 11200 -> loss: 0.09337832766069956\n",
      "    At iteration 11300 -> loss: 0.09335650372740972\n",
      "    At iteration 11400 -> loss: 0.09338262460971415\n",
      "    At iteration 11500 -> loss: 0.09336078724235503\n",
      "    At iteration 11600 -> loss: 0.09337625169652292\n",
      "    At iteration 11700 -> loss: 0.09341062473912964\n",
      "    At iteration 11800 -> loss: 0.09342171735335883\n",
      "    At iteration 11900 -> loss: 0.0934640446879742\n",
      "    At iteration 12000 -> loss: 0.09345231790428664\n",
      "    At iteration 12100 -> loss: 0.09342975645817235\n",
      "    At iteration 12200 -> loss: 0.09341714694021107\n",
      "    At iteration 12300 -> loss: 0.09340589756124441\n",
      "    At iteration 12400 -> loss: 0.09346027360092929\n",
      "    At iteration 12500 -> loss: 0.09343296698400913\n",
      "    At iteration 12600 -> loss: 0.09344534544558643\n",
      "    At iteration 12700 -> loss: 0.09345115438963\n",
      "    At iteration 12800 -> loss: 0.09343138730081277\n",
      "    At iteration 12900 -> loss: 0.09339986528678713\n",
      "    At iteration 13000 -> loss: 0.09338596782706601\n",
      "    At iteration 13100 -> loss: 0.09336908930351262\n",
      "    At iteration 13200 -> loss: 0.09336245268097122\n",
      "    At iteration 13300 -> loss: 0.09333352865022577\n",
      "    At iteration 13400 -> loss: 0.09331920716877466\n",
      "    At iteration 13500 -> loss: 0.09335353228982603\n",
      "    At iteration 13600 -> loss: 0.09332473838806568\n",
      "Staring Epoch 8\n",
      "    At iteration 0 -> loss: 0.08784887008368969\n",
      "    At iteration 100 -> loss: 0.09570405576578038\n",
      "    At iteration 200 -> loss: 0.09573834712188978\n",
      "    At iteration 300 -> loss: 0.0941687778393725\n",
      "    At iteration 400 -> loss: 0.09338184037788547\n",
      "    At iteration 500 -> loss: 0.09386268688414234\n",
      "    At iteration 600 -> loss: 0.09380636444709559\n",
      "    At iteration 700 -> loss: 0.09429303641816458\n",
      "    At iteration 800 -> loss: 0.09371897893093513\n",
      "    At iteration 900 -> loss: 0.09332534525439909\n",
      "    At iteration 1000 -> loss: 0.09301443180559106\n",
      "    At iteration 1100 -> loss: 0.09271783641830278\n",
      "    At iteration 1200 -> loss: 0.09260334214106675\n",
      "    At iteration 1300 -> loss: 0.09280274619706716\n",
      "    At iteration 1400 -> loss: 0.09283297111109565\n",
      "    At iteration 1500 -> loss: 0.09297523883659906\n",
      "    At iteration 1600 -> loss: 0.09290631780466135\n",
      "    At iteration 1700 -> loss: 0.09291799960974233\n",
      "    At iteration 1800 -> loss: 0.09270134415754983\n",
      "    At iteration 1900 -> loss: 0.09256107401642376\n",
      "    At iteration 2000 -> loss: 0.09246912464813847\n",
      "    At iteration 2100 -> loss: 0.09238998524916511\n",
      "    At iteration 2200 -> loss: 0.09231473774292112\n",
      "    At iteration 2300 -> loss: 0.09248066723762186\n",
      "    At iteration 2400 -> loss: 0.0923677002715613\n",
      "    At iteration 2500 -> loss: 0.09278313082096773\n",
      "    At iteration 2600 -> loss: 0.09270439800417934\n",
      "    At iteration 2700 -> loss: 0.09259419861181674\n",
      "    At iteration 2800 -> loss: 0.09244314271775199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2900 -> loss: 0.09236331625565311\n",
      "    At iteration 3000 -> loss: 0.09232085729987614\n",
      "    At iteration 3100 -> loss: 0.09247221608337007\n",
      "    At iteration 3200 -> loss: 0.09265349698524203\n",
      "    At iteration 3300 -> loss: 0.09263488213896866\n",
      "    At iteration 3400 -> loss: 0.09267151951415802\n",
      "    At iteration 3500 -> loss: 0.09269098288814001\n",
      "    At iteration 3600 -> loss: 0.09278497728436057\n",
      "    At iteration 3700 -> loss: 0.09271276109820728\n",
      "    At iteration 3800 -> loss: 0.0927370062470203\n",
      "    At iteration 3900 -> loss: 0.09268452356269803\n",
      "    At iteration 4000 -> loss: 0.09262554338413931\n",
      "    At iteration 4100 -> loss: 0.09255509497342003\n",
      "    At iteration 4200 -> loss: 0.09260396369221002\n",
      "    At iteration 4300 -> loss: 0.09260936225045947\n",
      "    At iteration 4400 -> loss: 0.09262224876334214\n",
      "    At iteration 4500 -> loss: 0.09265109824081696\n",
      "    At iteration 4600 -> loss: 0.09261266080837074\n",
      "    At iteration 4700 -> loss: 0.09253259852615478\n",
      "    At iteration 4800 -> loss: 0.09259040936238193\n",
      "    At iteration 4900 -> loss: 0.09253362569459861\n",
      "    At iteration 5000 -> loss: 0.09256502307454009\n",
      "    At iteration 5100 -> loss: 0.09251813588796796\n",
      "    At iteration 5200 -> loss: 0.0927680211424286\n",
      "    At iteration 5300 -> loss: 0.09276008979891744\n",
      "    At iteration 5400 -> loss: 0.09270692380473758\n",
      "    At iteration 5500 -> loss: 0.09270252417479184\n",
      "    At iteration 5600 -> loss: 0.09268811958669135\n",
      "    At iteration 5700 -> loss: 0.09278213111575442\n",
      "    At iteration 5800 -> loss: 0.09275113942191861\n",
      "    At iteration 5900 -> loss: 0.09280292119621933\n",
      "    At iteration 6000 -> loss: 0.09278228230189924\n",
      "    At iteration 6100 -> loss: 0.09275826732363028\n",
      "    At iteration 6200 -> loss: 0.0926870129232632\n",
      "    At iteration 6300 -> loss: 0.09268960079823965\n",
      "    At iteration 6400 -> loss: 0.09268263658659043\n",
      "    At iteration 6500 -> loss: 0.09263557951873577\n",
      "    At iteration 6600 -> loss: 0.09264933570579395\n",
      "    At iteration 6700 -> loss: 0.09261419746222843\n",
      "    At iteration 6800 -> loss: 0.09256153799498773\n",
      "    At iteration 6900 -> loss: 0.0925837708663758\n",
      "    At iteration 7000 -> loss: 0.09252580459061925\n",
      "    At iteration 7100 -> loss: 0.09250894934522512\n",
      "    At iteration 7200 -> loss: 0.09269162815587342\n",
      "    At iteration 7300 -> loss: 0.09265049459408237\n",
      "    At iteration 7400 -> loss: 0.09261584310776612\n",
      "    At iteration 7500 -> loss: 0.09263098549234888\n",
      "    At iteration 7600 -> loss: 0.09261589897723006\n",
      "    At iteration 7700 -> loss: 0.09261149458135572\n",
      "    At iteration 7800 -> loss: 0.09260044806478845\n",
      "    At iteration 7900 -> loss: 0.09272317227529304\n",
      "    At iteration 8000 -> loss: 0.0927164917606417\n",
      "    At iteration 8100 -> loss: 0.09269160823127376\n",
      "    At iteration 8200 -> loss: 0.09266444828221754\n",
      "    At iteration 8300 -> loss: 0.0926679309479305\n",
      "    At iteration 8400 -> loss: 0.09275659825437275\n",
      "    At iteration 8500 -> loss: 0.09273209292720477\n",
      "    At iteration 8600 -> loss: 0.09270638024370732\n",
      "    At iteration 8700 -> loss: 0.0926803672806558\n",
      "    At iteration 8800 -> loss: 0.09276064804804361\n",
      "    At iteration 8900 -> loss: 0.09275040534894789\n",
      "    At iteration 9000 -> loss: 0.09275317027774724\n",
      "    At iteration 9100 -> loss: 0.09277705207233468\n",
      "    At iteration 9200 -> loss: 0.09309548399473713\n",
      "    At iteration 9300 -> loss: 0.09306964289032946\n",
      "    At iteration 9400 -> loss: 0.09313205149316474\n",
      "    At iteration 9500 -> loss: 0.09318390903174956\n",
      "    At iteration 9600 -> loss: 0.09313685631962522\n",
      "    At iteration 9700 -> loss: 0.09312824244681935\n",
      "    At iteration 9800 -> loss: 0.09312603376022667\n",
      "    At iteration 9900 -> loss: 0.09342161747296131\n",
      "    At iteration 10000 -> loss: 0.09340974826476907\n",
      "    At iteration 10100 -> loss: 0.09342441008744605\n",
      "    At iteration 10200 -> loss: 0.09342071197574042\n",
      "    At iteration 10300 -> loss: 0.0933831716831953\n",
      "    At iteration 10400 -> loss: 0.09337159173760583\n",
      "    At iteration 10500 -> loss: 0.0933513152453506\n",
      "    At iteration 10600 -> loss: 0.09339868211966947\n",
      "    At iteration 10700 -> loss: 0.09344904939392416\n",
      "    At iteration 10800 -> loss: 0.09343154174335841\n",
      "    At iteration 10900 -> loss: 0.09341127397639423\n",
      "    At iteration 11000 -> loss: 0.09338260338972007\n",
      "    At iteration 11100 -> loss: 0.09334617962761999\n",
      "    At iteration 11200 -> loss: 0.0933145868011474\n",
      "    At iteration 11300 -> loss: 0.09331864335384933\n",
      "    At iteration 11400 -> loss: 0.09330584991961428\n",
      "    At iteration 11500 -> loss: 0.09333516930360282\n",
      "    At iteration 11600 -> loss: 0.09337636190925029\n",
      "    At iteration 11700 -> loss: 0.09334103511615134\n",
      "    At iteration 11800 -> loss: 0.09332866946932875\n",
      "    At iteration 11900 -> loss: 0.09330529343472753\n",
      "    At iteration 12000 -> loss: 0.09328574562083836\n",
      "    At iteration 12100 -> loss: 0.09333321168559315\n",
      "    At iteration 12200 -> loss: 0.09330851987008097\n",
      "    At iteration 12300 -> loss: 0.09329511103855537\n",
      "    At iteration 12400 -> loss: 0.09329105014610226\n",
      "    At iteration 12500 -> loss: 0.09330999744545984\n",
      "    At iteration 12600 -> loss: 0.09329223032087174\n",
      "    At iteration 12700 -> loss: 0.09330209984043915\n",
      "    At iteration 12800 -> loss: 0.09333538595600142\n",
      "    At iteration 12900 -> loss: 0.09330869588705976\n",
      "    At iteration 13000 -> loss: 0.09332820322945348\n",
      "    At iteration 13100 -> loss: 0.09330564887761984\n",
      "    At iteration 13200 -> loss: 0.09329858398682693\n",
      "    At iteration 13300 -> loss: 0.09328704156557927\n",
      "    At iteration 13400 -> loss: 0.09325923082991631\n",
      "    At iteration 13500 -> loss: 0.09326169783770169\n",
      "    At iteration 13600 -> loss: 0.09323771422666803\n",
      "Staring Epoch 9\n",
      "    At iteration 0 -> loss: 0.08438173297327012\n",
      "    At iteration 100 -> loss: 0.10194294968238689\n",
      "    At iteration 200 -> loss: 0.09636451666233646\n",
      "    At iteration 300 -> loss: 0.09488861022592548\n",
      "    At iteration 400 -> loss: 0.09402969205064851\n",
      "    At iteration 500 -> loss: 0.09365691525678764\n",
      "    At iteration 600 -> loss: 0.09291143397321008\n",
      "    At iteration 700 -> loss: 0.09297526755383376\n",
      "    At iteration 800 -> loss: 0.09295695842446383\n",
      "    At iteration 900 -> loss: 0.09262782204212025\n",
      "    At iteration 1000 -> loss: 0.09252127737965078\n",
      "    At iteration 1100 -> loss: 0.09242208327510161\n",
      "    At iteration 1200 -> loss: 0.09234901105551573\n",
      "    At iteration 1300 -> loss: 0.09236162199971583\n",
      "    At iteration 1400 -> loss: 0.09230429875513345\n",
      "    At iteration 1500 -> loss: 0.09274248527916175\n",
      "    At iteration 1600 -> loss: 0.09291982984787768\n",
      "    At iteration 1700 -> loss: 0.09283801160125622\n",
      "    At iteration 1800 -> loss: 0.0929624976831309\n",
      "    At iteration 1900 -> loss: 0.09287372531162492\n",
      "    At iteration 2000 -> loss: 0.09294102123558995\n",
      "    At iteration 2100 -> loss: 0.09308383345098482\n",
      "    At iteration 2200 -> loss: 0.09307271115099751\n",
      "    At iteration 2300 -> loss: 0.09329874245108603\n",
      "    At iteration 2400 -> loss: 0.09346331734356798\n",
      "    At iteration 2500 -> loss: 0.09342379415390598\n",
      "    At iteration 2600 -> loss: 0.09343421486328998\n",
      "    At iteration 2700 -> loss: 0.09366757712858195\n",
      "    At iteration 2800 -> loss: 0.0936736561471723\n",
      "    At iteration 2900 -> loss: 0.09365795222779572\n",
      "    At iteration 3000 -> loss: 0.0935213874253167\n",
      "    At iteration 3100 -> loss: 0.09349691732039193\n",
      "    At iteration 3200 -> loss: 0.09346649614446581\n",
      "    At iteration 3300 -> loss: 0.09350295666474802\n",
      "    At iteration 3400 -> loss: 0.09340177031070135\n",
      "    At iteration 3500 -> loss: 0.0933788025705722\n",
      "    At iteration 3600 -> loss: 0.09381375670783425\n",
      "    At iteration 3700 -> loss: 0.09371512009360726\n",
      "    At iteration 3800 -> loss: 0.09368824534170202\n",
      "    At iteration 3900 -> loss: 0.0936277748347342\n",
      "    At iteration 4000 -> loss: 0.09353105830117082\n",
      "    At iteration 4100 -> loss: 0.09361696369521065\n",
      "    At iteration 4200 -> loss: 0.09380271451558503\n",
      "    At iteration 4300 -> loss: 0.09377523167338282\n",
      "    At iteration 4400 -> loss: 0.09367952140532029\n",
      "    At iteration 4500 -> loss: 0.09363131799607997\n",
      "    At iteration 4600 -> loss: 0.09356564144476998\n",
      "    At iteration 4700 -> loss: 0.09374140681451988\n",
      "    At iteration 4800 -> loss: 0.0936535189118036\n",
      "    At iteration 4900 -> loss: 0.09354203332391542\n",
      "    At iteration 5000 -> loss: 0.093481146588006\n",
      "    At iteration 5100 -> loss: 0.09347650424802288\n",
      "    At iteration 5200 -> loss: 0.09342654133670296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5300 -> loss: 0.09342251216446595\n",
      "    At iteration 5400 -> loss: 0.09336816205462192\n",
      "    At iteration 5500 -> loss: 0.09349196903648267\n",
      "    At iteration 5600 -> loss: 0.09355535206865566\n",
      "    At iteration 5700 -> loss: 0.09354212438539702\n",
      "    At iteration 5800 -> loss: 0.09361172364160258\n",
      "    At iteration 5900 -> loss: 0.09360824746965811\n",
      "    At iteration 6000 -> loss: 0.09354059356251819\n",
      "    At iteration 6100 -> loss: 0.09352251835423807\n",
      "    At iteration 6200 -> loss: 0.09353115921263573\n",
      "    At iteration 6300 -> loss: 0.09350606089361264\n",
      "    At iteration 6400 -> loss: 0.09347218652248004\n",
      "    At iteration 6500 -> loss: 0.0934393464003878\n",
      "    At iteration 6600 -> loss: 0.09346687980213195\n",
      "    At iteration 6700 -> loss: 0.09358859916804008\n",
      "    At iteration 6800 -> loss: 0.09359950022911148\n",
      "    At iteration 6900 -> loss: 0.09358370686188641\n",
      "    At iteration 7000 -> loss: 0.09360263413435373\n",
      "    At iteration 7100 -> loss: 0.09357704918569255\n",
      "    At iteration 7200 -> loss: 0.09355390324637973\n",
      "    At iteration 7300 -> loss: 0.0935342254572895\n",
      "    At iteration 7400 -> loss: 0.09355410716965139\n",
      "    At iteration 7500 -> loss: 0.09354173038002388\n",
      "    At iteration 7600 -> loss: 0.09353642855194022\n",
      "    At iteration 7700 -> loss: 0.0934711284695013\n",
      "    At iteration 7800 -> loss: 0.09342420312017372\n",
      "    At iteration 7900 -> loss: 0.093373529852847\n",
      "    At iteration 8000 -> loss: 0.09334568574169443\n",
      "    At iteration 8100 -> loss: 0.09331225680815013\n",
      "    At iteration 8200 -> loss: 0.09333760174535467\n",
      "    At iteration 8300 -> loss: 0.09327689949173809\n",
      "    At iteration 8400 -> loss: 0.09327253916154934\n",
      "    At iteration 8500 -> loss: 0.09328460380107216\n",
      "    At iteration 8600 -> loss: 0.09327927991945689\n",
      "    At iteration 8700 -> loss: 0.09324769057497051\n",
      "    At iteration 8800 -> loss: 0.09322306030143701\n",
      "    At iteration 8900 -> loss: 0.09319130932371955\n",
      "    At iteration 9000 -> loss: 0.09317327584994271\n",
      "    At iteration 9100 -> loss: 0.09316345575809508\n",
      "    At iteration 9200 -> loss: 0.09323635454779886\n",
      "    At iteration 9300 -> loss: 0.0932251791829131\n",
      "    At iteration 9400 -> loss: 0.0932108641090696\n",
      "    At iteration 9500 -> loss: 0.09326173312612342\n",
      "    At iteration 9600 -> loss: 0.09325797908224066\n",
      "    At iteration 9700 -> loss: 0.09325305226591439\n",
      "    At iteration 9800 -> loss: 0.09322488203674166\n",
      "    At iteration 9900 -> loss: 0.09320955502045712\n",
      "    At iteration 10000 -> loss: 0.09318345501644114\n",
      "    At iteration 10100 -> loss: 0.09314609527000454\n",
      "    At iteration 10200 -> loss: 0.09312990646415525\n",
      "    At iteration 10300 -> loss: 0.0931136432287612\n",
      "    At iteration 10400 -> loss: 0.09309028980205554\n",
      "    At iteration 10500 -> loss: 0.09306595989679094\n",
      "    At iteration 10600 -> loss: 0.09336623917112012\n",
      "    At iteration 10700 -> loss: 0.09332222876010952\n",
      "    At iteration 10800 -> loss: 0.09334018678444801\n",
      "    At iteration 10900 -> loss: 0.09336026192568338\n",
      "    At iteration 11000 -> loss: 0.09336690756662909\n",
      "    At iteration 11100 -> loss: 0.09335259776219787\n",
      "    At iteration 11200 -> loss: 0.09333052584817537\n",
      "    At iteration 11300 -> loss: 0.09329386464815788\n",
      "    At iteration 11400 -> loss: 0.0932784559692768\n",
      "    At iteration 11500 -> loss: 0.09331429718664255\n",
      "    At iteration 11600 -> loss: 0.09329044816409489\n",
      "    At iteration 11700 -> loss: 0.09326288780565874\n",
      "    At iteration 11800 -> loss: 0.09331291547080219\n",
      "    At iteration 11900 -> loss: 0.09328848564046925\n",
      "    At iteration 12000 -> loss: 0.09329225001484952\n",
      "    At iteration 12100 -> loss: 0.09328406741470008\n",
      "    At iteration 12200 -> loss: 0.09326558967519981\n",
      "    At iteration 12300 -> loss: 0.09324361593098766\n",
      "    At iteration 12400 -> loss: 0.09321102009089424\n",
      "    At iteration 12500 -> loss: 0.09327387561744735\n",
      "    At iteration 12600 -> loss: 0.09328793497303214\n",
      "    At iteration 12700 -> loss: 0.0932743691788115\n",
      "    At iteration 12800 -> loss: 0.09324979680927432\n",
      "    At iteration 12900 -> loss: 0.09324269998980664\n",
      "    At iteration 13000 -> loss: 0.09325067169807488\n",
      "    At iteration 13100 -> loss: 0.09322724677062318\n",
      "    At iteration 13200 -> loss: 0.09327507652460268\n",
      "    At iteration 13300 -> loss: 0.09326522581688676\n",
      "    At iteration 13400 -> loss: 0.0932424640563082\n",
      "    At iteration 13500 -> loss: 0.09322399414289285\n",
      "    At iteration 13600 -> loss: 0.09321065063254687\n",
      "Staring Epoch 10\n",
      "    At iteration 0 -> loss: 0.08017780847876566\n",
      "    At iteration 100 -> loss: 0.09073240551377257\n",
      "    At iteration 200 -> loss: 0.09195727284530231\n",
      "    At iteration 300 -> loss: 0.09115356510426913\n",
      "    At iteration 400 -> loss: 0.09081164092537992\n",
      "    At iteration 500 -> loss: 0.0908889581318802\n",
      "    At iteration 600 -> loss: 0.09210028715262046\n",
      "    At iteration 700 -> loss: 0.0923912820604013\n",
      "    At iteration 800 -> loss: 0.09231235166067972\n",
      "    At iteration 900 -> loss: 0.0931400806048268\n",
      "    At iteration 1000 -> loss: 0.09292847915775286\n",
      "    At iteration 1100 -> loss: 0.09266126084700645\n",
      "    At iteration 1200 -> loss: 0.09285567298120716\n",
      "    At iteration 1300 -> loss: 0.09307223346236643\n",
      "    At iteration 1400 -> loss: 0.0933520296375291\n",
      "    At iteration 1500 -> loss: 0.09304481580590819\n",
      "    At iteration 1600 -> loss: 0.0929749083163068\n",
      "    At iteration 1700 -> loss: 0.0932370337232178\n",
      "    At iteration 1800 -> loss: 0.09310329045480305\n",
      "    At iteration 1900 -> loss: 0.0931159581157216\n",
      "    At iteration 2000 -> loss: 0.09299801873465816\n",
      "    At iteration 2100 -> loss: 0.09291560513693031\n",
      "    At iteration 2200 -> loss: 0.09298991267781052\n",
      "    At iteration 2300 -> loss: 0.09288183461833421\n",
      "    At iteration 2400 -> loss: 0.09281140167969149\n",
      "    At iteration 2500 -> loss: 0.09307592611801553\n",
      "    At iteration 2600 -> loss: 0.09304201610229537\n",
      "    At iteration 2700 -> loss: 0.09308600087272006\n",
      "    At iteration 2800 -> loss: 0.0930641071494524\n",
      "    At iteration 2900 -> loss: 0.09319211371593827\n",
      "    At iteration 3000 -> loss: 0.09423175437639894\n",
      "    At iteration 3100 -> loss: 0.0941644154625772\n",
      "    At iteration 3200 -> loss: 0.09412310880535343\n",
      "    At iteration 3300 -> loss: 0.09397098408010457\n",
      "    At iteration 3400 -> loss: 0.09392891903532256\n",
      "    At iteration 3500 -> loss: 0.09375742961586979\n",
      "    At iteration 3600 -> loss: 0.09371108838127457\n",
      "    At iteration 3700 -> loss: 0.09359999565159192\n",
      "    At iteration 3800 -> loss: 0.09363957051046469\n",
      "    At iteration 3900 -> loss: 0.09356868071560762\n",
      "    At iteration 4000 -> loss: 0.09349451325946005\n",
      "    At iteration 4100 -> loss: 0.09338483647398661\n",
      "    At iteration 4200 -> loss: 0.09329361192335846\n",
      "    At iteration 4300 -> loss: 0.09325345701456904\n",
      "    At iteration 4400 -> loss: 0.093210335490055\n",
      "    At iteration 4500 -> loss: 0.09319320224908878\n",
      "    At iteration 4600 -> loss: 0.09324495342004506\n",
      "    At iteration 4700 -> loss: 0.09343766741951351\n",
      "    At iteration 4800 -> loss: 0.0933647807483812\n",
      "    At iteration 4900 -> loss: 0.09339020020541028\n",
      "    At iteration 5000 -> loss: 0.09336602578837054\n",
      "    At iteration 5100 -> loss: 0.0934583969411123\n",
      "    At iteration 5200 -> loss: 0.09350057194972322\n",
      "    At iteration 5300 -> loss: 0.09351827093253727\n",
      "    At iteration 5400 -> loss: 0.09364363657320046\n",
      "    At iteration 5500 -> loss: 0.09363114758358171\n",
      "    At iteration 5600 -> loss: 0.09361359866619502\n",
      "    At iteration 5700 -> loss: 0.09356794475691353\n",
      "    At iteration 5800 -> loss: 0.09356477166202959\n",
      "    At iteration 5900 -> loss: 0.09351889710802522\n",
      "    At iteration 6000 -> loss: 0.0935183774421186\n",
      "    At iteration 6100 -> loss: 0.09371496827393515\n",
      "    At iteration 6200 -> loss: 0.0937736284623447\n",
      "    At iteration 6300 -> loss: 0.09374251524885242\n",
      "    At iteration 6400 -> loss: 0.09378916326611236\n",
      "    At iteration 6500 -> loss: 0.09375575693078958\n",
      "    At iteration 6600 -> loss: 0.09371637264116701\n",
      "    At iteration 6700 -> loss: 0.09376428304770251\n",
      "    At iteration 6800 -> loss: 0.09375311773310319\n",
      "    At iteration 6900 -> loss: 0.09372054130828524\n",
      "    At iteration 7000 -> loss: 0.09368174986213937\n",
      "    At iteration 7100 -> loss: 0.09368432816171356\n",
      "    At iteration 7200 -> loss: 0.09365153888916392\n",
      "    At iteration 7300 -> loss: 0.09365782277824926\n",
      "    At iteration 7400 -> loss: 0.09375600877005469\n",
      "    At iteration 7500 -> loss: 0.09372894645050857\n",
      "    At iteration 7600 -> loss: 0.09373044785835245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7700 -> loss: 0.0937276070571275\n",
      "    At iteration 7800 -> loss: 0.09369220270311912\n",
      "    At iteration 7900 -> loss: 0.09368569258175036\n",
      "    At iteration 8000 -> loss: 0.09368083503069183\n",
      "    At iteration 8100 -> loss: 0.09368658714120906\n",
      "    At iteration 8200 -> loss: 0.09369649491965604\n",
      "    At iteration 8300 -> loss: 0.09368110333266423\n",
      "    At iteration 8400 -> loss: 0.09364152448105938\n",
      "    At iteration 8500 -> loss: 0.09359100832401554\n",
      "    At iteration 8600 -> loss: 0.09356448462977077\n",
      "    At iteration 8700 -> loss: 0.09353589376478975\n",
      "    At iteration 8800 -> loss: 0.09355063783171856\n",
      "    At iteration 8900 -> loss: 0.0935180508386212\n",
      "    At iteration 9000 -> loss: 0.09353688148259208\n",
      "    At iteration 9100 -> loss: 0.09348863181884301\n",
      "    At iteration 9200 -> loss: 0.09347030494248816\n",
      "    At iteration 9300 -> loss: 0.09352089286366246\n",
      "    At iteration 9400 -> loss: 0.09357535377857217\n",
      "    At iteration 9500 -> loss: 0.09358660918512259\n",
      "    At iteration 9600 -> loss: 0.09354940705505448\n",
      "    At iteration 9700 -> loss: 0.09355452195349671\n",
      "    At iteration 9800 -> loss: 0.09354728842714027\n",
      "    At iteration 9900 -> loss: 0.09352423933563847\n",
      "    At iteration 10000 -> loss: 0.0934845789268961\n",
      "    At iteration 10100 -> loss: 0.09345170193557792\n",
      "    At iteration 10200 -> loss: 0.0934115277204901\n",
      "    At iteration 10300 -> loss: 0.09338228507370058\n",
      "    At iteration 10400 -> loss: 0.09335429007029981\n",
      "    At iteration 10500 -> loss: 0.09331016360591764\n",
      "    At iteration 10600 -> loss: 0.09329906863016321\n",
      "    At iteration 10700 -> loss: 0.09328255791315407\n",
      "    At iteration 10800 -> loss: 0.09330414793028444\n",
      "    At iteration 10900 -> loss: 0.09326337885629472\n",
      "    At iteration 11000 -> loss: 0.09327787717464453\n",
      "    At iteration 11100 -> loss: 0.09324797192448649\n",
      "    At iteration 11200 -> loss: 0.09322144326394645\n",
      "    At iteration 11300 -> loss: 0.09321706190683751\n",
      "    At iteration 11400 -> loss: 0.09317610371459044\n",
      "    At iteration 11500 -> loss: 0.09316715090021843\n",
      "    At iteration 11600 -> loss: 0.09315536294020871\n",
      "    At iteration 11700 -> loss: 0.09316083181428061\n",
      "    At iteration 11800 -> loss: 0.09314346881931446\n",
      "    At iteration 11900 -> loss: 0.09314249538275585\n",
      "    At iteration 12000 -> loss: 0.09313629841979883\n",
      "    At iteration 12100 -> loss: 0.09313276285876905\n",
      "    At iteration 12200 -> loss: 0.09313018523615595\n",
      "    At iteration 12300 -> loss: 0.09311085284151022\n",
      "    At iteration 12400 -> loss: 0.09316296210805389\n",
      "    At iteration 12500 -> loss: 0.09315160461145575\n",
      "    At iteration 12600 -> loss: 0.09318260470539164\n",
      "    At iteration 12700 -> loss: 0.09317831238997482\n",
      "    At iteration 12800 -> loss: 0.09319933545769929\n",
      "    At iteration 12900 -> loss: 0.09321268777428025\n",
      "    At iteration 13000 -> loss: 0.09321766806974316\n",
      "    At iteration 13100 -> loss: 0.0932681054091195\n",
      "    At iteration 13200 -> loss: 0.09324623687759223\n",
      "    At iteration 13300 -> loss: 0.0932620744520177\n",
      "    At iteration 13400 -> loss: 0.09326004104966744\n",
      "    At iteration 13500 -> loss: 0.09323942417273438\n",
      "    At iteration 13600 -> loss: 0.09321635967295963\n",
      "Staring Epoch 11\n",
      "    At iteration 0 -> loss: 0.08106826490256935\n",
      "    At iteration 100 -> loss: 0.0892084620113649\n",
      "    At iteration 200 -> loss: 0.09069460438366451\n",
      "    At iteration 300 -> loss: 0.09074743592291556\n",
      "    At iteration 400 -> loss: 0.09131579001893335\n",
      "    At iteration 500 -> loss: 0.09104375876477612\n",
      "    At iteration 600 -> loss: 0.09618784878453716\n",
      "    At iteration 700 -> loss: 0.09547254906226309\n",
      "    At iteration 800 -> loss: 0.09622863164445004\n",
      "    At iteration 900 -> loss: 0.09585012251769297\n",
      "    At iteration 1000 -> loss: 0.09596311585731747\n",
      "    At iteration 1100 -> loss: 0.09551574843478594\n",
      "    At iteration 1200 -> loss: 0.09586861063356479\n",
      "    At iteration 1300 -> loss: 0.09560576291238493\n",
      "    At iteration 1400 -> loss: 0.09533920886737166\n",
      "    At iteration 1500 -> loss: 0.0950788774281278\n",
      "    At iteration 1600 -> loss: 0.0951492459474205\n",
      "    At iteration 1700 -> loss: 0.0949178060536095\n",
      "    At iteration 1800 -> loss: 0.09469322131138304\n",
      "    At iteration 1900 -> loss: 0.09452369866764906\n",
      "    At iteration 2000 -> loss: 0.09439347736045779\n",
      "    At iteration 2100 -> loss: 0.09435631393169282\n",
      "    At iteration 2200 -> loss: 0.09448129591415366\n",
      "    At iteration 2300 -> loss: 0.09425668404716217\n",
      "    At iteration 2400 -> loss: 0.0940573438349463\n",
      "    At iteration 2500 -> loss: 0.09391890763132965\n",
      "    At iteration 2600 -> loss: 0.09384549337831352\n",
      "    At iteration 2700 -> loss: 0.0938487616021435\n",
      "    At iteration 2800 -> loss: 0.09408288460036657\n",
      "    At iteration 2900 -> loss: 0.09405202306134766\n",
      "    At iteration 3000 -> loss: 0.0942398340066942\n",
      "    At iteration 3100 -> loss: 0.09427192241775466\n",
      "    At iteration 3200 -> loss: 0.09431406780594094\n",
      "    At iteration 3300 -> loss: 0.09435586392816368\n",
      "    At iteration 3400 -> loss: 0.0942000343267317\n",
      "    At iteration 3500 -> loss: 0.09416515644278309\n",
      "    At iteration 3600 -> loss: 0.09424093916971035\n",
      "    At iteration 3700 -> loss: 0.09417466308253156\n",
      "    At iteration 3800 -> loss: 0.09406384808794388\n",
      "    At iteration 3900 -> loss: 0.0939494149573686\n",
      "    At iteration 4000 -> loss: 0.09403442940979495\n",
      "    At iteration 4100 -> loss: 0.0939971857434884\n",
      "    At iteration 4200 -> loss: 0.09389347725147011\n",
      "    At iteration 4300 -> loss: 0.09382638685544929\n",
      "    At iteration 4400 -> loss: 0.09394012461927678\n",
      "    At iteration 4500 -> loss: 0.0939180270766634\n",
      "    At iteration 4600 -> loss: 0.09387589119629029\n",
      "    At iteration 4700 -> loss: 0.0939749161357289\n",
      "    At iteration 4800 -> loss: 0.09400806598775131\n",
      "    At iteration 4900 -> loss: 0.0939785648351975\n",
      "    At iteration 5000 -> loss: 0.09391950032057572\n",
      "    At iteration 5100 -> loss: 0.09393832445590292\n",
      "    At iteration 5200 -> loss: 0.09390484316883814\n",
      "    At iteration 5300 -> loss: 0.09381969914935669\n",
      "    At iteration 5400 -> loss: 0.09377352275952446\n",
      "    At iteration 5500 -> loss: 0.09377639654670325\n",
      "    At iteration 5600 -> loss: 0.093773299585782\n",
      "    At iteration 5700 -> loss: 0.09372741176721655\n",
      "    At iteration 5800 -> loss: 0.09381495919422385\n",
      "    At iteration 5900 -> loss: 0.09379514335464229\n",
      "    At iteration 6000 -> loss: 0.09373407695565358\n",
      "    At iteration 6100 -> loss: 0.0937208159229995\n",
      "    At iteration 6200 -> loss: 0.09372313065305346\n",
      "    At iteration 6300 -> loss: 0.0937798262337002\n",
      "    At iteration 6400 -> loss: 0.09375107820139372\n",
      "    At iteration 6500 -> loss: 0.093695065074938\n",
      "    At iteration 6600 -> loss: 0.09377299759292092\n",
      "    At iteration 6700 -> loss: 0.09382194505878089\n",
      "    At iteration 6800 -> loss: 0.09381597760754215\n",
      "    At iteration 6900 -> loss: 0.0937970333255498\n",
      "    At iteration 7000 -> loss: 0.09375243266146174\n",
      "    At iteration 7100 -> loss: 0.09367514195626683\n",
      "    At iteration 7200 -> loss: 0.09370420487940226\n",
      "    At iteration 7300 -> loss: 0.09374647717881879\n",
      "    At iteration 7400 -> loss: 0.09382682187398805\n",
      "    At iteration 7500 -> loss: 0.09377460164336282\n",
      "    At iteration 7600 -> loss: 0.09372372862173262\n",
      "    At iteration 7700 -> loss: 0.09375387012924032\n",
      "    At iteration 7800 -> loss: 0.0938073745201912\n",
      "    At iteration 7900 -> loss: 0.0938116843754888\n",
      "    At iteration 8000 -> loss: 0.09378397765563824\n",
      "    At iteration 8100 -> loss: 0.09377469229077585\n",
      "    At iteration 8200 -> loss: 0.09370493565793997\n",
      "    At iteration 8300 -> loss: 0.09367784584023087\n",
      "    At iteration 8400 -> loss: 0.09381353389021839\n",
      "    At iteration 8500 -> loss: 0.09375905275026397\n",
      "    At iteration 8600 -> loss: 0.09369792306818095\n",
      "    At iteration 8700 -> loss: 0.09369199619969652\n",
      "    At iteration 8800 -> loss: 0.0936902963798141\n",
      "    At iteration 8900 -> loss: 0.09364322184080834\n",
      "    At iteration 9000 -> loss: 0.09360791529963032\n",
      "    At iteration 9100 -> loss: 0.09357360179575243\n",
      "    At iteration 9200 -> loss: 0.09361442719951803\n",
      "    At iteration 9300 -> loss: 0.09362576885967408\n",
      "    At iteration 9400 -> loss: 0.09358166722793662\n",
      "    At iteration 9500 -> loss: 0.09358537396284708\n",
      "    At iteration 9600 -> loss: 0.0935510545746793\n",
      "    At iteration 9700 -> loss: 0.0935378002898944\n",
      "    At iteration 9800 -> loss: 0.09351073916083565\n",
      "    At iteration 9900 -> loss: 0.0935254946307634\n",
      "    At iteration 10000 -> loss: 0.09353623610202527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10100 -> loss: 0.09352184717167014\n",
      "    At iteration 10200 -> loss: 0.0934831771847807\n",
      "    At iteration 10300 -> loss: 0.09348539232728316\n",
      "    At iteration 10400 -> loss: 0.09350460473544805\n",
      "    At iteration 10500 -> loss: 0.09349497168065082\n",
      "    At iteration 10600 -> loss: 0.09354630528758927\n",
      "    At iteration 10700 -> loss: 0.09353471978194387\n",
      "    At iteration 10800 -> loss: 0.09350658147357498\n",
      "    At iteration 10900 -> loss: 0.093468759409749\n",
      "    At iteration 11000 -> loss: 0.09348454204590577\n",
      "    At iteration 11100 -> loss: 0.09347555944545748\n",
      "    At iteration 11200 -> loss: 0.0934769141820874\n",
      "    At iteration 11300 -> loss: 0.0934282968654078\n",
      "    At iteration 11400 -> loss: 0.09340772744354235\n",
      "    At iteration 11500 -> loss: 0.09337426409669419\n",
      "    At iteration 11600 -> loss: 0.0933414077814365\n",
      "    At iteration 11700 -> loss: 0.09342630765316033\n",
      "    At iteration 11800 -> loss: 0.09339054124479643\n",
      "    At iteration 11900 -> loss: 0.09336840623667983\n",
      "    At iteration 12000 -> loss: 0.09339261046475286\n",
      "    At iteration 12100 -> loss: 0.09336427028788936\n",
      "    At iteration 12200 -> loss: 0.09334069481812242\n",
      "    At iteration 12300 -> loss: 0.0933237773803424\n",
      "    At iteration 12400 -> loss: 0.09332084119458856\n",
      "    At iteration 12500 -> loss: 0.09330355757578189\n",
      "    At iteration 12600 -> loss: 0.09327761888350448\n",
      "    At iteration 12700 -> loss: 0.09326419032068223\n",
      "    At iteration 12800 -> loss: 0.09323292759418467\n",
      "    At iteration 12900 -> loss: 0.09320892938938312\n",
      "    At iteration 13000 -> loss: 0.09320924885492225\n",
      "    At iteration 13100 -> loss: 0.09323225847742694\n",
      "    At iteration 13200 -> loss: 0.09320475313285338\n",
      "    At iteration 13300 -> loss: 0.09319005382504482\n",
      "    At iteration 13400 -> loss: 0.09317556924555158\n",
      "    At iteration 13500 -> loss: 0.09315703418849046\n",
      "    At iteration 13600 -> loss: 0.09319403570636363\n",
      "Staring Epoch 12\n",
      "    At iteration 0 -> loss: 0.14082620199769735\n",
      "    At iteration 100 -> loss: 0.09236506727379627\n",
      "    At iteration 200 -> loss: 0.09145565119571164\n",
      "    At iteration 300 -> loss: 0.09085840736115199\n",
      "    At iteration 400 -> loss: 0.09286841844060667\n",
      "    At iteration 500 -> loss: 0.0945323382249613\n",
      "    At iteration 600 -> loss: 0.0941710949763552\n",
      "    At iteration 700 -> loss: 0.09374860063065935\n",
      "    At iteration 800 -> loss: 0.09298296701090836\n",
      "    At iteration 900 -> loss: 0.09267921890006574\n",
      "    At iteration 1000 -> loss: 0.09274694284017167\n",
      "    At iteration 1100 -> loss: 0.09269262890691497\n",
      "    At iteration 1200 -> loss: 0.0925625294664731\n",
      "    At iteration 1300 -> loss: 0.09265590182275836\n",
      "    At iteration 1400 -> loss: 0.09285347821100405\n",
      "    At iteration 1500 -> loss: 0.09263586275711146\n",
      "    At iteration 1600 -> loss: 0.09268319591435055\n",
      "    At iteration 1700 -> loss: 0.09311931466558647\n",
      "    At iteration 1800 -> loss: 0.09297341198341713\n",
      "    At iteration 1900 -> loss: 0.09290466129230071\n",
      "    At iteration 2000 -> loss: 0.09321671585531743\n",
      "    At iteration 2100 -> loss: 0.09301210956703777\n",
      "    At iteration 2200 -> loss: 0.0931106680658887\n",
      "    At iteration 2300 -> loss: 0.0930139265891257\n",
      "    At iteration 2400 -> loss: 0.0929031210639234\n",
      "    At iteration 2500 -> loss: 0.09279286882113565\n",
      "    At iteration 2600 -> loss: 0.09270112795471737\n",
      "    At iteration 2700 -> loss: 0.09272979104330631\n",
      "    At iteration 2800 -> loss: 0.092653151124514\n",
      "    At iteration 2900 -> loss: 0.09272078045532027\n",
      "    At iteration 3000 -> loss: 0.09265548091018098\n",
      "    At iteration 3100 -> loss: 0.09261819958245443\n",
      "    At iteration 3200 -> loss: 0.09261820616122568\n",
      "    At iteration 3300 -> loss: 0.0926167479458187\n",
      "    At iteration 3400 -> loss: 0.09258892675478247\n",
      "    At iteration 3500 -> loss: 0.09257509126762904\n",
      "    At iteration 3600 -> loss: 0.09254863057663971\n",
      "    At iteration 3700 -> loss: 0.09251817863298337\n",
      "    At iteration 3800 -> loss: 0.0924526769528331\n",
      "    At iteration 3900 -> loss: 0.09243854856177168\n",
      "    At iteration 4000 -> loss: 0.0924317403687502\n",
      "    At iteration 4100 -> loss: 0.09244672859561823\n",
      "    At iteration 4200 -> loss: 0.09240543801521815\n",
      "    At iteration 4300 -> loss: 0.09236717594157241\n",
      "    At iteration 4400 -> loss: 0.09231232003149349\n",
      "    At iteration 4500 -> loss: 0.09247410208291361\n",
      "    At iteration 4600 -> loss: 0.09244259321352151\n",
      "    At iteration 4700 -> loss: 0.09241718472028118\n",
      "    At iteration 4800 -> loss: 0.09247821035695974\n",
      "    At iteration 4900 -> loss: 0.09245045541337059\n",
      "    At iteration 5000 -> loss: 0.09251939888880188\n",
      "    At iteration 5100 -> loss: 0.09262966060058161\n",
      "    At iteration 5200 -> loss: 0.09261273270416116\n",
      "    At iteration 5300 -> loss: 0.09258749456227586\n",
      "    At iteration 5400 -> loss: 0.09256339165763969\n",
      "    At iteration 5500 -> loss: 0.0926369736317165\n",
      "    At iteration 5600 -> loss: 0.0926387762213751\n",
      "    At iteration 5700 -> loss: 0.09280590965534007\n",
      "    At iteration 5800 -> loss: 0.09276348472422241\n",
      "    At iteration 5900 -> loss: 0.09274553215065001\n",
      "    At iteration 6000 -> loss: 0.09270920993282836\n",
      "    At iteration 6100 -> loss: 0.09271883722654403\n",
      "    At iteration 6200 -> loss: 0.0927842537176015\n",
      "    At iteration 6300 -> loss: 0.09274018831811008\n",
      "    At iteration 6400 -> loss: 0.09277593357029754\n",
      "    At iteration 6500 -> loss: 0.09287416185824492\n",
      "    At iteration 6600 -> loss: 0.092820738542138\n",
      "    At iteration 6700 -> loss: 0.0928176353333202\n",
      "    At iteration 6800 -> loss: 0.09277659402383263\n",
      "    At iteration 6900 -> loss: 0.09279273912020543\n",
      "    At iteration 7000 -> loss: 0.09283380094669379\n",
      "    At iteration 7100 -> loss: 0.0928339784676021\n",
      "    At iteration 7200 -> loss: 0.09279897822940587\n",
      "    At iteration 7300 -> loss: 0.0928929814531359\n",
      "    At iteration 7400 -> loss: 0.0929456089321745\n",
      "    At iteration 7500 -> loss: 0.09298088814872643\n",
      "    At iteration 7600 -> loss: 0.0929447003296992\n",
      "    At iteration 7700 -> loss: 0.09291710214684451\n",
      "    At iteration 7800 -> loss: 0.09288808360965833\n",
      "    At iteration 7900 -> loss: 0.09287547030960332\n",
      "    At iteration 8000 -> loss: 0.09291473052686536\n",
      "    At iteration 8100 -> loss: 0.09292285503377858\n",
      "    At iteration 8200 -> loss: 0.09289297305119582\n",
      "    At iteration 8300 -> loss: 0.09288986649011229\n",
      "    At iteration 8400 -> loss: 0.09284748028518368\n",
      "    At iteration 8500 -> loss: 0.09286908017665066\n",
      "    At iteration 8600 -> loss: 0.09285048669086181\n",
      "    At iteration 8700 -> loss: 0.09280954156458461\n",
      "    At iteration 8800 -> loss: 0.09287618766229955\n",
      "    At iteration 8900 -> loss: 0.09295805798755069\n",
      "    At iteration 9000 -> loss: 0.09302214706531718\n",
      "    At iteration 9100 -> loss: 0.09301162484767235\n",
      "    At iteration 9200 -> loss: 0.09297132792234547\n",
      "    At iteration 9300 -> loss: 0.09293486411954739\n",
      "    At iteration 9400 -> loss: 0.09294596843477289\n",
      "    At iteration 9500 -> loss: 0.09293426165039217\n",
      "    At iteration 9600 -> loss: 0.09291772305999117\n",
      "    At iteration 9700 -> loss: 0.0929418042674883\n",
      "    At iteration 9800 -> loss: 0.09296875126452153\n",
      "    At iteration 9900 -> loss: 0.09293144425971392\n",
      "    At iteration 10000 -> loss: 0.09300840764566996\n",
      "    At iteration 10100 -> loss: 0.0930503436588317\n",
      "    At iteration 10200 -> loss: 0.09307725928246113\n",
      "    At iteration 10300 -> loss: 0.09304324144694721\n",
      "    At iteration 10400 -> loss: 0.09303894652656088\n",
      "    At iteration 10500 -> loss: 0.09302115341962036\n",
      "    At iteration 10600 -> loss: 0.09301263071909298\n",
      "    At iteration 10700 -> loss: 0.0930045009661457\n",
      "    At iteration 10800 -> loss: 0.09308015414671725\n",
      "    At iteration 10900 -> loss: 0.0931058885542693\n",
      "    At iteration 11000 -> loss: 0.09310490669953576\n",
      "    At iteration 11100 -> loss: 0.09310589192978652\n",
      "    At iteration 11200 -> loss: 0.09307942777896068\n",
      "    At iteration 11300 -> loss: 0.09305403682593581\n",
      "    At iteration 11400 -> loss: 0.09302349290914248\n",
      "    At iteration 11500 -> loss: 0.09305804894625462\n",
      "    At iteration 11600 -> loss: 0.09308085704092431\n",
      "    At iteration 11700 -> loss: 0.09307740790682652\n",
      "    At iteration 11800 -> loss: 0.093058770375798\n",
      "    At iteration 11900 -> loss: 0.09308347182080944\n",
      "    At iteration 12000 -> loss: 0.09309544775406732\n",
      "    At iteration 12100 -> loss: 0.09307782106455229\n",
      "    At iteration 12200 -> loss: 0.09304678424711126\n",
      "    At iteration 12300 -> loss: 0.09303466234848774\n",
      "    At iteration 12400 -> loss: 0.09303075298172125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12500 -> loss: 0.09307496540351855\n",
      "    At iteration 12600 -> loss: 0.09304778177431494\n",
      "    At iteration 12700 -> loss: 0.09303563089913183\n",
      "    At iteration 12800 -> loss: 0.09327427861556563\n",
      "    At iteration 12900 -> loss: 0.0932743178787067\n",
      "    At iteration 13000 -> loss: 0.09326002460679764\n",
      "    At iteration 13100 -> loss: 0.09324409654460257\n",
      "    At iteration 13200 -> loss: 0.0932219893634351\n",
      "    At iteration 13300 -> loss: 0.09320205213920135\n",
      "    At iteration 13400 -> loss: 0.09322040144459774\n",
      "    At iteration 13500 -> loss: 0.09319986951795046\n",
      "    At iteration 13600 -> loss: 0.09318367835831565\n",
      "Staring Epoch 13\n",
      "    At iteration 0 -> loss: 0.08523364516440779\n",
      "    At iteration 100 -> loss: 0.08981751117061694\n",
      "    At iteration 200 -> loss: 0.09011835929665303\n",
      "    At iteration 300 -> loss: 0.0898673214872494\n",
      "    At iteration 400 -> loss: 0.08990748805509326\n",
      "    At iteration 500 -> loss: 0.09025726792262806\n",
      "    At iteration 600 -> loss: 0.0904513693214831\n",
      "    At iteration 700 -> loss: 0.0909175641381866\n",
      "    At iteration 800 -> loss: 0.09182993986221848\n",
      "    At iteration 900 -> loss: 0.09167774017566806\n",
      "    At iteration 1000 -> loss: 0.09180747907112918\n",
      "    At iteration 1100 -> loss: 0.092249324415208\n",
      "    At iteration 1200 -> loss: 0.09206267008462393\n",
      "    At iteration 1300 -> loss: 0.09206124914050945\n",
      "    At iteration 1400 -> loss: 0.09196421358939783\n",
      "    At iteration 1500 -> loss: 0.09184875731874349\n",
      "    At iteration 1600 -> loss: 0.09204975781128798\n",
      "    At iteration 1700 -> loss: 0.09201322795657525\n",
      "    At iteration 1800 -> loss: 0.0919970480001477\n",
      "    At iteration 1900 -> loss: 0.09223215026819191\n",
      "    At iteration 2000 -> loss: 0.09232760715059504\n",
      "    At iteration 2100 -> loss: 0.09223370844573137\n",
      "    At iteration 2200 -> loss: 0.09222027346499184\n",
      "    At iteration 2300 -> loss: 0.09239469391458426\n",
      "    At iteration 2400 -> loss: 0.0924956042145612\n",
      "    At iteration 2500 -> loss: 0.09247815530623804\n",
      "    At iteration 2600 -> loss: 0.09258988766467385\n",
      "    At iteration 2700 -> loss: 0.09295211655746122\n",
      "    At iteration 2800 -> loss: 0.09294895163760157\n",
      "    At iteration 2900 -> loss: 0.09285004184293154\n",
      "    At iteration 3000 -> loss: 0.092857240177348\n",
      "    At iteration 3100 -> loss: 0.0928418819304455\n",
      "    At iteration 3200 -> loss: 0.09280770276575566\n",
      "    At iteration 3300 -> loss: 0.0927710270855524\n",
      "    At iteration 3400 -> loss: 0.09293652211451195\n",
      "    At iteration 3500 -> loss: 0.09313105932026153\n",
      "    At iteration 3600 -> loss: 0.09304418942419798\n",
      "    At iteration 3700 -> loss: 0.09298061033413928\n",
      "    At iteration 3800 -> loss: 0.09290336866999463\n",
      "    At iteration 3900 -> loss: 0.09287618359923129\n",
      "    At iteration 4000 -> loss: 0.09282422349486333\n",
      "    At iteration 4100 -> loss: 0.09279510091020092\n",
      "    At iteration 4200 -> loss: 0.09287296550878986\n",
      "    At iteration 4300 -> loss: 0.09286376927067866\n",
      "    At iteration 4400 -> loss: 0.0928706784958683\n",
      "    At iteration 4500 -> loss: 0.09292016825510173\n",
      "    At iteration 4600 -> loss: 0.09287858988544428\n",
      "    At iteration 4700 -> loss: 0.09283474976301896\n",
      "    At iteration 4800 -> loss: 0.09290921614451185\n",
      "    At iteration 4900 -> loss: 0.09283599165186567\n",
      "    At iteration 5000 -> loss: 0.09298320020994343\n",
      "    At iteration 5100 -> loss: 0.0929027507695812\n",
      "    At iteration 5200 -> loss: 0.0928547968601458\n",
      "    At iteration 5300 -> loss: 0.09289270753274119\n",
      "    At iteration 5400 -> loss: 0.0930290570055654\n",
      "    At iteration 5500 -> loss: 0.09299686124917914\n",
      "    At iteration 5600 -> loss: 0.09299071824052046\n",
      "    At iteration 5700 -> loss: 0.09292848889978048\n",
      "    At iteration 5800 -> loss: 0.0929285225503145\n",
      "    At iteration 5900 -> loss: 0.09293410664562185\n",
      "    At iteration 6000 -> loss: 0.09295196761555768\n",
      "    At iteration 6100 -> loss: 0.09292823132543\n",
      "    At iteration 6200 -> loss: 0.09300124188092183\n",
      "    At iteration 6300 -> loss: 0.09297881686540126\n",
      "    At iteration 6400 -> loss: 0.09293506849825768\n",
      "    At iteration 6500 -> loss: 0.09289578363116344\n",
      "    At iteration 6600 -> loss: 0.09288673364626596\n",
      "    At iteration 6700 -> loss: 0.09287992314110542\n",
      "    At iteration 6800 -> loss: 0.09281786428351341\n",
      "    At iteration 6900 -> loss: 0.09282758357758042\n",
      "    At iteration 7000 -> loss: 0.0928058639646289\n",
      "    At iteration 7100 -> loss: 0.09276835286876293\n",
      "    At iteration 7200 -> loss: 0.09274622453272478\n",
      "    At iteration 7300 -> loss: 0.09275586129832139\n",
      "    At iteration 7400 -> loss: 0.09294159317889088\n",
      "    At iteration 7500 -> loss: 0.09305431902948544\n",
      "    At iteration 7600 -> loss: 0.09313179285690663\n",
      "    At iteration 7700 -> loss: 0.0931981361181766\n",
      "    At iteration 7800 -> loss: 0.09322539343833709\n",
      "    At iteration 7900 -> loss: 0.09321544151208545\n",
      "    At iteration 8000 -> loss: 0.09321306502028837\n",
      "    At iteration 8100 -> loss: 0.09317797063832652\n",
      "    At iteration 8200 -> loss: 0.09316137981188365\n",
      "    At iteration 8300 -> loss: 0.09316589295213373\n",
      "    At iteration 8400 -> loss: 0.093139202279945\n",
      "    At iteration 8500 -> loss: 0.0930964556246087\n",
      "    At iteration 8600 -> loss: 0.09309583308787839\n",
      "    At iteration 8700 -> loss: 0.09306919253034827\n",
      "    At iteration 8800 -> loss: 0.09304865466591805\n",
      "    At iteration 8900 -> loss: 0.09305321490862781\n",
      "    At iteration 9000 -> loss: 0.09301730089319776\n",
      "    At iteration 9100 -> loss: 0.09299009520154308\n",
      "    At iteration 9200 -> loss: 0.09296084109432017\n",
      "    At iteration 9300 -> loss: 0.0929501000718198\n",
      "    At iteration 9400 -> loss: 0.09292116223456162\n",
      "    At iteration 9500 -> loss: 0.09289147027421862\n",
      "    At iteration 9600 -> loss: 0.09289394889312631\n",
      "    At iteration 9700 -> loss: 0.09287861343727707\n",
      "    At iteration 9800 -> loss: 0.09284211374690055\n",
      "    At iteration 9900 -> loss: 0.09280615194403492\n",
      "    At iteration 10000 -> loss: 0.09278249034347806\n",
      "    At iteration 10100 -> loss: 0.09312275108513539\n",
      "    At iteration 10200 -> loss: 0.09309717773118834\n",
      "    At iteration 10300 -> loss: 0.09316164377052227\n",
      "    At iteration 10400 -> loss: 0.09333173420742219\n",
      "    At iteration 10500 -> loss: 0.09331865585107428\n",
      "    At iteration 10600 -> loss: 0.09330587153447442\n",
      "    At iteration 10700 -> loss: 0.0932853550832565\n",
      "    At iteration 10800 -> loss: 0.09327718349192797\n",
      "    At iteration 10900 -> loss: 0.09324959128794871\n",
      "    At iteration 11000 -> loss: 0.09325447042448604\n",
      "    At iteration 11100 -> loss: 0.09324772657179496\n",
      "    At iteration 11200 -> loss: 0.09324805470984256\n",
      "    At iteration 11300 -> loss: 0.09323202738538307\n",
      "    At iteration 11400 -> loss: 0.09323532579143594\n",
      "    At iteration 11500 -> loss: 0.09320395915671342\n",
      "    At iteration 11600 -> loss: 0.09319431248533226\n",
      "    At iteration 11700 -> loss: 0.09318630613498968\n",
      "    At iteration 11800 -> loss: 0.09318861178378494\n",
      "    At iteration 11900 -> loss: 0.09317146419369296\n",
      "    At iteration 12000 -> loss: 0.09319472530765478\n",
      "    At iteration 12100 -> loss: 0.09323137120828874\n",
      "    At iteration 12200 -> loss: 0.09321141469291543\n",
      "    At iteration 12300 -> loss: 0.0932417627264348\n",
      "    At iteration 12400 -> loss: 0.09325021747237605\n",
      "    At iteration 12500 -> loss: 0.09333861971950422\n",
      "    At iteration 12600 -> loss: 0.09331339454775489\n",
      "    At iteration 12700 -> loss: 0.09329482402693497\n",
      "    At iteration 12800 -> loss: 0.09327165505116738\n",
      "    At iteration 12900 -> loss: 0.09324484524340355\n",
      "    At iteration 13000 -> loss: 0.0932220752816096\n",
      "    At iteration 13100 -> loss: 0.09323072984224519\n",
      "    At iteration 13200 -> loss: 0.09324074050126113\n",
      "    At iteration 13300 -> loss: 0.0932178737879423\n",
      "    At iteration 13400 -> loss: 0.09318510676878483\n",
      "    At iteration 13500 -> loss: 0.09319451409117212\n",
      "    At iteration 13600 -> loss: 0.09316995547179778\n",
      "Staring Epoch 14\n",
      "    At iteration 0 -> loss: 0.08052770904032513\n",
      "    At iteration 100 -> loss: 0.09459451196126772\n",
      "    At iteration 200 -> loss: 0.09437446524003747\n",
      "    At iteration 300 -> loss: 0.09345160807276452\n",
      "    At iteration 400 -> loss: 0.09335537571599706\n",
      "    At iteration 500 -> loss: 0.09295391306038528\n",
      "    At iteration 600 -> loss: 0.09346775354722153\n",
      "    At iteration 700 -> loss: 0.09353371273462396\n",
      "    At iteration 800 -> loss: 0.0931638765853243\n",
      "    At iteration 900 -> loss: 0.09267728451097244\n",
      "    At iteration 1000 -> loss: 0.09260770489475385\n",
      "    At iteration 1100 -> loss: 0.0928476096349239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1200 -> loss: 0.09285844028314384\n",
      "    At iteration 1300 -> loss: 0.09283779484140477\n",
      "    At iteration 1400 -> loss: 0.09285575382805751\n",
      "    At iteration 1500 -> loss: 0.09296992785640008\n",
      "    At iteration 1600 -> loss: 0.09300334756798379\n",
      "    At iteration 1700 -> loss: 0.09468986024532534\n",
      "    At iteration 1800 -> loss: 0.09464555361588636\n",
      "    At iteration 1900 -> loss: 0.09460355354944681\n",
      "    At iteration 2000 -> loss: 0.0948416079479696\n",
      "    At iteration 2100 -> loss: 0.09459546890205467\n",
      "    At iteration 2200 -> loss: 0.09439087949133247\n",
      "    At iteration 2300 -> loss: 0.09431202183405818\n",
      "    At iteration 2400 -> loss: 0.09411914695465586\n",
      "    At iteration 2500 -> loss: 0.09401544942680498\n",
      "    At iteration 2600 -> loss: 0.09378457543078056\n",
      "    At iteration 2700 -> loss: 0.09365624335665881\n",
      "    At iteration 2800 -> loss: 0.09369674690599104\n",
      "    At iteration 2900 -> loss: 0.09362109986890879\n",
      "    At iteration 3000 -> loss: 0.09350932119487515\n",
      "    At iteration 3100 -> loss: 0.09342427947392422\n",
      "    At iteration 3200 -> loss: 0.09331399191488814\n",
      "    At iteration 3300 -> loss: 0.09321910935858156\n",
      "    At iteration 3400 -> loss: 0.09313603173715042\n",
      "    At iteration 3500 -> loss: 0.09304879634766677\n",
      "    At iteration 3600 -> loss: 0.09309608122417043\n",
      "    At iteration 3700 -> loss: 0.09302620217048443\n",
      "    At iteration 3800 -> loss: 0.0930005784163845\n",
      "    At iteration 3900 -> loss: 0.09297010807192463\n",
      "    At iteration 4000 -> loss: 0.0928987907214516\n",
      "    At iteration 4100 -> loss: 0.09289926746122162\n",
      "    At iteration 4200 -> loss: 0.09284309194296497\n",
      "    At iteration 4300 -> loss: 0.0929542124443419\n",
      "    At iteration 4400 -> loss: 0.09290827176059006\n",
      "    At iteration 4500 -> loss: 0.09283551168129744\n",
      "    At iteration 4600 -> loss: 0.09280213049872106\n",
      "    At iteration 4700 -> loss: 0.09310928221566668\n",
      "    At iteration 4800 -> loss: 0.09302477309377148\n",
      "    At iteration 4900 -> loss: 0.09296274501369552\n",
      "    At iteration 5000 -> loss: 0.09288828007470844\n",
      "    At iteration 5100 -> loss: 0.0929299539346332\n",
      "    At iteration 5200 -> loss: 0.09291245657039499\n",
      "    At iteration 5300 -> loss: 0.09294015107886397\n",
      "    At iteration 5400 -> loss: 0.09314327251565639\n",
      "    At iteration 5500 -> loss: 0.09318939930307105\n",
      "    At iteration 5600 -> loss: 0.09320465075928121\n",
      "    At iteration 5700 -> loss: 0.09330940006300241\n",
      "    At iteration 5800 -> loss: 0.09327104051115505\n",
      "    At iteration 5900 -> loss: 0.09330212854720062\n",
      "    At iteration 6000 -> loss: 0.09331417571076826\n",
      "    At iteration 6100 -> loss: 0.09326809010717818\n",
      "    At iteration 6200 -> loss: 0.0931967365674755\n",
      "    At iteration 6300 -> loss: 0.09334591761073717\n",
      "    At iteration 6400 -> loss: 0.09332658825569971\n",
      "    At iteration 6500 -> loss: 0.09326726624922983\n",
      "    At iteration 6600 -> loss: 0.09342338607229307\n",
      "    At iteration 6700 -> loss: 0.09342196118690568\n",
      "    At iteration 6800 -> loss: 0.09337267619045526\n",
      "    At iteration 6900 -> loss: 0.09335408156679334\n",
      "    At iteration 7000 -> loss: 0.09330945835306083\n",
      "    At iteration 7100 -> loss: 0.09347543615810687\n",
      "    At iteration 7200 -> loss: 0.09346649375782384\n",
      "    At iteration 7300 -> loss: 0.09343902336688911\n",
      "    At iteration 7400 -> loss: 0.09343649887966392\n",
      "    At iteration 7500 -> loss: 0.09344454472195603\n",
      "    At iteration 7600 -> loss: 0.09345011417423055\n",
      "    At iteration 7700 -> loss: 0.09350212506378712\n",
      "    At iteration 7800 -> loss: 0.09349362852169528\n",
      "    At iteration 7900 -> loss: 0.09349410940648849\n",
      "    At iteration 8000 -> loss: 0.0934447624360826\n",
      "    At iteration 8100 -> loss: 0.09342055813283846\n",
      "    At iteration 8200 -> loss: 0.0933821708940181\n",
      "    At iteration 8300 -> loss: 0.09345535595685062\n",
      "    At iteration 8400 -> loss: 0.09339764945078831\n",
      "    At iteration 8500 -> loss: 0.09338591096111747\n",
      "    At iteration 8600 -> loss: 0.09341900916518964\n",
      "    At iteration 8700 -> loss: 0.09340663473931773\n",
      "    At iteration 8800 -> loss: 0.09343738294855519\n",
      "    At iteration 8900 -> loss: 0.09343101780619319\n",
      "    At iteration 9000 -> loss: 0.09341046796638172\n",
      "    At iteration 9100 -> loss: 0.0933959707084225\n",
      "    At iteration 9200 -> loss: 0.09338520239626458\n",
      "    At iteration 9300 -> loss: 0.09335706622288772\n",
      "    At iteration 9400 -> loss: 0.09331538747172125\n",
      "    At iteration 9500 -> loss: 0.0933321259853927\n",
      "    At iteration 9600 -> loss: 0.09327907593934186\n",
      "    At iteration 9700 -> loss: 0.09324824813194328\n",
      "    At iteration 9800 -> loss: 0.09322460089239532\n",
      "    At iteration 9900 -> loss: 0.09320520202718251\n",
      "    At iteration 10000 -> loss: 0.09324804638917482\n",
      "    At iteration 10100 -> loss: 0.09324026261876835\n",
      "    At iteration 10200 -> loss: 0.09321853536648807\n",
      "    At iteration 10300 -> loss: 0.0932172940278213\n",
      "    At iteration 10400 -> loss: 0.09319499869805944\n",
      "    At iteration 10500 -> loss: 0.09326812718473251\n",
      "    At iteration 10600 -> loss: 0.09324824321173357\n",
      "    At iteration 10700 -> loss: 0.09321306614914612\n",
      "    At iteration 10800 -> loss: 0.09327132408282628\n",
      "    At iteration 10900 -> loss: 0.09325410727596994\n",
      "    At iteration 11000 -> loss: 0.09331799829924821\n",
      "    At iteration 11100 -> loss: 0.09330169840566838\n",
      "    At iteration 11200 -> loss: 0.09329007823560137\n",
      "    At iteration 11300 -> loss: 0.09328746488913817\n",
      "    At iteration 11400 -> loss: 0.09329449276744953\n",
      "    At iteration 11500 -> loss: 0.09327174357291507\n",
      "    At iteration 11600 -> loss: 0.09328763040070934\n",
      "    At iteration 11700 -> loss: 0.09326593469208533\n",
      "    At iteration 11800 -> loss: 0.09325019948276513\n",
      "    At iteration 11900 -> loss: 0.09322335365109445\n",
      "    At iteration 12000 -> loss: 0.09326818602058631\n",
      "    At iteration 12100 -> loss: 0.09329528270472273\n",
      "    At iteration 12200 -> loss: 0.09326903265638214\n",
      "    At iteration 12300 -> loss: 0.0932461948156397\n",
      "    At iteration 12400 -> loss: 0.09322141083663785\n",
      "    At iteration 12500 -> loss: 0.09320848352764781\n",
      "    At iteration 12600 -> loss: 0.0932117538170644\n",
      "    At iteration 12700 -> loss: 0.09322140905931399\n",
      "    At iteration 12800 -> loss: 0.09319801039039174\n",
      "    At iteration 12900 -> loss: 0.0931906729218369\n",
      "    At iteration 13000 -> loss: 0.09320441506419434\n",
      "    At iteration 13100 -> loss: 0.0932145683726622\n",
      "    At iteration 13200 -> loss: 0.09319249728427338\n",
      "    At iteration 13300 -> loss: 0.09319788057460678\n",
      "    At iteration 13400 -> loss: 0.09322726678313925\n",
      "    At iteration 13500 -> loss: 0.09321130842107886\n",
      "    At iteration 13600 -> loss: 0.09318725097877065\n",
      "Staring Epoch 15\n",
      "    At iteration 0 -> loss: 0.07998543040957884\n",
      "    At iteration 100 -> loss: 0.09753070552995433\n",
      "    At iteration 200 -> loss: 0.09505630915507182\n",
      "    At iteration 300 -> loss: 0.09357317223356766\n",
      "    At iteration 400 -> loss: 0.0954591401901231\n",
      "    At iteration 500 -> loss: 0.0951289238565119\n",
      "    At iteration 600 -> loss: 0.09458488113439134\n",
      "    At iteration 700 -> loss: 0.09422148166433082\n",
      "    At iteration 800 -> loss: 0.09398490642483955\n",
      "    At iteration 900 -> loss: 0.09353091556783723\n",
      "    At iteration 1000 -> loss: 0.09349997439213864\n",
      "    At iteration 1100 -> loss: 0.0932236299575099\n",
      "    At iteration 1200 -> loss: 0.09297632628120049\n",
      "    At iteration 1300 -> loss: 0.09266223191135095\n",
      "    At iteration 1400 -> loss: 0.09250048031417701\n",
      "    At iteration 1500 -> loss: 0.09245842777004322\n",
      "    At iteration 1600 -> loss: 0.09232509324958892\n",
      "    At iteration 1700 -> loss: 0.0922691075507415\n",
      "    At iteration 1800 -> loss: 0.09253329046328493\n",
      "    At iteration 1900 -> loss: 0.09262173443009769\n",
      "    At iteration 2000 -> loss: 0.09288322984935797\n",
      "    At iteration 2100 -> loss: 0.09276324152873598\n",
      "    At iteration 2200 -> loss: 0.09289269103243455\n",
      "    At iteration 2300 -> loss: 0.09327049086326804\n",
      "    At iteration 2400 -> loss: 0.09314003233643896\n",
      "    At iteration 2500 -> loss: 0.09303908654844323\n",
      "    At iteration 2600 -> loss: 0.09287749409948236\n",
      "    At iteration 2700 -> loss: 0.09279642948342096\n",
      "    At iteration 2800 -> loss: 0.09279970928824946\n",
      "    At iteration 2900 -> loss: 0.09281227668242095\n",
      "    At iteration 3000 -> loss: 0.09316837732629554\n",
      "    At iteration 3100 -> loss: 0.09336123797804874\n",
      "    At iteration 3200 -> loss: 0.0934034463451753\n",
      "    At iteration 3300 -> loss: 0.09349391900083893\n",
      "    At iteration 3400 -> loss: 0.09338240367187939\n",
      "    At iteration 3500 -> loss: 0.09335622390143632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3600 -> loss: 0.09337717428491736\n",
      "    At iteration 3700 -> loss: 0.0932871127265962\n",
      "    At iteration 3800 -> loss: 0.09322188243351294\n",
      "    At iteration 3900 -> loss: 0.09337048733754698\n",
      "    At iteration 4000 -> loss: 0.09327533850053978\n",
      "    At iteration 4100 -> loss: 0.09326929679945657\n",
      "    At iteration 4200 -> loss: 0.09319172583444513\n",
      "    At iteration 4300 -> loss: 0.09319956811107617\n",
      "    At iteration 4400 -> loss: 0.0931162673013359\n",
      "    At iteration 4500 -> loss: 0.09305250351755874\n",
      "    At iteration 4600 -> loss: 0.09307765755167928\n",
      "    At iteration 4700 -> loss: 0.09301662090181206\n",
      "    At iteration 4800 -> loss: 0.0929741132282559\n",
      "    At iteration 4900 -> loss: 0.09300772861624768\n",
      "    At iteration 5000 -> loss: 0.0929716632443095\n",
      "    At iteration 5100 -> loss: 0.09294741015249085\n",
      "    At iteration 5200 -> loss: 0.09291718843962443\n",
      "    At iteration 5300 -> loss: 0.09291255238181917\n",
      "    At iteration 5400 -> loss: 0.09286536817917893\n",
      "    At iteration 5500 -> loss: 0.09282701998392295\n",
      "    At iteration 5600 -> loss: 0.09301235463338031\n",
      "    At iteration 5700 -> loss: 0.09297170553177696\n",
      "    At iteration 5800 -> loss: 0.09290611965392456\n",
      "    At iteration 5900 -> loss: 0.09300344557049989\n",
      "    At iteration 6000 -> loss: 0.09305659543678647\n",
      "    At iteration 6100 -> loss: 0.09304085260409836\n",
      "    At iteration 6200 -> loss: 0.09300153851120108\n",
      "    At iteration 6300 -> loss: 0.09295957727067115\n",
      "    At iteration 6400 -> loss: 0.09295824738985646\n",
      "    At iteration 6500 -> loss: 0.09296222808237996\n",
      "    At iteration 6600 -> loss: 0.09294910868768162\n",
      "    At iteration 6700 -> loss: 0.0931220963059193\n",
      "    At iteration 6800 -> loss: 0.09313130012979547\n",
      "    At iteration 6900 -> loss: 0.09310155777935449\n",
      "    At iteration 7000 -> loss: 0.09308930817126186\n",
      "    At iteration 7100 -> loss: 0.09309543482611081\n",
      "    At iteration 7200 -> loss: 0.09311554889211085\n",
      "    At iteration 7300 -> loss: 0.09311789178588731\n",
      "    At iteration 7400 -> loss: 0.09309214090551349\n",
      "    At iteration 7500 -> loss: 0.09308777425885524\n",
      "    At iteration 7600 -> loss: 0.09306864588113509\n",
      "    At iteration 7700 -> loss: 0.0930226269997073\n",
      "    At iteration 7800 -> loss: 0.09300634421687966\n",
      "    At iteration 7900 -> loss: 0.09300536680267156\n",
      "    At iteration 8000 -> loss: 0.09298607057685425\n",
      "    At iteration 8100 -> loss: 0.09295560559108705\n",
      "    At iteration 8200 -> loss: 0.09297033444862308\n",
      "    At iteration 8300 -> loss: 0.09292422422407505\n",
      "    At iteration 8400 -> loss: 0.09289692608232343\n",
      "    At iteration 8500 -> loss: 0.09292444721315848\n",
      "    At iteration 8600 -> loss: 0.09290730553334178\n",
      "    At iteration 8700 -> loss: 0.09288787477427998\n",
      "    At iteration 8800 -> loss: 0.09286327164579584\n",
      "    At iteration 8900 -> loss: 0.09285505882533535\n",
      "    At iteration 9000 -> loss: 0.09281924272854562\n",
      "    At iteration 9100 -> loss: 0.09287202581311144\n",
      "    At iteration 9200 -> loss: 0.09286849765686148\n",
      "    At iteration 9300 -> loss: 0.09286124893386218\n",
      "    At iteration 9400 -> loss: 0.09283382029179657\n",
      "    At iteration 9500 -> loss: 0.09281438119921774\n",
      "    At iteration 9600 -> loss: 0.09279928539903803\n",
      "    At iteration 9700 -> loss: 0.09286019326184976\n",
      "    At iteration 9800 -> loss: 0.09282014585565376\n",
      "    At iteration 9900 -> loss: 0.09284749792007438\n",
      "    At iteration 10000 -> loss: 0.09292242507327993\n",
      "    At iteration 10100 -> loss: 0.09301099169307175\n",
      "    At iteration 10200 -> loss: 0.09305023560222221\n",
      "    At iteration 10300 -> loss: 0.0930130723869041\n",
      "    At iteration 10400 -> loss: 0.09301042630109058\n",
      "    At iteration 10500 -> loss: 0.09300126301549731\n",
      "    At iteration 10600 -> loss: 0.09298770510679112\n",
      "    At iteration 10700 -> loss: 0.09296059456644352\n",
      "    At iteration 10800 -> loss: 0.09295804026971216\n",
      "    At iteration 10900 -> loss: 0.09297981415001796\n",
      "    At iteration 11000 -> loss: 0.09298160328254637\n",
      "    At iteration 11100 -> loss: 0.09298282873820306\n",
      "    At iteration 11200 -> loss: 0.09309107809562614\n",
      "    At iteration 11300 -> loss: 0.09312240314087578\n",
      "    At iteration 11400 -> loss: 0.09310792950176634\n",
      "    At iteration 11500 -> loss: 0.09307774805329747\n",
      "    At iteration 11600 -> loss: 0.09311242715790316\n",
      "    At iteration 11700 -> loss: 0.09309738571913327\n",
      "    At iteration 11800 -> loss: 0.09309099825425605\n",
      "    At iteration 11900 -> loss: 0.09306146655815693\n",
      "    At iteration 12000 -> loss: 0.09335593664052984\n",
      "    At iteration 12100 -> loss: 0.09334822259776744\n",
      "    At iteration 12200 -> loss: 0.09336906168317077\n",
      "    At iteration 12300 -> loss: 0.09335347879335766\n",
      "    At iteration 12400 -> loss: 0.09333978997602693\n",
      "    At iteration 12500 -> loss: 0.09333403748527039\n",
      "    At iteration 12600 -> loss: 0.09330200482775598\n",
      "    At iteration 12700 -> loss: 0.09327007730601955\n",
      "    At iteration 12800 -> loss: 0.09327539427753542\n",
      "    At iteration 12900 -> loss: 0.09324683458528814\n",
      "    At iteration 13000 -> loss: 0.09324106519343474\n",
      "    At iteration 13100 -> loss: 0.09323940275519937\n",
      "    At iteration 13200 -> loss: 0.09323338459964901\n",
      "    At iteration 13300 -> loss: 0.09322822060603515\n",
      "    At iteration 13400 -> loss: 0.09318721417716784\n",
      "    At iteration 13500 -> loss: 0.09317188212757875\n",
      "    At iteration 13600 -> loss: 0.09315637527575628\n",
      "Staring Epoch 16\n",
      "    At iteration 0 -> loss: 0.10296857031062245\n",
      "    At iteration 100 -> loss: 0.08952413952478339\n",
      "    At iteration 200 -> loss: 0.09207068250073842\n",
      "    At iteration 300 -> loss: 0.09450260295577365\n",
      "    At iteration 400 -> loss: 0.09368967114128332\n",
      "    At iteration 500 -> loss: 0.09305212545774341\n",
      "    At iteration 600 -> loss: 0.09251300764134027\n",
      "    At iteration 700 -> loss: 0.09221532469575737\n",
      "    At iteration 800 -> loss: 0.09234179402770638\n",
      "    At iteration 900 -> loss: 0.09207431312760443\n",
      "    At iteration 1000 -> loss: 0.09188930779603065\n",
      "    At iteration 1100 -> loss: 0.09258893892179818\n",
      "    At iteration 1200 -> loss: 0.09354001466793162\n",
      "    At iteration 1300 -> loss: 0.09329102749987725\n",
      "    At iteration 1400 -> loss: 0.0930906255998316\n",
      "    At iteration 1500 -> loss: 0.09311525265403138\n",
      "    At iteration 1600 -> loss: 0.0928733745851927\n",
      "    At iteration 1700 -> loss: 0.09314746994174652\n",
      "    At iteration 1800 -> loss: 0.093047329529667\n",
      "    At iteration 1900 -> loss: 0.09300605397376549\n",
      "    At iteration 2000 -> loss: 0.09300610759180836\n",
      "    At iteration 2100 -> loss: 0.09307786738163966\n",
      "    At iteration 2200 -> loss: 0.09296145604100586\n",
      "    At iteration 2300 -> loss: 0.0929711441058426\n",
      "    At iteration 2400 -> loss: 0.09315256243614631\n",
      "    At iteration 2500 -> loss: 0.09308604853389899\n",
      "    At iteration 2600 -> loss: 0.09316207023423677\n",
      "    At iteration 2700 -> loss: 0.09304665214593619\n",
      "    At iteration 2800 -> loss: 0.09298822808212236\n",
      "    At iteration 2900 -> loss: 0.09290188124219881\n",
      "    At iteration 3000 -> loss: 0.09297396282104335\n",
      "    At iteration 3100 -> loss: 0.09290629061160369\n",
      "    At iteration 3200 -> loss: 0.09292660464528249\n",
      "    At iteration 3300 -> loss: 0.0929785778613876\n",
      "    At iteration 3400 -> loss: 0.09311533292971866\n",
      "    At iteration 3500 -> loss: 0.09307806161204399\n",
      "    At iteration 3600 -> loss: 0.09309076111123667\n",
      "    At iteration 3700 -> loss: 0.0931287690454314\n",
      "    At iteration 3800 -> loss: 0.09310739380809083\n",
      "    At iteration 3900 -> loss: 0.09328288244986951\n",
      "    At iteration 4000 -> loss: 0.09318655135549425\n",
      "    At iteration 4100 -> loss: 0.0931889477170357\n",
      "    At iteration 4200 -> loss: 0.0931120247043584\n",
      "    At iteration 4300 -> loss: 0.09306582168872311\n",
      "    At iteration 4400 -> loss: 0.09302848952269507\n",
      "    At iteration 4500 -> loss: 0.09317751139213468\n",
      "    At iteration 4600 -> loss: 0.09313561856473941\n",
      "    At iteration 4700 -> loss: 0.09311597617953957\n",
      "    At iteration 4800 -> loss: 0.09316096375485514\n",
      "    At iteration 4900 -> loss: 0.0931512606254439\n",
      "    At iteration 5000 -> loss: 0.09319062306563032\n",
      "    At iteration 5100 -> loss: 0.09313909699565143\n",
      "    At iteration 5200 -> loss: 0.09313694362257598\n",
      "    At iteration 5300 -> loss: 0.09305689551753198\n",
      "    At iteration 5400 -> loss: 0.09324697686186487\n",
      "    At iteration 5500 -> loss: 0.0932195104261938\n",
      "    At iteration 5600 -> loss: 0.0931838690419635\n",
      "    At iteration 5700 -> loss: 0.09313772744344258\n",
      "    At iteration 5800 -> loss: 0.09311001647184618\n",
      "    At iteration 5900 -> loss: 0.09315889866856629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6000 -> loss: 0.09312455905508552\n",
      "    At iteration 6100 -> loss: 0.09308984336204645\n",
      "    At iteration 6200 -> loss: 0.0931856617065706\n",
      "    At iteration 6300 -> loss: 0.09312094592309027\n",
      "    At iteration 6400 -> loss: 0.09313690462267826\n",
      "    At iteration 6500 -> loss: 0.09307116493121387\n",
      "    At iteration 6600 -> loss: 0.09307030353986098\n",
      "    At iteration 6700 -> loss: 0.09303958072751431\n",
      "    At iteration 6800 -> loss: 0.09305520116226663\n",
      "    At iteration 6900 -> loss: 0.09298548093172915\n",
      "    At iteration 7000 -> loss: 0.09314402160426331\n",
      "    At iteration 7100 -> loss: 0.09311817860522024\n",
      "    At iteration 7200 -> loss: 0.09312182251738896\n",
      "    At iteration 7300 -> loss: 0.0930973154517498\n",
      "    At iteration 7400 -> loss: 0.09306518253444242\n",
      "    At iteration 7500 -> loss: 0.09301712481302805\n",
      "    At iteration 7600 -> loss: 0.09344270031673077\n",
      "    At iteration 7700 -> loss: 0.09344093375279838\n",
      "    At iteration 7800 -> loss: 0.0935031153410648\n",
      "    At iteration 7900 -> loss: 0.09347276109514559\n",
      "    At iteration 8000 -> loss: 0.09346879431138845\n",
      "    At iteration 8100 -> loss: 0.09345290820683044\n",
      "    At iteration 8200 -> loss: 0.09349132182365884\n",
      "    At iteration 8300 -> loss: 0.09346077699295892\n",
      "    At iteration 8400 -> loss: 0.09347741891707546\n",
      "    At iteration 8500 -> loss: 0.09343325714517388\n",
      "    At iteration 8600 -> loss: 0.0933894129062908\n",
      "    At iteration 8700 -> loss: 0.0933642756899533\n",
      "    At iteration 8800 -> loss: 0.09335956047315054\n",
      "    At iteration 8900 -> loss: 0.09343552820400752\n",
      "    At iteration 9000 -> loss: 0.09340214312756076\n",
      "    At iteration 9100 -> loss: 0.09338266371860937\n",
      "    At iteration 9200 -> loss: 0.09338089604848454\n",
      "    At iteration 9300 -> loss: 0.09335806778339958\n",
      "    At iteration 9400 -> loss: 0.09333377764440985\n",
      "    At iteration 9500 -> loss: 0.09331100922940569\n",
      "    At iteration 9600 -> loss: 0.09328765881446484\n",
      "    At iteration 9700 -> loss: 0.09326531394774276\n",
      "    At iteration 9800 -> loss: 0.09329822374752818\n",
      "    At iteration 9900 -> loss: 0.09325293118965844\n",
      "    At iteration 10000 -> loss: 0.09322709404943286\n",
      "    At iteration 10100 -> loss: 0.09319906663693411\n",
      "    At iteration 10200 -> loss: 0.0931778296085617\n",
      "    At iteration 10300 -> loss: 0.09316425152395477\n",
      "    At iteration 10400 -> loss: 0.09313069372565023\n",
      "    At iteration 10500 -> loss: 0.09313363250829253\n",
      "    At iteration 10600 -> loss: 0.09310472017586305\n",
      "    At iteration 10700 -> loss: 0.09309610653947356\n",
      "    At iteration 10800 -> loss: 0.09307928163445414\n",
      "    At iteration 10900 -> loss: 0.09318308676108711\n",
      "    At iteration 11000 -> loss: 0.09321341679970362\n",
      "    At iteration 11100 -> loss: 0.09322343586304424\n",
      "    At iteration 11200 -> loss: 0.09326698725440286\n",
      "    At iteration 11300 -> loss: 0.09329691758974017\n",
      "    At iteration 11400 -> loss: 0.09326758032093499\n",
      "    At iteration 11500 -> loss: 0.09323558839387046\n",
      "    At iteration 11600 -> loss: 0.09320262327320344\n",
      "    At iteration 11700 -> loss: 0.09323272499647921\n",
      "    At iteration 11800 -> loss: 0.09326138908245998\n",
      "    At iteration 11900 -> loss: 0.09329188777919785\n",
      "    At iteration 12000 -> loss: 0.0932740448024491\n",
      "    At iteration 12100 -> loss: 0.09326242573959\n",
      "    At iteration 12200 -> loss: 0.09324589350348501\n",
      "    At iteration 12300 -> loss: 0.09327023787357057\n",
      "    At iteration 12400 -> loss: 0.09325673046748126\n",
      "    At iteration 12500 -> loss: 0.0932529874160873\n",
      "    At iteration 12600 -> loss: 0.09324603262729846\n",
      "    At iteration 12700 -> loss: 0.09322743169566182\n",
      "    At iteration 12800 -> loss: 0.0932327711381791\n",
      "    At iteration 12900 -> loss: 0.09322612833737219\n",
      "    At iteration 13000 -> loss: 0.0931996835435597\n",
      "    At iteration 13100 -> loss: 0.09319621357775065\n",
      "    At iteration 13200 -> loss: 0.09319226669560696\n",
      "    At iteration 13300 -> loss: 0.09317570222352083\n",
      "    At iteration 13400 -> loss: 0.09315074052650325\n",
      "    At iteration 13500 -> loss: 0.09313628545422122\n",
      "    At iteration 13600 -> loss: 0.09312590030055674\n",
      "Staring Epoch 17\n",
      "    At iteration 0 -> loss: 0.09525477653369308\n",
      "    At iteration 100 -> loss: 0.09227233013183675\n",
      "    At iteration 200 -> loss: 0.09080676792635217\n",
      "    At iteration 300 -> loss: 0.09253961318220819\n",
      "    At iteration 400 -> loss: 0.09300725104271697\n",
      "    At iteration 500 -> loss: 0.0930176776787638\n",
      "    At iteration 600 -> loss: 0.09379413555274221\n",
      "    At iteration 700 -> loss: 0.09383904672280691\n",
      "    At iteration 800 -> loss: 0.09353799829797334\n",
      "    At iteration 900 -> loss: 0.09359441157294887\n",
      "    At iteration 1000 -> loss: 0.09322933044061772\n",
      "    At iteration 1100 -> loss: 0.09300901176240517\n",
      "    At iteration 1200 -> loss: 0.0928103324562615\n",
      "    At iteration 1300 -> loss: 0.092640851211883\n",
      "    At iteration 1400 -> loss: 0.09254318713323645\n",
      "    At iteration 1500 -> loss: 0.09266101943274888\n",
      "    At iteration 1600 -> loss: 0.09265284367462104\n",
      "    At iteration 1700 -> loss: 0.09264192040302459\n",
      "    At iteration 1800 -> loss: 0.09254040232011755\n",
      "    At iteration 1900 -> loss: 0.09239936993651326\n",
      "    At iteration 2000 -> loss: 0.09245495089009166\n",
      "    At iteration 2100 -> loss: 0.09224982602569216\n",
      "    At iteration 2200 -> loss: 0.09224436952471511\n",
      "    At iteration 2300 -> loss: 0.09222631281948057\n",
      "    At iteration 2400 -> loss: 0.09220667646101909\n",
      "    At iteration 2500 -> loss: 0.0922449818748475\n",
      "    At iteration 2600 -> loss: 0.09215987004208545\n",
      "    At iteration 2700 -> loss: 0.09220429751832337\n",
      "    At iteration 2800 -> loss: 0.09237484793947429\n",
      "    At iteration 2900 -> loss: 0.09250574422153124\n",
      "    At iteration 3000 -> loss: 0.09296597322169071\n",
      "    At iteration 3100 -> loss: 0.09289547451070723\n",
      "    At iteration 3200 -> loss: 0.09288960820990062\n",
      "    At iteration 3300 -> loss: 0.09287500442181566\n",
      "    At iteration 3400 -> loss: 0.09282992894209677\n",
      "    At iteration 3500 -> loss: 0.0927507115574583\n",
      "    At iteration 3600 -> loss: 0.09271314545766864\n",
      "    At iteration 3700 -> loss: 0.09266103471191747\n",
      "    At iteration 3800 -> loss: 0.09261680859922707\n",
      "    At iteration 3900 -> loss: 0.09271907466407439\n",
      "    At iteration 4000 -> loss: 0.09272239188267509\n",
      "    At iteration 4100 -> loss: 0.09268303484262747\n",
      "    At iteration 4200 -> loss: 0.09263576406607725\n",
      "    At iteration 4300 -> loss: 0.09261986607646963\n",
      "    At iteration 4400 -> loss: 0.09259414281760255\n",
      "    At iteration 4500 -> loss: 0.09266169457147114\n",
      "    At iteration 4600 -> loss: 0.09266229070912624\n",
      "    At iteration 4700 -> loss: 0.09257317200546958\n",
      "    At iteration 4800 -> loss: 0.09250190771276522\n",
      "    At iteration 4900 -> loss: 0.092513879158216\n",
      "    At iteration 5000 -> loss: 0.09270428338493712\n",
      "    At iteration 5100 -> loss: 0.09303660043272\n",
      "    At iteration 5200 -> loss: 0.09300514891173536\n",
      "    At iteration 5300 -> loss: 0.09292916969511614\n",
      "    At iteration 5400 -> loss: 0.09288403248447008\n",
      "    At iteration 5500 -> loss: 0.0928972487374925\n",
      "    At iteration 5600 -> loss: 0.09288585636673101\n",
      "    At iteration 5700 -> loss: 0.09294084013840813\n",
      "    At iteration 5800 -> loss: 0.09290928908772295\n",
      "    At iteration 5900 -> loss: 0.09295539034709842\n",
      "    At iteration 6000 -> loss: 0.0929224496806199\n",
      "    At iteration 6100 -> loss: 0.09290032010289667\n",
      "    At iteration 6200 -> loss: 0.09287094031451949\n",
      "    At iteration 6300 -> loss: 0.09295573857512454\n",
      "    At iteration 6400 -> loss: 0.09292203788222053\n",
      "    At iteration 6500 -> loss: 0.09294730710851402\n",
      "    At iteration 6600 -> loss: 0.09307861321273687\n",
      "    At iteration 6700 -> loss: 0.09303954925925503\n",
      "    At iteration 6800 -> loss: 0.09318968629995557\n",
      "    At iteration 6900 -> loss: 0.0931579430711115\n",
      "    At iteration 7000 -> loss: 0.09314114252754709\n",
      "    At iteration 7100 -> loss: 0.09314928685148888\n",
      "    At iteration 7200 -> loss: 0.09311780033775995\n",
      "    At iteration 7300 -> loss: 0.09305561960868527\n",
      "    At iteration 7400 -> loss: 0.09305110702684853\n",
      "    At iteration 7500 -> loss: 0.09300143948881981\n",
      "    At iteration 7600 -> loss: 0.09298814447342621\n",
      "    At iteration 7700 -> loss: 0.09301362628516954\n",
      "    At iteration 7800 -> loss: 0.0929633939531013\n",
      "    At iteration 7900 -> loss: 0.09294373807177515\n",
      "    At iteration 8000 -> loss: 0.09300734995130998\n",
      "    At iteration 8100 -> loss: 0.0929802777263501\n",
      "    At iteration 8200 -> loss: 0.09292934991377723\n",
      "    At iteration 8300 -> loss: 0.09291441785594062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8400 -> loss: 0.09288987697762978\n",
      "    At iteration 8500 -> loss: 0.09291787655665137\n",
      "    At iteration 8600 -> loss: 0.09291828837278145\n",
      "    At iteration 8700 -> loss: 0.09297862739498335\n",
      "    At iteration 8800 -> loss: 0.09297432903030454\n",
      "    At iteration 8900 -> loss: 0.09294663126335469\n",
      "    At iteration 9000 -> loss: 0.09293001248618757\n",
      "    At iteration 9100 -> loss: 0.09291883969660115\n",
      "    At iteration 9200 -> loss: 0.09290165775812115\n",
      "    At iteration 9300 -> loss: 0.0929216732643052\n",
      "    At iteration 9400 -> loss: 0.09293137301211767\n",
      "    At iteration 9500 -> loss: 0.09290102539015288\n",
      "    At iteration 9600 -> loss: 0.09290297708941325\n",
      "    At iteration 9700 -> loss: 0.09290853871907818\n",
      "    At iteration 9800 -> loss: 0.09291305920923491\n",
      "    At iteration 9900 -> loss: 0.09292036642884927\n",
      "    At iteration 10000 -> loss: 0.09293628115992357\n",
      "    At iteration 10100 -> loss: 0.09294830798369123\n",
      "    At iteration 10200 -> loss: 0.09294546294616476\n",
      "    At iteration 10300 -> loss: 0.0930399184714401\n",
      "    At iteration 10400 -> loss: 0.0930044125932905\n",
      "    At iteration 10500 -> loss: 0.09302081771154856\n",
      "    At iteration 10600 -> loss: 0.09307773983110476\n",
      "    At iteration 10700 -> loss: 0.09306554560961372\n",
      "    At iteration 10800 -> loss: 0.09303999014767673\n",
      "    At iteration 10900 -> loss: 0.0930247860430253\n",
      "    At iteration 11000 -> loss: 0.09299441312726114\n",
      "    At iteration 11100 -> loss: 0.0929867882169576\n",
      "    At iteration 11200 -> loss: 0.09296666041253454\n",
      "    At iteration 11300 -> loss: 0.09293632090948055\n",
      "    At iteration 11400 -> loss: 0.09297649034454962\n",
      "    At iteration 11500 -> loss: 0.09293826767007245\n",
      "    At iteration 11600 -> loss: 0.09291205672936995\n",
      "    At iteration 11700 -> loss: 0.09290402004444628\n",
      "    At iteration 11800 -> loss: 0.09290067842728433\n",
      "    At iteration 11900 -> loss: 0.09291145237395156\n",
      "    At iteration 12000 -> loss: 0.09292410067958341\n",
      "    At iteration 12100 -> loss: 0.09292266907836337\n",
      "    At iteration 12200 -> loss: 0.09292360729296269\n",
      "    At iteration 12300 -> loss: 0.09295341923363944\n",
      "    At iteration 12400 -> loss: 0.09297181005086773\n",
      "    At iteration 12500 -> loss: 0.09319041146478886\n",
      "    At iteration 12600 -> loss: 0.09315864603832714\n",
      "    At iteration 12700 -> loss: 0.09312688363817483\n",
      "    At iteration 12800 -> loss: 0.09309818031611951\n",
      "    At iteration 12900 -> loss: 0.0931243677964489\n",
      "    At iteration 13000 -> loss: 0.09313276343679679\n",
      "    At iteration 13100 -> loss: 0.09313343913291687\n",
      "    At iteration 13200 -> loss: 0.09314483214214576\n",
      "    At iteration 13300 -> loss: 0.09311740153419798\n",
      "    At iteration 13400 -> loss: 0.09312329639525821\n",
      "    At iteration 13500 -> loss: 0.0931236093289037\n",
      "    At iteration 13600 -> loss: 0.09310460109503577\n",
      "Staring Epoch 18\n",
      "    At iteration 0 -> loss: 0.08179278770694509\n",
      "    At iteration 100 -> loss: 0.10122907431718774\n",
      "    At iteration 200 -> loss: 0.09497767063279712\n",
      "    At iteration 300 -> loss: 0.09456451812065678\n",
      "    At iteration 400 -> loss: 0.0944301705144844\n",
      "    At iteration 500 -> loss: 0.09345575663448238\n",
      "    At iteration 600 -> loss: 0.09834616087963112\n",
      "    At iteration 700 -> loss: 0.09902350497468888\n",
      "    At iteration 800 -> loss: 0.0978977278789186\n",
      "    At iteration 900 -> loss: 0.09742405620091081\n",
      "    At iteration 1000 -> loss: 0.09679605252455833\n",
      "    At iteration 1100 -> loss: 0.09600524792118063\n",
      "    At iteration 1200 -> loss: 0.09565466668923024\n",
      "    At iteration 1300 -> loss: 0.09551210069247947\n",
      "    At iteration 1400 -> loss: 0.09513955242144867\n",
      "    At iteration 1500 -> loss: 0.09495988966204232\n",
      "    At iteration 1600 -> loss: 0.09465376844442543\n",
      "    At iteration 1700 -> loss: 0.09436134098049297\n",
      "    At iteration 1800 -> loss: 0.09420951952743789\n",
      "    At iteration 1900 -> loss: 0.0940012445245333\n",
      "    At iteration 2000 -> loss: 0.09384294250058516\n",
      "    At iteration 2100 -> loss: 0.09379641090780848\n",
      "    At iteration 2200 -> loss: 0.09393411925663016\n",
      "    At iteration 2300 -> loss: 0.0937619639381024\n",
      "    At iteration 2400 -> loss: 0.09358859680075135\n",
      "    At iteration 2500 -> loss: 0.09378565673046087\n",
      "    At iteration 2600 -> loss: 0.09393999909852933\n",
      "    At iteration 2700 -> loss: 0.09388084623757906\n",
      "    At iteration 2800 -> loss: 0.09382830468762252\n",
      "    At iteration 2900 -> loss: 0.09382399601088201\n",
      "    At iteration 3000 -> loss: 0.09384983121637844\n",
      "    At iteration 3100 -> loss: 0.09382222662959622\n",
      "    At iteration 3200 -> loss: 0.09379644574961592\n",
      "    At iteration 3300 -> loss: 0.09375511052114446\n",
      "    At iteration 3400 -> loss: 0.09365114563413918\n",
      "    At iteration 3500 -> loss: 0.0935856389192848\n",
      "    At iteration 3600 -> loss: 0.09352662543259571\n",
      "    At iteration 3700 -> loss: 0.0934621794963051\n",
      "    At iteration 3800 -> loss: 0.09344310352418907\n",
      "    At iteration 3900 -> loss: 0.0933634326800948\n",
      "    At iteration 4000 -> loss: 0.09326143520065903\n",
      "    At iteration 4100 -> loss: 0.09324325439431422\n",
      "    At iteration 4200 -> loss: 0.09314041234477581\n",
      "    At iteration 4300 -> loss: 0.09311710593125316\n",
      "    At iteration 4400 -> loss: 0.09311235919572114\n",
      "    At iteration 4500 -> loss: 0.09311227223572352\n",
      "    At iteration 4600 -> loss: 0.09306484909033418\n",
      "    At iteration 4700 -> loss: 0.09299928312654623\n",
      "    At iteration 4800 -> loss: 0.09298283179286358\n",
      "    At iteration 4900 -> loss: 0.09307202414624663\n",
      "    At iteration 5000 -> loss: 0.09299592046390526\n",
      "    At iteration 5100 -> loss: 0.09301412412189511\n",
      "    At iteration 5200 -> loss: 0.09298654341370319\n",
      "    At iteration 5300 -> loss: 0.092910472580782\n",
      "    At iteration 5400 -> loss: 0.09284687978631737\n",
      "    At iteration 5500 -> loss: 0.09284620943826315\n",
      "    At iteration 5600 -> loss: 0.09285253680418044\n",
      "    At iteration 5700 -> loss: 0.09292043249194912\n",
      "    At iteration 5800 -> loss: 0.09290631225561712\n",
      "    At iteration 5900 -> loss: 0.09294534912707858\n",
      "    At iteration 6000 -> loss: 0.0929040112898711\n",
      "    At iteration 6100 -> loss: 0.09285308175192875\n",
      "    At iteration 6200 -> loss: 0.0928526481636075\n",
      "    At iteration 6300 -> loss: 0.0929388815753302\n",
      "    At iteration 6400 -> loss: 0.09291927021563434\n",
      "    At iteration 6500 -> loss: 0.09294469344212652\n",
      "    At iteration 6600 -> loss: 0.09291156791461355\n",
      "    At iteration 6700 -> loss: 0.09292349349923042\n",
      "    At iteration 6800 -> loss: 0.0929250186920628\n",
      "    At iteration 6900 -> loss: 0.09288624103702486\n",
      "    At iteration 7000 -> loss: 0.09289444459804198\n",
      "    At iteration 7100 -> loss: 0.09290690144813821\n",
      "    At iteration 7200 -> loss: 0.09294275527854061\n",
      "    At iteration 7300 -> loss: 0.09299229586726286\n",
      "    At iteration 7400 -> loss: 0.09299876509078484\n",
      "    At iteration 7500 -> loss: 0.09307807848712608\n",
      "    At iteration 7600 -> loss: 0.0930929970440584\n",
      "    At iteration 7700 -> loss: 0.09309946145836162\n",
      "    At iteration 7800 -> loss: 0.0930974400716071\n",
      "    At iteration 7900 -> loss: 0.093078460618486\n",
      "    At iteration 8000 -> loss: 0.09304670441561792\n",
      "    At iteration 8100 -> loss: 0.09301001343599409\n",
      "    At iteration 8200 -> loss: 0.09297394619730875\n",
      "    At iteration 8300 -> loss: 0.09301868538822684\n",
      "    At iteration 8400 -> loss: 0.09305778711415708\n",
      "    At iteration 8500 -> loss: 0.09302724574776208\n",
      "    At iteration 8600 -> loss: 0.09303655343906549\n",
      "    At iteration 8700 -> loss: 0.09299386954362114\n",
      "    At iteration 8800 -> loss: 0.09302368728089488\n",
      "    At iteration 8900 -> loss: 0.09302087828222848\n",
      "    At iteration 9000 -> loss: 0.09316179956104387\n",
      "    At iteration 9100 -> loss: 0.09316005057502531\n",
      "    At iteration 9200 -> loss: 0.09316165272599135\n",
      "    At iteration 9300 -> loss: 0.09318539644118361\n",
      "    At iteration 9400 -> loss: 0.09323052127440987\n",
      "    At iteration 9500 -> loss: 0.09320865520590028\n",
      "    At iteration 9600 -> loss: 0.09319508557108636\n",
      "    At iteration 9700 -> loss: 0.09322083627882058\n",
      "    At iteration 9800 -> loss: 0.09322286248746924\n",
      "    At iteration 9900 -> loss: 0.09320276850801773\n",
      "    At iteration 10000 -> loss: 0.09319514863156554\n",
      "    At iteration 10100 -> loss: 0.09319113075241803\n",
      "    At iteration 10200 -> loss: 0.0931846970192306\n",
      "    At iteration 10300 -> loss: 0.09318111048047514\n",
      "    At iteration 10400 -> loss: 0.09318251213503627\n",
      "    At iteration 10500 -> loss: 0.09317354114642554\n",
      "    At iteration 10600 -> loss: 0.09319040710740686\n",
      "    At iteration 10700 -> loss: 0.09322234744564105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10800 -> loss: 0.09320234146292758\n",
      "    At iteration 10900 -> loss: 0.09318388563873244\n",
      "    At iteration 11000 -> loss: 0.09314569193172381\n",
      "    At iteration 11100 -> loss: 0.0931276938653972\n",
      "    At iteration 11200 -> loss: 0.09309568455740165\n",
      "    At iteration 11300 -> loss: 0.0931898195122608\n",
      "    At iteration 11400 -> loss: 0.0931707686201816\n",
      "    At iteration 11500 -> loss: 0.09325139239962994\n",
      "    At iteration 11600 -> loss: 0.09322310934224165\n",
      "    At iteration 11700 -> loss: 0.0931812396682403\n",
      "    At iteration 11800 -> loss: 0.0931839438489809\n",
      "    At iteration 11900 -> loss: 0.09316376888186872\n",
      "    At iteration 12000 -> loss: 0.09318739553135842\n",
      "    At iteration 12100 -> loss: 0.09316349857246738\n",
      "    At iteration 12200 -> loss: 0.09314789536768521\n",
      "    At iteration 12300 -> loss: 0.09312513729551299\n",
      "    At iteration 12400 -> loss: 0.09313659879111054\n",
      "    At iteration 12500 -> loss: 0.09314214910806977\n",
      "    At iteration 12600 -> loss: 0.09322011003534554\n",
      "    At iteration 12700 -> loss: 0.09319958613334636\n",
      "    At iteration 12800 -> loss: 0.09318279127736043\n",
      "    At iteration 12900 -> loss: 0.0931936053003672\n",
      "    At iteration 13000 -> loss: 0.09317345638280827\n",
      "    At iteration 13100 -> loss: 0.09318260983662023\n",
      "    At iteration 13200 -> loss: 0.09314967454292027\n",
      "    At iteration 13300 -> loss: 0.09316853774680302\n",
      "    At iteration 13400 -> loss: 0.09314792085996106\n",
      "    At iteration 13500 -> loss: 0.09315912844161331\n",
      "    At iteration 13600 -> loss: 0.09313776902978708\n",
      "Staring Epoch 19\n",
      "    At iteration 0 -> loss: 0.08212486346019432\n",
      "    At iteration 100 -> loss: 0.09229419701099606\n",
      "    At iteration 200 -> loss: 0.09185741659873448\n",
      "    At iteration 300 -> loss: 0.09468699416196334\n",
      "    At iteration 400 -> loss: 0.09505474765908686\n",
      "    At iteration 500 -> loss: 0.09551685612251154\n",
      "    At iteration 600 -> loss: 0.09575051420921428\n",
      "    At iteration 700 -> loss: 0.09505550307146737\n",
      "    At iteration 800 -> loss: 0.09541803832568573\n",
      "    At iteration 900 -> loss: 0.09497162917711971\n",
      "    At iteration 1000 -> loss: 0.09449902704326239\n",
      "    At iteration 1100 -> loss: 0.0941916318681863\n",
      "    At iteration 1200 -> loss: 0.0944295750526728\n",
      "    At iteration 1300 -> loss: 0.0947615573869186\n",
      "    At iteration 1400 -> loss: 0.09440407614346043\n",
      "    At iteration 1500 -> loss: 0.09396604246047346\n",
      "    At iteration 1600 -> loss: 0.09373007214111544\n",
      "    At iteration 1700 -> loss: 0.0934730887737951\n",
      "    At iteration 1800 -> loss: 0.09336160815539209\n",
      "    At iteration 1900 -> loss: 0.09327725403374912\n",
      "    At iteration 2000 -> loss: 0.09316644078357829\n",
      "    At iteration 2100 -> loss: 0.09298414494307325\n",
      "    At iteration 2200 -> loss: 0.09285879546921844\n",
      "    At iteration 2300 -> loss: 0.0928051040952105\n",
      "    At iteration 2400 -> loss: 0.09277802605343906\n",
      "    At iteration 2500 -> loss: 0.09291334322262718\n",
      "    At iteration 2600 -> loss: 0.09289235981619191\n",
      "    At iteration 2700 -> loss: 0.09288451259813278\n",
      "    At iteration 2800 -> loss: 0.09283258337663618\n",
      "    At iteration 2900 -> loss: 0.09282893641624013\n",
      "    At iteration 3000 -> loss: 0.09276307617123603\n",
      "    At iteration 3100 -> loss: 0.09280820536180862\n",
      "    At iteration 3200 -> loss: 0.09319235285068066\n",
      "    At iteration 3300 -> loss: 0.0930736369696792\n",
      "    At iteration 3400 -> loss: 0.09299516784891003\n",
      "    At iteration 3500 -> loss: 0.09307939188941382\n",
      "    At iteration 3600 -> loss: 0.09306148723795614\n",
      "    At iteration 3700 -> loss: 0.09302512048815996\n",
      "    At iteration 3800 -> loss: 0.09295510917168343\n",
      "    At iteration 3900 -> loss: 0.09303534996466044\n",
      "    At iteration 4000 -> loss: 0.09302076044793581\n",
      "    At iteration 4100 -> loss: 0.09299265126637427\n",
      "    At iteration 4200 -> loss: 0.09308327800204744\n",
      "    At iteration 4300 -> loss: 0.09298540857166084\n",
      "    At iteration 4400 -> loss: 0.09293777283579224\n",
      "    At iteration 4500 -> loss: 0.09297547547611071\n",
      "    At iteration 4600 -> loss: 0.0930677990114226\n",
      "    At iteration 4700 -> loss: 0.09304499745919187\n",
      "    At iteration 4800 -> loss: 0.09302982695207906\n",
      "    At iteration 4900 -> loss: 0.09302596677227258\n",
      "    At iteration 5000 -> loss: 0.09297040481815841\n",
      "    At iteration 5100 -> loss: 0.09298233403772457\n",
      "    At iteration 5200 -> loss: 0.09294242854655045\n",
      "    At iteration 5300 -> loss: 0.09307412634876366\n",
      "    At iteration 5400 -> loss: 0.09313209954881713\n",
      "    At iteration 5500 -> loss: 0.09309625670583237\n",
      "    At iteration 5600 -> loss: 0.09317803860000425\n",
      "    At iteration 5700 -> loss: 0.09318616976447741\n",
      "    At iteration 5800 -> loss: 0.09319744423481702\n",
      "    At iteration 5900 -> loss: 0.09326122607909007\n",
      "    At iteration 6000 -> loss: 0.09322304416449814\n",
      "    At iteration 6100 -> loss: 0.0933449673446933\n",
      "    At iteration 6200 -> loss: 0.09334416721227567\n",
      "    At iteration 6300 -> loss: 0.09330254208085273\n",
      "    At iteration 6400 -> loss: 0.09331132554866271\n",
      "    At iteration 6500 -> loss: 0.09335893779126116\n",
      "    At iteration 6600 -> loss: 0.09336835533203146\n",
      "    At iteration 6700 -> loss: 0.09334805832147386\n",
      "    At iteration 6800 -> loss: 0.09336111108881286\n",
      "    At iteration 6900 -> loss: 0.09339617211189223\n",
      "    At iteration 7000 -> loss: 0.0934405001379742\n",
      "    At iteration 7100 -> loss: 0.09342732268877103\n",
      "    At iteration 7200 -> loss: 0.09343285784034548\n",
      "    At iteration 7300 -> loss: 0.09343793227580559\n",
      "    At iteration 7400 -> loss: 0.09342547931216702\n",
      "    At iteration 7500 -> loss: 0.09340218317254276\n",
      "    At iteration 7600 -> loss: 0.09341180696913913\n",
      "    At iteration 7700 -> loss: 0.09340092064026845\n",
      "    At iteration 7800 -> loss: 0.0934178135002512\n",
      "    At iteration 7900 -> loss: 0.09338699128320875\n",
      "    At iteration 8000 -> loss: 0.09338523431780986\n",
      "    At iteration 8100 -> loss: 0.09336792775476227\n",
      "    At iteration 8200 -> loss: 0.09333491065034225\n",
      "    At iteration 8300 -> loss: 0.0933110538953713\n",
      "    At iteration 8400 -> loss: 0.09326144499845948\n",
      "    At iteration 8500 -> loss: 0.0932219675268384\n",
      "    At iteration 8600 -> loss: 0.09332451625275132\n",
      "    At iteration 8700 -> loss: 0.09328645512911828\n",
      "    At iteration 8800 -> loss: 0.09359059562328786\n",
      "    At iteration 8900 -> loss: 0.09354682852078634\n",
      "    At iteration 9000 -> loss: 0.09350443599774104\n",
      "    At iteration 9100 -> loss: 0.09348758939950472\n",
      "    At iteration 9200 -> loss: 0.09346508647035316\n",
      "    At iteration 9300 -> loss: 0.09345505514309208\n",
      "    At iteration 9400 -> loss: 0.09342945165865363\n",
      "    At iteration 9500 -> loss: 0.0934171167595393\n",
      "    At iteration 9600 -> loss: 0.09340156827681813\n",
      "    At iteration 9700 -> loss: 0.09339049696663332\n",
      "    At iteration 9800 -> loss: 0.09337851440104768\n",
      "    At iteration 9900 -> loss: 0.09335825977394516\n",
      "    At iteration 10000 -> loss: 0.09334314777463232\n",
      "    At iteration 10100 -> loss: 0.09332018332883252\n",
      "    At iteration 10200 -> loss: 0.09329707250082978\n",
      "    At iteration 10300 -> loss: 0.09326965789742536\n",
      "    At iteration 10400 -> loss: 0.09330685652137634\n",
      "    At iteration 10500 -> loss: 0.09327035689512733\n",
      "    At iteration 10600 -> loss: 0.09326274715719146\n",
      "    At iteration 10700 -> loss: 0.09323692415263433\n",
      "    At iteration 10800 -> loss: 0.09324260883809185\n",
      "    At iteration 10900 -> loss: 0.09323716021289406\n",
      "    At iteration 11000 -> loss: 0.09321746317471086\n",
      "    At iteration 11100 -> loss: 0.09319985970429268\n",
      "    At iteration 11200 -> loss: 0.09317347633669774\n",
      "    At iteration 11300 -> loss: 0.09315445959508922\n",
      "    At iteration 11400 -> loss: 0.09314184636131248\n",
      "    At iteration 11500 -> loss: 0.09312611842014677\n",
      "    At iteration 11600 -> loss: 0.09313880486674025\n",
      "    At iteration 11700 -> loss: 0.09311656931135905\n",
      "    At iteration 11800 -> loss: 0.09313044703336143\n",
      "    At iteration 11900 -> loss: 0.09309562093412592\n",
      "    At iteration 12000 -> loss: 0.0930696351939282\n",
      "    At iteration 12100 -> loss: 0.09308433786436011\n",
      "    At iteration 12200 -> loss: 0.09304177136488401\n",
      "    At iteration 12300 -> loss: 0.0930399143325911\n",
      "    At iteration 12400 -> loss: 0.09306933256546326\n",
      "    At iteration 12500 -> loss: 0.09304499912010199\n",
      "    At iteration 12600 -> loss: 0.09302079549683402\n",
      "    At iteration 12700 -> loss: 0.09309628633801693\n",
      "    At iteration 12800 -> loss: 0.09308827058860387\n",
      "    At iteration 12900 -> loss: 0.09307772551995949\n",
      "    At iteration 13000 -> loss: 0.09310696715039947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 13100 -> loss: 0.09314702790303495\n",
      "    At iteration 13200 -> loss: 0.09319458919130226\n",
      "    At iteration 13300 -> loss: 0.09317747106398508\n",
      "    At iteration 13400 -> loss: 0.0931648580871357\n",
      "    At iteration 13500 -> loss: 0.09318422659743837\n",
      "    At iteration 13600 -> loss: 0.09317472073255215\n",
      "Staring Epoch 20\n",
      "    At iteration 0 -> loss: 0.09500360116362572\n",
      "    At iteration 100 -> loss: 0.08885620691158143\n",
      "    At iteration 200 -> loss: 0.0951312953907297\n",
      "    At iteration 300 -> loss: 0.09505228463904783\n",
      "    At iteration 400 -> loss: 0.09374505517916809\n",
      "    At iteration 500 -> loss: 0.09398990292440834\n",
      "    At iteration 600 -> loss: 0.09330757867938289\n",
      "    At iteration 700 -> loss: 0.0928645646170637\n",
      "    At iteration 800 -> loss: 0.09282510386956343\n",
      "    At iteration 900 -> loss: 0.09284321978158312\n",
      "    At iteration 1000 -> loss: 0.0930417838386366\n",
      "    At iteration 1100 -> loss: 0.09353994643083677\n",
      "    At iteration 1200 -> loss: 0.09321355424662923\n",
      "    At iteration 1300 -> loss: 0.09339532790627061\n",
      "    At iteration 1400 -> loss: 0.09336440364483871\n",
      "    At iteration 1500 -> loss: 0.09346437124420995\n",
      "    At iteration 1600 -> loss: 0.093250495774995\n",
      "    At iteration 1700 -> loss: 0.0930507940593441\n",
      "    At iteration 1800 -> loss: 0.09292439197876533\n",
      "    At iteration 1900 -> loss: 0.09320783910382362\n",
      "    At iteration 2000 -> loss: 0.09336293337830666\n",
      "    At iteration 2100 -> loss: 0.09346238749253646\n",
      "    At iteration 2200 -> loss: 0.09355456158765167\n",
      "    At iteration 2300 -> loss: 0.09358962057713263\n",
      "    At iteration 2400 -> loss: 0.09378147562953855\n",
      "    At iteration 2500 -> loss: 0.09373723594408905\n",
      "    At iteration 2600 -> loss: 0.093620246267476\n",
      "    At iteration 2700 -> loss: 0.09354741474691403\n",
      "    At iteration 2800 -> loss: 0.09353428738033714\n",
      "    At iteration 2900 -> loss: 0.09347668332919229\n",
      "    At iteration 3000 -> loss: 0.09347097744957757\n",
      "    At iteration 3100 -> loss: 0.09334162229117263\n",
      "    At iteration 3200 -> loss: 0.0933776265425304\n",
      "    At iteration 3300 -> loss: 0.09326463388538567\n",
      "    At iteration 3400 -> loss: 0.09336251598431372\n",
      "    At iteration 3500 -> loss: 0.09327446840172741\n",
      "    At iteration 3600 -> loss: 0.09318220496867713\n",
      "    At iteration 3700 -> loss: 0.09344982453592802\n",
      "    At iteration 3800 -> loss: 0.09346814088792109\n",
      "    At iteration 3900 -> loss: 0.09343172087325553\n",
      "    At iteration 4000 -> loss: 0.09333708568584972\n",
      "    At iteration 4100 -> loss: 0.09323856436385412\n",
      "    At iteration 4200 -> loss: 0.09328542872989078\n",
      "    At iteration 4300 -> loss: 0.09318927986607431\n",
      "    At iteration 4400 -> loss: 0.09316081341024776\n",
      "    At iteration 4500 -> loss: 0.0931950543844073\n",
      "    At iteration 4600 -> loss: 0.09313986070705157\n",
      "    At iteration 4700 -> loss: 0.09306377295973518\n",
      "    At iteration 4800 -> loss: 0.09306994196627823\n",
      "    At iteration 4900 -> loss: 0.09305037490767565\n",
      "    At iteration 5000 -> loss: 0.09304225941993863\n",
      "    At iteration 5100 -> loss: 0.09306892988318911\n",
      "    At iteration 5200 -> loss: 0.09331678895120452\n",
      "    At iteration 5300 -> loss: 0.09331152832293742\n",
      "    At iteration 5400 -> loss: 0.09327454109127589\n",
      "    At iteration 5500 -> loss: 0.09330593431757382\n",
      "    At iteration 5600 -> loss: 0.09336506177400052\n",
      "    At iteration 5700 -> loss: 0.09336817437967897\n",
      "    At iteration 5800 -> loss: 0.09330276824552582\n",
      "    At iteration 5900 -> loss: 0.09331326351670019\n",
      "    At iteration 6000 -> loss: 0.09330150454135466\n",
      "    At iteration 6100 -> loss: 0.0932452789858546\n",
      "    At iteration 6200 -> loss: 0.09320003657176323\n",
      "    At iteration 6300 -> loss: 0.09317850655384147\n",
      "    At iteration 6400 -> loss: 0.0931098156744974\n",
      "    At iteration 6500 -> loss: 0.09314464251048149\n",
      "    At iteration 6600 -> loss: 0.09309547991762934\n",
      "    At iteration 6700 -> loss: 0.09305315658736403\n",
      "    At iteration 6800 -> loss: 0.09308125116503178\n",
      "    At iteration 6900 -> loss: 0.09304657774045877\n",
      "    At iteration 7000 -> loss: 0.09310465354357478\n",
      "    At iteration 7100 -> loss: 0.09304985739595448\n",
      "    At iteration 7200 -> loss: 0.09317009663104615\n",
      "    At iteration 7300 -> loss: 0.09320694662693468\n",
      "    At iteration 7400 -> loss: 0.09317771862807743\n",
      "    At iteration 7500 -> loss: 0.09315433494985785\n",
      "    At iteration 7600 -> loss: 0.093125193200767\n",
      "    At iteration 7700 -> loss: 0.09311845238006403\n",
      "    At iteration 7800 -> loss: 0.09308116126697659\n",
      "    At iteration 7900 -> loss: 0.09310208585834556\n",
      "    At iteration 8000 -> loss: 0.09308939755867825\n",
      "    At iteration 8100 -> loss: 0.09308454373608395\n",
      "    At iteration 8200 -> loss: 0.09312823283884024\n",
      "    At iteration 8300 -> loss: 0.09317661791319276\n",
      "    At iteration 8400 -> loss: 0.0931463040566138\n",
      "    At iteration 8500 -> loss: 0.09310292993136818\n",
      "    At iteration 8600 -> loss: 0.0930875480313415\n",
      "    At iteration 8700 -> loss: 0.09305739734367822\n",
      "    At iteration 8800 -> loss: 0.09300731167146611\n",
      "    At iteration 8900 -> loss: 0.0929736544558375\n",
      "    At iteration 9000 -> loss: 0.09298055957430584\n",
      "    At iteration 9100 -> loss: 0.09298976246749995\n",
      "    At iteration 9200 -> loss: 0.09294970352050043\n",
      "    At iteration 9300 -> loss: 0.09301894251586522\n",
      "    At iteration 9400 -> loss: 0.09309713262576462\n",
      "    At iteration 9500 -> loss: 0.09308652051917835\n",
      "    At iteration 9600 -> loss: 0.09307081758610934\n",
      "    At iteration 9700 -> loss: 0.0930372621497423\n",
      "    At iteration 9800 -> loss: 0.09302032236436501\n",
      "    At iteration 9900 -> loss: 0.0930016855454752\n",
      "    At iteration 10000 -> loss: 0.09297248384482167\n",
      "    At iteration 10100 -> loss: 0.0932756152661653\n",
      "    At iteration 10200 -> loss: 0.09326455014615033\n",
      "    At iteration 10300 -> loss: 0.09327331080943721\n",
      "    At iteration 10400 -> loss: 0.09326287177271883\n",
      "    At iteration 10500 -> loss: 0.09325557444316773\n",
      "    At iteration 10600 -> loss: 0.09322520203492238\n",
      "    At iteration 10700 -> loss: 0.09322574448374929\n",
      "    At iteration 10800 -> loss: 0.09329743305712808\n",
      "    At iteration 10900 -> loss: 0.09324082999606098\n",
      "    At iteration 11000 -> loss: 0.09324696639964693\n",
      "    At iteration 11100 -> loss: 0.09326641685863393\n",
      "    At iteration 11200 -> loss: 0.09326637553115599\n",
      "    At iteration 11300 -> loss: 0.09325891061771632\n",
      "    At iteration 11400 -> loss: 0.09324564288039346\n",
      "    At iteration 11500 -> loss: 0.09330477502158607\n",
      "    At iteration 11600 -> loss: 0.09329378452471106\n",
      "    At iteration 11700 -> loss: 0.09325665365263366\n",
      "    At iteration 11800 -> loss: 0.09322844451363078\n",
      "    At iteration 11900 -> loss: 0.09319960872669958\n",
      "    At iteration 12000 -> loss: 0.09323394113989585\n",
      "    At iteration 12100 -> loss: 0.09321751936043858\n",
      "    At iteration 12200 -> loss: 0.09318907640502962\n",
      "    At iteration 12300 -> loss: 0.09322915481926143\n",
      "    At iteration 12400 -> loss: 0.09319202819478295\n",
      "    At iteration 12500 -> loss: 0.09316971242156052\n",
      "    At iteration 12600 -> loss: 0.09321370053871508\n",
      "    At iteration 12700 -> loss: 0.09321156379224561\n",
      "    At iteration 12800 -> loss: 0.09320513104831064\n",
      "    At iteration 12900 -> loss: 0.09318105430275583\n",
      "    At iteration 13000 -> loss: 0.09318383549858845\n",
      "    At iteration 13100 -> loss: 0.09320286204288537\n",
      "    At iteration 13200 -> loss: 0.09319747778663796\n",
      "    At iteration 13300 -> loss: 0.09319107492511461\n",
      "    At iteration 13400 -> loss: 0.09319091162974118\n",
      "    At iteration 13500 -> loss: 0.09319889710766911\n",
      "    At iteration 13600 -> loss: 0.09316927998051634\n",
      "Staring Epoch 21\n",
      "    At iteration 0 -> loss: 0.08890837780199945\n",
      "    At iteration 100 -> loss: 0.09272579006645372\n",
      "    At iteration 200 -> loss: 0.09768646685051664\n",
      "    At iteration 300 -> loss: 0.09680104152992448\n",
      "    At iteration 400 -> loss: 0.0958985389085765\n",
      "    At iteration 500 -> loss: 0.09481416752642204\n",
      "    At iteration 600 -> loss: 0.094552828827766\n",
      "    At iteration 700 -> loss: 0.09387049895505213\n",
      "    At iteration 800 -> loss: 0.09379800106817121\n",
      "    At iteration 900 -> loss: 0.09348111144677758\n",
      "    At iteration 1000 -> loss: 0.09345737191169276\n",
      "    At iteration 1100 -> loss: 0.09343293654213564\n",
      "    At iteration 1200 -> loss: 0.09340801300482245\n",
      "    At iteration 1300 -> loss: 0.09365441846934199\n",
      "    At iteration 1400 -> loss: 0.09344482386953508\n",
      "    At iteration 1500 -> loss: 0.09382851377655639\n",
      "    At iteration 1600 -> loss: 0.09354402095652621\n",
      "    At iteration 1700 -> loss: 0.0936613490055049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1800 -> loss: 0.0938997321599432\n",
      "    At iteration 1900 -> loss: 0.09376725026517288\n",
      "    At iteration 2000 -> loss: 0.09364062589386918\n",
      "    At iteration 2100 -> loss: 0.09345922370279554\n",
      "    At iteration 2200 -> loss: 0.0934976055033121\n",
      "    At iteration 2300 -> loss: 0.09337725285579034\n",
      "    At iteration 2400 -> loss: 0.09334089841614389\n",
      "    At iteration 2500 -> loss: 0.09326367777807204\n",
      "    At iteration 2600 -> loss: 0.09305387863911145\n",
      "    At iteration 2700 -> loss: 0.09305471852123692\n",
      "    At iteration 2800 -> loss: 0.09294581676104155\n",
      "    At iteration 2900 -> loss: 0.09298047565523643\n",
      "    At iteration 3000 -> loss: 0.0929391606624411\n",
      "    At iteration 3100 -> loss: 0.09304786727589785\n",
      "    At iteration 3200 -> loss: 0.09292964239847126\n",
      "    At iteration 3300 -> loss: 0.09285462778822835\n",
      "    At iteration 3400 -> loss: 0.09285094137906748\n",
      "    At iteration 3500 -> loss: 0.09276578116151062\n",
      "    At iteration 3600 -> loss: 0.0926765307873012\n",
      "    At iteration 3700 -> loss: 0.09263374044584632\n",
      "    At iteration 3800 -> loss: 0.09254798068763348\n",
      "    At iteration 3900 -> loss: 0.09256545631523784\n",
      "    At iteration 4000 -> loss: 0.09256046338744306\n",
      "    At iteration 4100 -> loss: 0.09256072814942189\n",
      "    At iteration 4200 -> loss: 0.09252025055800366\n",
      "    At iteration 4300 -> loss: 0.09254463059307805\n",
      "    At iteration 4400 -> loss: 0.09258155739080458\n",
      "    At iteration 4500 -> loss: 0.09268176987866847\n",
      "    At iteration 4600 -> loss: 0.09261747266773306\n",
      "    At iteration 4700 -> loss: 0.09269150375093493\n",
      "    At iteration 4800 -> loss: 0.09271144157620191\n",
      "    At iteration 4900 -> loss: 0.09275326949572672\n",
      "    At iteration 5000 -> loss: 0.09272687817374842\n",
      "    At iteration 5100 -> loss: 0.09268802396010908\n",
      "    At iteration 5200 -> loss: 0.09260346405557564\n",
      "    At iteration 5300 -> loss: 0.09276134754163214\n",
      "    At iteration 5400 -> loss: 0.09270087027240538\n",
      "    At iteration 5500 -> loss: 0.09269786027480928\n",
      "    At iteration 5600 -> loss: 0.09275323875122166\n",
      "    At iteration 5700 -> loss: 0.09283676800164586\n",
      "    At iteration 5800 -> loss: 0.0929581598802544\n",
      "    At iteration 5900 -> loss: 0.09292293904082358\n",
      "    At iteration 6000 -> loss: 0.09305186130349487\n",
      "    At iteration 6100 -> loss: 0.09305367730277593\n",
      "    At iteration 6200 -> loss: 0.09305780855358607\n",
      "    At iteration 6300 -> loss: 0.09305179218567933\n",
      "    At iteration 6400 -> loss: 0.09301686551176527\n",
      "    At iteration 6500 -> loss: 0.09312354453854059\n",
      "    At iteration 6600 -> loss: 0.09316842115061078\n",
      "    At iteration 6700 -> loss: 0.09330596927266878\n",
      "    At iteration 6800 -> loss: 0.09341472187140053\n",
      "    At iteration 6900 -> loss: 0.09336370850039288\n",
      "    At iteration 7000 -> loss: 0.09344842716819818\n",
      "    At iteration 7100 -> loss: 0.09344502039454412\n",
      "    At iteration 7200 -> loss: 0.09343808644180114\n",
      "    At iteration 7300 -> loss: 0.09381768800254225\n",
      "    At iteration 7400 -> loss: 0.09377700352003718\n",
      "    At iteration 7500 -> loss: 0.09377216792278924\n",
      "    At iteration 7600 -> loss: 0.09376219009614638\n",
      "    At iteration 7700 -> loss: 0.09371460317616008\n",
      "    At iteration 7800 -> loss: 0.09368189115264708\n",
      "    At iteration 7900 -> loss: 0.09363344279669575\n",
      "    At iteration 8000 -> loss: 0.09364426183987697\n",
      "    At iteration 8100 -> loss: 0.09378071180832658\n",
      "    At iteration 8200 -> loss: 0.09373489130978008\n",
      "    At iteration 8300 -> loss: 0.09370153454223205\n",
      "    At iteration 8400 -> loss: 0.0936330102446289\n",
      "    At iteration 8500 -> loss: 0.09361824320642566\n",
      "    At iteration 8600 -> loss: 0.09360256910992774\n",
      "    At iteration 8700 -> loss: 0.09356923737207025\n",
      "    At iteration 8800 -> loss: 0.09352622925453657\n",
      "    At iteration 8900 -> loss: 0.09357402886255684\n",
      "    At iteration 9000 -> loss: 0.09354274765998762\n",
      "    At iteration 9100 -> loss: 0.09351225955228812\n",
      "    At iteration 9200 -> loss: 0.09350284591577507\n",
      "    At iteration 9300 -> loss: 0.09346526412639491\n",
      "    At iteration 9400 -> loss: 0.0935216365120063\n",
      "    At iteration 9500 -> loss: 0.09353299666560583\n",
      "    At iteration 9600 -> loss: 0.09350715521458042\n",
      "    At iteration 9700 -> loss: 0.0934969157499578\n",
      "    At iteration 9800 -> loss: 0.09348076756968553\n",
      "    At iteration 9900 -> loss: 0.0934534613722678\n",
      "    At iteration 10000 -> loss: 0.09341998482718365\n",
      "    At iteration 10100 -> loss: 0.09340182525084077\n",
      "    At iteration 10200 -> loss: 0.09340290800961347\n",
      "    At iteration 10300 -> loss: 0.09342136171306135\n",
      "    At iteration 10400 -> loss: 0.09338469010659904\n",
      "    At iteration 10500 -> loss: 0.09340433195305645\n",
      "    At iteration 10600 -> loss: 0.09338750043279506\n",
      "    At iteration 10700 -> loss: 0.0933461495398035\n",
      "    At iteration 10800 -> loss: 0.09334236059920004\n",
      "    At iteration 10900 -> loss: 0.09331582158135635\n",
      "    At iteration 11000 -> loss: 0.09332727622730379\n",
      "    At iteration 11100 -> loss: 0.09331737457697742\n",
      "    At iteration 11200 -> loss: 0.09330044020184754\n",
      "    At iteration 11300 -> loss: 0.09330050761392927\n",
      "    At iteration 11400 -> loss: 0.09334433891390541\n",
      "    At iteration 11500 -> loss: 0.09333048564322541\n",
      "    At iteration 11600 -> loss: 0.09334077649608494\n",
      "    At iteration 11700 -> loss: 0.09331768971175759\n",
      "    At iteration 11800 -> loss: 0.09330312280568884\n",
      "    At iteration 11900 -> loss: 0.09337330629925406\n",
      "    At iteration 12000 -> loss: 0.09340564281156025\n",
      "    At iteration 12100 -> loss: 0.093410563682704\n",
      "    At iteration 12200 -> loss: 0.09338535742982772\n",
      "    At iteration 12300 -> loss: 0.09335234870686497\n",
      "    At iteration 12400 -> loss: 0.09333471725517684\n",
      "    At iteration 12500 -> loss: 0.0933195269381794\n",
      "    At iteration 12600 -> loss: 0.09333200867322257\n",
      "    At iteration 12700 -> loss: 0.09331539865721226\n",
      "    At iteration 12800 -> loss: 0.09330159930685693\n",
      "    At iteration 12900 -> loss: 0.09328512498161169\n",
      "    At iteration 13000 -> loss: 0.09327978831410359\n",
      "    At iteration 13100 -> loss: 0.09325041155801662\n",
      "    At iteration 13200 -> loss: 0.09322193037407811\n",
      "    At iteration 13300 -> loss: 0.09320713429013108\n",
      "    At iteration 13400 -> loss: 0.09319157524768475\n",
      "    At iteration 13500 -> loss: 0.09317287509988624\n",
      "    At iteration 13600 -> loss: 0.09315981481973759\n",
      "Staring Epoch 22\n",
      "    At iteration 0 -> loss: 0.09547865390777588\n",
      "    At iteration 100 -> loss: 0.09415414591605423\n",
      "    At iteration 200 -> loss: 0.09237376943753316\n",
      "    At iteration 300 -> loss: 0.09174332144855948\n",
      "    At iteration 400 -> loss: 0.09168793147965076\n",
      "    At iteration 500 -> loss: 0.09224859100361299\n",
      "    At iteration 600 -> loss: 0.09255206967152485\n",
      "    At iteration 700 -> loss: 0.09203701540689996\n",
      "    At iteration 800 -> loss: 0.0922278238330327\n",
      "    At iteration 900 -> loss: 0.09227240854350632\n",
      "    At iteration 1000 -> loss: 0.09241007518022529\n",
      "    At iteration 1100 -> loss: 0.09223504719698289\n",
      "    At iteration 1200 -> loss: 0.09219491586861688\n",
      "    At iteration 1300 -> loss: 0.09241723119424664\n",
      "    At iteration 1400 -> loss: 0.092189044939127\n",
      "    At iteration 1500 -> loss: 0.0924910362305861\n",
      "    At iteration 1600 -> loss: 0.09251778910883351\n",
      "    At iteration 1700 -> loss: 0.09238029462200463\n",
      "    At iteration 1800 -> loss: 0.09239618418637052\n",
      "    At iteration 1900 -> loss: 0.09244138465496078\n",
      "    At iteration 2000 -> loss: 0.09234416533172524\n",
      "    At iteration 2100 -> loss: 0.0923170261573747\n",
      "    At iteration 2200 -> loss: 0.09253968622436964\n",
      "    At iteration 2300 -> loss: 0.09261614576848623\n",
      "    At iteration 2400 -> loss: 0.09245281456525044\n",
      "    At iteration 2500 -> loss: 0.09249822985609844\n",
      "    At iteration 2600 -> loss: 0.0926475569104891\n",
      "    At iteration 2700 -> loss: 0.09255269359427504\n",
      "    At iteration 2800 -> loss: 0.09270382347125769\n",
      "    At iteration 2900 -> loss: 0.09281889721049602\n",
      "    At iteration 3000 -> loss: 0.09271706255340288\n",
      "    At iteration 3100 -> loss: 0.09266824398370148\n",
      "    At iteration 3200 -> loss: 0.09276356217673426\n",
      "    At iteration 3300 -> loss: 0.0927049700124677\n",
      "    At iteration 3400 -> loss: 0.09276328749375831\n",
      "    At iteration 3500 -> loss: 0.09275946113254653\n",
      "    At iteration 3600 -> loss: 0.09284935238651106\n",
      "    At iteration 3700 -> loss: 0.09279326962175863\n",
      "    At iteration 3800 -> loss: 0.09281832441051006\n",
      "    At iteration 3900 -> loss: 0.09276527006988043\n",
      "    At iteration 4000 -> loss: 0.09271830216110848\n",
      "    At iteration 4100 -> loss: 0.09272840237319134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4200 -> loss: 0.09264277321811909\n",
      "    At iteration 4300 -> loss: 0.09262677873453368\n",
      "    At iteration 4400 -> loss: 0.09265401018949737\n",
      "    At iteration 4500 -> loss: 0.09262640458014391\n",
      "    At iteration 4600 -> loss: 0.09263586781560705\n",
      "    At iteration 4700 -> loss: 0.092550151850859\n",
      "    At iteration 4800 -> loss: 0.09265050487879767\n",
      "    At iteration 4900 -> loss: 0.0926490691642175\n",
      "    At iteration 5000 -> loss: 0.0926191068930282\n",
      "    At iteration 5100 -> loss: 0.09315861710096825\n",
      "    At iteration 5200 -> loss: 0.09314043770411891\n",
      "    At iteration 5300 -> loss: 0.09310362601914177\n",
      "    At iteration 5400 -> loss: 0.09305930020224544\n",
      "    At iteration 5500 -> loss: 0.09316666015543913\n",
      "    At iteration 5600 -> loss: 0.09319470280159253\n",
      "    At iteration 5700 -> loss: 0.09317232788054217\n",
      "    At iteration 5800 -> loss: 0.0931104744521794\n",
      "    At iteration 5900 -> loss: 0.09304975351502943\n",
      "    At iteration 6000 -> loss: 0.09307827120224355\n",
      "    At iteration 6100 -> loss: 0.09306472384940134\n",
      "    At iteration 6200 -> loss: 0.0930225464932043\n",
      "    At iteration 6300 -> loss: 0.09299792113828467\n",
      "    At iteration 6400 -> loss: 0.09300987991474206\n",
      "    At iteration 6500 -> loss: 0.0929537708045587\n",
      "    At iteration 6600 -> loss: 0.0930942331172055\n",
      "    At iteration 6700 -> loss: 0.09308339061358763\n",
      "    At iteration 6800 -> loss: 0.09317899780972311\n",
      "    At iteration 6900 -> loss: 0.09329477457339728\n",
      "    At iteration 7000 -> loss: 0.09322256441524579\n",
      "    At iteration 7100 -> loss: 0.09321597866251466\n",
      "    At iteration 7200 -> loss: 0.09317324829603761\n",
      "    At iteration 7300 -> loss: 0.09318497785705361\n",
      "    At iteration 7400 -> loss: 0.09313737013178645\n",
      "    At iteration 7500 -> loss: 0.09312022079575234\n",
      "    At iteration 7600 -> loss: 0.093209897158328\n",
      "    At iteration 7700 -> loss: 0.0932586665915955\n",
      "    At iteration 7800 -> loss: 0.09321626707984378\n",
      "    At iteration 7900 -> loss: 0.0932249656745972\n",
      "    At iteration 8000 -> loss: 0.09332921886149861\n",
      "    At iteration 8100 -> loss: 0.09330257765859254\n",
      "    At iteration 8200 -> loss: 0.0932809909573313\n",
      "    At iteration 8300 -> loss: 0.09327590340258705\n",
      "    At iteration 8400 -> loss: 0.09326109231400716\n",
      "    At iteration 8500 -> loss: 0.09321955528766632\n",
      "    At iteration 8600 -> loss: 0.09319235703650201\n",
      "    At iteration 8700 -> loss: 0.093208300345226\n",
      "    At iteration 8800 -> loss: 0.0932509785777869\n",
      "    At iteration 8900 -> loss: 0.0932221612151475\n",
      "    At iteration 9000 -> loss: 0.09323177731245096\n",
      "    At iteration 9100 -> loss: 0.09320515192486431\n",
      "    At iteration 9200 -> loss: 0.09317665636447085\n",
      "    At iteration 9300 -> loss: 0.09317722131228831\n",
      "    At iteration 9400 -> loss: 0.0931940352116749\n",
      "    At iteration 9500 -> loss: 0.0931640255600152\n",
      "    At iteration 9600 -> loss: 0.09316627430932808\n",
      "    At iteration 9700 -> loss: 0.09318716545183622\n",
      "    At iteration 9800 -> loss: 0.09319808604103084\n",
      "    At iteration 9900 -> loss: 0.09320088884505669\n",
      "    At iteration 10000 -> loss: 0.09319650252381956\n",
      "    At iteration 10100 -> loss: 0.09319750592159014\n",
      "    At iteration 10200 -> loss: 0.09317105477839172\n",
      "    At iteration 10300 -> loss: 0.09312089764664688\n",
      "    At iteration 10400 -> loss: 0.0932129587765289\n",
      "    At iteration 10500 -> loss: 0.09319739117006166\n",
      "    At iteration 10600 -> loss: 0.09317696363955712\n",
      "    At iteration 10700 -> loss: 0.09318522396823482\n",
      "    At iteration 10800 -> loss: 0.0931577347532046\n",
      "    At iteration 10900 -> loss: 0.09312013534080349\n",
      "    At iteration 11000 -> loss: 0.09318336380531721\n",
      "    At iteration 11100 -> loss: 0.09320129836444524\n",
      "    At iteration 11200 -> loss: 0.09317955241351\n",
      "    At iteration 11300 -> loss: 0.09317732120390516\n",
      "    At iteration 11400 -> loss: 0.09321138583515165\n",
      "    At iteration 11500 -> loss: 0.0931958280583156\n",
      "    At iteration 11600 -> loss: 0.09317924493090847\n",
      "    At iteration 11700 -> loss: 0.09318191693163391\n",
      "    At iteration 11800 -> loss: 0.0931847624385637\n",
      "    At iteration 11900 -> loss: 0.09322480872340401\n",
      "    At iteration 12000 -> loss: 0.09319058041413675\n",
      "    At iteration 12100 -> loss: 0.09318497733765747\n",
      "    At iteration 12200 -> loss: 0.09326051632607231\n",
      "    At iteration 12300 -> loss: 0.09324575043500895\n",
      "    At iteration 12400 -> loss: 0.09326308315846547\n",
      "    At iteration 12500 -> loss: 0.09322686552556919\n",
      "    At iteration 12600 -> loss: 0.09323100414198826\n",
      "    At iteration 12700 -> loss: 0.09321566630871372\n",
      "    At iteration 12800 -> loss: 0.09321491730907247\n",
      "    At iteration 12900 -> loss: 0.09318870318734211\n",
      "    At iteration 13000 -> loss: 0.09316745141556516\n",
      "    At iteration 13100 -> loss: 0.09314790758691482\n",
      "    At iteration 13200 -> loss: 0.09311398305250403\n",
      "    At iteration 13300 -> loss: 0.09310211887427387\n",
      "    At iteration 13400 -> loss: 0.09309681912407321\n",
      "    At iteration 13500 -> loss: 0.09310159903228149\n",
      "    At iteration 13600 -> loss: 0.0931023832949205\n",
      "Staring Epoch 23\n",
      "    At iteration 0 -> loss: 0.08250603941269219\n",
      "    At iteration 100 -> loss: 0.09294394281213689\n",
      "    At iteration 200 -> loss: 0.09229903386589157\n",
      "    At iteration 300 -> loss: 0.09163340410937523\n",
      "    At iteration 400 -> loss: 0.0939411354426272\n",
      "    At iteration 500 -> loss: 0.0939954309087548\n",
      "    At iteration 600 -> loss: 0.09396472534476465\n",
      "    At iteration 700 -> loss: 0.0934132602112913\n",
      "    At iteration 800 -> loss: 0.09316269558726017\n",
      "    At iteration 900 -> loss: 0.09381241434743963\n",
      "    At iteration 1000 -> loss: 0.09366272321115708\n",
      "    At iteration 1100 -> loss: 0.09346014383420426\n",
      "    At iteration 1200 -> loss: 0.09412222247197977\n",
      "    At iteration 1300 -> loss: 0.09420582863300564\n",
      "    At iteration 1400 -> loss: 0.09471619388214005\n",
      "    At iteration 1500 -> loss: 0.0946545774478427\n",
      "    At iteration 1600 -> loss: 0.09453337185109487\n",
      "    At iteration 1700 -> loss: 0.09463338416798044\n",
      "    At iteration 1800 -> loss: 0.09444322240028649\n",
      "    At iteration 1900 -> loss: 0.09416397692929603\n",
      "    At iteration 2000 -> loss: 0.09440746868153234\n",
      "    At iteration 2100 -> loss: 0.09420816516307955\n",
      "    At iteration 2200 -> loss: 0.09426593837503046\n",
      "    At iteration 2300 -> loss: 0.09425374696899746\n",
      "    At iteration 2400 -> loss: 0.09421399297334186\n",
      "    At iteration 2500 -> loss: 0.09404676139306768\n",
      "    At iteration 2600 -> loss: 0.09389657293719718\n",
      "    At iteration 2700 -> loss: 0.09379829374195416\n",
      "    At iteration 2800 -> loss: 0.09366620364453054\n",
      "    At iteration 2900 -> loss: 0.09349080357200976\n",
      "    At iteration 3000 -> loss: 0.0934167315429494\n",
      "    At iteration 3100 -> loss: 0.09340023606125379\n",
      "    At iteration 3200 -> loss: 0.09324068516832887\n",
      "    At iteration 3300 -> loss: 0.0931858938841793\n",
      "    At iteration 3400 -> loss: 0.09315140687624623\n",
      "    At iteration 3500 -> loss: 0.09314603349198915\n",
      "    At iteration 3600 -> loss: 0.09302763797668682\n",
      "    At iteration 3700 -> loss: 0.09303684590325939\n",
      "    At iteration 3800 -> loss: 0.09300077954960942\n",
      "    At iteration 3900 -> loss: 0.09288238069086965\n",
      "    At iteration 4000 -> loss: 0.09289776511326192\n",
      "    At iteration 4100 -> loss: 0.09284979193798021\n",
      "    At iteration 4200 -> loss: 0.09287244186596508\n",
      "    At iteration 4300 -> loss: 0.09292905826561254\n",
      "    At iteration 4400 -> loss: 0.09294359761654948\n",
      "    At iteration 4500 -> loss: 0.09292913024407212\n",
      "    At iteration 4600 -> loss: 0.09296570774463954\n",
      "    At iteration 4700 -> loss: 0.0929880219677164\n",
      "    At iteration 4800 -> loss: 0.09290549264185949\n",
      "    At iteration 4900 -> loss: 0.09287799952737433\n",
      "    At iteration 5000 -> loss: 0.09281262710611564\n",
      "    At iteration 5100 -> loss: 0.09281581886853206\n",
      "    At iteration 5200 -> loss: 0.09281917759903413\n",
      "    At iteration 5300 -> loss: 0.09277714966519779\n",
      "    At iteration 5400 -> loss: 0.09277980088774553\n",
      "    At iteration 5500 -> loss: 0.092700580848496\n",
      "    At iteration 5600 -> loss: 0.09271029775628147\n",
      "    At iteration 5700 -> loss: 0.09281809287168095\n",
      "    At iteration 5800 -> loss: 0.0931263081137379\n",
      "    At iteration 5900 -> loss: 0.09309038501050004\n",
      "    At iteration 6000 -> loss: 0.0931098924561064\n",
      "    At iteration 6100 -> loss: 0.09312420643367345\n",
      "    At iteration 6200 -> loss: 0.09308122844596463\n",
      "    At iteration 6300 -> loss: 0.0930633379233761\n",
      "    At iteration 6400 -> loss: 0.09308343921764904\n",
      "    At iteration 6500 -> loss: 0.09307537060459074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6600 -> loss: 0.09304748588178181\n",
      "    At iteration 6700 -> loss: 0.09305127571862185\n",
      "    At iteration 6800 -> loss: 0.09298339974443888\n",
      "    At iteration 6900 -> loss: 0.0929393195753241\n",
      "    At iteration 7000 -> loss: 0.09295897553295925\n",
      "    At iteration 7100 -> loss: 0.09294455151678274\n",
      "    At iteration 7200 -> loss: 0.09292103949577038\n",
      "    At iteration 7300 -> loss: 0.09295283702187863\n",
      "    At iteration 7400 -> loss: 0.09296282084457029\n",
      "    At iteration 7500 -> loss: 0.09297319810202696\n",
      "    At iteration 7600 -> loss: 0.09307353786263198\n",
      "    At iteration 7700 -> loss: 0.09305637196795226\n",
      "    At iteration 7800 -> loss: 0.09302539486925027\n",
      "    At iteration 7900 -> loss: 0.09317234866899639\n",
      "    At iteration 8000 -> loss: 0.09314331044218721\n",
      "    At iteration 8100 -> loss: 0.0933087619967569\n",
      "    At iteration 8200 -> loss: 0.09327808747118985\n",
      "    At iteration 8300 -> loss: 0.09332877297888201\n",
      "    At iteration 8400 -> loss: 0.09329200238130624\n",
      "    At iteration 8500 -> loss: 0.09326000683666903\n",
      "    At iteration 8600 -> loss: 0.09327080058748637\n",
      "    At iteration 8700 -> loss: 0.09329169790011936\n",
      "    At iteration 8800 -> loss: 0.09324841765381872\n",
      "    At iteration 8900 -> loss: 0.09320184997646458\n",
      "    At iteration 9000 -> loss: 0.09315452377575137\n",
      "    At iteration 9100 -> loss: 0.09312370890781169\n",
      "    At iteration 9200 -> loss: 0.09309613630823425\n",
      "    At iteration 9300 -> loss: 0.09304232151177466\n",
      "    At iteration 9400 -> loss: 0.09304950237326436\n",
      "    At iteration 9500 -> loss: 0.0930175819221782\n",
      "    At iteration 9600 -> loss: 0.09298212534136117\n",
      "    At iteration 9700 -> loss: 0.0929646490760604\n",
      "    At iteration 9800 -> loss: 0.09294088002799408\n",
      "    At iteration 9900 -> loss: 0.09295417989646892\n",
      "    At iteration 10000 -> loss: 0.0931039156649289\n",
      "    At iteration 10100 -> loss: 0.09312751861124255\n",
      "    At iteration 10200 -> loss: 0.09315164416750164\n",
      "    At iteration 10300 -> loss: 0.09313894276097\n",
      "    At iteration 10400 -> loss: 0.09313081129144526\n",
      "    At iteration 10500 -> loss: 0.09310507740071822\n",
      "    At iteration 10600 -> loss: 0.09310279522398726\n",
      "    At iteration 10700 -> loss: 0.09311449820898308\n",
      "    At iteration 10800 -> loss: 0.09309416124898486\n",
      "    At iteration 10900 -> loss: 0.09307004797297898\n",
      "    At iteration 11000 -> loss: 0.09307617315201129\n",
      "    At iteration 11100 -> loss: 0.09306950547469309\n",
      "    At iteration 11200 -> loss: 0.09305602290586017\n",
      "    At iteration 11300 -> loss: 0.09302726355395868\n",
      "    At iteration 11400 -> loss: 0.09302401911091175\n",
      "    At iteration 11500 -> loss: 0.09298000975167327\n",
      "    At iteration 11600 -> loss: 0.09295670390728629\n",
      "    At iteration 11700 -> loss: 0.09296317142715778\n",
      "    At iteration 11800 -> loss: 0.09299773795863481\n",
      "    At iteration 11900 -> loss: 0.09298170207585096\n",
      "    At iteration 12000 -> loss: 0.09296710277587912\n",
      "    At iteration 12100 -> loss: 0.0929341555889357\n",
      "    At iteration 12200 -> loss: 0.09298685513908522\n",
      "    At iteration 12300 -> loss: 0.09299979574121067\n",
      "    At iteration 12400 -> loss: 0.0932426420181249\n",
      "    At iteration 12500 -> loss: 0.09323168095430928\n",
      "    At iteration 12600 -> loss: 0.09320716823685461\n",
      "    At iteration 12700 -> loss: 0.09320612699065399\n",
      "    At iteration 12800 -> loss: 0.09318299995524826\n",
      "    At iteration 12900 -> loss: 0.09320421742504863\n",
      "    At iteration 13000 -> loss: 0.09319037390559377\n",
      "    At iteration 13100 -> loss: 0.09316694636785992\n",
      "    At iteration 13200 -> loss: 0.09315537024237958\n",
      "    At iteration 13300 -> loss: 0.09314396725166546\n",
      "    At iteration 13400 -> loss: 0.09316388614085658\n",
      "    At iteration 13500 -> loss: 0.09313335400570011\n",
      "    At iteration 13600 -> loss: 0.0931283232965536\n",
      "Staring Epoch 24\n",
      "    At iteration 0 -> loss: 0.09725041734054685\n",
      "    At iteration 100 -> loss: 0.09249944616303787\n",
      "    At iteration 200 -> loss: 0.09302001435272901\n",
      "    At iteration 300 -> loss: 0.09308179028065333\n",
      "    At iteration 400 -> loss: 0.09223795676593864\n",
      "    At iteration 500 -> loss: 0.09280561501170957\n",
      "    At iteration 600 -> loss: 0.09285520604914078\n",
      "    At iteration 700 -> loss: 0.09284074079828608\n",
      "    At iteration 800 -> loss: 0.09291279332492952\n",
      "    At iteration 900 -> loss: 0.09288165961309809\n",
      "    At iteration 1000 -> loss: 0.09280408722641949\n",
      "    At iteration 1100 -> loss: 0.09261560301794593\n",
      "    At iteration 1200 -> loss: 0.09251960329440226\n",
      "    At iteration 1300 -> loss: 0.09224573677323625\n",
      "    At iteration 1400 -> loss: 0.09229180018223072\n",
      "    At iteration 1500 -> loss: 0.09268071886773438\n",
      "    At iteration 1600 -> loss: 0.09256397798486067\n",
      "    At iteration 1700 -> loss: 0.09237571529281892\n",
      "    At iteration 1800 -> loss: 0.09227864860661746\n",
      "    At iteration 1900 -> loss: 0.09214209429957455\n",
      "    At iteration 2000 -> loss: 0.09225728706746295\n",
      "    At iteration 2100 -> loss: 0.09273069995196594\n",
      "    At iteration 2200 -> loss: 0.09262448118793756\n",
      "    At iteration 2300 -> loss: 0.09263109594148371\n",
      "    At iteration 2400 -> loss: 0.09249420832495578\n",
      "    At iteration 2500 -> loss: 0.09260190240277481\n",
      "    At iteration 2600 -> loss: 0.09257639269150741\n",
      "    At iteration 2700 -> loss: 0.09252592491965746\n",
      "    At iteration 2800 -> loss: 0.09268272478810681\n",
      "    At iteration 2900 -> loss: 0.09281559904683045\n",
      "    At iteration 3000 -> loss: 0.09269811074854199\n",
      "    At iteration 3100 -> loss: 0.09274401128663029\n",
      "    At iteration 3200 -> loss: 0.09271392210168439\n",
      "    At iteration 3300 -> loss: 0.09275948015381505\n",
      "    At iteration 3400 -> loss: 0.09270025908513654\n",
      "    At iteration 3500 -> loss: 0.09282836390025935\n",
      "    At iteration 3600 -> loss: 0.09278898032720984\n",
      "    At iteration 3700 -> loss: 0.09279368639360197\n",
      "    At iteration 3800 -> loss: 0.09277687318032693\n",
      "    At iteration 3900 -> loss: 0.09284292850427212\n",
      "    At iteration 4000 -> loss: 0.09282345855710868\n",
      "    At iteration 4100 -> loss: 0.09286476750159094\n",
      "    At iteration 4200 -> loss: 0.09278787616886155\n",
      "    At iteration 4300 -> loss: 0.09276260174960108\n",
      "    At iteration 4400 -> loss: 0.09274312952010845\n",
      "    At iteration 4500 -> loss: 0.09271079552307668\n",
      "    At iteration 4600 -> loss: 0.09273913755308799\n",
      "    At iteration 4700 -> loss: 0.0927931458999991\n",
      "    At iteration 4800 -> loss: 0.09274369042720249\n",
      "    At iteration 4900 -> loss: 0.09270190491286627\n",
      "    At iteration 5000 -> loss: 0.09263733861801764\n",
      "    At iteration 5100 -> loss: 0.09283292655703715\n",
      "    At iteration 5200 -> loss: 0.09281325380639342\n",
      "    At iteration 5300 -> loss: 0.09275780703208963\n",
      "    At iteration 5400 -> loss: 0.09273281999793219\n",
      "    At iteration 5500 -> loss: 0.0926899685438587\n",
      "    At iteration 5600 -> loss: 0.09264579908819262\n",
      "    At iteration 5700 -> loss: 0.09261917057418735\n",
      "    At iteration 5800 -> loss: 0.09259748483767324\n",
      "    At iteration 5900 -> loss: 0.09261564266060557\n",
      "    At iteration 6000 -> loss: 0.09255928109605754\n",
      "    At iteration 6100 -> loss: 0.09253299631254763\n",
      "    At iteration 6200 -> loss: 0.09252228507094268\n",
      "    At iteration 6300 -> loss: 0.0925373114249629\n",
      "    At iteration 6400 -> loss: 0.09250229424231579\n",
      "    At iteration 6500 -> loss: 0.09246825809677758\n",
      "    At iteration 6600 -> loss: 0.09245943485615166\n",
      "    At iteration 6700 -> loss: 0.09240858878251126\n",
      "    At iteration 6800 -> loss: 0.09238996230603598\n",
      "    At iteration 6900 -> loss: 0.09239550415883566\n",
      "    At iteration 7000 -> loss: 0.09240429626002007\n",
      "    At iteration 7100 -> loss: 0.09235356068866908\n",
      "    At iteration 7200 -> loss: 0.09232907341577369\n",
      "    At iteration 7300 -> loss: 0.09232652834682313\n",
      "    At iteration 7400 -> loss: 0.09230580269461068\n",
      "    At iteration 7500 -> loss: 0.09233071637146258\n",
      "    At iteration 7600 -> loss: 0.09235080311154722\n",
      "    At iteration 7700 -> loss: 0.09232476196937711\n",
      "    At iteration 7800 -> loss: 0.09251492870586635\n",
      "    At iteration 7900 -> loss: 0.09248333591646093\n",
      "    At iteration 8000 -> loss: 0.09247580245913357\n",
      "    At iteration 8100 -> loss: 0.09250335060655558\n",
      "    At iteration 8200 -> loss: 0.09245959686999726\n",
      "    At iteration 8300 -> loss: 0.09246111829271729\n",
      "    At iteration 8400 -> loss: 0.09243365273254231\n",
      "    At iteration 8500 -> loss: 0.09246026395383931\n",
      "    At iteration 8600 -> loss: 0.09241187973165578\n",
      "    At iteration 8700 -> loss: 0.09240078651605102\n",
      "    At iteration 8800 -> loss: 0.0924051738202742\n",
      "    At iteration 8900 -> loss: 0.09243666412242664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9000 -> loss: 0.09243304779359603\n",
      "    At iteration 9100 -> loss: 0.09239081791372915\n",
      "    At iteration 9200 -> loss: 0.09239259473794087\n",
      "    At iteration 9300 -> loss: 0.09239907686751046\n",
      "    At iteration 9400 -> loss: 0.09239284719647713\n",
      "    At iteration 9500 -> loss: 0.09237672643192708\n",
      "    At iteration 9600 -> loss: 0.09239144575917746\n",
      "    At iteration 9700 -> loss: 0.09247030066661249\n",
      "    At iteration 9800 -> loss: 0.0924545486844354\n",
      "    At iteration 9900 -> loss: 0.09255151802510177\n",
      "    At iteration 10000 -> loss: 0.09259159627928266\n",
      "    At iteration 10100 -> loss: 0.09258585957055536\n",
      "    At iteration 10200 -> loss: 0.09257864268546454\n",
      "    At iteration 10300 -> loss: 0.09259534518562205\n",
      "    At iteration 10400 -> loss: 0.0925587568687012\n",
      "    At iteration 10500 -> loss: 0.09260263768782265\n",
      "    At iteration 10600 -> loss: 0.09260450925396074\n",
      "    At iteration 10700 -> loss: 0.09259499045698703\n",
      "    At iteration 10800 -> loss: 0.09260101800822461\n",
      "    At iteration 10900 -> loss: 0.09258414753883232\n",
      "    At iteration 11000 -> loss: 0.09272356936790602\n",
      "    At iteration 11100 -> loss: 0.09275367234229664\n",
      "    At iteration 11200 -> loss: 0.09280988882345491\n",
      "    At iteration 11300 -> loss: 0.09278771045912838\n",
      "    At iteration 11400 -> loss: 0.09279486830799286\n",
      "    At iteration 11500 -> loss: 0.09278281577683653\n",
      "    At iteration 11600 -> loss: 0.0927513889033051\n",
      "    At iteration 11700 -> loss: 0.09275679696971087\n",
      "    At iteration 11800 -> loss: 0.0927595067680829\n",
      "    At iteration 11900 -> loss: 0.09286320562466031\n",
      "    At iteration 12000 -> loss: 0.09285137048245339\n",
      "    At iteration 12100 -> loss: 0.09282383501278561\n",
      "    At iteration 12200 -> loss: 0.09287260103489321\n",
      "    At iteration 12300 -> loss: 0.09286600099666599\n",
      "    At iteration 12400 -> loss: 0.09286382620271774\n",
      "    At iteration 12500 -> loss: 0.0929511157238636\n",
      "    At iteration 12600 -> loss: 0.09294608940473804\n",
      "    At iteration 12700 -> loss: 0.09293145035176803\n",
      "    At iteration 12800 -> loss: 0.09291032686040332\n",
      "    At iteration 12900 -> loss: 0.09289827998079228\n",
      "    At iteration 13000 -> loss: 0.09292730907800026\n",
      "    At iteration 13100 -> loss: 0.09290687135060609\n",
      "    At iteration 13200 -> loss: 0.09314442590105984\n",
      "    At iteration 13300 -> loss: 0.09315127497361635\n",
      "    At iteration 13400 -> loss: 0.09316532803311997\n",
      "    At iteration 13500 -> loss: 0.0931529873938413\n",
      "    At iteration 13600 -> loss: 0.09312997232121331\n",
      "Staring Epoch 25\n",
      "    At iteration 0 -> loss: 0.08705141581594944\n",
      "    At iteration 100 -> loss: 0.09630341624539268\n",
      "    At iteration 200 -> loss: 0.09436904072315859\n",
      "    At iteration 300 -> loss: 0.09535041506748032\n",
      "    At iteration 400 -> loss: 0.09393451499478066\n",
      "    At iteration 500 -> loss: 0.09329150264619222\n",
      "    At iteration 600 -> loss: 0.09401614959243335\n",
      "    At iteration 700 -> loss: 0.09371394483924406\n",
      "    At iteration 800 -> loss: 0.09372641438634188\n",
      "    At iteration 900 -> loss: 0.09338118995675788\n",
      "    At iteration 1000 -> loss: 0.09331835975372502\n",
      "    At iteration 1100 -> loss: 0.09344902821489634\n",
      "    At iteration 1200 -> loss: 0.0933964447756099\n",
      "    At iteration 1300 -> loss: 0.09320960764912811\n",
      "    At iteration 1400 -> loss: 0.09296726810233588\n",
      "    At iteration 1500 -> loss: 0.09299327612000949\n",
      "    At iteration 1600 -> loss: 0.09296494280296042\n",
      "    At iteration 1700 -> loss: 0.09306874033046911\n",
      "    At iteration 1800 -> loss: 0.09327721934690908\n",
      "    At iteration 1900 -> loss: 0.0931572073981854\n",
      "    At iteration 2000 -> loss: 0.09305516660047261\n",
      "    At iteration 2100 -> loss: 0.09290906226972054\n",
      "    At iteration 2200 -> loss: 0.09297469972854774\n",
      "    At iteration 2300 -> loss: 0.09288062804421357\n",
      "    At iteration 2400 -> loss: 0.09278197101265778\n",
      "    At iteration 2500 -> loss: 0.09262423638945377\n",
      "    At iteration 2600 -> loss: 0.09257560780104182\n",
      "    At iteration 2700 -> loss: 0.09248596270255664\n",
      "    At iteration 2800 -> loss: 0.09267136787342801\n",
      "    At iteration 2900 -> loss: 0.09260539301960509\n",
      "    At iteration 3000 -> loss: 0.09257601765290892\n",
      "    At iteration 3100 -> loss: 0.09271714277155557\n",
      "    At iteration 3200 -> loss: 0.09281535981694004\n",
      "    At iteration 3300 -> loss: 0.09281024021724883\n",
      "    At iteration 3400 -> loss: 0.09284658662079193\n",
      "    At iteration 3500 -> loss: 0.09281181914434687\n",
      "    At iteration 3600 -> loss: 0.0927356333626234\n",
      "    At iteration 3700 -> loss: 0.09274335167774504\n",
      "    At iteration 3800 -> loss: 0.09277087853601533\n",
      "    At iteration 3900 -> loss: 0.09268923659387891\n",
      "    At iteration 4000 -> loss: 0.09272268684759702\n",
      "    At iteration 4100 -> loss: 0.0926723639471312\n",
      "    At iteration 4200 -> loss: 0.09272012500618801\n",
      "    At iteration 4300 -> loss: 0.09270103572490916\n",
      "    At iteration 4400 -> loss: 0.0928333403515046\n",
      "    At iteration 4500 -> loss: 0.09283608955940757\n",
      "    At iteration 4600 -> loss: 0.09277052833316321\n",
      "    At iteration 4700 -> loss: 0.09275819831462925\n",
      "    At iteration 4800 -> loss: 0.09268265787217302\n",
      "    At iteration 4900 -> loss: 0.09264939056809295\n",
      "    At iteration 5000 -> loss: 0.09264777806056505\n",
      "    At iteration 5100 -> loss: 0.09259884682378884\n",
      "    At iteration 5200 -> loss: 0.09259913539592357\n",
      "    At iteration 5300 -> loss: 0.09268611258038872\n",
      "    At iteration 5400 -> loss: 0.09262290931722324\n",
      "    At iteration 5500 -> loss: 0.09259419868198372\n",
      "    At iteration 5600 -> loss: 0.09277632393958167\n",
      "    At iteration 5700 -> loss: 0.09273271294024384\n",
      "    At iteration 5800 -> loss: 0.09270928182067531\n",
      "    At iteration 5900 -> loss: 0.09267309057539384\n",
      "    At iteration 6000 -> loss: 0.09263863385083206\n",
      "    At iteration 6100 -> loss: 0.09268786197656052\n",
      "    At iteration 6200 -> loss: 0.09266615768119177\n",
      "    At iteration 6300 -> loss: 0.09261620733573102\n",
      "    At iteration 6400 -> loss: 0.09262262194763347\n",
      "    At iteration 6500 -> loss: 0.09282564861013888\n",
      "    At iteration 6600 -> loss: 0.0927914719975063\n",
      "    At iteration 6700 -> loss: 0.09282609838218976\n",
      "    At iteration 6800 -> loss: 0.09281394225465575\n",
      "    At iteration 6900 -> loss: 0.09278704777336792\n",
      "    At iteration 7000 -> loss: 0.09275658287592463\n",
      "    At iteration 7100 -> loss: 0.09271548157838003\n",
      "    At iteration 7200 -> loss: 0.09288600201128112\n",
      "    At iteration 7300 -> loss: 0.09292219866899186\n",
      "    At iteration 7400 -> loss: 0.09290472881429813\n",
      "    At iteration 7500 -> loss: 0.09289654179856231\n",
      "    At iteration 7600 -> loss: 0.09290701342300008\n",
      "    At iteration 7700 -> loss: 0.09291893364316078\n",
      "    At iteration 7800 -> loss: 0.09289825219047426\n",
      "    At iteration 7900 -> loss: 0.09288854243702248\n",
      "    At iteration 8000 -> loss: 0.09285011761528897\n",
      "    At iteration 8100 -> loss: 0.09289324985654239\n",
      "    At iteration 8200 -> loss: 0.0929237125824913\n",
      "    At iteration 8300 -> loss: 0.0929230192790419\n",
      "    At iteration 8400 -> loss: 0.0928912395750849\n",
      "    At iteration 8500 -> loss: 0.09298742642093738\n",
      "    At iteration 8600 -> loss: 0.09296605540480361\n",
      "    At iteration 8700 -> loss: 0.09295439144815752\n",
      "    At iteration 8800 -> loss: 0.09292337387150812\n",
      "    At iteration 8900 -> loss: 0.09287973295094527\n",
      "    At iteration 9000 -> loss: 0.09284311790411655\n",
      "    At iteration 9100 -> loss: 0.0928914798878487\n",
      "    At iteration 9200 -> loss: 0.0928792092437619\n",
      "    At iteration 9300 -> loss: 0.09285364759650805\n",
      "    At iteration 9400 -> loss: 0.09283965230173238\n",
      "    At iteration 9500 -> loss: 0.09285159533686152\n",
      "    At iteration 9600 -> loss: 0.09281222729968497\n",
      "    At iteration 9700 -> loss: 0.09279460094456778\n",
      "    At iteration 9800 -> loss: 0.09276612770083685\n",
      "    At iteration 9900 -> loss: 0.09278668558816476\n",
      "    At iteration 10000 -> loss: 0.09276696124364324\n",
      "    At iteration 10100 -> loss: 0.09276986058415405\n",
      "    At iteration 10200 -> loss: 0.09277981507477111\n",
      "    At iteration 10300 -> loss: 0.09276522579740774\n",
      "    At iteration 10400 -> loss: 0.0927861747011892\n",
      "    At iteration 10500 -> loss: 0.09278184935572227\n",
      "    At iteration 10600 -> loss: 0.09278281343719527\n",
      "    At iteration 10700 -> loss: 0.09278251075206263\n",
      "    At iteration 10800 -> loss: 0.0927812900273347\n",
      "    At iteration 10900 -> loss: 0.09274853827538519\n",
      "    At iteration 11000 -> loss: 0.09274496539653812\n",
      "    At iteration 11100 -> loss: 0.09285918488064342\n",
      "    At iteration 11200 -> loss: 0.09285489332036413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11300 -> loss: 0.09282455274166905\n",
      "    At iteration 11400 -> loss: 0.09280569794203439\n",
      "    At iteration 11500 -> loss: 0.09279822134070989\n",
      "    At iteration 11600 -> loss: 0.09314119316796036\n",
      "    At iteration 11700 -> loss: 0.09315229042646953\n",
      "    At iteration 11800 -> loss: 0.09313041015126897\n",
      "    At iteration 11900 -> loss: 0.09318307909441774\n",
      "    At iteration 12000 -> loss: 0.09317931727158545\n",
      "    At iteration 12100 -> loss: 0.09315775533280028\n",
      "    At iteration 12200 -> loss: 0.09316271914260411\n",
      "    At iteration 12300 -> loss: 0.09319155431930466\n",
      "    At iteration 12400 -> loss: 0.09317870437857749\n",
      "    At iteration 12500 -> loss: 0.09316302276740254\n",
      "    At iteration 12600 -> loss: 0.09319895535188881\n",
      "    At iteration 12700 -> loss: 0.09321140727739047\n",
      "    At iteration 12800 -> loss: 0.09318777132004608\n",
      "    At iteration 12900 -> loss: 0.09316193893105251\n",
      "    At iteration 13000 -> loss: 0.09316125236798874\n",
      "    At iteration 13100 -> loss: 0.09316146558545933\n",
      "    At iteration 13200 -> loss: 0.09313651851078102\n",
      "    At iteration 13300 -> loss: 0.09316475478365309\n",
      "    At iteration 13400 -> loss: 0.09315789043819572\n",
      "    At iteration 13500 -> loss: 0.09317867709733403\n",
      "    At iteration 13600 -> loss: 0.09316789650971954\n",
      "Staring Epoch 26\n",
      "    At iteration 0 -> loss: 0.09881467744708061\n",
      "    At iteration 100 -> loss: 0.0899778580814524\n",
      "    At iteration 200 -> loss: 0.09214003232335898\n",
      "    At iteration 300 -> loss: 0.09417196413134465\n",
      "    At iteration 400 -> loss: 0.09368576361691346\n",
      "    At iteration 500 -> loss: 0.0928700959417756\n",
      "    At iteration 600 -> loss: 0.09254057468693623\n",
      "    At iteration 700 -> loss: 0.09322110391008336\n",
      "    At iteration 800 -> loss: 0.09286561616479269\n",
      "    At iteration 900 -> loss: 0.09278489244261033\n",
      "    At iteration 1000 -> loss: 0.09296842176908628\n",
      "    At iteration 1100 -> loss: 0.09270845101334306\n",
      "    At iteration 1200 -> loss: 0.09234012625418571\n",
      "    At iteration 1300 -> loss: 0.09257393620125359\n",
      "    At iteration 1400 -> loss: 0.0926133606441748\n",
      "    At iteration 1500 -> loss: 0.0924983066842675\n",
      "    At iteration 1600 -> loss: 0.09258720710775838\n",
      "    At iteration 1700 -> loss: 0.09429012052555373\n",
      "    At iteration 1800 -> loss: 0.09410277032359948\n",
      "    At iteration 1900 -> loss: 0.0937527989218529\n",
      "    At iteration 2000 -> loss: 0.09355582142661821\n",
      "    At iteration 2100 -> loss: 0.09349419389856295\n",
      "    At iteration 2200 -> loss: 0.09369401748436765\n",
      "    At iteration 2300 -> loss: 0.09368698438718825\n",
      "    At iteration 2400 -> loss: 0.09391274905169919\n",
      "    At iteration 2500 -> loss: 0.09391595337308963\n",
      "    At iteration 2600 -> loss: 0.09376615035101175\n",
      "    At iteration 2700 -> loss: 0.09379240930301512\n",
      "    At iteration 2800 -> loss: 0.09371033311510397\n",
      "    At iteration 2900 -> loss: 0.09363645048117794\n",
      "    At iteration 3000 -> loss: 0.09369453585571835\n",
      "    At iteration 3100 -> loss: 0.09359449586639058\n",
      "    At iteration 3200 -> loss: 0.09352217113658745\n",
      "    At iteration 3300 -> loss: 0.0935281727788096\n",
      "    At iteration 3400 -> loss: 0.0935016436978692\n",
      "    At iteration 3500 -> loss: 0.09344900395322588\n",
      "    At iteration 3600 -> loss: 0.09336601089991134\n",
      "    At iteration 3700 -> loss: 0.09337994064531813\n",
      "    At iteration 3800 -> loss: 0.09330518630906137\n",
      "    At iteration 3900 -> loss: 0.09331291740944848\n",
      "    At iteration 4000 -> loss: 0.09334709337639316\n",
      "    At iteration 4100 -> loss: 0.09326700054017682\n",
      "    At iteration 4200 -> loss: 0.09349642370939219\n",
      "    At iteration 4300 -> loss: 0.09353774294781576\n",
      "    At iteration 4400 -> loss: 0.09347291110919043\n",
      "    At iteration 4500 -> loss: 0.09344452574281141\n",
      "    At iteration 4600 -> loss: 0.09338236768588801\n",
      "    At iteration 4700 -> loss: 0.09334404418963942\n",
      "    At iteration 4800 -> loss: 0.09341243029030087\n",
      "    At iteration 4900 -> loss: 0.09337733692276282\n",
      "    At iteration 5000 -> loss: 0.09330960890790636\n",
      "    At iteration 5100 -> loss: 0.09328644545375885\n",
      "    At iteration 5200 -> loss: 0.09321717735449994\n",
      "    At iteration 5300 -> loss: 0.0932948112752135\n",
      "    At iteration 5400 -> loss: 0.09332770012670277\n",
      "    At iteration 5500 -> loss: 0.0932819911609996\n",
      "    At iteration 5600 -> loss: 0.0932577411806032\n",
      "    At iteration 5700 -> loss: 0.09326220049212576\n",
      "    At iteration 5800 -> loss: 0.09327230793131447\n",
      "    At iteration 5900 -> loss: 0.0932668691632516\n",
      "    At iteration 6000 -> loss: 0.09322598322198802\n",
      "    At iteration 6100 -> loss: 0.09321053437264296\n",
      "    At iteration 6200 -> loss: 0.09321059460127545\n",
      "    At iteration 6300 -> loss: 0.09315378741742306\n",
      "    At iteration 6400 -> loss: 0.09314348831805068\n",
      "    At iteration 6500 -> loss: 0.09316683618042687\n",
      "    At iteration 6600 -> loss: 0.0931153164503461\n",
      "    At iteration 6700 -> loss: 0.09305745281416122\n",
      "    At iteration 6800 -> loss: 0.09301798459495582\n",
      "    At iteration 6900 -> loss: 0.09300078925056626\n",
      "    At iteration 7000 -> loss: 0.09301090899957565\n",
      "    At iteration 7100 -> loss: 0.09300174086422594\n",
      "    At iteration 7200 -> loss: 0.09303804901682713\n",
      "    At iteration 7300 -> loss: 0.093051960250269\n",
      "    At iteration 7400 -> loss: 0.09305029770902658\n",
      "    At iteration 7500 -> loss: 0.09302269366409079\n",
      "    At iteration 7600 -> loss: 0.09302209928623158\n",
      "    At iteration 7700 -> loss: 0.09297442304737216\n",
      "    At iteration 7800 -> loss: 0.09293977194917294\n",
      "    At iteration 7900 -> loss: 0.0929161193689101\n",
      "    At iteration 8000 -> loss: 0.09289617027797283\n",
      "    At iteration 8100 -> loss: 0.09287719261273523\n",
      "    At iteration 8200 -> loss: 0.09285034253846648\n",
      "    At iteration 8300 -> loss: 0.09283339000603677\n",
      "    At iteration 8400 -> loss: 0.09281855223142212\n",
      "    At iteration 8500 -> loss: 0.09278787861874702\n",
      "    At iteration 8600 -> loss: 0.09275418426870567\n",
      "    At iteration 8700 -> loss: 0.09275359432554\n",
      "    At iteration 8800 -> loss: 0.09273497700618814\n",
      "    At iteration 8900 -> loss: 0.0927283983100869\n",
      "    At iteration 9000 -> loss: 0.09270301023579883\n",
      "    At iteration 9100 -> loss: 0.09267815022816345\n",
      "    At iteration 9200 -> loss: 0.09271609125143929\n",
      "    At iteration 9300 -> loss: 0.09270155826458094\n",
      "    At iteration 9400 -> loss: 0.09271184556091329\n",
      "    At iteration 9500 -> loss: 0.09269529860099851\n",
      "    At iteration 9600 -> loss: 0.09272563413835193\n",
      "    At iteration 9700 -> loss: 0.09274418434434414\n",
      "    At iteration 9800 -> loss: 0.0927241179388305\n",
      "    At iteration 9900 -> loss: 0.09274228519882383\n",
      "    At iteration 10000 -> loss: 0.09270503360453504\n",
      "    At iteration 10100 -> loss: 0.09270719619503502\n",
      "    At iteration 10200 -> loss: 0.0927016891685051\n",
      "    At iteration 10300 -> loss: 0.09267219608201951\n",
      "    At iteration 10400 -> loss: 0.09266139870165466\n",
      "    At iteration 10500 -> loss: 0.09264679914176709\n",
      "    At iteration 10600 -> loss: 0.09265932383931216\n",
      "    At iteration 10700 -> loss: 0.09266413046959081\n",
      "    At iteration 10800 -> loss: 0.09268886497713705\n",
      "    At iteration 10900 -> loss: 0.09278439832726224\n",
      "    At iteration 11000 -> loss: 0.09278114746647195\n",
      "    At iteration 11100 -> loss: 0.09283547035554597\n",
      "    At iteration 11200 -> loss: 0.09283675120306041\n",
      "    At iteration 11300 -> loss: 0.09289319642812183\n",
      "    At iteration 11400 -> loss: 0.0928637853700947\n",
      "    At iteration 11500 -> loss: 0.09283076213281666\n",
      "    At iteration 11600 -> loss: 0.09284942249683636\n",
      "    At iteration 11700 -> loss: 0.0928548370436371\n",
      "    At iteration 11800 -> loss: 0.09283379493249055\n",
      "    At iteration 11900 -> loss: 0.0928545656616339\n",
      "    At iteration 12000 -> loss: 0.09282638796166674\n",
      "    At iteration 12100 -> loss: 0.09279962566965488\n",
      "    At iteration 12200 -> loss: 0.09280179426407968\n",
      "    At iteration 12300 -> loss: 0.09280695534972014\n",
      "    At iteration 12400 -> loss: 0.09285179251915364\n",
      "    At iteration 12500 -> loss: 0.09282617024046687\n",
      "    At iteration 12600 -> loss: 0.09287765439882924\n",
      "    At iteration 12700 -> loss: 0.09290020067906814\n",
      "    At iteration 12800 -> loss: 0.09297850968994294\n",
      "    At iteration 12900 -> loss: 0.09297206852320684\n",
      "    At iteration 13000 -> loss: 0.09296833054281253\n",
      "    At iteration 13100 -> loss: 0.09296085769622654\n",
      "    At iteration 13200 -> loss: 0.09295304307995257\n",
      "    At iteration 13300 -> loss: 0.09294505370102325\n",
      "    At iteration 13400 -> loss: 0.09293976240973463\n",
      "    At iteration 13500 -> loss: 0.09296471579554513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 13600 -> loss: 0.0930821601325508\n",
      "Staring Epoch 27\n",
      "    At iteration 0 -> loss: 0.08885245420970023\n",
      "    At iteration 100 -> loss: 0.0909985354362477\n",
      "    At iteration 200 -> loss: 0.09252106144449204\n",
      "    At iteration 300 -> loss: 0.09196401300983315\n",
      "    At iteration 400 -> loss: 0.09212610890524972\n",
      "    At iteration 500 -> loss: 0.09201030582160515\n",
      "    At iteration 600 -> loss: 0.09273719219939361\n",
      "    At iteration 700 -> loss: 0.09333757385732518\n",
      "    At iteration 800 -> loss: 0.09297746788570684\n",
      "    At iteration 900 -> loss: 0.09276806826648036\n",
      "    At iteration 1000 -> loss: 0.09246474384440324\n",
      "    At iteration 1100 -> loss: 0.09252249375183853\n",
      "    At iteration 1200 -> loss: 0.09240726809145218\n",
      "    At iteration 1300 -> loss: 0.09228205656626318\n",
      "    At iteration 1400 -> loss: 0.09277639445556192\n",
      "    At iteration 1500 -> loss: 0.09295798384153284\n",
      "    At iteration 1600 -> loss: 0.09307525999208842\n",
      "    At iteration 1700 -> loss: 0.09302906433376097\n",
      "    At iteration 1800 -> loss: 0.09279883966310067\n",
      "    At iteration 1900 -> loss: 0.09293847639128648\n",
      "    At iteration 2000 -> loss: 0.0929837467522621\n",
      "    At iteration 2100 -> loss: 0.09303821147766039\n",
      "    At iteration 2200 -> loss: 0.0929417148329063\n",
      "    At iteration 2300 -> loss: 0.09283571728872113\n",
      "    At iteration 2400 -> loss: 0.09300642180637486\n",
      "    At iteration 2500 -> loss: 0.09302363866292811\n",
      "    At iteration 2600 -> loss: 0.09324217649910889\n",
      "    At iteration 2700 -> loss: 0.0931669777116594\n",
      "    At iteration 2800 -> loss: 0.09306848851701575\n",
      "    At iteration 2900 -> loss: 0.09321742981485806\n",
      "    At iteration 3000 -> loss: 0.09325581789940894\n",
      "    At iteration 3100 -> loss: 0.09318563865948683\n",
      "    At iteration 3200 -> loss: 0.09316397311863507\n",
      "    At iteration 3300 -> loss: 0.09306524812430451\n",
      "    At iteration 3400 -> loss: 0.09427735706500547\n",
      "    At iteration 3500 -> loss: 0.09417280683697017\n",
      "    At iteration 3600 -> loss: 0.09408590707219937\n",
      "    At iteration 3700 -> loss: 0.09414668723544488\n",
      "    At iteration 3800 -> loss: 0.0940505068310026\n",
      "    At iteration 3900 -> loss: 0.0942655836854651\n",
      "    At iteration 4000 -> loss: 0.09414703056644526\n",
      "    At iteration 4100 -> loss: 0.09410072271795868\n",
      "    At iteration 4200 -> loss: 0.09412859190509087\n",
      "    At iteration 4300 -> loss: 0.09412783189335211\n",
      "    At iteration 4400 -> loss: 0.09409225519438778\n",
      "    At iteration 4500 -> loss: 0.09409771761750062\n",
      "    At iteration 4600 -> loss: 0.09419538211090056\n",
      "    At iteration 4700 -> loss: 0.09415164148477613\n",
      "    At iteration 4800 -> loss: 0.09413738849838098\n",
      "    At iteration 4900 -> loss: 0.09413193480656513\n",
      "    At iteration 5000 -> loss: 0.09405254371282021\n",
      "    At iteration 5100 -> loss: 0.09399559155931166\n",
      "    At iteration 5200 -> loss: 0.0939283976285658\n",
      "    At iteration 5300 -> loss: 0.09385559964975342\n",
      "    At iteration 5400 -> loss: 0.09379668925104895\n",
      "    At iteration 5500 -> loss: 0.09372551239673212\n",
      "    At iteration 5600 -> loss: 0.09375656955579438\n",
      "    At iteration 5700 -> loss: 0.09370258477728553\n",
      "    At iteration 5800 -> loss: 0.09370039072484086\n",
      "    At iteration 5900 -> loss: 0.09365666406960471\n",
      "    At iteration 6000 -> loss: 0.09360769989610879\n",
      "    At iteration 6100 -> loss: 0.09354538656300895\n",
      "    At iteration 6200 -> loss: 0.09350809001424154\n",
      "    At iteration 6300 -> loss: 0.09343719432084993\n",
      "    At iteration 6400 -> loss: 0.09359449053221479\n",
      "    At iteration 6500 -> loss: 0.0935208773204252\n",
      "    At iteration 6600 -> loss: 0.09352057875685281\n",
      "    At iteration 6700 -> loss: 0.09375702099736408\n",
      "    At iteration 6800 -> loss: 0.09369816815637819\n",
      "    At iteration 6900 -> loss: 0.09363018160321679\n",
      "    At iteration 7000 -> loss: 0.09358414505392759\n",
      "    At iteration 7100 -> loss: 0.09362722992370553\n",
      "    At iteration 7200 -> loss: 0.09359129012693951\n",
      "    At iteration 7300 -> loss: 0.09355864623325338\n",
      "    At iteration 7400 -> loss: 0.09351248177275939\n",
      "    At iteration 7500 -> loss: 0.09348818546393643\n",
      "    At iteration 7600 -> loss: 0.09363531862031706\n",
      "    At iteration 7700 -> loss: 0.09359972270127459\n",
      "    At iteration 7800 -> loss: 0.093553562271311\n",
      "    At iteration 7900 -> loss: 0.09352130655104389\n",
      "    At iteration 8000 -> loss: 0.0934884997147939\n",
      "    At iteration 8100 -> loss: 0.09352313838308997\n",
      "    At iteration 8200 -> loss: 0.09347689054239534\n",
      "    At iteration 8300 -> loss: 0.09355143357516404\n",
      "    At iteration 8400 -> loss: 0.09356879913462991\n",
      "    At iteration 8500 -> loss: 0.09352261448234049\n",
      "    At iteration 8600 -> loss: 0.09347508810859086\n",
      "    At iteration 8700 -> loss: 0.093471559773717\n",
      "    At iteration 8800 -> loss: 0.093416299845883\n",
      "    At iteration 8900 -> loss: 0.0933710521019414\n",
      "    At iteration 9000 -> loss: 0.09337046142407132\n",
      "    At iteration 9100 -> loss: 0.09335739157579998\n",
      "    At iteration 9200 -> loss: 0.09331873977058897\n",
      "    At iteration 9300 -> loss: 0.09332973490389807\n",
      "    At iteration 9400 -> loss: 0.09332792584482728\n",
      "    At iteration 9500 -> loss: 0.09328717546140243\n",
      "    At iteration 9600 -> loss: 0.09323147001216926\n",
      "    At iteration 9700 -> loss: 0.09318239094164742\n",
      "    At iteration 9800 -> loss: 0.09316023656617617\n",
      "    At iteration 9900 -> loss: 0.09319063941818956\n",
      "    At iteration 10000 -> loss: 0.09316290316022885\n",
      "    At iteration 10100 -> loss: 0.09314033343839054\n",
      "    At iteration 10200 -> loss: 0.09316776801772209\n",
      "    At iteration 10300 -> loss: 0.09313762624461283\n",
      "    At iteration 10400 -> loss: 0.0931689565378223\n",
      "    At iteration 10500 -> loss: 0.0931547965922986\n",
      "    At iteration 10600 -> loss: 0.09323046669387927\n",
      "    At iteration 10700 -> loss: 0.09322367629817814\n",
      "    At iteration 10800 -> loss: 0.09321777533914291\n",
      "    At iteration 10900 -> loss: 0.09322538612156277\n",
      "    At iteration 11000 -> loss: 0.09322767936742841\n",
      "    At iteration 11100 -> loss: 0.09320253549550912\n",
      "    At iteration 11200 -> loss: 0.09323767315156012\n",
      "    At iteration 11300 -> loss: 0.0932073055163771\n",
      "    At iteration 11400 -> loss: 0.09319461470825838\n",
      "    At iteration 11500 -> loss: 0.09317463390333072\n",
      "    At iteration 11600 -> loss: 0.0931602384307768\n",
      "    At iteration 11700 -> loss: 0.09314061859917656\n",
      "    At iteration 11800 -> loss: 0.09314599194146918\n",
      "    At iteration 11900 -> loss: 0.09315346490967932\n",
      "    At iteration 12000 -> loss: 0.09315764041581347\n",
      "    At iteration 12100 -> loss: 0.09314752065226711\n",
      "    At iteration 12200 -> loss: 0.09312336384247463\n",
      "    At iteration 12300 -> loss: 0.09318819497460758\n",
      "    At iteration 12400 -> loss: 0.09316773301287089\n",
      "    At iteration 12500 -> loss: 0.0932333912480715\n",
      "    At iteration 12600 -> loss: 0.09321806770375732\n",
      "    At iteration 12700 -> loss: 0.09319358540101005\n",
      "    At iteration 12800 -> loss: 0.09321123643384102\n",
      "    At iteration 12900 -> loss: 0.09321867049165011\n",
      "    At iteration 13000 -> loss: 0.0931856836251244\n",
      "    At iteration 13100 -> loss: 0.09317012195361132\n",
      "    At iteration 13200 -> loss: 0.0931631990411959\n",
      "    At iteration 13300 -> loss: 0.0931339997651435\n",
      "    At iteration 13400 -> loss: 0.09313283961205147\n",
      "    At iteration 13500 -> loss: 0.09312765551772162\n",
      "    At iteration 13600 -> loss: 0.0931191559260398\n",
      "Staring Epoch 28\n",
      "    At iteration 0 -> loss: 0.18983832556114066\n",
      "    At iteration 100 -> loss: 0.09554896387407251\n",
      "    At iteration 200 -> loss: 0.09508582304392983\n",
      "    At iteration 300 -> loss: 0.0981646600790683\n",
      "    At iteration 400 -> loss: 0.0971074991731789\n",
      "    At iteration 500 -> loss: 0.09725839315163143\n",
      "    At iteration 600 -> loss: 0.09775657383652316\n",
      "    At iteration 700 -> loss: 0.09653838437084838\n",
      "    At iteration 800 -> loss: 0.09560682111674229\n",
      "    At iteration 900 -> loss: 0.09552772932836806\n",
      "    At iteration 1000 -> loss: 0.09505445633553583\n",
      "    At iteration 1100 -> loss: 0.09452214758168387\n",
      "    At iteration 1200 -> loss: 0.09785783320226767\n",
      "    At iteration 1300 -> loss: 0.09725831954753422\n",
      "    At iteration 1400 -> loss: 0.09663589706281836\n",
      "    At iteration 1500 -> loss: 0.09635288391563628\n",
      "    At iteration 1600 -> loss: 0.0959996547616572\n",
      "    At iteration 1700 -> loss: 0.09591414706722802\n",
      "    At iteration 1800 -> loss: 0.09570434737597087\n",
      "    At iteration 1900 -> loss: 0.09537073195323251\n",
      "    At iteration 2000 -> loss: 0.09537251721818853\n",
      "    At iteration 2100 -> loss: 0.09519355942420785\n",
      "    At iteration 2200 -> loss: 0.09507981018551337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2300 -> loss: 0.09532179941711054\n",
      "    At iteration 2400 -> loss: 0.09508537118624011\n",
      "    At iteration 2500 -> loss: 0.09487733543961582\n",
      "    At iteration 2600 -> loss: 0.0947768878592959\n",
      "    At iteration 2700 -> loss: 0.094618388627199\n",
      "    At iteration 2800 -> loss: 0.09475616448613786\n",
      "    At iteration 2900 -> loss: 0.09476519968565815\n",
      "    At iteration 3000 -> loss: 0.09471496950996038\n",
      "    At iteration 3100 -> loss: 0.0946811134208466\n",
      "    At iteration 3200 -> loss: 0.09452756066874243\n",
      "    At iteration 3300 -> loss: 0.09443970571781368\n",
      "    At iteration 3400 -> loss: 0.09433952030423036\n",
      "    At iteration 3500 -> loss: 0.09423467172373147\n",
      "    At iteration 3600 -> loss: 0.09413019411493351\n",
      "    At iteration 3700 -> loss: 0.09405416512149604\n",
      "    At iteration 3800 -> loss: 0.09395905072890107\n",
      "    At iteration 3900 -> loss: 0.09394814079379767\n",
      "    At iteration 4000 -> loss: 0.09384098853673152\n",
      "    At iteration 4100 -> loss: 0.09384240868475534\n",
      "    At iteration 4200 -> loss: 0.09375245837726336\n",
      "    At iteration 4300 -> loss: 0.09367172251744947\n",
      "    At iteration 4400 -> loss: 0.09363056064939242\n",
      "    At iteration 4500 -> loss: 0.09360373749985385\n",
      "    At iteration 4600 -> loss: 0.0935130712377612\n",
      "    At iteration 4700 -> loss: 0.09349481950178876\n",
      "    At iteration 4800 -> loss: 0.09342440455001531\n",
      "    At iteration 4900 -> loss: 0.09345182486022897\n",
      "    At iteration 5000 -> loss: 0.09351869192740479\n",
      "    At iteration 5100 -> loss: 0.09375231454666029\n",
      "    At iteration 5200 -> loss: 0.09375096060279804\n",
      "    At iteration 5300 -> loss: 0.0937719842298409\n",
      "    At iteration 5400 -> loss: 0.0937145254466297\n",
      "    At iteration 5500 -> loss: 0.09370422054744754\n",
      "    At iteration 5600 -> loss: 0.09365700855073612\n",
      "    At iteration 5700 -> loss: 0.09364725381522218\n",
      "    At iteration 5800 -> loss: 0.09359677411433259\n",
      "    At iteration 5900 -> loss: 0.09363478418360995\n",
      "    At iteration 6000 -> loss: 0.09360124032406526\n",
      "    At iteration 6100 -> loss: 0.09357366647087402\n",
      "    At iteration 6200 -> loss: 0.09364480643905596\n",
      "    At iteration 6300 -> loss: 0.09363637582146107\n",
      "    At iteration 6400 -> loss: 0.09356927513136824\n",
      "    At iteration 6500 -> loss: 0.09349238324337161\n",
      "    At iteration 6600 -> loss: 0.09345575679623544\n",
      "    At iteration 6700 -> loss: 0.09353351930057609\n",
      "    At iteration 6800 -> loss: 0.09357318618653426\n",
      "    At iteration 6900 -> loss: 0.09353171096455053\n",
      "    At iteration 7000 -> loss: 0.09349523680217887\n",
      "    At iteration 7100 -> loss: 0.09348873198227704\n",
      "    At iteration 7200 -> loss: 0.09345324834377162\n",
      "    At iteration 7300 -> loss: 0.09341197587287708\n",
      "    At iteration 7400 -> loss: 0.09337366827587355\n",
      "    At iteration 7500 -> loss: 0.09334528134053571\n",
      "    At iteration 7600 -> loss: 0.09329135805296222\n",
      "    At iteration 7700 -> loss: 0.09328021296683357\n",
      "    At iteration 7800 -> loss: 0.0933746640500956\n",
      "    At iteration 7900 -> loss: 0.09343238806355092\n",
      "    At iteration 8000 -> loss: 0.09342215361493125\n",
      "    At iteration 8100 -> loss: 0.0934121652508061\n",
      "    At iteration 8200 -> loss: 0.09336368428454447\n",
      "    At iteration 8300 -> loss: 0.09336934643299138\n",
      "    At iteration 8400 -> loss: 0.09340243870561166\n",
      "    At iteration 8500 -> loss: 0.0934556638304806\n",
      "    At iteration 8600 -> loss: 0.09352545244156556\n",
      "    At iteration 8700 -> loss: 0.09352573489573268\n",
      "    At iteration 8800 -> loss: 0.09360311525275643\n",
      "    At iteration 8900 -> loss: 0.09358942384848198\n",
      "    At iteration 9000 -> loss: 0.09353878805358262\n",
      "    At iteration 9100 -> loss: 0.09353694008545221\n",
      "    At iteration 9200 -> loss: 0.09354662777097267\n",
      "    At iteration 9300 -> loss: 0.09351362894969302\n",
      "    At iteration 9400 -> loss: 0.09347242677437348\n",
      "    At iteration 9500 -> loss: 0.09343736007846727\n",
      "    At iteration 9600 -> loss: 0.09344980197794857\n",
      "    At iteration 9700 -> loss: 0.09340184986374406\n",
      "    At iteration 9800 -> loss: 0.09336778978692341\n",
      "    At iteration 9900 -> loss: 0.09338535286837166\n",
      "    At iteration 10000 -> loss: 0.09342214218192504\n",
      "    At iteration 10100 -> loss: 0.09341736051437051\n",
      "    At iteration 10200 -> loss: 0.09344214547570512\n",
      "    At iteration 10300 -> loss: 0.09347172637658532\n",
      "    At iteration 10400 -> loss: 0.09344273802995344\n",
      "    At iteration 10500 -> loss: 0.0934803790522495\n",
      "    At iteration 10600 -> loss: 0.0934467178175022\n",
      "    At iteration 10700 -> loss: 0.09344007172818068\n",
      "    At iteration 10800 -> loss: 0.09341917339676055\n",
      "    At iteration 10900 -> loss: 0.09339985506680419\n",
      "    At iteration 11000 -> loss: 0.09338008175332654\n",
      "    At iteration 11100 -> loss: 0.0933741734646361\n",
      "    At iteration 11200 -> loss: 0.09336320190165831\n",
      "    At iteration 11300 -> loss: 0.09338190525563651\n",
      "    At iteration 11400 -> loss: 0.09335300952221429\n",
      "    At iteration 11500 -> loss: 0.09335072674405039\n",
      "    At iteration 11600 -> loss: 0.09334299395490876\n",
      "    At iteration 11700 -> loss: 0.09332302125492432\n",
      "    At iteration 11800 -> loss: 0.09329872292523898\n",
      "    At iteration 11900 -> loss: 0.09325996637438515\n",
      "    At iteration 12000 -> loss: 0.09326351363387539\n",
      "    At iteration 12100 -> loss: 0.09324392965014486\n",
      "    At iteration 12200 -> loss: 0.09321683517770146\n",
      "    At iteration 12300 -> loss: 0.09319021546739231\n",
      "    At iteration 12400 -> loss: 0.09320894244802606\n",
      "    At iteration 12500 -> loss: 0.09321072943869332\n",
      "    At iteration 12600 -> loss: 0.0931832351679184\n",
      "    At iteration 12700 -> loss: 0.09316930865520215\n",
      "    At iteration 12800 -> loss: 0.09316643322690946\n",
      "    At iteration 12900 -> loss: 0.0931641605664968\n",
      "    At iteration 13000 -> loss: 0.09317006762679685\n",
      "    At iteration 13100 -> loss: 0.09318593451748787\n",
      "    At iteration 13200 -> loss: 0.09317182678207529\n",
      "    At iteration 13300 -> loss: 0.09315454791288856\n",
      "    At iteration 13400 -> loss: 0.09314696959389022\n",
      "    At iteration 13500 -> loss: 0.09316649009546764\n",
      "    At iteration 13600 -> loss: 0.09314932963504657\n",
      "Staring Epoch 29\n",
      "    At iteration 0 -> loss: 0.08794224727898836\n",
      "    At iteration 100 -> loss: 0.08923751196823256\n",
      "    At iteration 200 -> loss: 0.09584329368813191\n",
      "    At iteration 300 -> loss: 0.09541107793420883\n",
      "    At iteration 400 -> loss: 0.09533399101398246\n",
      "    At iteration 500 -> loss: 0.09516848685052254\n",
      "    At iteration 600 -> loss: 0.09446234811032626\n",
      "    At iteration 700 -> loss: 0.09516464892722397\n",
      "    At iteration 800 -> loss: 0.09600742682508488\n",
      "    At iteration 900 -> loss: 0.09545521409357902\n",
      "    At iteration 1000 -> loss: 0.0951404134226841\n",
      "    At iteration 1100 -> loss: 0.09470729561938765\n",
      "    At iteration 1200 -> loss: 0.09470593057475471\n",
      "    At iteration 1300 -> loss: 0.09454831376267779\n",
      "    At iteration 1400 -> loss: 0.09424228818460832\n",
      "    At iteration 1500 -> loss: 0.09480228238206552\n",
      "    At iteration 1600 -> loss: 0.09450965009641311\n",
      "    At iteration 1700 -> loss: 0.09433566541448707\n",
      "    At iteration 1800 -> loss: 0.0946010570102698\n",
      "    At iteration 1900 -> loss: 0.09434581376914702\n",
      "    At iteration 2000 -> loss: 0.09416052363249061\n",
      "    At iteration 2100 -> loss: 0.0942873104664293\n",
      "    At iteration 2200 -> loss: 0.09411504996056765\n",
      "    At iteration 2300 -> loss: 0.09402966823979077\n",
      "    At iteration 2400 -> loss: 0.09404842660744604\n",
      "    At iteration 2500 -> loss: 0.09411090380731257\n",
      "    At iteration 2600 -> loss: 0.09421094666631967\n",
      "    At iteration 2700 -> loss: 0.09408732292220148\n",
      "    At iteration 2800 -> loss: 0.09405362809433998\n",
      "    At iteration 2900 -> loss: 0.09391329095593866\n",
      "    At iteration 3000 -> loss: 0.09397700488998112\n",
      "    At iteration 3100 -> loss: 0.0939539845944805\n",
      "    At iteration 3200 -> loss: 0.0938352172730861\n",
      "    At iteration 3300 -> loss: 0.09380327376944626\n",
      "    At iteration 3400 -> loss: 0.09391460688751874\n",
      "    At iteration 3500 -> loss: 0.09380115029013793\n",
      "    At iteration 3600 -> loss: 0.09373121440711833\n",
      "    At iteration 3700 -> loss: 0.09361987051726596\n",
      "    At iteration 3800 -> loss: 0.09359787578451996\n",
      "    At iteration 3900 -> loss: 0.0936374742749837\n",
      "    At iteration 4000 -> loss: 0.09371019403612058\n",
      "    At iteration 4100 -> loss: 0.09363867452766861\n",
      "    At iteration 4200 -> loss: 0.09364107737623564\n",
      "    At iteration 4300 -> loss: 0.09365467906312121\n",
      "    At iteration 4400 -> loss: 0.0935674486592798\n",
      "    At iteration 4500 -> loss: 0.09362716090695586\n",
      "    At iteration 4600 -> loss: 0.09370028622160502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4700 -> loss: 0.09361611833687115\n",
      "    At iteration 4800 -> loss: 0.09355669590701085\n",
      "    At iteration 4900 -> loss: 0.09355015042928755\n",
      "    At iteration 5000 -> loss: 0.09345693463115236\n",
      "    At iteration 5100 -> loss: 0.09340597274195855\n",
      "    At iteration 5200 -> loss: 0.09334996740820556\n",
      "    At iteration 5300 -> loss: 0.09329691128745711\n",
      "    At iteration 5400 -> loss: 0.09323161560304467\n",
      "    At iteration 5500 -> loss: 0.09324685711557816\n",
      "    At iteration 5600 -> loss: 0.09327894265529295\n",
      "    At iteration 5700 -> loss: 0.09325778576141898\n",
      "    At iteration 5800 -> loss: 0.0932135282960051\n",
      "    At iteration 5900 -> loss: 0.09327816069450144\n",
      "    At iteration 6000 -> loss: 0.09343788452651541\n",
      "    At iteration 6100 -> loss: 0.09338193536889305\n",
      "    At iteration 6200 -> loss: 0.09332361232738848\n",
      "    At iteration 6300 -> loss: 0.09327139452740985\n",
      "    At iteration 6400 -> loss: 0.09330064319092057\n",
      "    At iteration 6500 -> loss: 0.09331850799761422\n",
      "    At iteration 6600 -> loss: 0.09335299471764905\n",
      "    At iteration 6700 -> loss: 0.09331948677840729\n",
      "    At iteration 6800 -> loss: 0.09329998960897125\n",
      "    At iteration 6900 -> loss: 0.09327796708090912\n",
      "    At iteration 7000 -> loss: 0.09326943753204117\n",
      "    At iteration 7100 -> loss: 0.0932266674377965\n",
      "    At iteration 7200 -> loss: 0.09322488850969951\n",
      "    At iteration 7300 -> loss: 0.09321199332082593\n",
      "    At iteration 7400 -> loss: 0.09324623753539614\n",
      "    At iteration 7500 -> loss: 0.0932262356927875\n",
      "    At iteration 7600 -> loss: 0.09322324064831118\n",
      "    At iteration 7700 -> loss: 0.09320421146064915\n",
      "    At iteration 7800 -> loss: 0.09317739410081168\n",
      "    At iteration 7900 -> loss: 0.09316597588853931\n",
      "    At iteration 8000 -> loss: 0.09314162884522757\n",
      "    At iteration 8100 -> loss: 0.09313667043650607\n",
      "    At iteration 8200 -> loss: 0.09313556488376383\n",
      "    At iteration 8300 -> loss: 0.09315238780606117\n",
      "    At iteration 8400 -> loss: 0.09316606955869146\n",
      "    At iteration 8500 -> loss: 0.0932357691627229\n",
      "    At iteration 8600 -> loss: 0.09327275054141458\n",
      "    At iteration 8700 -> loss: 0.09325908268246785\n",
      "    At iteration 8800 -> loss: 0.0932922196323796\n",
      "    At iteration 8900 -> loss: 0.09336102359679663\n",
      "    At iteration 9000 -> loss: 0.09333400969317715\n",
      "    At iteration 9100 -> loss: 0.0932972295758161\n",
      "    At iteration 9200 -> loss: 0.09327091538813784\n",
      "    At iteration 9300 -> loss: 0.09325972817195805\n",
      "    At iteration 9400 -> loss: 0.09329149392004327\n",
      "    At iteration 9500 -> loss: 0.09328995052505726\n",
      "    At iteration 9600 -> loss: 0.09326224287212775\n",
      "    At iteration 9700 -> loss: 0.09329390760549447\n",
      "    At iteration 9800 -> loss: 0.09325964216205748\n",
      "    At iteration 9900 -> loss: 0.09327694031462629\n",
      "    At iteration 10000 -> loss: 0.09323878289804909\n",
      "    At iteration 10100 -> loss: 0.09323185987104685\n",
      "    At iteration 10200 -> loss: 0.09319428089370871\n",
      "    At iteration 10300 -> loss: 0.09319631948973453\n",
      "    At iteration 10400 -> loss: 0.09318011506732556\n",
      "    At iteration 10500 -> loss: 0.09317578124348719\n",
      "    At iteration 10600 -> loss: 0.09315077501512542\n",
      "    At iteration 10700 -> loss: 0.09310786399152107\n",
      "    At iteration 10800 -> loss: 0.09310040510785089\n",
      "    At iteration 10900 -> loss: 0.09308074509272338\n",
      "    At iteration 11000 -> loss: 0.09308654592200084\n",
      "    At iteration 11100 -> loss: 0.09311059756448105\n",
      "    At iteration 11200 -> loss: 0.09307235991359013\n",
      "    At iteration 11300 -> loss: 0.09307921767841176\n",
      "    At iteration 11400 -> loss: 0.09314469903420193\n",
      "    At iteration 11500 -> loss: 0.09314775436183952\n",
      "    At iteration 11600 -> loss: 0.09319795036162347\n",
      "    At iteration 11700 -> loss: 0.09317461693807223\n",
      "    At iteration 11800 -> loss: 0.09315841784639504\n",
      "    At iteration 11900 -> loss: 0.09313143444406413\n",
      "    At iteration 12000 -> loss: 0.0931110317131329\n",
      "    At iteration 12100 -> loss: 0.09311059457779904\n",
      "    At iteration 12200 -> loss: 0.09310324101402424\n",
      "    At iteration 12300 -> loss: 0.09308025919035424\n",
      "    At iteration 12400 -> loss: 0.09308551738267684\n",
      "    At iteration 12500 -> loss: 0.09307059216889602\n",
      "    At iteration 12600 -> loss: 0.09304153801245368\n",
      "    At iteration 12700 -> loss: 0.0932854692619445\n",
      "    At iteration 12800 -> loss: 0.09327156328653616\n",
      "    At iteration 12900 -> loss: 0.09324251236047026\n",
      "    At iteration 13000 -> loss: 0.09322374208857517\n",
      "    At iteration 13100 -> loss: 0.09321815457637636\n",
      "    At iteration 13200 -> loss: 0.09320674544260228\n",
      "    At iteration 13300 -> loss: 0.09318651429645326\n",
      "    At iteration 13400 -> loss: 0.09319394238457132\n",
      "    At iteration 13500 -> loss: 0.09318220985595396\n",
      "    At iteration 13600 -> loss: 0.09316393299165851\n",
      "Staring Epoch 30\n",
      "    At iteration 0 -> loss: 0.08632979611866176\n",
      "    At iteration 100 -> loss: 0.0969083614438136\n",
      "    At iteration 200 -> loss: 0.09261526200418559\n",
      "    At iteration 300 -> loss: 0.09240486758999944\n",
      "    At iteration 400 -> loss: 0.09254245920124515\n",
      "    At iteration 500 -> loss: 0.09187515936614968\n",
      "    At iteration 600 -> loss: 0.09129409458991201\n",
      "    At iteration 700 -> loss: 0.09134761979191791\n",
      "    At iteration 800 -> loss: 0.091948463319562\n",
      "    At iteration 900 -> loss: 0.09149336439208723\n",
      "    At iteration 1000 -> loss: 0.09145623953854583\n",
      "    At iteration 1100 -> loss: 0.09135817530592691\n",
      "    At iteration 1200 -> loss: 0.09165172982377166\n",
      "    At iteration 1300 -> loss: 0.09148244195328736\n",
      "    At iteration 1400 -> loss: 0.09151470132050839\n",
      "    At iteration 1500 -> loss: 0.09173376299907217\n",
      "    At iteration 1600 -> loss: 0.09159774662000267\n",
      "    At iteration 1700 -> loss: 0.09164291202180637\n",
      "    At iteration 1800 -> loss: 0.09169562924351911\n",
      "    At iteration 1900 -> loss: 0.09160727219665227\n",
      "    At iteration 2000 -> loss: 0.09147963946455934\n",
      "    At iteration 2100 -> loss: 0.09152586391716326\n",
      "    At iteration 2200 -> loss: 0.09171264205222329\n",
      "    At iteration 2300 -> loss: 0.09185164703688183\n",
      "    At iteration 2400 -> loss: 0.09244126260722586\n",
      "    At iteration 2500 -> loss: 0.09275509337613627\n",
      "    At iteration 2600 -> loss: 0.09281042531062014\n",
      "    At iteration 2700 -> loss: 0.09273313114734874\n",
      "    At iteration 2800 -> loss: 0.0926472746425547\n",
      "    At iteration 2900 -> loss: 0.09300883719045847\n",
      "    At iteration 3000 -> loss: 0.0929647196719919\n",
      "    At iteration 3100 -> loss: 0.09292168705841299\n",
      "    At iteration 3200 -> loss: 0.09297744118211537\n",
      "    At iteration 3300 -> loss: 0.09288470909303188\n",
      "    At iteration 3400 -> loss: 0.09288475744971651\n",
      "    At iteration 3500 -> loss: 0.09282994603039692\n",
      "    At iteration 3600 -> loss: 0.09287425252370812\n",
      "    At iteration 3700 -> loss: 0.09301759179746545\n",
      "    At iteration 3800 -> loss: 0.09290508695450536\n",
      "    At iteration 3900 -> loss: 0.09286452875890364\n",
      "    At iteration 4000 -> loss: 0.09294106158345077\n",
      "    At iteration 4100 -> loss: 0.09290723221056926\n",
      "    At iteration 4200 -> loss: 0.09296928316566098\n",
      "    At iteration 4300 -> loss: 0.09294960623199613\n",
      "    At iteration 4400 -> loss: 0.09293190941371225\n",
      "    At iteration 4500 -> loss: 0.09289899421846766\n",
      "    At iteration 4600 -> loss: 0.09285612756391858\n",
      "    At iteration 4700 -> loss: 0.09282026970320073\n",
      "    At iteration 4800 -> loss: 0.09289733323737266\n",
      "    At iteration 4900 -> loss: 0.09289908611232524\n",
      "    At iteration 5000 -> loss: 0.09292097171709733\n",
      "    At iteration 5100 -> loss: 0.0929523580014805\n",
      "    At iteration 5200 -> loss: 0.09292206956491002\n",
      "    At iteration 5300 -> loss: 0.09297953418155086\n",
      "    At iteration 5400 -> loss: 0.09295158137680856\n",
      "    At iteration 5500 -> loss: 0.09296642219688415\n",
      "    At iteration 5600 -> loss: 0.09295742439488042\n",
      "    At iteration 5700 -> loss: 0.09291550641580056\n",
      "    At iteration 5800 -> loss: 0.09294217620679636\n",
      "    At iteration 5900 -> loss: 0.09287616753690402\n",
      "    At iteration 6000 -> loss: 0.09288782959190509\n",
      "    At iteration 6100 -> loss: 0.09285324376875656\n",
      "    At iteration 6200 -> loss: 0.09289568760343975\n",
      "    At iteration 6300 -> loss: 0.09299010175768474\n",
      "    At iteration 6400 -> loss: 0.09295819698562625\n",
      "    At iteration 6500 -> loss: 0.09300237481456582\n",
      "    At iteration 6600 -> loss: 0.09295797289507883\n",
      "    At iteration 6700 -> loss: 0.09294838145225372\n",
      "    At iteration 6800 -> loss: 0.09297427776829449\n",
      "    At iteration 6900 -> loss: 0.09293380516113063\n",
      "    At iteration 7000 -> loss: 0.09292067604406701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7100 -> loss: 0.09293103741585941\n",
      "    At iteration 7200 -> loss: 0.09290333896920835\n",
      "    At iteration 7300 -> loss: 0.09291177930410002\n",
      "    At iteration 7400 -> loss: 0.09286565374088884\n",
      "    At iteration 7500 -> loss: 0.09281202671935937\n",
      "    At iteration 7600 -> loss: 0.09284313225347961\n",
      "    At iteration 7700 -> loss: 0.0928766440945519\n",
      "    At iteration 7800 -> loss: 0.09284462040553794\n",
      "    At iteration 7900 -> loss: 0.09288765046523473\n",
      "    At iteration 8000 -> loss: 0.09286681843430555\n",
      "    At iteration 8100 -> loss: 0.09325608294939962\n",
      "    At iteration 8200 -> loss: 0.09319931557633843\n",
      "    At iteration 8300 -> loss: 0.09316059665261404\n",
      "    At iteration 8400 -> loss: 0.09311635365483333\n",
      "    At iteration 8500 -> loss: 0.09321753733190884\n",
      "    At iteration 8600 -> loss: 0.09319970194636899\n",
      "    At iteration 8700 -> loss: 0.0931819161335769\n",
      "    At iteration 8800 -> loss: 0.09317039638830274\n",
      "    At iteration 8900 -> loss: 0.0932752348540048\n",
      "    At iteration 9000 -> loss: 0.09327430251095806\n",
      "    At iteration 9100 -> loss: 0.09322646261556373\n",
      "    At iteration 9200 -> loss: 0.09321453038366155\n",
      "    At iteration 9300 -> loss: 0.09332235193033744\n",
      "    At iteration 9400 -> loss: 0.09329298060590803\n",
      "    At iteration 9500 -> loss: 0.09326904398252762\n",
      "    At iteration 9600 -> loss: 0.09324354171203444\n",
      "    At iteration 9700 -> loss: 0.09326390559479009\n",
      "    At iteration 9800 -> loss: 0.09323789819776465\n",
      "    At iteration 9900 -> loss: 0.09319822250718884\n",
      "    At iteration 10000 -> loss: 0.09319104361918402\n",
      "    At iteration 10100 -> loss: 0.09318368043990434\n",
      "    At iteration 10200 -> loss: 0.0931852573640249\n",
      "    At iteration 10300 -> loss: 0.0931974639082161\n",
      "    At iteration 10400 -> loss: 0.0931578805755028\n",
      "    At iteration 10500 -> loss: 0.09324870925466419\n",
      "    At iteration 10600 -> loss: 0.0932213956376884\n",
      "    At iteration 10700 -> loss: 0.09331096990259617\n",
      "    At iteration 10800 -> loss: 0.09329133415210902\n",
      "    At iteration 10900 -> loss: 0.09330121239592676\n",
      "    At iteration 11000 -> loss: 0.09329875151842713\n",
      "    At iteration 11100 -> loss: 0.09334407129371002\n",
      "    At iteration 11200 -> loss: 0.09332113055553401\n",
      "    At iteration 11300 -> loss: 0.0932868417248476\n",
      "    At iteration 11400 -> loss: 0.09331010907089832\n",
      "    At iteration 11500 -> loss: 0.09327691856727278\n",
      "    At iteration 11600 -> loss: 0.09327654741783745\n",
      "    At iteration 11700 -> loss: 0.09325046524458376\n",
      "    At iteration 11800 -> loss: 0.09321284908607476\n",
      "    At iteration 11900 -> loss: 0.09319252149941024\n",
      "    At iteration 12000 -> loss: 0.09315207027392318\n",
      "    At iteration 12100 -> loss: 0.09316587912665926\n",
      "    At iteration 12200 -> loss: 0.0931776419381903\n",
      "    At iteration 12300 -> loss: 0.09315662933748556\n",
      "    At iteration 12400 -> loss: 0.09315879060710834\n",
      "    At iteration 12500 -> loss: 0.09315079255624557\n",
      "    At iteration 12600 -> loss: 0.09313293341710056\n",
      "    At iteration 12700 -> loss: 0.09310911175102786\n",
      "    At iteration 12800 -> loss: 0.09315574965277913\n",
      "    At iteration 12900 -> loss: 0.09314093095562678\n",
      "    At iteration 13000 -> loss: 0.09311928295288371\n",
      "    At iteration 13100 -> loss: 0.09314642860184828\n",
      "    At iteration 13200 -> loss: 0.09315677165088965\n",
      "    At iteration 13300 -> loss: 0.09316446709889788\n",
      "    At iteration 13400 -> loss: 0.09313642311452193\n",
      "    At iteration 13500 -> loss: 0.093145882803955\n",
      "    At iteration 13600 -> loss: 0.0931373336397003\n",
      "Staring Epoch 31\n",
      "    At iteration 0 -> loss: 0.08008371491541766\n",
      "    At iteration 100 -> loss: 0.090891660900577\n",
      "    At iteration 200 -> loss: 0.094929432790941\n",
      "    At iteration 300 -> loss: 0.09359453224263166\n",
      "    At iteration 400 -> loss: 0.09498354578290027\n",
      "    At iteration 500 -> loss: 0.09464050199255708\n",
      "    At iteration 600 -> loss: 0.09979265887724162\n",
      "    At iteration 700 -> loss: 0.09841398404408937\n",
      "    At iteration 800 -> loss: 0.0974978808773842\n",
      "    At iteration 900 -> loss: 0.09690276020652329\n",
      "    At iteration 1000 -> loss: 0.09621223303526234\n",
      "    At iteration 1100 -> loss: 0.09554402893674778\n",
      "    At iteration 1200 -> loss: 0.09588801274123972\n",
      "    At iteration 1300 -> loss: 0.09598390702947225\n",
      "    At iteration 1400 -> loss: 0.09552768196467573\n",
      "    At iteration 1500 -> loss: 0.09519373452571628\n",
      "    At iteration 1600 -> loss: 0.09488936673914611\n",
      "    At iteration 1700 -> loss: 0.09478559523753227\n",
      "    At iteration 1800 -> loss: 0.09465825013900557\n",
      "    At iteration 1900 -> loss: 0.09443280969406256\n",
      "    At iteration 2000 -> loss: 0.09437775308006108\n",
      "    At iteration 2100 -> loss: 0.09427026533976358\n",
      "    At iteration 2200 -> loss: 0.09414701273082357\n",
      "    At iteration 2300 -> loss: 0.09427136605571607\n",
      "    At iteration 2400 -> loss: 0.09455165475008279\n",
      "    At iteration 2500 -> loss: 0.09453747943353676\n",
      "    At iteration 2600 -> loss: 0.09439589379995054\n",
      "    At iteration 2700 -> loss: 0.09437134029596435\n",
      "    At iteration 2800 -> loss: 0.09420352127637051\n",
      "    At iteration 2900 -> loss: 0.09420946380484498\n",
      "    At iteration 3000 -> loss: 0.09439076681322037\n",
      "    At iteration 3100 -> loss: 0.09427155625103997\n",
      "    At iteration 3200 -> loss: 0.09422435932771235\n",
      "    At iteration 3300 -> loss: 0.09410826443904491\n",
      "    At iteration 3400 -> loss: 0.09400028761308256\n",
      "    At iteration 3500 -> loss: 0.09393732459184266\n",
      "    At iteration 3600 -> loss: 0.09384759604626133\n",
      "    At iteration 3700 -> loss: 0.09377002672203046\n",
      "    At iteration 3800 -> loss: 0.09370018738763702\n",
      "    At iteration 3900 -> loss: 0.09361499169044872\n",
      "    At iteration 4000 -> loss: 0.09355105833026342\n",
      "    At iteration 4100 -> loss: 0.09352567715382866\n",
      "    At iteration 4200 -> loss: 0.09346412771894815\n",
      "    At iteration 4300 -> loss: 0.09340504463775977\n",
      "    At iteration 4400 -> loss: 0.09342040866374855\n",
      "    At iteration 4500 -> loss: 0.09350541185731268\n",
      "    At iteration 4600 -> loss: 0.09352918857282198\n",
      "    At iteration 4700 -> loss: 0.09351822940460976\n",
      "    At iteration 4800 -> loss: 0.09356900251232718\n",
      "    At iteration 4900 -> loss: 0.09354988557831907\n",
      "    At iteration 5000 -> loss: 0.0935807667246257\n",
      "    At iteration 5100 -> loss: 0.09352496248841291\n",
      "    At iteration 5200 -> loss: 0.09344782009755875\n",
      "    At iteration 5300 -> loss: 0.09350866679859823\n",
      "    At iteration 5400 -> loss: 0.09345056526920516\n",
      "    At iteration 5500 -> loss: 0.09340196099418872\n",
      "    At iteration 5600 -> loss: 0.09334045980730658\n",
      "    At iteration 5700 -> loss: 0.09333759897554085\n",
      "    At iteration 5800 -> loss: 0.09331644789847651\n",
      "    At iteration 5900 -> loss: 0.09325507105837402\n",
      "    At iteration 6000 -> loss: 0.09318769182372603\n",
      "    At iteration 6100 -> loss: 0.09325689550265756\n",
      "    At iteration 6200 -> loss: 0.09332085049255845\n",
      "    At iteration 6300 -> loss: 0.09326038930461643\n",
      "    At iteration 6400 -> loss: 0.09326022560552578\n",
      "    At iteration 6500 -> loss: 0.09323463595078299\n",
      "    At iteration 6600 -> loss: 0.09326086823393023\n",
      "    At iteration 6700 -> loss: 0.09326976605236109\n",
      "    At iteration 6800 -> loss: 0.09323706156098549\n",
      "    At iteration 6900 -> loss: 0.09323104015394756\n",
      "    At iteration 7000 -> loss: 0.09327210787097028\n",
      "    At iteration 7100 -> loss: 0.09321865310444113\n",
      "    At iteration 7200 -> loss: 0.09318843226736813\n",
      "    At iteration 7300 -> loss: 0.09318255611994561\n",
      "    At iteration 7400 -> loss: 0.09315162324517222\n",
      "    At iteration 7500 -> loss: 0.09315721907924968\n",
      "    At iteration 7600 -> loss: 0.0931972222894468\n",
      "    At iteration 7700 -> loss: 0.09322891715499237\n",
      "    At iteration 7800 -> loss: 0.09317981103452776\n",
      "    At iteration 7900 -> loss: 0.09315529727623728\n",
      "    At iteration 8000 -> loss: 0.09321955960883796\n",
      "    At iteration 8100 -> loss: 0.09322771160365027\n",
      "    At iteration 8200 -> loss: 0.09319965476046131\n",
      "    At iteration 8300 -> loss: 0.09325813067331072\n",
      "    At iteration 8400 -> loss: 0.09327766397431098\n",
      "    At iteration 8500 -> loss: 0.09325818580680256\n",
      "    At iteration 8600 -> loss: 0.09325016043844817\n",
      "    At iteration 8700 -> loss: 0.09334970935131068\n",
      "    At iteration 8800 -> loss: 0.09330014861011826\n",
      "    At iteration 8900 -> loss: 0.09325784151894889\n",
      "    At iteration 9000 -> loss: 0.0932089493515414\n",
      "    At iteration 9100 -> loss: 0.09317770802499556\n",
      "    At iteration 9200 -> loss: 0.09316922696133458\n",
      "    At iteration 9300 -> loss: 0.09314110003916663\n",
      "    At iteration 9400 -> loss: 0.09312059838214874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9500 -> loss: 0.09309336416475406\n",
      "    At iteration 9600 -> loss: 0.0931652119693178\n",
      "    At iteration 9700 -> loss: 0.09311420145094305\n",
      "    At iteration 9800 -> loss: 0.09307690602055635\n",
      "    At iteration 9900 -> loss: 0.09307397420945542\n",
      "    At iteration 10000 -> loss: 0.0930490965319061\n",
      "    At iteration 10100 -> loss: 0.09301717425276552\n",
      "    At iteration 10200 -> loss: 0.09299654177324564\n",
      "    At iteration 10300 -> loss: 0.09298632283202331\n",
      "    At iteration 10400 -> loss: 0.09301574915024402\n",
      "    At iteration 10500 -> loss: 0.09297946171543653\n",
      "    At iteration 10600 -> loss: 0.09298853864974048\n",
      "    At iteration 10700 -> loss: 0.09295896273644091\n",
      "    At iteration 10800 -> loss: 0.09292697762960404\n",
      "    At iteration 10900 -> loss: 0.09291219294458396\n",
      "    At iteration 11000 -> loss: 0.0929845665378665\n",
      "    At iteration 11100 -> loss: 0.09302206074635488\n",
      "    At iteration 11200 -> loss: 0.09304770011427291\n",
      "    At iteration 11300 -> loss: 0.09306820770612799\n",
      "    At iteration 11400 -> loss: 0.09314658748021358\n",
      "    At iteration 11500 -> loss: 0.09315288676547605\n",
      "    At iteration 11600 -> loss: 0.09312667419415596\n",
      "    At iteration 11700 -> loss: 0.09310851778309867\n",
      "    At iteration 11800 -> loss: 0.09310131750222497\n",
      "    At iteration 11900 -> loss: 0.09307504709416477\n",
      "    At iteration 12000 -> loss: 0.09305451104109792\n",
      "    At iteration 12100 -> loss: 0.09304114270967571\n",
      "    At iteration 12200 -> loss: 0.09308283447925712\n",
      "    At iteration 12300 -> loss: 0.09304927976959923\n",
      "    At iteration 12400 -> loss: 0.09307639941988015\n",
      "    At iteration 12500 -> loss: 0.09308010480112051\n",
      "    At iteration 12600 -> loss: 0.0931161136864194\n",
      "    At iteration 12700 -> loss: 0.09309849255312777\n",
      "    At iteration 12800 -> loss: 0.09307516822403439\n",
      "    At iteration 12900 -> loss: 0.09305215113185242\n",
      "    At iteration 13000 -> loss: 0.09305148634606938\n",
      "    At iteration 13100 -> loss: 0.09307239598030115\n",
      "    At iteration 13200 -> loss: 0.0930530038687608\n",
      "    At iteration 13300 -> loss: 0.09303852728468874\n",
      "    At iteration 13400 -> loss: 0.09304959856101896\n",
      "    At iteration 13500 -> loss: 0.0930494666073328\n",
      "    At iteration 13600 -> loss: 0.0930926432095139\n",
      "Staring Epoch 32\n",
      "    At iteration 0 -> loss: 0.08048137317382498\n",
      "    At iteration 100 -> loss: 0.09057779241148899\n",
      "    At iteration 200 -> loss: 0.09051192934734968\n",
      "    At iteration 300 -> loss: 0.09047889131591394\n",
      "    At iteration 400 -> loss: 0.09108910057319317\n",
      "    At iteration 500 -> loss: 0.09137560342337447\n",
      "    At iteration 600 -> loss: 0.091134150829577\n",
      "    At iteration 700 -> loss: 0.09112373924681419\n",
      "    At iteration 800 -> loss: 0.09113108677630131\n",
      "    At iteration 900 -> loss: 0.0910861769764191\n",
      "    At iteration 1000 -> loss: 0.09106714694800175\n",
      "    At iteration 1100 -> loss: 0.09100722547801146\n",
      "    At iteration 1200 -> loss: 0.09151345852628616\n",
      "    At iteration 1300 -> loss: 0.09177622246160283\n",
      "    At iteration 1400 -> loss: 0.0921495186579987\n",
      "    At iteration 1500 -> loss: 0.09258201828904024\n",
      "    At iteration 1600 -> loss: 0.09298176517315339\n",
      "    At iteration 1700 -> loss: 0.0928369206480397\n",
      "    At iteration 1800 -> loss: 0.09282951502824065\n",
      "    At iteration 1900 -> loss: 0.09264819510868236\n",
      "    At iteration 2000 -> loss: 0.09471995878066156\n",
      "    At iteration 2100 -> loss: 0.09449653063366667\n",
      "    At iteration 2200 -> loss: 0.0942889054421469\n",
      "    At iteration 2300 -> loss: 0.09421134468405394\n",
      "    At iteration 2400 -> loss: 0.09410049780238589\n",
      "    At iteration 2500 -> loss: 0.09391250710947853\n",
      "    At iteration 2600 -> loss: 0.09414104666545683\n",
      "    At iteration 2700 -> loss: 0.09411096421475837\n",
      "    At iteration 2800 -> loss: 0.09406747301247632\n",
      "    At iteration 2900 -> loss: 0.09390620503826079\n",
      "    At iteration 3000 -> loss: 0.09380917263900516\n",
      "    At iteration 3100 -> loss: 0.09369358531570755\n",
      "    At iteration 3200 -> loss: 0.09379529253314182\n",
      "    At iteration 3300 -> loss: 0.09388746297059031\n",
      "    At iteration 3400 -> loss: 0.09378214884607629\n",
      "    At iteration 3500 -> loss: 0.09380820482961334\n",
      "    At iteration 3600 -> loss: 0.09378351609895061\n",
      "    At iteration 3700 -> loss: 0.09370980615293444\n",
      "    At iteration 3800 -> loss: 0.09360223812329169\n",
      "    At iteration 3900 -> loss: 0.09358104044563029\n",
      "    At iteration 4000 -> loss: 0.09350635706711512\n",
      "    At iteration 4100 -> loss: 0.09346037541267527\n",
      "    At iteration 4200 -> loss: 0.09353264282529748\n",
      "    At iteration 4300 -> loss: 0.09339256811753434\n",
      "    At iteration 4400 -> loss: 0.09337421783407626\n",
      "    At iteration 4500 -> loss: 0.09341049337025582\n",
      "    At iteration 4600 -> loss: 0.09337219353286985\n",
      "    At iteration 4700 -> loss: 0.0933101796761852\n",
      "    At iteration 4800 -> loss: 0.09346684808980421\n",
      "    At iteration 4900 -> loss: 0.09338658698173614\n",
      "    At iteration 5000 -> loss: 0.09341335634626155\n",
      "    At iteration 5100 -> loss: 0.0933477723774147\n",
      "    At iteration 5200 -> loss: 0.09328101431673717\n",
      "    At iteration 5300 -> loss: 0.09338151884494036\n",
      "    At iteration 5400 -> loss: 0.09333174522714721\n",
      "    At iteration 5500 -> loss: 0.09328186409039634\n",
      "    At iteration 5600 -> loss: 0.09324667979045896\n",
      "    At iteration 5700 -> loss: 0.09323331124671894\n",
      "    At iteration 5800 -> loss: 0.0933103629677341\n",
      "    At iteration 5900 -> loss: 0.09336591186629846\n",
      "    At iteration 6000 -> loss: 0.09339496288631532\n",
      "    At iteration 6100 -> loss: 0.09344518345370559\n",
      "    At iteration 6200 -> loss: 0.09343676085293424\n",
      "    At iteration 6300 -> loss: 0.09341649266575462\n",
      "    At iteration 6400 -> loss: 0.09336939053204003\n",
      "    At iteration 6500 -> loss: 0.09338489211402662\n",
      "    At iteration 6600 -> loss: 0.09334144023877353\n",
      "    At iteration 6700 -> loss: 0.09330423036511616\n",
      "    At iteration 6800 -> loss: 0.0934237005419826\n",
      "    At iteration 6900 -> loss: 0.09344384007044154\n",
      "    At iteration 7000 -> loss: 0.09345363030546538\n",
      "    At iteration 7100 -> loss: 0.09340693638834752\n",
      "    At iteration 7200 -> loss: 0.09340216053398664\n",
      "    At iteration 7300 -> loss: 0.09343182815453463\n",
      "    At iteration 7400 -> loss: 0.09340883108838464\n",
      "    At iteration 7500 -> loss: 0.09339506661201037\n",
      "    At iteration 7600 -> loss: 0.09341639177304706\n",
      "    At iteration 7700 -> loss: 0.09339453230059655\n",
      "    At iteration 7800 -> loss: 0.09335539509295554\n",
      "    At iteration 7900 -> loss: 0.09335057832159128\n",
      "    At iteration 8000 -> loss: 0.09334534640332318\n",
      "    At iteration 8100 -> loss: 0.09337791683732827\n",
      "    At iteration 8200 -> loss: 0.09340720444204559\n",
      "    At iteration 8300 -> loss: 0.09349450440909955\n",
      "    At iteration 8400 -> loss: 0.09344877800079905\n",
      "    At iteration 8500 -> loss: 0.09345541013247784\n",
      "    At iteration 8600 -> loss: 0.09341618159272258\n",
      "    At iteration 8700 -> loss: 0.09340127594790575\n",
      "    At iteration 8800 -> loss: 0.09337130814644796\n",
      "    At iteration 8900 -> loss: 0.09335277804279074\n",
      "    At iteration 9000 -> loss: 0.09333005459259026\n",
      "    At iteration 9100 -> loss: 0.09336273231958811\n",
      "    At iteration 9200 -> loss: 0.09334195582076196\n",
      "    At iteration 9300 -> loss: 0.09331134830493876\n",
      "    At iteration 9400 -> loss: 0.09329533920364498\n",
      "    At iteration 9500 -> loss: 0.09327949294547452\n",
      "    At iteration 9600 -> loss: 0.09324135442323579\n",
      "    At iteration 9700 -> loss: 0.09324405665052951\n",
      "    At iteration 9800 -> loss: 0.09322166005122054\n",
      "    At iteration 9900 -> loss: 0.09318670756773233\n",
      "    At iteration 10000 -> loss: 0.09316603334201645\n",
      "    At iteration 10100 -> loss: 0.09316446354645133\n",
      "    At iteration 10200 -> loss: 0.09314477982000621\n",
      "    At iteration 10300 -> loss: 0.09313512045862773\n",
      "    At iteration 10400 -> loss: 0.09317734312780214\n",
      "    At iteration 10500 -> loss: 0.09314934052775901\n",
      "    At iteration 10600 -> loss: 0.09314336770841786\n",
      "    At iteration 10700 -> loss: 0.09315624010413129\n",
      "    At iteration 10800 -> loss: 0.09315317893019125\n",
      "    At iteration 10900 -> loss: 0.09313795535224145\n",
      "    At iteration 11000 -> loss: 0.09311753980220966\n",
      "    At iteration 11100 -> loss: 0.09317423005881052\n",
      "    At iteration 11200 -> loss: 0.09320843847668402\n",
      "    At iteration 11300 -> loss: 0.09319930092621037\n",
      "    At iteration 11400 -> loss: 0.09319174170181774\n",
      "    At iteration 11500 -> loss: 0.09317073986842692\n",
      "    At iteration 11600 -> loss: 0.09318884089882198\n",
      "    At iteration 11700 -> loss: 0.09317497946506167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11800 -> loss: 0.09320659333516919\n",
      "    At iteration 11900 -> loss: 0.0932313196880567\n",
      "    At iteration 12000 -> loss: 0.09320650652833931\n",
      "    At iteration 12100 -> loss: 0.09317518551879371\n",
      "    At iteration 12200 -> loss: 0.09321237084378238\n",
      "    At iteration 12300 -> loss: 0.09319830738299607\n",
      "    At iteration 12400 -> loss: 0.09318678752548215\n",
      "    At iteration 12500 -> loss: 0.09324713938004114\n",
      "    At iteration 12600 -> loss: 0.09321017155054136\n",
      "    At iteration 12700 -> loss: 0.09317917002842896\n",
      "    At iteration 12800 -> loss: 0.09319018510804868\n",
      "    At iteration 12900 -> loss: 0.09319702009614197\n",
      "    At iteration 13000 -> loss: 0.09316388058959973\n",
      "    At iteration 13100 -> loss: 0.09315243185920244\n",
      "    At iteration 13200 -> loss: 0.09313152099952914\n",
      "    At iteration 13300 -> loss: 0.09312711137056252\n",
      "    At iteration 13400 -> loss: 0.09311762394935869\n",
      "    At iteration 13500 -> loss: 0.09310616620546282\n",
      "    At iteration 13600 -> loss: 0.09313196367129563\n",
      "Staring Epoch 33\n",
      "    At iteration 0 -> loss: 0.08191997354151681\n",
      "    At iteration 100 -> loss: 0.09855130297753678\n",
      "    At iteration 200 -> loss: 0.09574047183451063\n",
      "    At iteration 300 -> loss: 0.09420385999944184\n",
      "    At iteration 400 -> loss: 0.0939531828552072\n",
      "    At iteration 500 -> loss: 0.0936500961818898\n",
      "    At iteration 600 -> loss: 0.09464792910195835\n",
      "    At iteration 700 -> loss: 0.0943509689609969\n",
      "    At iteration 800 -> loss: 0.09405435797669612\n",
      "    At iteration 900 -> loss: 0.09376977100567296\n",
      "    At iteration 1000 -> loss: 0.09365385535434184\n",
      "    At iteration 1100 -> loss: 0.09339795722642219\n",
      "    At iteration 1200 -> loss: 0.09342509919974226\n",
      "    At iteration 1300 -> loss: 0.09326473011414928\n",
      "    At iteration 1400 -> loss: 0.09335830044730149\n",
      "    At iteration 1500 -> loss: 0.09375186712372899\n",
      "    At iteration 1600 -> loss: 0.09346423781786797\n",
      "    At iteration 1700 -> loss: 0.09366584462008617\n",
      "    At iteration 1800 -> loss: 0.09353488608631531\n",
      "    At iteration 1900 -> loss: 0.09404333546065172\n",
      "    At iteration 2000 -> loss: 0.09391550448280545\n",
      "    At iteration 2100 -> loss: 0.09381105565189267\n",
      "    At iteration 2200 -> loss: 0.09389582173481507\n",
      "    At iteration 2300 -> loss: 0.09390644686819585\n",
      "    At iteration 2400 -> loss: 0.09398599032103355\n",
      "    At iteration 2500 -> loss: 0.09399614822713581\n",
      "    At iteration 2600 -> loss: 0.0937997739285816\n",
      "    At iteration 2700 -> loss: 0.09368549222391402\n",
      "    At iteration 2800 -> loss: 0.0937903274798635\n",
      "    At iteration 2900 -> loss: 0.09389597151094242\n",
      "    At iteration 3000 -> loss: 0.09381777606633333\n",
      "    At iteration 3100 -> loss: 0.09366674775586843\n",
      "    At iteration 3200 -> loss: 0.09369846973071053\n",
      "    At iteration 3300 -> loss: 0.0936677558307563\n",
      "    At iteration 3400 -> loss: 0.09362093226487751\n",
      "    At iteration 3500 -> loss: 0.09359668614776244\n",
      "    At iteration 3600 -> loss: 0.09347462963746948\n",
      "    At iteration 3700 -> loss: 0.09345177581828162\n",
      "    At iteration 3800 -> loss: 0.09337729072118153\n",
      "    At iteration 3900 -> loss: 0.09341021854287304\n",
      "    At iteration 4000 -> loss: 0.09337393917754692\n",
      "    At iteration 4100 -> loss: 0.09333754270660591\n",
      "    At iteration 4200 -> loss: 0.09338989724058258\n",
      "    At iteration 4300 -> loss: 0.09331858649331543\n",
      "    At iteration 4400 -> loss: 0.09318917456052382\n",
      "    At iteration 4500 -> loss: 0.0932264736108832\n",
      "    At iteration 4600 -> loss: 0.09334783538811971\n",
      "    At iteration 4700 -> loss: 0.09329207788014877\n",
      "    At iteration 4800 -> loss: 0.09325464699548884\n",
      "    At iteration 4900 -> loss: 0.09316097355384505\n",
      "    At iteration 5000 -> loss: 0.09320679245403721\n",
      "    At iteration 5100 -> loss: 0.09315591998950708\n",
      "    At iteration 5200 -> loss: 0.09333521498145454\n",
      "    At iteration 5300 -> loss: 0.09329942409005235\n",
      "    At iteration 5400 -> loss: 0.09331012230790829\n",
      "    At iteration 5500 -> loss: 0.09323466846508299\n",
      "    At iteration 5600 -> loss: 0.09322328653028816\n",
      "    At iteration 5700 -> loss: 0.09323297323485437\n",
      "    At iteration 5800 -> loss: 0.09313841713981964\n",
      "    At iteration 5900 -> loss: 0.09313960306083978\n",
      "    At iteration 6000 -> loss: 0.09311008537902926\n",
      "    At iteration 6100 -> loss: 0.09308799348260925\n",
      "    At iteration 6200 -> loss: 0.09309945062419815\n",
      "    At iteration 6300 -> loss: 0.09311598470135651\n",
      "    At iteration 6400 -> loss: 0.09308186743103722\n",
      "    At iteration 6500 -> loss: 0.09302661237109643\n",
      "    At iteration 6600 -> loss: 0.0929964945652352\n",
      "    At iteration 6700 -> loss: 0.09298545191043504\n",
      "    At iteration 6800 -> loss: 0.09293889284644903\n",
      "    At iteration 6900 -> loss: 0.09291283278135958\n",
      "    At iteration 7000 -> loss: 0.09285074363701255\n",
      "    At iteration 7100 -> loss: 0.09281565923820104\n",
      "    At iteration 7200 -> loss: 0.09279613458497685\n",
      "    At iteration 7300 -> loss: 0.09279429344825071\n",
      "    At iteration 7400 -> loss: 0.09277493701845288\n",
      "    At iteration 7500 -> loss: 0.09276900941807154\n",
      "    At iteration 7600 -> loss: 0.09282207699299001\n",
      "    At iteration 7700 -> loss: 0.09281441041662994\n",
      "    At iteration 7800 -> loss: 0.09288633213456525\n",
      "    At iteration 7900 -> loss: 0.0928497549018623\n",
      "    At iteration 8000 -> loss: 0.0928422294330072\n",
      "    At iteration 8100 -> loss: 0.09281511900359117\n",
      "    At iteration 8200 -> loss: 0.09290146960486369\n",
      "    At iteration 8300 -> loss: 0.09292548757121873\n",
      "    At iteration 8400 -> loss: 0.09291758669034143\n",
      "    At iteration 8500 -> loss: 0.09292258118814845\n",
      "    At iteration 8600 -> loss: 0.09290131317670637\n",
      "    At iteration 8700 -> loss: 0.0929353044839373\n",
      "    At iteration 8800 -> loss: 0.09294468232684015\n",
      "    At iteration 8900 -> loss: 0.09292142053257701\n",
      "    At iteration 9000 -> loss: 0.09288865283413457\n",
      "    At iteration 9100 -> loss: 0.09297567414846271\n",
      "    At iteration 9200 -> loss: 0.09295750241990411\n",
      "    At iteration 9300 -> loss: 0.09293215779094488\n",
      "    At iteration 9400 -> loss: 0.09294042148427202\n",
      "    At iteration 9500 -> loss: 0.09292631120084915\n",
      "    At iteration 9600 -> loss: 0.09287601832336088\n",
      "    At iteration 9700 -> loss: 0.09283770671173289\n",
      "    At iteration 9800 -> loss: 0.09291308720370348\n",
      "    At iteration 9900 -> loss: 0.09290980576599304\n",
      "    At iteration 10000 -> loss: 0.09289161094578358\n",
      "    At iteration 10100 -> loss: 0.09290986679962564\n",
      "    At iteration 10200 -> loss: 0.09290336382163503\n",
      "    At iteration 10300 -> loss: 0.09286796118788916\n",
      "    At iteration 10400 -> loss: 0.09283847880180195\n",
      "    At iteration 10500 -> loss: 0.09282160617772937\n",
      "    At iteration 10600 -> loss: 0.0928266320158673\n",
      "    At iteration 10700 -> loss: 0.09285098366907715\n",
      "    At iteration 10800 -> loss: 0.09285625636408147\n",
      "    At iteration 10900 -> loss: 0.092862244535561\n",
      "    At iteration 11000 -> loss: 0.09284685694167587\n",
      "    At iteration 11100 -> loss: 0.0928187834658794\n",
      "    At iteration 11200 -> loss: 0.09278893364686681\n",
      "    At iteration 11300 -> loss: 0.09277092900924132\n",
      "    At iteration 11400 -> loss: 0.09278150220951366\n",
      "    At iteration 11500 -> loss: 0.092765484442094\n",
      "    At iteration 11600 -> loss: 0.09284870642753933\n",
      "    At iteration 11700 -> loss: 0.0928426197691026\n",
      "    At iteration 11800 -> loss: 0.0928432536414652\n",
      "    At iteration 11900 -> loss: 0.0928546614995029\n",
      "    At iteration 12000 -> loss: 0.09281801467356189\n",
      "    At iteration 12100 -> loss: 0.09285561059346324\n",
      "    At iteration 12200 -> loss: 0.09309648238200333\n",
      "    At iteration 12300 -> loss: 0.09315053132468902\n",
      "    At iteration 12400 -> loss: 0.09319417971225245\n",
      "    At iteration 12500 -> loss: 0.09316653344371151\n",
      "    At iteration 12600 -> loss: 0.09315434154120625\n",
      "    At iteration 12700 -> loss: 0.09312200851662818\n",
      "    At iteration 12800 -> loss: 0.09310235539650918\n",
      "    At iteration 12900 -> loss: 0.09309902480744305\n",
      "    At iteration 13000 -> loss: 0.09311293947172809\n",
      "    At iteration 13100 -> loss: 0.09309767591385114\n",
      "    At iteration 13200 -> loss: 0.09311266668332174\n",
      "    At iteration 13300 -> loss: 0.09309883579120812\n",
      "    At iteration 13400 -> loss: 0.09311486967877036\n",
      "    At iteration 13500 -> loss: 0.09310185305147353\n",
      "    At iteration 13600 -> loss: 0.09309242449480802\n",
      "Staring Epoch 34\n",
      "    At iteration 0 -> loss: 0.09094032761640847\n",
      "    At iteration 100 -> loss: 0.09128316877874369\n",
      "    At iteration 200 -> loss: 0.09068460849390242\n",
      "    At iteration 300 -> loss: 0.09298641210674237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 400 -> loss: 0.09290502667487409\n",
      "    At iteration 500 -> loss: 0.09190859118194791\n",
      "    At iteration 600 -> loss: 0.09218809083360298\n",
      "    At iteration 700 -> loss: 0.09205481400264595\n",
      "    At iteration 800 -> loss: 0.09211911945910134\n",
      "    At iteration 900 -> loss: 0.0929608584578428\n",
      "    At iteration 1000 -> loss: 0.09281449009423076\n",
      "    At iteration 1100 -> loss: 0.09264842211112059\n",
      "    At iteration 1200 -> loss: 0.09256059166646569\n",
      "    At iteration 1300 -> loss: 0.09294917129314473\n",
      "    At iteration 1400 -> loss: 0.09348714126099873\n",
      "    At iteration 1500 -> loss: 0.09376653402143426\n",
      "    At iteration 1600 -> loss: 0.09399041216922849\n",
      "    At iteration 1700 -> loss: 0.09369096521106916\n",
      "    At iteration 1800 -> loss: 0.09345982457974655\n",
      "    At iteration 1900 -> loss: 0.09350464651658358\n",
      "    At iteration 2000 -> loss: 0.09333137742787861\n",
      "    At iteration 2100 -> loss: 0.09322597299413672\n",
      "    At iteration 2200 -> loss: 0.09326756449677256\n",
      "    At iteration 2300 -> loss: 0.09328375169588389\n",
      "    At iteration 2400 -> loss: 0.09342889402275033\n",
      "    At iteration 2500 -> loss: 0.09349960428896609\n",
      "    At iteration 2600 -> loss: 0.09348847917336554\n",
      "    At iteration 2700 -> loss: 0.09337889497009555\n",
      "    At iteration 2800 -> loss: 0.09340650506116002\n",
      "    At iteration 2900 -> loss: 0.09342278682384314\n",
      "    At iteration 3000 -> loss: 0.09328615599378387\n",
      "    At iteration 3100 -> loss: 0.0933566370918121\n",
      "    At iteration 3200 -> loss: 0.09329856051064549\n",
      "    At iteration 3300 -> loss: 0.09320648016308336\n",
      "    At iteration 3400 -> loss: 0.0932749774547807\n",
      "    At iteration 3500 -> loss: 0.0932264257970329\n",
      "    At iteration 3600 -> loss: 0.09318198841382896\n",
      "    At iteration 3700 -> loss: 0.09319628437163155\n",
      "    At iteration 3800 -> loss: 0.09307264014548093\n",
      "    At iteration 3900 -> loss: 0.0930610007989132\n",
      "    At iteration 4000 -> loss: 0.09313884344500778\n",
      "    At iteration 4100 -> loss: 0.09309840101392329\n",
      "    At iteration 4200 -> loss: 0.09310808000735966\n",
      "    At iteration 4300 -> loss: 0.09304903444803513\n",
      "    At iteration 4400 -> loss: 0.09298478301307614\n",
      "    At iteration 4500 -> loss: 0.09320963311118788\n",
      "    At iteration 4600 -> loss: 0.0931220632484622\n",
      "    At iteration 4700 -> loss: 0.09308176944167744\n",
      "    At iteration 4800 -> loss: 0.09307335284489228\n",
      "    At iteration 4900 -> loss: 0.0931365970235283\n",
      "    At iteration 5000 -> loss: 0.09309928983356147\n",
      "    At iteration 5100 -> loss: 0.0930357270952665\n",
      "    At iteration 5200 -> loss: 0.09302913048508939\n",
      "    At iteration 5300 -> loss: 0.09313678261541437\n",
      "    At iteration 5400 -> loss: 0.09307637097671943\n",
      "    At iteration 5500 -> loss: 0.09299859163564866\n",
      "    At iteration 5600 -> loss: 0.09297515776814659\n",
      "    At iteration 5700 -> loss: 0.09293496865503297\n",
      "    At iteration 5800 -> loss: 0.09291580705077938\n",
      "    At iteration 5900 -> loss: 0.09302615814327908\n",
      "    At iteration 6000 -> loss: 0.09297342938020106\n",
      "    At iteration 6100 -> loss: 0.09293130267537385\n",
      "    At iteration 6200 -> loss: 0.09289437326255216\n",
      "    At iteration 6300 -> loss: 0.09304734603862529\n",
      "    At iteration 6400 -> loss: 0.09313263822257134\n",
      "    At iteration 6500 -> loss: 0.09309751481723695\n",
      "    At iteration 6600 -> loss: 0.09305859816937213\n",
      "    At iteration 6700 -> loss: 0.09299575450303947\n",
      "    At iteration 6800 -> loss: 0.0929804074425597\n",
      "    At iteration 6900 -> loss: 0.09303231445979404\n",
      "    At iteration 7000 -> loss: 0.09297214909288555\n",
      "    At iteration 7100 -> loss: 0.09298883865423155\n",
      "    At iteration 7200 -> loss: 0.09299815136386731\n",
      "    At iteration 7300 -> loss: 0.09298771498662638\n",
      "    At iteration 7400 -> loss: 0.09299966368734841\n",
      "    At iteration 7500 -> loss: 0.09295545508597901\n",
      "    At iteration 7600 -> loss: 0.09293398127022356\n",
      "    At iteration 7700 -> loss: 0.09289920019749441\n",
      "    At iteration 7800 -> loss: 0.09292288412542833\n",
      "    At iteration 7900 -> loss: 0.09292299987503563\n",
      "    At iteration 8000 -> loss: 0.09293913809350406\n",
      "    At iteration 8100 -> loss: 0.0929576820534017\n",
      "    At iteration 8200 -> loss: 0.09295254635397207\n",
      "    At iteration 8300 -> loss: 0.09293890433896312\n",
      "    At iteration 8400 -> loss: 0.09297333385381427\n",
      "    At iteration 8500 -> loss: 0.09305430174972128\n",
      "    At iteration 8600 -> loss: 0.09299686194782691\n",
      "    At iteration 8700 -> loss: 0.09296270516533257\n",
      "    At iteration 8800 -> loss: 0.09296059380649688\n",
      "    At iteration 8900 -> loss: 0.0933263716766722\n",
      "    At iteration 9000 -> loss: 0.09330428135862141\n",
      "    At iteration 9100 -> loss: 0.09332587984942523\n",
      "    At iteration 9200 -> loss: 0.09333052244982531\n",
      "    At iteration 9300 -> loss: 0.09327476397777257\n",
      "    At iteration 9400 -> loss: 0.09326667820030024\n",
      "    At iteration 9500 -> loss: 0.09323099822485972\n",
      "    At iteration 9600 -> loss: 0.0932204077449767\n",
      "    At iteration 9700 -> loss: 0.09320774579658359\n",
      "    At iteration 9800 -> loss: 0.09320477766843889\n",
      "    At iteration 9900 -> loss: 0.09326149542447094\n",
      "    At iteration 10000 -> loss: 0.09322419473772624\n",
      "    At iteration 10100 -> loss: 0.09320849101249573\n",
      "    At iteration 10200 -> loss: 0.09319758301121299\n",
      "    At iteration 10300 -> loss: 0.09324588571699785\n",
      "    At iteration 10400 -> loss: 0.09322997849006245\n",
      "    At iteration 10500 -> loss: 0.09326968784647109\n",
      "    At iteration 10600 -> loss: 0.09324153952103298\n",
      "    At iteration 10700 -> loss: 0.09321616304635327\n",
      "    At iteration 10800 -> loss: 0.09318053205253063\n",
      "    At iteration 10900 -> loss: 0.09321633765437969\n",
      "    At iteration 11000 -> loss: 0.09320795595989759\n",
      "    At iteration 11100 -> loss: 0.09318855450271787\n",
      "    At iteration 11200 -> loss: 0.09318752869467302\n",
      "    At iteration 11300 -> loss: 0.09323739999364394\n",
      "    At iteration 11400 -> loss: 0.09322090973140928\n",
      "    At iteration 11500 -> loss: 0.09319779999213332\n",
      "    At iteration 11600 -> loss: 0.09321615820626096\n",
      "    At iteration 11700 -> loss: 0.0932013316370749\n",
      "    At iteration 11800 -> loss: 0.09321060989668323\n",
      "    At iteration 11900 -> loss: 0.09320788619835806\n",
      "    At iteration 12000 -> loss: 0.0931866226883086\n",
      "    At iteration 12100 -> loss: 0.09315979405653538\n",
      "    At iteration 12200 -> loss: 0.09318770872994615\n",
      "    At iteration 12300 -> loss: 0.09323435930516823\n",
      "    At iteration 12400 -> loss: 0.0931992283169637\n",
      "    At iteration 12500 -> loss: 0.09317838536047462\n",
      "    At iteration 12600 -> loss: 0.09316752706828757\n",
      "    At iteration 12700 -> loss: 0.09312926043589516\n",
      "    At iteration 12800 -> loss: 0.09317275775245809\n",
      "    At iteration 12900 -> loss: 0.09316432364482198\n",
      "    At iteration 13000 -> loss: 0.09315570091400775\n",
      "    At iteration 13100 -> loss: 0.09312917784155711\n",
      "    At iteration 13200 -> loss: 0.09313891057203795\n",
      "    At iteration 13300 -> loss: 0.0931012178907465\n",
      "    At iteration 13400 -> loss: 0.09311191530187773\n",
      "    At iteration 13500 -> loss: 0.0931189901323297\n",
      "    At iteration 13600 -> loss: 0.09312150804565875\n",
      "Staring Epoch 35\n",
      "    At iteration 0 -> loss: 0.08006027683950379\n",
      "    At iteration 100 -> loss: 0.09463896504314112\n",
      "    At iteration 200 -> loss: 0.09519240158419395\n",
      "    At iteration 300 -> loss: 0.09365112862991702\n",
      "    At iteration 400 -> loss: 0.09281325641063087\n",
      "    At iteration 500 -> loss: 0.0928249098351198\n",
      "    At iteration 600 -> loss: 0.09359355187163448\n",
      "    At iteration 700 -> loss: 0.0933743497425169\n",
      "    At iteration 800 -> loss: 0.09352958818667083\n",
      "    At iteration 900 -> loss: 0.09342831134164645\n",
      "    At iteration 1000 -> loss: 0.09294986058033858\n",
      "    At iteration 1100 -> loss: 0.0930318066126534\n",
      "    At iteration 1200 -> loss: 0.09266054743850363\n",
      "    At iteration 1300 -> loss: 0.09260674189269724\n",
      "    At iteration 1400 -> loss: 0.09250426952179124\n",
      "    At iteration 1500 -> loss: 0.09231727391135681\n",
      "    At iteration 1600 -> loss: 0.09234045051339693\n",
      "    At iteration 1700 -> loss: 0.09410794193362548\n",
      "    At iteration 1800 -> loss: 0.09475309042826424\n",
      "    At iteration 1900 -> loss: 0.0944956844930429\n",
      "    At iteration 2000 -> loss: 0.09425758467504579\n",
      "    At iteration 2100 -> loss: 0.09409253588843816\n",
      "    At iteration 2200 -> loss: 0.09406505494075376\n",
      "    At iteration 2300 -> loss: 0.09389792839363599\n",
      "    At iteration 2400 -> loss: 0.09382481811153393\n",
      "    At iteration 2500 -> loss: 0.09374594428370962\n",
      "    At iteration 2600 -> loss: 0.09353881788786726\n",
      "    At iteration 2700 -> loss: 0.09357000031802477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2800 -> loss: 0.09398084895289491\n",
      "    At iteration 2900 -> loss: 0.0938731857603141\n",
      "    At iteration 3000 -> loss: 0.09384288355065455\n",
      "    At iteration 3100 -> loss: 0.09383086166475567\n",
      "    At iteration 3200 -> loss: 0.0938787392749894\n",
      "    At iteration 3300 -> loss: 0.09389009678485881\n",
      "    At iteration 3400 -> loss: 0.0939257967382594\n",
      "    At iteration 3500 -> loss: 0.09393750507467659\n",
      "    At iteration 3600 -> loss: 0.09391004091478479\n",
      "    At iteration 3700 -> loss: 0.09390055851895941\n",
      "    At iteration 3800 -> loss: 0.09377402674193087\n",
      "    At iteration 3900 -> loss: 0.09379752246832763\n",
      "    At iteration 4000 -> loss: 0.09368412732204853\n",
      "    At iteration 4100 -> loss: 0.09360217759408172\n",
      "    At iteration 4200 -> loss: 0.09358808098688418\n",
      "    At iteration 4300 -> loss: 0.09357040009251799\n",
      "    At iteration 4400 -> loss: 0.09357586092896653\n",
      "    At iteration 4500 -> loss: 0.0934914516246226\n",
      "    At iteration 4600 -> loss: 0.0934135767404592\n",
      "    At iteration 4700 -> loss: 0.09340370505577145\n",
      "    At iteration 4800 -> loss: 0.09334057887608972\n",
      "    At iteration 4900 -> loss: 0.09340233548874662\n",
      "    At iteration 5000 -> loss: 0.09337162911115324\n",
      "    At iteration 5100 -> loss: 0.09335155881503808\n",
      "    At iteration 5200 -> loss: 0.09334275662189957\n",
      "    At iteration 5300 -> loss: 0.09331017739947992\n",
      "    At iteration 5400 -> loss: 0.09335914594786343\n",
      "    At iteration 5500 -> loss: 0.09328879885279528\n",
      "    At iteration 5600 -> loss: 0.09327582186895311\n",
      "    At iteration 5700 -> loss: 0.09321808704118326\n",
      "    At iteration 5800 -> loss: 0.09316784838978054\n",
      "    At iteration 5900 -> loss: 0.09312452177323981\n",
      "    At iteration 6000 -> loss: 0.09308274288730892\n",
      "    At iteration 6100 -> loss: 0.09302921183593088\n",
      "    At iteration 6200 -> loss: 0.09298911561030962\n",
      "    At iteration 6300 -> loss: 0.09307373194591363\n",
      "    At iteration 6400 -> loss: 0.09311404862615777\n",
      "    At iteration 6500 -> loss: 0.09307444596550314\n",
      "    At iteration 6600 -> loss: 0.0930519089382663\n",
      "    At iteration 6700 -> loss: 0.09302908801718793\n",
      "    At iteration 6800 -> loss: 0.09302792122745324\n",
      "    At iteration 6900 -> loss: 0.09301526107265477\n",
      "    At iteration 7000 -> loss: 0.09298251927254639\n",
      "    At iteration 7100 -> loss: 0.09291003042526291\n",
      "    At iteration 7200 -> loss: 0.09289755637194785\n",
      "    At iteration 7300 -> loss: 0.09295205479172822\n",
      "    At iteration 7400 -> loss: 0.09293250701102505\n",
      "    At iteration 7500 -> loss: 0.0929594440041365\n",
      "    At iteration 7600 -> loss: 0.09297032178309593\n",
      "    At iteration 7700 -> loss: 0.09296915768912775\n",
      "    At iteration 7800 -> loss: 0.09293586964410262\n",
      "    At iteration 7900 -> loss: 0.09293702196315554\n",
      "    At iteration 8000 -> loss: 0.09293556123817183\n",
      "    At iteration 8100 -> loss: 0.09293728777911585\n",
      "    At iteration 8200 -> loss: 0.09295183195439681\n",
      "    At iteration 8300 -> loss: 0.09296098264866175\n",
      "    At iteration 8400 -> loss: 0.09293309110954937\n",
      "    At iteration 8500 -> loss: 0.09308529535014844\n",
      "    At iteration 8600 -> loss: 0.09310173349368439\n",
      "    At iteration 8700 -> loss: 0.09307491298394972\n",
      "    At iteration 8800 -> loss: 0.09310114727803515\n",
      "    At iteration 8900 -> loss: 0.0930633723360414\n",
      "    At iteration 9000 -> loss: 0.09302766324448264\n",
      "    At iteration 9100 -> loss: 0.09308736912268942\n",
      "    At iteration 9200 -> loss: 0.0930943172641666\n",
      "    At iteration 9300 -> loss: 0.09311690511764502\n",
      "    At iteration 9400 -> loss: 0.09312967410296955\n",
      "    At iteration 9500 -> loss: 0.09311779793382735\n",
      "    At iteration 9600 -> loss: 0.09307584067001112\n",
      "    At iteration 9700 -> loss: 0.09317110117685916\n",
      "    At iteration 9800 -> loss: 0.09317282069773367\n",
      "    At iteration 9900 -> loss: 0.09316302255230767\n",
      "    At iteration 10000 -> loss: 0.09313512991110408\n",
      "    At iteration 10100 -> loss: 0.09313597907877093\n",
      "    At iteration 10200 -> loss: 0.09316720999077985\n",
      "    At iteration 10300 -> loss: 0.09319623664196207\n",
      "    At iteration 10400 -> loss: 0.09316664243067059\n",
      "    At iteration 10500 -> loss: 0.09313534056362249\n",
      "    At iteration 10600 -> loss: 0.09309884977671115\n",
      "    At iteration 10700 -> loss: 0.09312801283835273\n",
      "    At iteration 10800 -> loss: 0.09315559990949383\n",
      "    At iteration 10900 -> loss: 0.09312775609260485\n",
      "    At iteration 11000 -> loss: 0.09311440966793916\n",
      "    At iteration 11100 -> loss: 0.09309858502272549\n",
      "    At iteration 11200 -> loss: 0.093108112163312\n",
      "    At iteration 11300 -> loss: 0.09324295575996533\n",
      "    At iteration 11400 -> loss: 0.09322091597275806\n",
      "    At iteration 11500 -> loss: 0.09321213767337136\n",
      "    At iteration 11600 -> loss: 0.09324749758329753\n",
      "    At iteration 11700 -> loss: 0.09326875179513236\n",
      "    At iteration 11800 -> loss: 0.09324197215889268\n",
      "    At iteration 11900 -> loss: 0.09322247662793133\n",
      "    At iteration 12000 -> loss: 0.0931878694938572\n",
      "    At iteration 12100 -> loss: 0.09318169953386839\n",
      "    At iteration 12200 -> loss: 0.09322506262768046\n",
      "    At iteration 12300 -> loss: 0.0932085864406224\n",
      "    At iteration 12400 -> loss: 0.09320964493255057\n",
      "    At iteration 12500 -> loss: 0.09318751651714972\n",
      "    At iteration 12600 -> loss: 0.09314884639149472\n",
      "    At iteration 12700 -> loss: 0.09314750093586917\n",
      "    At iteration 12800 -> loss: 0.09312773014978751\n",
      "    At iteration 12900 -> loss: 0.09310502661541914\n",
      "    At iteration 13000 -> loss: 0.093116537630141\n",
      "    At iteration 13100 -> loss: 0.09309156784337026\n",
      "    At iteration 13200 -> loss: 0.0931530811954112\n",
      "    At iteration 13300 -> loss: 0.09313500741965047\n",
      "    At iteration 13400 -> loss: 0.09311463154356567\n",
      "    At iteration 13500 -> loss: 0.09313366036346113\n",
      "    At iteration 13600 -> loss: 0.09313376163357419\n",
      "Staring Epoch 36\n",
      "    At iteration 0 -> loss: 0.08484630659222603\n",
      "    At iteration 100 -> loss: 0.08878849473173696\n",
      "    At iteration 200 -> loss: 0.09318056894416903\n",
      "    At iteration 300 -> loss: 0.09293494716894768\n",
      "    At iteration 400 -> loss: 0.09227893799627727\n",
      "    At iteration 500 -> loss: 0.09257998304838962\n",
      "    At iteration 600 -> loss: 0.09442309205732478\n",
      "    At iteration 700 -> loss: 0.09451592860389432\n",
      "    At iteration 800 -> loss: 0.09445873178994263\n",
      "    At iteration 900 -> loss: 0.09399764436983721\n",
      "    At iteration 1000 -> loss: 0.09353513999687157\n",
      "    At iteration 1100 -> loss: 0.093708220512792\n",
      "    At iteration 1200 -> loss: 0.09360029224093933\n",
      "    At iteration 1300 -> loss: 0.09355174282016296\n",
      "    At iteration 1400 -> loss: 0.09350319112130488\n",
      "    At iteration 1500 -> loss: 0.09338542238835632\n",
      "    At iteration 1600 -> loss: 0.09345608605396107\n",
      "    At iteration 1700 -> loss: 0.09326827149898488\n",
      "    At iteration 1800 -> loss: 0.09329538323107725\n",
      "    At iteration 1900 -> loss: 0.09318712707016075\n",
      "    At iteration 2000 -> loss: 0.09324790853852959\n",
      "    At iteration 2100 -> loss: 0.09328229274991014\n",
      "    At iteration 2200 -> loss: 0.09308437038164058\n",
      "    At iteration 2300 -> loss: 0.0930884848050296\n",
      "    At iteration 2400 -> loss: 0.09317685758097176\n",
      "    At iteration 2500 -> loss: 0.09318736088578095\n",
      "    At iteration 2600 -> loss: 0.09305893597859587\n",
      "    At iteration 2700 -> loss: 0.09305863974141139\n",
      "    At iteration 2800 -> loss: 0.09326860388158598\n",
      "    At iteration 2900 -> loss: 0.09314807439132093\n",
      "    At iteration 3000 -> loss: 0.09319803611783682\n",
      "    At iteration 3100 -> loss: 0.0932151567615462\n",
      "    At iteration 3200 -> loss: 0.09317697848919428\n",
      "    At iteration 3300 -> loss: 0.09324968345270979\n",
      "    At iteration 3400 -> loss: 0.09319762508072865\n",
      "    At iteration 3500 -> loss: 0.09331244930835833\n",
      "    At iteration 3600 -> loss: 0.09323911029985575\n",
      "    At iteration 3700 -> loss: 0.09320105492343306\n",
      "    At iteration 3800 -> loss: 0.09325199509882243\n",
      "    At iteration 3900 -> loss: 0.09314570846966981\n",
      "    At iteration 4000 -> loss: 0.09318019029803676\n",
      "    At iteration 4100 -> loss: 0.09329715358711364\n",
      "    At iteration 4200 -> loss: 0.09326488840516199\n",
      "    At iteration 4300 -> loss: 0.09321906449268517\n",
      "    At iteration 4400 -> loss: 0.09334920751027279\n",
      "    At iteration 4500 -> loss: 0.09334288742237462\n",
      "    At iteration 4600 -> loss: 0.09333803007119706\n",
      "    At iteration 4700 -> loss: 0.09330378845029437\n",
      "    At iteration 4800 -> loss: 0.0933898787788692\n",
      "    At iteration 4900 -> loss: 0.09334161239925695\n",
      "    At iteration 5000 -> loss: 0.09326844685312435\n",
      "    At iteration 5100 -> loss: 0.09319948889958239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5200 -> loss: 0.09319797923464114\n",
      "    At iteration 5300 -> loss: 0.09318331855993302\n",
      "    At iteration 5400 -> loss: 0.09312742288333288\n",
      "    At iteration 5500 -> loss: 0.09311495410201182\n",
      "    At iteration 5600 -> loss: 0.09318758484807713\n",
      "    At iteration 5700 -> loss: 0.09313881182592924\n",
      "    At iteration 5800 -> loss: 0.09326184349940633\n",
      "    At iteration 5900 -> loss: 0.0933234038925396\n",
      "    At iteration 6000 -> loss: 0.09334146048521592\n",
      "    At iteration 6100 -> loss: 0.09331519370947478\n",
      "    At iteration 6200 -> loss: 0.09326418123455663\n",
      "    At iteration 6300 -> loss: 0.09325518393683306\n",
      "    At iteration 6400 -> loss: 0.09322484458488783\n",
      "    At iteration 6500 -> loss: 0.09317358197050452\n",
      "    At iteration 6600 -> loss: 0.09313952717365094\n",
      "    At iteration 6700 -> loss: 0.09314357279194085\n",
      "    At iteration 6800 -> loss: 0.09312788487474176\n",
      "    At iteration 6900 -> loss: 0.0931023979355955\n",
      "    At iteration 7000 -> loss: 0.09305357152199287\n",
      "    At iteration 7100 -> loss: 0.09319975256043106\n",
      "    At iteration 7200 -> loss: 0.09315592577696255\n",
      "    At iteration 7300 -> loss: 0.09319219150717956\n",
      "    At iteration 7400 -> loss: 0.0931885916719326\n",
      "    At iteration 7500 -> loss: 0.09314607413501129\n",
      "    At iteration 7600 -> loss: 0.09313048396900102\n",
      "    At iteration 7700 -> loss: 0.09309129584452307\n",
      "    At iteration 7800 -> loss: 0.0930413268332091\n",
      "    At iteration 7900 -> loss: 0.09304376160038842\n",
      "    At iteration 8000 -> loss: 0.09304132029199051\n",
      "    At iteration 8100 -> loss: 0.09301504386170102\n",
      "    At iteration 8200 -> loss: 0.09299758273903791\n",
      "    At iteration 8300 -> loss: 0.0930042103197594\n",
      "    At iteration 8400 -> loss: 0.0929545196715918\n",
      "    At iteration 8500 -> loss: 0.09328045666130766\n",
      "    At iteration 8600 -> loss: 0.09330297907859456\n",
      "    At iteration 8700 -> loss: 0.09329699217141198\n",
      "    At iteration 8800 -> loss: 0.09326146484084344\n",
      "    At iteration 8900 -> loss: 0.09330833653256752\n",
      "    At iteration 9000 -> loss: 0.0932755228294374\n",
      "    At iteration 9100 -> loss: 0.09324841565174995\n",
      "    At iteration 9200 -> loss: 0.09325965065515746\n",
      "    At iteration 9300 -> loss: 0.09333525190350396\n",
      "    At iteration 9400 -> loss: 0.0933233060949407\n",
      "    At iteration 9500 -> loss: 0.09336858486512278\n",
      "    At iteration 9600 -> loss: 0.09333083032559164\n",
      "    At iteration 9700 -> loss: 0.09330832003260217\n",
      "    At iteration 9800 -> loss: 0.09325941184916484\n",
      "    At iteration 9900 -> loss: 0.09326009422526639\n",
      "    At iteration 10000 -> loss: 0.09323771752527661\n",
      "    At iteration 10100 -> loss: 0.0932110268752178\n",
      "    At iteration 10200 -> loss: 0.09321493601773831\n",
      "    At iteration 10300 -> loss: 0.0932130653988951\n",
      "    At iteration 10400 -> loss: 0.09321835483846676\n",
      "    At iteration 10500 -> loss: 0.09320366807529268\n",
      "    At iteration 10600 -> loss: 0.09323229335540476\n",
      "    At iteration 10700 -> loss: 0.09322171177929736\n",
      "    At iteration 10800 -> loss: 0.0932751380997144\n",
      "    At iteration 10900 -> loss: 0.09327657839399328\n",
      "    At iteration 11000 -> loss: 0.0933431743887066\n",
      "    At iteration 11100 -> loss: 0.09333317159585314\n",
      "    At iteration 11200 -> loss: 0.09334245938197493\n",
      "    At iteration 11300 -> loss: 0.09331197006058915\n",
      "    At iteration 11400 -> loss: 0.09331080850981723\n",
      "    At iteration 11500 -> loss: 0.09328154361058391\n",
      "    At iteration 11600 -> loss: 0.09331521781176302\n",
      "    At iteration 11700 -> loss: 0.09332444951099536\n",
      "    At iteration 11800 -> loss: 0.09332975924995532\n",
      "    At iteration 11900 -> loss: 0.09331168271646287\n",
      "    At iteration 12000 -> loss: 0.09328009011424922\n",
      "    At iteration 12100 -> loss: 0.09324618820841264\n",
      "    At iteration 12200 -> loss: 0.09324921867766175\n",
      "    At iteration 12300 -> loss: 0.09321706480722029\n",
      "    At iteration 12400 -> loss: 0.09320188437809783\n",
      "    At iteration 12500 -> loss: 0.0932248925938008\n",
      "    At iteration 12600 -> loss: 0.0932533246012872\n",
      "    At iteration 12700 -> loss: 0.09322150340265885\n",
      "    At iteration 12800 -> loss: 0.09319695923126428\n",
      "    At iteration 12900 -> loss: 0.0931750679348213\n",
      "    At iteration 13000 -> loss: 0.09315372199564557\n",
      "    At iteration 13100 -> loss: 0.0931370057248187\n",
      "    At iteration 13200 -> loss: 0.09314148938521624\n",
      "    At iteration 13300 -> loss: 0.09315745624899602\n",
      "    At iteration 13400 -> loss: 0.09315420953299869\n",
      "    At iteration 13500 -> loss: 0.09314911288417181\n",
      "    At iteration 13600 -> loss: 0.09312782475566392\n",
      "Staring Epoch 37\n",
      "    At iteration 0 -> loss: 0.080140363494138\n",
      "    At iteration 100 -> loss: 0.08937018391263633\n",
      "    At iteration 200 -> loss: 0.08966840600901921\n",
      "    At iteration 300 -> loss: 0.08955578885006979\n",
      "    At iteration 400 -> loss: 0.09139781729527557\n",
      "    At iteration 500 -> loss: 0.09183262959627274\n",
      "    At iteration 600 -> loss: 0.09270894713325079\n",
      "    At iteration 700 -> loss: 0.09269803808001832\n",
      "    At iteration 800 -> loss: 0.09222623028080729\n",
      "    At iteration 900 -> loss: 0.09204625193527571\n",
      "    At iteration 1000 -> loss: 0.0919111382025112\n",
      "    At iteration 1100 -> loss: 0.0918567273594025\n",
      "    At iteration 1200 -> loss: 0.09195201388939873\n",
      "    At iteration 1300 -> loss: 0.09184248116113457\n",
      "    At iteration 1400 -> loss: 0.09173291391797325\n",
      "    At iteration 1500 -> loss: 0.09198173901922105\n",
      "    At iteration 1600 -> loss: 0.09185236499218287\n",
      "    At iteration 1700 -> loss: 0.09188411386557142\n",
      "    At iteration 1800 -> loss: 0.09215328752158228\n",
      "    At iteration 1900 -> loss: 0.09194858824288972\n",
      "    At iteration 2000 -> loss: 0.09194241605849082\n",
      "    At iteration 2100 -> loss: 0.09224240086597811\n",
      "    At iteration 2200 -> loss: 0.09242056231759303\n",
      "    At iteration 2300 -> loss: 0.09244036429096199\n",
      "    At iteration 2400 -> loss: 0.09252217448618363\n",
      "    At iteration 2500 -> loss: 0.09292229846385423\n",
      "    At iteration 2600 -> loss: 0.09277755153404514\n",
      "    At iteration 2700 -> loss: 0.09271716144909206\n",
      "    At iteration 2800 -> loss: 0.09261577274025509\n",
      "    At iteration 2900 -> loss: 0.0927545489032061\n",
      "    At iteration 3000 -> loss: 0.09280046649470296\n",
      "    At iteration 3100 -> loss: 0.09287539989937844\n",
      "    At iteration 3200 -> loss: 0.09277888640795702\n",
      "    At iteration 3300 -> loss: 0.0926699544476806\n",
      "    At iteration 3400 -> loss: 0.09275939859458293\n",
      "    At iteration 3500 -> loss: 0.09270516363849998\n",
      "    At iteration 3600 -> loss: 0.09280512007570198\n",
      "    At iteration 3700 -> loss: 0.09272661940284647\n",
      "    At iteration 3800 -> loss: 0.09266014433238931\n",
      "    At iteration 3900 -> loss: 0.09271306225900108\n",
      "    At iteration 4000 -> loss: 0.09297985940614494\n",
      "    At iteration 4100 -> loss: 0.09326857214782568\n",
      "    At iteration 4200 -> loss: 0.09323937179521907\n",
      "    At iteration 4300 -> loss: 0.09319342743272428\n",
      "    At iteration 4400 -> loss: 0.09315374326494585\n",
      "    At iteration 4500 -> loss: 0.09308846031802655\n",
      "    At iteration 4600 -> loss: 0.09313895833428427\n",
      "    At iteration 4700 -> loss: 0.09308252182645028\n",
      "    At iteration 4800 -> loss: 0.09302904286890264\n",
      "    At iteration 4900 -> loss: 0.09298352304753973\n",
      "    At iteration 5000 -> loss: 0.09293551046651194\n",
      "    At iteration 5100 -> loss: 0.09288138319662062\n",
      "    At iteration 5200 -> loss: 0.09301128805167769\n",
      "    At iteration 5300 -> loss: 0.09300436149180318\n",
      "    At iteration 5400 -> loss: 0.09298221442061687\n",
      "    At iteration 5500 -> loss: 0.09308958866340278\n",
      "    At iteration 5600 -> loss: 0.09317152043632541\n",
      "    At iteration 5700 -> loss: 0.09311307404974128\n",
      "    At iteration 5800 -> loss: 0.0930647608728632\n",
      "    At iteration 5900 -> loss: 0.09304525127495343\n",
      "    At iteration 6000 -> loss: 0.09310422819838896\n",
      "    At iteration 6100 -> loss: 0.0932245791848881\n",
      "    At iteration 6200 -> loss: 0.09325822203904775\n",
      "    At iteration 6300 -> loss: 0.09322847379190528\n",
      "    At iteration 6400 -> loss: 0.09320804288528062\n",
      "    At iteration 6500 -> loss: 0.09312642575115804\n",
      "    At iteration 6600 -> loss: 0.09307191343427867\n",
      "    At iteration 6700 -> loss: 0.09354274848592395\n",
      "    At iteration 6800 -> loss: 0.09355873049476787\n",
      "    At iteration 6900 -> loss: 0.09351931906434531\n",
      "    At iteration 7000 -> loss: 0.09350576380682545\n",
      "    At iteration 7100 -> loss: 0.09351242556568644\n",
      "    At iteration 7200 -> loss: 0.09346602437229427\n",
      "    At iteration 7300 -> loss: 0.09344770268727384\n",
      "    At iteration 7400 -> loss: 0.09350347106492009\n",
      "    At iteration 7500 -> loss: 0.09346314681244859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7600 -> loss: 0.09343121349114795\n",
      "    At iteration 7700 -> loss: 0.09337723572302849\n",
      "    At iteration 7800 -> loss: 0.09337567026821103\n",
      "    At iteration 7900 -> loss: 0.09337005718625702\n",
      "    At iteration 8000 -> loss: 0.09333260187266636\n",
      "    At iteration 8100 -> loss: 0.09331277739909896\n",
      "    At iteration 8200 -> loss: 0.09328834117565486\n",
      "    At iteration 8300 -> loss: 0.09325299165583424\n",
      "    At iteration 8400 -> loss: 0.09327172453235354\n",
      "    At iteration 8500 -> loss: 0.09327072784308157\n",
      "    At iteration 8600 -> loss: 0.09324029722417121\n",
      "    At iteration 8700 -> loss: 0.09327014350295765\n",
      "    At iteration 8800 -> loss: 0.09323906961265085\n",
      "    At iteration 8900 -> loss: 0.09320802869363684\n",
      "    At iteration 9000 -> loss: 0.09320138397567787\n",
      "    At iteration 9100 -> loss: 0.09316409724745411\n",
      "    At iteration 9200 -> loss: 0.09316477392955\n",
      "    At iteration 9300 -> loss: 0.09314800563317492\n",
      "    At iteration 9400 -> loss: 0.09312330949684809\n",
      "    At iteration 9500 -> loss: 0.09308811785033037\n",
      "    At iteration 9600 -> loss: 0.09312198533107525\n",
      "    At iteration 9700 -> loss: 0.09309601899729594\n",
      "    At iteration 9800 -> loss: 0.09305986163420492\n",
      "    At iteration 9900 -> loss: 0.09302265333183785\n",
      "    At iteration 10000 -> loss: 0.09306212454995448\n",
      "    At iteration 10100 -> loss: 0.0930562246365485\n",
      "    At iteration 10200 -> loss: 0.09303062296031668\n",
      "    At iteration 10300 -> loss: 0.0930191692802885\n",
      "    At iteration 10400 -> loss: 0.09300258542971365\n",
      "    At iteration 10500 -> loss: 0.09299995010036702\n",
      "    At iteration 10600 -> loss: 0.09298203337992293\n",
      "    At iteration 10700 -> loss: 0.09296372535282683\n",
      "    At iteration 10800 -> loss: 0.09293188923992086\n",
      "    At iteration 10900 -> loss: 0.09299771670651352\n",
      "    At iteration 11000 -> loss: 0.09297748343355607\n",
      "    At iteration 11100 -> loss: 0.09295154127619004\n",
      "    At iteration 11200 -> loss: 0.09298294141938125\n",
      "    At iteration 11300 -> loss: 0.09300306118575234\n",
      "    At iteration 11400 -> loss: 0.09302439587275586\n",
      "    At iteration 11500 -> loss: 0.09299463435734634\n",
      "    At iteration 11600 -> loss: 0.09301095224449482\n",
      "    At iteration 11700 -> loss: 0.09303792748615786\n",
      "    At iteration 11800 -> loss: 0.09302439391344115\n",
      "    At iteration 11900 -> loss: 0.09299106188582337\n",
      "    At iteration 12000 -> loss: 0.09296936491769428\n",
      "    At iteration 12100 -> loss: 0.09298732555703537\n",
      "    At iteration 12200 -> loss: 0.09296192877063254\n",
      "    At iteration 12300 -> loss: 0.09299716828553589\n",
      "    At iteration 12400 -> loss: 0.09300046988731299\n",
      "    At iteration 12500 -> loss: 0.09300985903706514\n",
      "    At iteration 12600 -> loss: 0.0929984432457403\n",
      "    At iteration 12700 -> loss: 0.09302610823504419\n",
      "    At iteration 12800 -> loss: 0.09301426146350866\n",
      "    At iteration 12900 -> loss: 0.09301873494892395\n",
      "    At iteration 13000 -> loss: 0.09304923206785173\n",
      "    At iteration 13100 -> loss: 0.0930532652815959\n",
      "    At iteration 13200 -> loss: 0.09304824886130739\n",
      "    At iteration 13300 -> loss: 0.09303620976689929\n",
      "    At iteration 13400 -> loss: 0.09304385369765791\n",
      "    At iteration 13500 -> loss: 0.0930357015621915\n",
      "    At iteration 13600 -> loss: 0.09306188546252214\n",
      "Staring Epoch 38\n",
      "    At iteration 0 -> loss: 0.08036470157821896\n",
      "    At iteration 100 -> loss: 0.09132471590472183\n",
      "    At iteration 200 -> loss: 0.0905971150592097\n",
      "    At iteration 300 -> loss: 0.0914102966996849\n",
      "    At iteration 400 -> loss: 0.09158811255092829\n",
      "    At iteration 500 -> loss: 0.09258832010955878\n",
      "    At iteration 600 -> loss: 0.09228608811133558\n",
      "    At iteration 700 -> loss: 0.0920868298045692\n",
      "    At iteration 800 -> loss: 0.09228099495508178\n",
      "    At iteration 900 -> loss: 0.09188942406294047\n",
      "    At iteration 1000 -> loss: 0.09273019120127009\n",
      "    At iteration 1100 -> loss: 0.09255148002408828\n",
      "    At iteration 1200 -> loss: 0.0925651292984378\n",
      "    At iteration 1300 -> loss: 0.09250347374801583\n",
      "    At iteration 1400 -> loss: 0.09237785735752024\n",
      "    At iteration 1500 -> loss: 0.09239226659678602\n",
      "    At iteration 1600 -> loss: 0.09243757156230872\n",
      "    At iteration 1700 -> loss: 0.09222301290756031\n",
      "    At iteration 1800 -> loss: 0.09236939161622268\n",
      "    At iteration 1900 -> loss: 0.09245839425712116\n",
      "    At iteration 2000 -> loss: 0.09238790992553546\n",
      "    At iteration 2100 -> loss: 0.09238785172382293\n",
      "    At iteration 2200 -> loss: 0.09242017903048885\n",
      "    At iteration 2300 -> loss: 0.09235847938984752\n",
      "    At iteration 2400 -> loss: 0.0923185223192264\n",
      "    At iteration 2500 -> loss: 0.0922629989160387\n",
      "    At iteration 2600 -> loss: 0.09244719380999741\n",
      "    At iteration 2700 -> loss: 0.09252439470019456\n",
      "    At iteration 2800 -> loss: 0.0924623129131181\n",
      "    At iteration 2900 -> loss: 0.092506578514797\n",
      "    At iteration 3000 -> loss: 0.09255415596463037\n",
      "    At iteration 3100 -> loss: 0.09266156702490597\n",
      "    At iteration 3200 -> loss: 0.09275031998852297\n",
      "    At iteration 3300 -> loss: 0.09270225582148661\n",
      "    At iteration 3400 -> loss: 0.09269248449108909\n",
      "    At iteration 3500 -> loss: 0.09268669514416734\n",
      "    At iteration 3600 -> loss: 0.09293068244222549\n",
      "    At iteration 3700 -> loss: 0.09291250288959506\n",
      "    At iteration 3800 -> loss: 0.09278924791267695\n",
      "    At iteration 3900 -> loss: 0.0927406574997524\n",
      "    At iteration 4000 -> loss: 0.09266609678227633\n",
      "    At iteration 4100 -> loss: 0.09263631641074288\n",
      "    At iteration 4200 -> loss: 0.09266297729912859\n",
      "    At iteration 4300 -> loss: 0.09263613895493578\n",
      "    At iteration 4400 -> loss: 0.09264040681062133\n",
      "    At iteration 4500 -> loss: 0.09277635093435835\n",
      "    At iteration 4600 -> loss: 0.09276071688415735\n",
      "    At iteration 4700 -> loss: 0.09290949030301163\n",
      "    At iteration 4800 -> loss: 0.09287367741539634\n",
      "    At iteration 4900 -> loss: 0.09287488952400902\n",
      "    At iteration 5000 -> loss: 0.09287451233796937\n",
      "    At iteration 5100 -> loss: 0.09276973070951876\n",
      "    At iteration 5200 -> loss: 0.09272338597789266\n",
      "    At iteration 5300 -> loss: 0.09272043136592002\n",
      "    At iteration 5400 -> loss: 0.0927126146931327\n",
      "    At iteration 5500 -> loss: 0.09265775334961653\n",
      "    At iteration 5600 -> loss: 0.09276040884579273\n",
      "    At iteration 5700 -> loss: 0.09286841750729916\n",
      "    At iteration 5800 -> loss: 0.09282735205781474\n",
      "    At iteration 5900 -> loss: 0.09277910974746162\n",
      "    At iteration 6000 -> loss: 0.09272429213550543\n",
      "    At iteration 6100 -> loss: 0.0927617890410508\n",
      "    At iteration 6200 -> loss: 0.09285373585529216\n",
      "    At iteration 6300 -> loss: 0.09280740939030738\n",
      "    At iteration 6400 -> loss: 0.09279610679687204\n",
      "    At iteration 6500 -> loss: 0.0928292442114922\n",
      "    At iteration 6600 -> loss: 0.09292699457445382\n",
      "    At iteration 6700 -> loss: 0.09293686120264712\n",
      "    At iteration 6800 -> loss: 0.09305020733413906\n",
      "    At iteration 6900 -> loss: 0.09302183467877677\n",
      "    At iteration 7000 -> loss: 0.09303991252909759\n",
      "    At iteration 7100 -> loss: 0.09309071123807118\n",
      "    At iteration 7200 -> loss: 0.09306682915397284\n",
      "    At iteration 7300 -> loss: 0.09315526300014187\n",
      "    At iteration 7400 -> loss: 0.09309448788870511\n",
      "    At iteration 7500 -> loss: 0.09308591638408105\n",
      "    At iteration 7600 -> loss: 0.09304483270422638\n",
      "    At iteration 7700 -> loss: 0.09301241726238144\n",
      "    At iteration 7800 -> loss: 0.09301713216813612\n",
      "    At iteration 7900 -> loss: 0.09298355278251547\n",
      "    At iteration 8000 -> loss: 0.09297798431579007\n",
      "    At iteration 8100 -> loss: 0.09305854376834788\n",
      "    At iteration 8200 -> loss: 0.09303898729670941\n",
      "    At iteration 8300 -> loss: 0.09317674625934588\n",
      "    At iteration 8400 -> loss: 0.09319248385119602\n",
      "    At iteration 8500 -> loss: 0.0932383190452584\n",
      "    At iteration 8600 -> loss: 0.09323983782010564\n",
      "    At iteration 8700 -> loss: 0.09320740328763194\n",
      "    At iteration 8800 -> loss: 0.09319556566799368\n",
      "    At iteration 8900 -> loss: 0.09317293909951398\n",
      "    At iteration 9000 -> loss: 0.09315568259782746\n",
      "    At iteration 9100 -> loss: 0.09316857579781453\n",
      "    At iteration 9200 -> loss: 0.09324106367957156\n",
      "    At iteration 9300 -> loss: 0.09322530480503087\n",
      "    At iteration 9400 -> loss: 0.09319850153998449\n",
      "    At iteration 9500 -> loss: 0.09318904350915003\n",
      "    At iteration 9600 -> loss: 0.09317284776490763\n",
      "    At iteration 9700 -> loss: 0.09316122307328302\n",
      "    At iteration 9800 -> loss: 0.09313940180168434\n",
      "    At iteration 9900 -> loss: 0.09312657034921612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10000 -> loss: 0.09314309293869348\n",
      "    At iteration 10100 -> loss: 0.09312123273714738\n",
      "    At iteration 10200 -> loss: 0.09311025321256804\n",
      "    At iteration 10300 -> loss: 0.09307723931237369\n",
      "    At iteration 10400 -> loss: 0.09306722852262858\n",
      "    At iteration 10500 -> loss: 0.09304965260772821\n",
      "    At iteration 10600 -> loss: 0.09305413549941823\n",
      "    At iteration 10700 -> loss: 0.09331095461168071\n",
      "    At iteration 10800 -> loss: 0.09331315223938857\n",
      "    At iteration 10900 -> loss: 0.09336370968358246\n",
      "    At iteration 11000 -> loss: 0.09332216436535838\n",
      "    At iteration 11100 -> loss: 0.09329476205947992\n",
      "    At iteration 11200 -> loss: 0.09327608522164423\n",
      "    At iteration 11300 -> loss: 0.0932820566541445\n",
      "    At iteration 11400 -> loss: 0.09327614633522635\n",
      "    At iteration 11500 -> loss: 0.09327241459920198\n",
      "    At iteration 11600 -> loss: 0.0932352307660846\n",
      "    At iteration 11700 -> loss: 0.09325751372804689\n",
      "    At iteration 11800 -> loss: 0.09323462814860332\n",
      "    At iteration 11900 -> loss: 0.09324494632158893\n",
      "    At iteration 12000 -> loss: 0.0932828314023227\n",
      "    At iteration 12100 -> loss: 0.09325081050736388\n",
      "    At iteration 12200 -> loss: 0.09324722168166584\n",
      "    At iteration 12300 -> loss: 0.09325275754309037\n",
      "    At iteration 12400 -> loss: 0.09321766363736994\n",
      "    At iteration 12500 -> loss: 0.09324353604400096\n",
      "    At iteration 12600 -> loss: 0.09324002217911234\n",
      "    At iteration 12700 -> loss: 0.09324996597433091\n",
      "    At iteration 12800 -> loss: 0.09324198764576927\n",
      "    At iteration 12900 -> loss: 0.09327260172174306\n",
      "    At iteration 13000 -> loss: 0.09326227852403451\n",
      "    At iteration 13100 -> loss: 0.09324033071904358\n",
      "    At iteration 13200 -> loss: 0.09322773214946921\n",
      "    At iteration 13300 -> loss: 0.09319269487970779\n",
      "    At iteration 13400 -> loss: 0.0931716606456869\n",
      "    At iteration 13500 -> loss: 0.09314574686936747\n",
      "    At iteration 13600 -> loss: 0.09313626370881709\n",
      "Staring Epoch 39\n",
      "    At iteration 0 -> loss: 0.08115194935817271\n",
      "    At iteration 100 -> loss: 0.09036085710095625\n",
      "    At iteration 200 -> loss: 0.08954202168289191\n",
      "    At iteration 300 -> loss: 0.09065852222876808\n",
      "    At iteration 400 -> loss: 0.09161927184866983\n",
      "    At iteration 500 -> loss: 0.09219166470599764\n",
      "    At iteration 600 -> loss: 0.09205524892276512\n",
      "    At iteration 700 -> loss: 0.09181980841780901\n",
      "    At iteration 800 -> loss: 0.09196436267649907\n",
      "    At iteration 900 -> loss: 0.09164536556173024\n",
      "    At iteration 1000 -> loss: 0.09151828535417292\n",
      "    At iteration 1100 -> loss: 0.09176985686714985\n",
      "    At iteration 1200 -> loss: 0.09152999162946825\n",
      "    At iteration 1300 -> loss: 0.09146448617841667\n",
      "    At iteration 1400 -> loss: 0.09148338191124428\n",
      "    At iteration 1500 -> loss: 0.0917275022540844\n",
      "    At iteration 1600 -> loss: 0.0916637599414972\n",
      "    At iteration 1700 -> loss: 0.09181686041427603\n",
      "    At iteration 1800 -> loss: 0.09164036088295144\n",
      "    At iteration 1900 -> loss: 0.09179061508548716\n",
      "    At iteration 2000 -> loss: 0.09173066069489845\n",
      "    At iteration 2100 -> loss: 0.09202472752784722\n",
      "    At iteration 2200 -> loss: 0.09200689292329087\n",
      "    At iteration 2300 -> loss: 0.09224932779958006\n",
      "    At iteration 2400 -> loss: 0.09221570620186012\n",
      "    At iteration 2500 -> loss: 0.09222977688787408\n",
      "    At iteration 2600 -> loss: 0.09216127357220537\n",
      "    At iteration 2700 -> loss: 0.09206993656494496\n",
      "    At iteration 2800 -> loss: 0.09207517360243685\n",
      "    At iteration 2900 -> loss: 0.09224558541455478\n",
      "    At iteration 3000 -> loss: 0.09226596937590725\n",
      "    At iteration 3100 -> loss: 0.09216269959599754\n",
      "    At iteration 3200 -> loss: 0.09213867034229259\n",
      "    At iteration 3300 -> loss: 0.09208057931664225\n",
      "    At iteration 3400 -> loss: 0.09207987871768139\n",
      "    At iteration 3500 -> loss: 0.0920703885831281\n",
      "    At iteration 3600 -> loss: 0.09204648735947496\n",
      "    At iteration 3700 -> loss: 0.09215961208325588\n",
      "    At iteration 3800 -> loss: 0.09215846385331816\n",
      "    At iteration 3900 -> loss: 0.09222484744467743\n",
      "    At iteration 4000 -> loss: 0.09218138337673207\n",
      "    At iteration 4100 -> loss: 0.09217409100105502\n",
      "    At iteration 4200 -> loss: 0.09215041476949352\n",
      "    At iteration 4300 -> loss: 0.09215033741452662\n",
      "    At iteration 4400 -> loss: 0.0920801703742239\n",
      "    At iteration 4500 -> loss: 0.09208937235176846\n",
      "    At iteration 4600 -> loss: 0.09207034614702111\n",
      "    At iteration 4700 -> loss: 0.09204192295656106\n",
      "    At iteration 4800 -> loss: 0.09195632616174788\n",
      "    At iteration 4900 -> loss: 0.09202697250135294\n",
      "    At iteration 5000 -> loss: 0.09213613401074579\n",
      "    At iteration 5100 -> loss: 0.09233111293041199\n",
      "    At iteration 5200 -> loss: 0.09232445698636833\n",
      "    At iteration 5300 -> loss: 0.09235444019851008\n",
      "    At iteration 5400 -> loss: 0.09273326479239237\n",
      "    At iteration 5500 -> loss: 0.09275069999327643\n",
      "    At iteration 5600 -> loss: 0.0926849806878186\n",
      "    At iteration 5700 -> loss: 0.09263741298904485\n",
      "    At iteration 5800 -> loss: 0.0926588427340444\n",
      "    At iteration 5900 -> loss: 0.09272011834386401\n",
      "    At iteration 6000 -> loss: 0.09270455658772508\n",
      "    At iteration 6100 -> loss: 0.09271568913049298\n",
      "    At iteration 6200 -> loss: 0.09267564452464284\n",
      "    At iteration 6300 -> loss: 0.09265904605135937\n",
      "    At iteration 6400 -> loss: 0.09263140006450203\n",
      "    At iteration 6500 -> loss: 0.09261959873410897\n",
      "    At iteration 6600 -> loss: 0.09261239920624541\n",
      "    At iteration 6700 -> loss: 0.09257411894292945\n",
      "    At iteration 6800 -> loss: 0.09258694731055951\n",
      "    At iteration 6900 -> loss: 0.0926797527262839\n",
      "    At iteration 7000 -> loss: 0.09264939549108375\n",
      "    At iteration 7100 -> loss: 0.0930284359949594\n",
      "    At iteration 7200 -> loss: 0.09300533427700644\n",
      "    At iteration 7300 -> loss: 0.09300051288357003\n",
      "    At iteration 7400 -> loss: 0.09301539462161877\n",
      "    At iteration 7500 -> loss: 0.09299026908541634\n",
      "    At iteration 7600 -> loss: 0.09297266029073799\n",
      "    At iteration 7700 -> loss: 0.0930801036957026\n",
      "    At iteration 7800 -> loss: 0.09304648454674047\n",
      "    At iteration 7900 -> loss: 0.09318189488472352\n",
      "    At iteration 8000 -> loss: 0.09315886852067669\n",
      "    At iteration 8100 -> loss: 0.09322018161465925\n",
      "    At iteration 8200 -> loss: 0.09318939953131737\n",
      "    At iteration 8300 -> loss: 0.09316788570598386\n",
      "    At iteration 8400 -> loss: 0.09315392634443093\n",
      "    At iteration 8500 -> loss: 0.09331204496429854\n",
      "    At iteration 8600 -> loss: 0.09329215008183736\n",
      "    At iteration 8700 -> loss: 0.09326494915877616\n",
      "    At iteration 8800 -> loss: 0.0932843136318373\n",
      "    At iteration 8900 -> loss: 0.09328519507634624\n",
      "    At iteration 9000 -> loss: 0.09328421122450693\n",
      "    At iteration 9100 -> loss: 0.09331588031849747\n",
      "    At iteration 9200 -> loss: 0.09327968177687303\n",
      "    At iteration 9300 -> loss: 0.09325383008900676\n",
      "    At iteration 9400 -> loss: 0.09334945773073652\n",
      "    At iteration 9500 -> loss: 0.0933541654907685\n",
      "    At iteration 9600 -> loss: 0.09330351499600674\n",
      "    At iteration 9700 -> loss: 0.09332528819715902\n",
      "    At iteration 9800 -> loss: 0.09331047078173173\n",
      "    At iteration 9900 -> loss: 0.0933387356830763\n",
      "    At iteration 10000 -> loss: 0.09330382630485368\n",
      "    At iteration 10100 -> loss: 0.09331323298322046\n",
      "    At iteration 10200 -> loss: 0.09330741369129675\n",
      "    At iteration 10300 -> loss: 0.09327957833075044\n",
      "    At iteration 10400 -> loss: 0.09324225575478566\n",
      "    At iteration 10500 -> loss: 0.09330926330381538\n",
      "    At iteration 10600 -> loss: 0.09329549243243038\n",
      "    At iteration 10700 -> loss: 0.09328348595172704\n",
      "    At iteration 10800 -> loss: 0.09326117990931476\n",
      "    At iteration 10900 -> loss: 0.09325481335640856\n",
      "    At iteration 11000 -> loss: 0.09323091757875722\n",
      "    At iteration 11100 -> loss: 0.09320732549448789\n",
      "    At iteration 11200 -> loss: 0.0931611172339077\n",
      "    At iteration 11300 -> loss: 0.09326971441759503\n",
      "    At iteration 11400 -> loss: 0.09323627824996135\n",
      "    At iteration 11500 -> loss: 0.09319002558348116\n",
      "    At iteration 11600 -> loss: 0.09320410304811921\n",
      "    At iteration 11700 -> loss: 0.09319698721410254\n",
      "    At iteration 11800 -> loss: 0.0931738786067003\n",
      "    At iteration 11900 -> loss: 0.09320886214916985\n",
      "    At iteration 12000 -> loss: 0.09317711912065105\n",
      "    At iteration 12100 -> loss: 0.09319589734641107\n",
      "    At iteration 12200 -> loss: 0.0931757718163792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12300 -> loss: 0.0931639173835234\n",
      "    At iteration 12400 -> loss: 0.09317784743011848\n",
      "    At iteration 12500 -> loss: 0.09316892897125105\n",
      "    At iteration 12600 -> loss: 0.09324348110298245\n",
      "    At iteration 12700 -> loss: 0.0932116036305703\n",
      "    At iteration 12800 -> loss: 0.09322501893336214\n",
      "    At iteration 12900 -> loss: 0.09320254031890945\n",
      "    At iteration 13000 -> loss: 0.09320056255992838\n",
      "    At iteration 13100 -> loss: 0.09320263927924383\n",
      "    At iteration 13200 -> loss: 0.09317942807102213\n",
      "    At iteration 13300 -> loss: 0.0931559868227074\n",
      "    At iteration 13400 -> loss: 0.09313548856865432\n",
      "    At iteration 13500 -> loss: 0.09312494330276919\n",
      "    At iteration 13600 -> loss: 0.09311826608221922\n",
      "Staring Epoch 40\n",
      "    At iteration 0 -> loss: 0.08734395424835384\n",
      "    At iteration 100 -> loss: 0.09697349865644576\n",
      "    At iteration 200 -> loss: 0.09427307560233272\n",
      "    At iteration 300 -> loss: 0.09416213021281288\n",
      "    At iteration 400 -> loss: 0.09352857837745107\n",
      "    At iteration 500 -> loss: 0.09307392739719729\n",
      "    At iteration 600 -> loss: 0.092812104859635\n",
      "    At iteration 700 -> loss: 0.09235773558802525\n",
      "    At iteration 800 -> loss: 0.0925032647929496\n",
      "    At iteration 900 -> loss: 0.09304281991797879\n",
      "    At iteration 1000 -> loss: 0.09351093198059066\n",
      "    At iteration 1100 -> loss: 0.09390928310170234\n",
      "    At iteration 1200 -> loss: 0.09362576301687435\n",
      "    At iteration 1300 -> loss: 0.09337791495502279\n",
      "    At iteration 1400 -> loss: 0.09310392680998679\n",
      "    At iteration 1500 -> loss: 0.09322504431330647\n",
      "    At iteration 1600 -> loss: 0.09337702627027389\n",
      "    At iteration 1700 -> loss: 0.09346818704081934\n",
      "    At iteration 1800 -> loss: 0.09346256189033492\n",
      "    At iteration 1900 -> loss: 0.09534242653926471\n",
      "    At iteration 2000 -> loss: 0.09545455796014837\n",
      "    At iteration 2100 -> loss: 0.09598081773707504\n",
      "    At iteration 2200 -> loss: 0.09603673973610397\n",
      "    At iteration 2300 -> loss: 0.09585606593775624\n",
      "    At iteration 2400 -> loss: 0.09570092385334274\n",
      "    At iteration 2500 -> loss: 0.09560954621197751\n",
      "    At iteration 2600 -> loss: 0.09537102134699064\n",
      "    At iteration 2700 -> loss: 0.09514884388822158\n",
      "    At iteration 2800 -> loss: 0.09504138630007544\n",
      "    At iteration 2900 -> loss: 0.09503044804283774\n",
      "    At iteration 3000 -> loss: 0.09486809157008358\n",
      "    At iteration 3100 -> loss: 0.09478604406348222\n",
      "    At iteration 3200 -> loss: 0.09471752719853399\n",
      "    At iteration 3300 -> loss: 0.09465161254045908\n",
      "    At iteration 3400 -> loss: 0.09459927352620981\n",
      "    At iteration 3500 -> loss: 0.09452444591372232\n",
      "    At iteration 3600 -> loss: 0.09465185409979504\n",
      "    At iteration 3700 -> loss: 0.09451515976251723\n",
      "    At iteration 3800 -> loss: 0.0944463882439009\n",
      "    At iteration 3900 -> loss: 0.09435349535981347\n",
      "    At iteration 4000 -> loss: 0.09422955220475643\n",
      "    At iteration 4100 -> loss: 0.0941769521892971\n",
      "    At iteration 4200 -> loss: 0.09414248461225859\n",
      "    At iteration 4300 -> loss: 0.09426198123367084\n",
      "    At iteration 4400 -> loss: 0.09420981295560014\n",
      "    At iteration 4500 -> loss: 0.0941268581997824\n",
      "    At iteration 4600 -> loss: 0.09405016947226581\n",
      "    At iteration 4700 -> loss: 0.09395901695239753\n",
      "    At iteration 4800 -> loss: 0.09426543038258108\n",
      "    At iteration 4900 -> loss: 0.09432461049622984\n",
      "    At iteration 5000 -> loss: 0.09423102152633274\n",
      "    At iteration 5100 -> loss: 0.0941911629032987\n",
      "    At iteration 5200 -> loss: 0.09423582620345515\n",
      "    At iteration 5300 -> loss: 0.09428246187298923\n",
      "    At iteration 5400 -> loss: 0.09421526372503579\n",
      "    At iteration 5500 -> loss: 0.09416527050681935\n",
      "    At iteration 5600 -> loss: 0.09409081319628129\n",
      "    At iteration 5700 -> loss: 0.09402918932481145\n",
      "    At iteration 5800 -> loss: 0.09405203211146207\n",
      "    At iteration 5900 -> loss: 0.09397775992866765\n",
      "    At iteration 6000 -> loss: 0.09392484829860281\n",
      "    At iteration 6100 -> loss: 0.09385039597997011\n",
      "    At iteration 6200 -> loss: 0.093784849610175\n",
      "    At iteration 6300 -> loss: 0.0937806787301554\n",
      "    At iteration 6400 -> loss: 0.09383990641090437\n",
      "    At iteration 6500 -> loss: 0.09381813079578219\n",
      "    At iteration 6600 -> loss: 0.09392971077575621\n",
      "    At iteration 6700 -> loss: 0.0938780011514654\n",
      "    At iteration 6800 -> loss: 0.09380807616249343\n",
      "    At iteration 6900 -> loss: 0.0937993365567174\n",
      "    At iteration 7000 -> loss: 0.093773042760773\n",
      "    At iteration 7100 -> loss: 0.09373766243381239\n",
      "    At iteration 7200 -> loss: 0.09372691789871668\n",
      "    At iteration 7300 -> loss: 0.09369195383824457\n",
      "    At iteration 7400 -> loss: 0.09366332628144146\n",
      "    At iteration 7500 -> loss: 0.0936681934312535\n",
      "    At iteration 7600 -> loss: 0.0936324264221594\n",
      "    At iteration 7700 -> loss: 0.09365284658451468\n",
      "    At iteration 7800 -> loss: 0.09360525355899946\n",
      "    At iteration 7900 -> loss: 0.09360844526388555\n",
      "    At iteration 8000 -> loss: 0.09366312337167677\n",
      "    At iteration 8100 -> loss: 0.0936417371557351\n",
      "    At iteration 8200 -> loss: 0.09360522431737296\n",
      "    At iteration 8300 -> loss: 0.09358513030063594\n",
      "    At iteration 8400 -> loss: 0.09358974661473488\n",
      "    At iteration 8500 -> loss: 0.09356602829675588\n",
      "    At iteration 8600 -> loss: 0.0935316715241223\n",
      "    At iteration 8700 -> loss: 0.09349375064773184\n",
      "    At iteration 8800 -> loss: 0.09346994956331565\n",
      "    At iteration 8900 -> loss: 0.09348022253014361\n",
      "    At iteration 9000 -> loss: 0.09349747084174684\n",
      "    At iteration 9100 -> loss: 0.09351737479874783\n",
      "    At iteration 9200 -> loss: 0.09350552426121822\n",
      "    At iteration 9300 -> loss: 0.09347018316348032\n",
      "    At iteration 9400 -> loss: 0.09344745855027643\n",
      "    At iteration 9500 -> loss: 0.09346809656390431\n",
      "    At iteration 9600 -> loss: 0.09345267113282708\n",
      "    At iteration 9700 -> loss: 0.09343822564737672\n",
      "    At iteration 9800 -> loss: 0.09342692568068704\n",
      "    At iteration 9900 -> loss: 0.09342236939276696\n",
      "    At iteration 10000 -> loss: 0.09337979736086165\n",
      "    At iteration 10100 -> loss: 0.09342154902278199\n",
      "    At iteration 10200 -> loss: 0.09342490688191334\n",
      "    At iteration 10300 -> loss: 0.093474005769718\n",
      "    At iteration 10400 -> loss: 0.09345945232436885\n",
      "    At iteration 10500 -> loss: 0.09348245421151366\n",
      "    At iteration 10600 -> loss: 0.09345573918497786\n",
      "    At iteration 10700 -> loss: 0.09342151769658237\n",
      "    At iteration 10800 -> loss: 0.09343246079160829\n",
      "    At iteration 10900 -> loss: 0.0934153979743393\n",
      "    At iteration 11000 -> loss: 0.09337455851046655\n",
      "    At iteration 11100 -> loss: 0.09339087904837318\n",
      "    At iteration 11200 -> loss: 0.09337361659476835\n",
      "    At iteration 11300 -> loss: 0.09337918342303166\n",
      "    At iteration 11400 -> loss: 0.09334668547662957\n",
      "    At iteration 11500 -> loss: 0.09344832669156151\n",
      "    At iteration 11600 -> loss: 0.09342895297744841\n",
      "    At iteration 11700 -> loss: 0.09340049087194695\n",
      "    At iteration 11800 -> loss: 0.09339584702277469\n",
      "    At iteration 11900 -> loss: 0.09336530677212046\n",
      "    At iteration 12000 -> loss: 0.0933874029826411\n",
      "    At iteration 12100 -> loss: 0.09336476933914753\n",
      "    At iteration 12200 -> loss: 0.0933419907168821\n",
      "    At iteration 12300 -> loss: 0.09331475762632283\n",
      "    At iteration 12400 -> loss: 0.09331435560023187\n",
      "    At iteration 12500 -> loss: 0.09331308459739251\n",
      "    At iteration 12600 -> loss: 0.09332054788233837\n",
      "    At iteration 12700 -> loss: 0.09329966780697888\n",
      "    At iteration 12800 -> loss: 0.09327494057981345\n",
      "    At iteration 12900 -> loss: 0.09325918754988331\n",
      "    At iteration 13000 -> loss: 0.0932618766421828\n",
      "    At iteration 13100 -> loss: 0.09323162083472177\n",
      "    At iteration 13200 -> loss: 0.09320574227481584\n",
      "    At iteration 13300 -> loss: 0.09318614764177285\n",
      "    At iteration 13400 -> loss: 0.09316352391338577\n",
      "    At iteration 13500 -> loss: 0.09314273327768431\n",
      "    At iteration 13600 -> loss: 0.09313242340321062\n",
      "Staring Epoch 41\n",
      "    At iteration 0 -> loss: 0.08690527570433915\n",
      "    At iteration 100 -> loss: 0.10345667031602526\n",
      "    At iteration 200 -> loss: 0.09635486617504394\n",
      "    At iteration 300 -> loss: 0.09647585016502129\n",
      "    At iteration 400 -> loss: 0.09501902913165472\n",
      "    At iteration 500 -> loss: 0.09497474627314272\n",
      "    At iteration 600 -> loss: 0.09389946911780074\n",
      "    At iteration 700 -> loss: 0.0934963550318357\n",
      "    At iteration 800 -> loss: 0.09323278945043637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 900 -> loss: 0.09319358993525545\n",
      "    At iteration 1000 -> loss: 0.09318861272177167\n",
      "    At iteration 1100 -> loss: 0.09296599871636226\n",
      "    At iteration 1200 -> loss: 0.09269087395789816\n",
      "    At iteration 1300 -> loss: 0.09279232112998277\n",
      "    At iteration 1400 -> loss: 0.09275852281685755\n",
      "    At iteration 1500 -> loss: 0.09273208589627581\n",
      "    At iteration 1600 -> loss: 0.09244929194785827\n",
      "    At iteration 1700 -> loss: 0.092478629055268\n",
      "    At iteration 1800 -> loss: 0.09244753853963485\n",
      "    At iteration 1900 -> loss: 0.09234692603919345\n",
      "    At iteration 2000 -> loss: 0.09226140222276455\n",
      "    At iteration 2100 -> loss: 0.09211085962861619\n",
      "    At iteration 2200 -> loss: 0.09205729104969224\n",
      "    At iteration 2300 -> loss: 0.09205082664020342\n",
      "    At iteration 2400 -> loss: 0.09198259539714816\n",
      "    At iteration 2500 -> loss: 0.09189535656504956\n",
      "    At iteration 2600 -> loss: 0.09197735621825397\n",
      "    At iteration 2700 -> loss: 0.09188327886501667\n",
      "    At iteration 2800 -> loss: 0.09191531656794326\n",
      "    At iteration 2900 -> loss: 0.09186111674945234\n",
      "    At iteration 3000 -> loss: 0.09190701044424482\n",
      "    At iteration 3100 -> loss: 0.09221807147692258\n",
      "    At iteration 3200 -> loss: 0.09212556484508634\n",
      "    At iteration 3300 -> loss: 0.0920875758132043\n",
      "    At iteration 3400 -> loss: 0.0920776950903291\n",
      "    At iteration 3500 -> loss: 0.0923855820310329\n",
      "    At iteration 3600 -> loss: 0.09239830410002235\n",
      "    At iteration 3700 -> loss: 0.0923234850186717\n",
      "    At iteration 3800 -> loss: 0.09242453382554944\n",
      "    At iteration 3900 -> loss: 0.09240610096839519\n",
      "    At iteration 4000 -> loss: 0.09238877694674953\n",
      "    At iteration 4100 -> loss: 0.09254993146519849\n",
      "    At iteration 4200 -> loss: 0.09266765783718231\n",
      "    At iteration 4300 -> loss: 0.09269884927216374\n",
      "    At iteration 4400 -> loss: 0.09299386292969897\n",
      "    At iteration 4500 -> loss: 0.0930027765714822\n",
      "    At iteration 4600 -> loss: 0.09311114760493605\n",
      "    At iteration 4700 -> loss: 0.09309278192412034\n",
      "    At iteration 4800 -> loss: 0.0931963621015608\n",
      "    At iteration 4900 -> loss: 0.0931734411626985\n",
      "    At iteration 5000 -> loss: 0.093101854777059\n",
      "    At iteration 5100 -> loss: 0.09309378461923619\n",
      "    At iteration 5200 -> loss: 0.09304560857131182\n",
      "    At iteration 5300 -> loss: 0.09303471569551887\n",
      "    At iteration 5400 -> loss: 0.09298479247083688\n",
      "    At iteration 5500 -> loss: 0.09305201434506362\n",
      "    At iteration 5600 -> loss: 0.09309630560398735\n",
      "    At iteration 5700 -> loss: 0.09311939034265605\n",
      "    At iteration 5800 -> loss: 0.09313184500461193\n",
      "    At iteration 5900 -> loss: 0.09305975104431356\n",
      "    At iteration 6000 -> loss: 0.09300812485958658\n",
      "    At iteration 6100 -> loss: 0.09299686346079826\n",
      "    At iteration 6200 -> loss: 0.09300895313406536\n",
      "    At iteration 6300 -> loss: 0.09302903717725244\n",
      "    At iteration 6400 -> loss: 0.0930713525595365\n",
      "    At iteration 6500 -> loss: 0.09351959593502908\n",
      "    At iteration 6600 -> loss: 0.09349312724792277\n",
      "    At iteration 6700 -> loss: 0.09345873761982501\n",
      "    At iteration 6800 -> loss: 0.09343680116759391\n",
      "    At iteration 6900 -> loss: 0.09351079457707376\n",
      "    At iteration 7000 -> loss: 0.09343889248608975\n",
      "    At iteration 7100 -> loss: 0.09341523672932106\n",
      "    At iteration 7200 -> loss: 0.09337422758007002\n",
      "    At iteration 7300 -> loss: 0.09336217657991767\n",
      "    At iteration 7400 -> loss: 0.09329937122614798\n",
      "    At iteration 7500 -> loss: 0.09328364838242213\n",
      "    At iteration 7600 -> loss: 0.09324612824536417\n",
      "    At iteration 7700 -> loss: 0.09325499498331911\n",
      "    At iteration 7800 -> loss: 0.09335688719793866\n",
      "    At iteration 7900 -> loss: 0.09336250629860651\n",
      "    At iteration 8000 -> loss: 0.0933604218491532\n",
      "    At iteration 8100 -> loss: 0.09337277072008784\n",
      "    At iteration 8200 -> loss: 0.09336951293043876\n",
      "    At iteration 8300 -> loss: 0.09342353003563862\n",
      "    At iteration 8400 -> loss: 0.0933711070202339\n",
      "    At iteration 8500 -> loss: 0.09334544302097784\n",
      "    At iteration 8600 -> loss: 0.09333345486019218\n",
      "    At iteration 8700 -> loss: 0.09335830666618158\n",
      "    At iteration 8800 -> loss: 0.09336004689123027\n",
      "    At iteration 8900 -> loss: 0.09334416988665548\n",
      "    At iteration 9000 -> loss: 0.09338533994352342\n",
      "    At iteration 9100 -> loss: 0.0933875481016255\n",
      "    At iteration 9200 -> loss: 0.09338218836857742\n",
      "    At iteration 9300 -> loss: 0.09345909574121267\n",
      "    At iteration 9400 -> loss: 0.09350091536062619\n",
      "    At iteration 9500 -> loss: 0.09347926157175099\n",
      "    At iteration 9600 -> loss: 0.09349445696814446\n",
      "    At iteration 9700 -> loss: 0.09349723611042869\n",
      "    At iteration 9800 -> loss: 0.0934922352991268\n",
      "    At iteration 9900 -> loss: 0.09349777321112245\n",
      "    At iteration 10000 -> loss: 0.09350284425481721\n",
      "    At iteration 10100 -> loss: 0.0934812437895249\n",
      "    At iteration 10200 -> loss: 0.09349628106541322\n",
      "    At iteration 10300 -> loss: 0.09345180661309686\n",
      "    At iteration 10400 -> loss: 0.09340261904357956\n",
      "    At iteration 10500 -> loss: 0.0934043333031805\n",
      "    At iteration 10600 -> loss: 0.09339214660138048\n",
      "    At iteration 10700 -> loss: 0.09338430726063443\n",
      "    At iteration 10800 -> loss: 0.09338361430460176\n",
      "    At iteration 10900 -> loss: 0.09336094537353357\n",
      "    At iteration 11000 -> loss: 0.09335463256998756\n",
      "    At iteration 11100 -> loss: 0.09333759820227881\n",
      "    At iteration 11200 -> loss: 0.093312100642474\n",
      "    At iteration 11300 -> loss: 0.09330054254779116\n",
      "    At iteration 11400 -> loss: 0.09325402639055771\n",
      "    At iteration 11500 -> loss: 0.09324266872969982\n",
      "    At iteration 11600 -> loss: 0.09327115904180065\n",
      "    At iteration 11700 -> loss: 0.09324787502973075\n",
      "    At iteration 11800 -> loss: 0.09322071115032177\n",
      "    At iteration 11900 -> loss: 0.09319163572703551\n",
      "    At iteration 12000 -> loss: 0.09317044233643723\n",
      "    At iteration 12100 -> loss: 0.09314709088732741\n",
      "    At iteration 12200 -> loss: 0.09314272749453133\n",
      "    At iteration 12300 -> loss: 0.09311165602555582\n",
      "    At iteration 12400 -> loss: 0.09320013709784936\n",
      "    At iteration 12500 -> loss: 0.09324045717261664\n",
      "    At iteration 12600 -> loss: 0.0932202403916828\n",
      "    At iteration 12700 -> loss: 0.09321560833866574\n",
      "    At iteration 12800 -> loss: 0.09321254304205195\n",
      "    At iteration 12900 -> loss: 0.09319754375895341\n",
      "    At iteration 13000 -> loss: 0.09318071858802687\n",
      "    At iteration 13100 -> loss: 0.09317379789801465\n",
      "    At iteration 13200 -> loss: 0.09315975252906367\n",
      "    At iteration 13300 -> loss: 0.09315233716529515\n",
      "    At iteration 13400 -> loss: 0.09315469551856406\n",
      "    At iteration 13500 -> loss: 0.0931341362405239\n",
      "    At iteration 13600 -> loss: 0.09310695690719864\n",
      "Staring Epoch 42\n",
      "    At iteration 0 -> loss: 0.08437278319615871\n",
      "    At iteration 100 -> loss: 0.09774062108720985\n",
      "    At iteration 200 -> loss: 0.0951090097902501\n",
      "    At iteration 300 -> loss: 0.09449822756683741\n",
      "    At iteration 400 -> loss: 0.09327296591350204\n",
      "    At iteration 500 -> loss: 0.09505132762570856\n",
      "    At iteration 600 -> loss: 0.09405669150600651\n",
      "    At iteration 700 -> loss: 0.09398645736032178\n",
      "    At iteration 800 -> loss: 0.09394789574087979\n",
      "    At iteration 900 -> loss: 0.09428685999396143\n",
      "    At iteration 1000 -> loss: 0.09383199215704711\n",
      "    At iteration 1100 -> loss: 0.09346151142313988\n",
      "    At iteration 1200 -> loss: 0.09304828676515792\n",
      "    At iteration 1300 -> loss: 0.09287449877721433\n",
      "    At iteration 1400 -> loss: 0.0927308946033728\n",
      "    At iteration 1500 -> loss: 0.09266460912233324\n",
      "    At iteration 1600 -> loss: 0.09304516313229252\n",
      "    At iteration 1700 -> loss: 0.09327725638694166\n",
      "    At iteration 1800 -> loss: 0.0933004562494789\n",
      "    At iteration 1900 -> loss: 0.09319762339870226\n",
      "    At iteration 2000 -> loss: 0.09316786825037282\n",
      "    At iteration 2100 -> loss: 0.09316800735376674\n",
      "    At iteration 2200 -> loss: 0.09328240330155108\n",
      "    At iteration 2300 -> loss: 0.09360265638274769\n",
      "    At iteration 2400 -> loss: 0.09359703422136062\n",
      "    At iteration 2500 -> loss: 0.09338758494351714\n",
      "    At iteration 2600 -> loss: 0.09334219065741113\n",
      "    At iteration 2700 -> loss: 0.09345416774284068\n",
      "    At iteration 2800 -> loss: 0.09335955503574898\n",
      "    At iteration 2900 -> loss: 0.09334706956708436\n",
      "    At iteration 3000 -> loss: 0.09339358159641564\n",
      "    At iteration 3100 -> loss: 0.09333910277624684\n",
      "    At iteration 3200 -> loss: 0.09327249021225661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3300 -> loss: 0.09318922931213802\n",
      "    At iteration 3400 -> loss: 0.0931581247984931\n",
      "    At iteration 3500 -> loss: 0.0932702816864049\n",
      "    At iteration 3600 -> loss: 0.09315671223540516\n",
      "    At iteration 3700 -> loss: 0.09309589604631363\n",
      "    At iteration 3800 -> loss: 0.09330484624561373\n",
      "    At iteration 3900 -> loss: 0.09327839715416976\n",
      "    At iteration 4000 -> loss: 0.09339171705242362\n",
      "    At iteration 4100 -> loss: 0.09335572762714824\n",
      "    At iteration 4200 -> loss: 0.09327042019214894\n",
      "    At iteration 4300 -> loss: 0.0931943296972783\n",
      "    At iteration 4400 -> loss: 0.09315266827990432\n",
      "    At iteration 4500 -> loss: 0.09384444087097536\n",
      "    At iteration 4600 -> loss: 0.09381078549144752\n",
      "    At iteration 4700 -> loss: 0.09373428889200772\n",
      "    At iteration 4800 -> loss: 0.09373650783036494\n",
      "    At iteration 4900 -> loss: 0.09369078358553108\n",
      "    At iteration 5000 -> loss: 0.09377566127387203\n",
      "    At iteration 5100 -> loss: 0.09371540310323617\n",
      "    At iteration 5200 -> loss: 0.09373037447106707\n",
      "    At iteration 5300 -> loss: 0.09371224329330004\n",
      "    At iteration 5400 -> loss: 0.09372859111442484\n",
      "    At iteration 5500 -> loss: 0.09372552289625162\n",
      "    At iteration 5600 -> loss: 0.0936686245298321\n",
      "    At iteration 5700 -> loss: 0.09359727493880607\n",
      "    At iteration 5800 -> loss: 0.09352825197543314\n",
      "    At iteration 5900 -> loss: 0.09348119928903367\n",
      "    At iteration 6000 -> loss: 0.09345588055252523\n",
      "    At iteration 6100 -> loss: 0.09339197349375164\n",
      "    At iteration 6200 -> loss: 0.09341807728032396\n",
      "    At iteration 6300 -> loss: 0.09338972040925589\n",
      "    At iteration 6400 -> loss: 0.09344434346356455\n",
      "    At iteration 6500 -> loss: 0.09345916179106299\n",
      "    At iteration 6600 -> loss: 0.09350702864642912\n",
      "    At iteration 6700 -> loss: 0.09343531613716778\n",
      "    At iteration 6800 -> loss: 0.09350709769231276\n",
      "    At iteration 6900 -> loss: 0.09351312012047473\n",
      "    At iteration 7000 -> loss: 0.09348876260093576\n",
      "    At iteration 7100 -> loss: 0.09346893639585699\n",
      "    At iteration 7200 -> loss: 0.09341927549793426\n",
      "    At iteration 7300 -> loss: 0.09346626331772474\n",
      "    At iteration 7400 -> loss: 0.09340594296191536\n",
      "    At iteration 7500 -> loss: 0.09349217833502123\n",
      "    At iteration 7600 -> loss: 0.09346870926174732\n",
      "    At iteration 7700 -> loss: 0.0934552668380884\n",
      "    At iteration 7800 -> loss: 0.09348361336604447\n",
      "    At iteration 7900 -> loss: 0.09346271398538801\n",
      "    At iteration 8000 -> loss: 0.09345311817600063\n",
      "    At iteration 8100 -> loss: 0.09349805345446409\n",
      "    At iteration 8200 -> loss: 0.09351888070788432\n",
      "    At iteration 8300 -> loss: 0.09354518904023974\n",
      "    At iteration 8400 -> loss: 0.09349250988913885\n",
      "    At iteration 8500 -> loss: 0.09347863724321036\n",
      "    At iteration 8600 -> loss: 0.09349589379412791\n",
      "    At iteration 8700 -> loss: 0.09344516832148285\n",
      "    At iteration 8800 -> loss: 0.09341499603411747\n",
      "    At iteration 8900 -> loss: 0.09342530983046492\n",
      "    At iteration 9000 -> loss: 0.09340025043643545\n",
      "    At iteration 9100 -> loss: 0.09337371495042374\n",
      "    At iteration 9200 -> loss: 0.09339931612120628\n",
      "    At iteration 9300 -> loss: 0.09340042404977117\n",
      "    At iteration 9400 -> loss: 0.09339835922900011\n",
      "    At iteration 9500 -> loss: 0.09341455185728342\n",
      "    At iteration 9600 -> loss: 0.09340355157869205\n",
      "    At iteration 9700 -> loss: 0.09342477155371637\n",
      "    At iteration 9800 -> loss: 0.09338062810302651\n",
      "    At iteration 9900 -> loss: 0.09335030952802775\n",
      "    At iteration 10000 -> loss: 0.0933254667495621\n",
      "    At iteration 10100 -> loss: 0.09329023385264523\n",
      "    At iteration 10200 -> loss: 0.09329885347235031\n",
      "    At iteration 10300 -> loss: 0.09329191185387373\n",
      "    At iteration 10400 -> loss: 0.0932869230889051\n",
      "    At iteration 10500 -> loss: 0.09327196197258321\n",
      "    At iteration 10600 -> loss: 0.0932346223414974\n",
      "    At iteration 10700 -> loss: 0.09321217468719423\n",
      "    At iteration 10800 -> loss: 0.09317921821298399\n",
      "    At iteration 10900 -> loss: 0.09317325021431214\n",
      "    At iteration 11000 -> loss: 0.09315775975303069\n",
      "    At iteration 11100 -> loss: 0.09318157820506506\n",
      "    At iteration 11200 -> loss: 0.09315162676031208\n",
      "    At iteration 11300 -> loss: 0.09313099786519007\n",
      "    At iteration 11400 -> loss: 0.09310092433628464\n",
      "    At iteration 11500 -> loss: 0.093084682609698\n",
      "    At iteration 11600 -> loss: 0.09304528333158553\n",
      "    At iteration 11700 -> loss: 0.09302478695412511\n",
      "    At iteration 11800 -> loss: 0.09303128122922699\n",
      "    At iteration 11900 -> loss: 0.09313392436178516\n",
      "    At iteration 12000 -> loss: 0.0930985249022801\n",
      "    At iteration 12100 -> loss: 0.09309264840659598\n",
      "    At iteration 12200 -> loss: 0.09308142943279542\n",
      "    At iteration 12300 -> loss: 0.09318117904949039\n",
      "    At iteration 12400 -> loss: 0.093164778071983\n",
      "    At iteration 12500 -> loss: 0.09314095337032906\n",
      "    At iteration 12600 -> loss: 0.09311627473424926\n",
      "    At iteration 12700 -> loss: 0.09313361586024696\n",
      "    At iteration 12800 -> loss: 0.0931611142269459\n",
      "    At iteration 12900 -> loss: 0.09322069810384134\n",
      "    At iteration 13000 -> loss: 0.0932231216180317\n",
      "    At iteration 13100 -> loss: 0.09323318129059359\n",
      "    At iteration 13200 -> loss: 0.09321451680809542\n",
      "    At iteration 13300 -> loss: 0.09321381918910746\n",
      "    At iteration 13400 -> loss: 0.09317870783507008\n",
      "    At iteration 13500 -> loss: 0.09315564642313443\n",
      "    At iteration 13600 -> loss: 0.0931278135812304\n",
      "Staring Epoch 43\n",
      "    At iteration 0 -> loss: 0.10119074257090688\n",
      "    At iteration 100 -> loss: 0.0908123425851493\n",
      "    At iteration 200 -> loss: 0.0911619708523353\n",
      "    At iteration 300 -> loss: 0.09314146610370756\n",
      "    At iteration 400 -> loss: 0.09284734164175491\n",
      "    At iteration 500 -> loss: 0.09385328999694985\n",
      "    At iteration 600 -> loss: 0.0931568223574467\n",
      "    At iteration 700 -> loss: 0.09264528178762531\n",
      "    At iteration 800 -> loss: 0.09233720166970168\n",
      "    At iteration 900 -> loss: 0.09284396682526107\n",
      "    At iteration 1000 -> loss: 0.09251612008893388\n",
      "    At iteration 1100 -> loss: 0.09232549467915274\n",
      "    At iteration 1200 -> loss: 0.09247063449524985\n",
      "    At iteration 1300 -> loss: 0.0928141724105401\n",
      "    At iteration 1400 -> loss: 0.09274000874197466\n",
      "    At iteration 1500 -> loss: 0.09310815630758011\n",
      "    At iteration 1600 -> loss: 0.09306175575556044\n",
      "    At iteration 1700 -> loss: 0.09298175510250341\n",
      "    At iteration 1800 -> loss: 0.09273302479011512\n",
      "    At iteration 1900 -> loss: 0.09289887902243184\n",
      "    At iteration 2000 -> loss: 0.09284192921475043\n",
      "    At iteration 2100 -> loss: 0.09272455775359831\n",
      "    At iteration 2200 -> loss: 0.092611451683102\n",
      "    At iteration 2300 -> loss: 0.09255603594675987\n",
      "    At iteration 2400 -> loss: 0.09244013331213356\n",
      "    At iteration 2500 -> loss: 0.09247286048025678\n",
      "    At iteration 2600 -> loss: 0.09247509885755119\n",
      "    At iteration 2700 -> loss: 0.09278145001406032\n",
      "    At iteration 2800 -> loss: 0.09266348499760771\n",
      "    At iteration 2900 -> loss: 0.0927000704363363\n",
      "    At iteration 3000 -> loss: 0.09292439883760734\n",
      "    At iteration 3100 -> loss: 0.09289678813278496\n",
      "    At iteration 3200 -> loss: 0.09285867740350776\n",
      "    At iteration 3300 -> loss: 0.09277210061200432\n",
      "    At iteration 3400 -> loss: 0.09299212501345958\n",
      "    At iteration 3500 -> loss: 0.09290905138460646\n",
      "    At iteration 3600 -> loss: 0.09289017066063325\n",
      "    At iteration 3700 -> loss: 0.09282749929028665\n",
      "    At iteration 3800 -> loss: 0.09277617416141919\n",
      "    At iteration 3900 -> loss: 0.09303042697961268\n",
      "    At iteration 4000 -> loss: 0.09299326195339465\n",
      "    At iteration 4100 -> loss: 0.0929175030139669\n",
      "    At iteration 4200 -> loss: 0.09288357556638041\n",
      "    At iteration 4300 -> loss: 0.09283797461436119\n",
      "    At iteration 4400 -> loss: 0.09290183549489385\n",
      "    At iteration 4500 -> loss: 0.09328571296942866\n",
      "    At iteration 4600 -> loss: 0.0932325543570768\n",
      "    At iteration 4700 -> loss: 0.09317059229407634\n",
      "    At iteration 4800 -> loss: 0.09314457769952443\n",
      "    At iteration 4900 -> loss: 0.09307487338018999\n",
      "    At iteration 5000 -> loss: 0.09300076465346756\n",
      "    At iteration 5100 -> loss: 0.09301499159070524\n",
      "    At iteration 5200 -> loss: 0.09311205845327863\n",
      "    At iteration 5300 -> loss: 0.09321337893445808\n",
      "    At iteration 5400 -> loss: 0.09319771649700313\n",
      "    At iteration 5500 -> loss: 0.09314129184962462\n",
      "    At iteration 5600 -> loss: 0.09312128117691171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5700 -> loss: 0.09322248123017653\n",
      "    At iteration 5800 -> loss: 0.09318925022972258\n",
      "    At iteration 5900 -> loss: 0.09316487649581506\n",
      "    At iteration 6000 -> loss: 0.09322418990497522\n",
      "    At iteration 6100 -> loss: 0.09320749293937704\n",
      "    At iteration 6200 -> loss: 0.09320839805508493\n",
      "    At iteration 6300 -> loss: 0.09320146324881634\n",
      "    At iteration 6400 -> loss: 0.09317393205914269\n",
      "    At iteration 6500 -> loss: 0.09314251278887681\n",
      "    At iteration 6600 -> loss: 0.09307813673980427\n",
      "    At iteration 6700 -> loss: 0.09307384306408853\n",
      "    At iteration 6800 -> loss: 0.09306700501872886\n",
      "    At iteration 6900 -> loss: 0.09347869287634969\n",
      "    At iteration 7000 -> loss: 0.09347688407115771\n",
      "    At iteration 7100 -> loss: 0.09346444066481582\n",
      "    At iteration 7200 -> loss: 0.0933917302544946\n",
      "    At iteration 7300 -> loss: 0.0934110286871257\n",
      "    At iteration 7400 -> loss: 0.09339441024301402\n",
      "    At iteration 7500 -> loss: 0.09334576177681858\n",
      "    At iteration 7600 -> loss: 0.09331299919629181\n",
      "    At iteration 7700 -> loss: 0.09330632907598765\n",
      "    At iteration 7800 -> loss: 0.09325740007644882\n",
      "    At iteration 7900 -> loss: 0.09320848046886912\n",
      "    At iteration 8000 -> loss: 0.09316600232178786\n",
      "    At iteration 8100 -> loss: 0.09313234832517091\n",
      "    At iteration 8200 -> loss: 0.09310431717509342\n",
      "    At iteration 8300 -> loss: 0.09307305285010706\n",
      "    At iteration 8400 -> loss: 0.0930688438891996\n",
      "    At iteration 8500 -> loss: 0.09317219689609071\n",
      "    At iteration 8600 -> loss: 0.09314902309349807\n",
      "    At iteration 8700 -> loss: 0.09319398111364201\n",
      "    At iteration 8800 -> loss: 0.09324210043435853\n",
      "    At iteration 8900 -> loss: 0.09325401137353222\n",
      "    At iteration 9000 -> loss: 0.093251107395906\n",
      "    At iteration 9100 -> loss: 0.0932138294071005\n",
      "    At iteration 9200 -> loss: 0.09328921461750303\n",
      "    At iteration 9300 -> loss: 0.0932768890055434\n",
      "    At iteration 9400 -> loss: 0.09325278497718813\n",
      "    At iteration 9500 -> loss: 0.09327174657493076\n",
      "    At iteration 9600 -> loss: 0.09327358216068929\n",
      "    At iteration 9700 -> loss: 0.09330590727981049\n",
      "    At iteration 9800 -> loss: 0.09325822043932544\n",
      "    At iteration 9900 -> loss: 0.0932180089968157\n",
      "    At iteration 10000 -> loss: 0.09319654510163236\n",
      "    At iteration 10100 -> loss: 0.0931548814689291\n",
      "    At iteration 10200 -> loss: 0.09312967158691648\n",
      "    At iteration 10300 -> loss: 0.09318021059731352\n",
      "    At iteration 10400 -> loss: 0.09316530951454788\n",
      "    At iteration 10500 -> loss: 0.09322922220419098\n",
      "    At iteration 10600 -> loss: 0.09322465771135449\n",
      "    At iteration 10700 -> loss: 0.09324675867188302\n",
      "    At iteration 10800 -> loss: 0.09319049529538738\n",
      "    At iteration 10900 -> loss: 0.09321126858975051\n",
      "    At iteration 11000 -> loss: 0.0931849959504716\n",
      "    At iteration 11100 -> loss: 0.09316482034661752\n",
      "    At iteration 11200 -> loss: 0.0931675596549731\n",
      "    At iteration 11300 -> loss: 0.09316196708525308\n",
      "    At iteration 11400 -> loss: 0.09316831322695467\n",
      "    At iteration 11500 -> loss: 0.09314384009830258\n",
      "    At iteration 11600 -> loss: 0.09316102737503802\n",
      "    At iteration 11700 -> loss: 0.09314074967644886\n",
      "    At iteration 11800 -> loss: 0.09313371768038951\n",
      "    At iteration 11900 -> loss: 0.09315412949573436\n",
      "    At iteration 12000 -> loss: 0.09314452095194536\n",
      "    At iteration 12100 -> loss: 0.09312663809284327\n",
      "    At iteration 12200 -> loss: 0.09314289981911819\n",
      "    At iteration 12300 -> loss: 0.09312446200672884\n",
      "    At iteration 12400 -> loss: 0.09311637548363395\n",
      "    At iteration 12500 -> loss: 0.09314102336881301\n",
      "    At iteration 12600 -> loss: 0.09311634214857423\n",
      "    At iteration 12700 -> loss: 0.09313513849089741\n",
      "    At iteration 12800 -> loss: 0.09313692134048986\n",
      "    At iteration 12900 -> loss: 0.09310314111063478\n",
      "    At iteration 13000 -> loss: 0.09310332745432517\n",
      "    At iteration 13100 -> loss: 0.09310235178634405\n",
      "    At iteration 13200 -> loss: 0.09310253882853814\n",
      "    At iteration 13300 -> loss: 0.09315864284602667\n",
      "    At iteration 13400 -> loss: 0.09313561204422748\n",
      "    At iteration 13500 -> loss: 0.09313059587464197\n",
      "    At iteration 13600 -> loss: 0.09311298953388208\n",
      "Staring Epoch 44\n",
      "    At iteration 0 -> loss: 0.08116092436830513\n",
      "    At iteration 100 -> loss: 0.09317623155661715\n",
      "    At iteration 200 -> loss: 0.0918411347058766\n",
      "    At iteration 300 -> loss: 0.09176361418040317\n",
      "    At iteration 400 -> loss: 0.09127058914380459\n",
      "    At iteration 500 -> loss: 0.0914176900521012\n",
      "    At iteration 600 -> loss: 0.09111037501429327\n",
      "    At iteration 700 -> loss: 0.09128464650602185\n",
      "    At iteration 800 -> loss: 0.09217516664480882\n",
      "    At iteration 900 -> loss: 0.09198893740403194\n",
      "    At iteration 1000 -> loss: 0.09197154814639506\n",
      "    At iteration 1100 -> loss: 0.09165263043719064\n",
      "    At iteration 1200 -> loss: 0.09150513388838065\n",
      "    At iteration 1300 -> loss: 0.09142196001894475\n",
      "    At iteration 1400 -> loss: 0.09172050909998844\n",
      "    At iteration 1500 -> loss: 0.09176568109680111\n",
      "    At iteration 1600 -> loss: 0.09162096396474861\n",
      "    At iteration 1700 -> loss: 0.09153207017908405\n",
      "    At iteration 1800 -> loss: 0.09138387552746906\n",
      "    At iteration 1900 -> loss: 0.09124950183400733\n",
      "    At iteration 2000 -> loss: 0.09126469685098282\n",
      "    At iteration 2100 -> loss: 0.0912478297872206\n",
      "    At iteration 2200 -> loss: 0.0912137994187358\n",
      "    At iteration 2300 -> loss: 0.09128337703270295\n",
      "    At iteration 2400 -> loss: 0.09125468288807705\n",
      "    At iteration 2500 -> loss: 0.09118851295380989\n",
      "    At iteration 2600 -> loss: 0.0912584245582618\n",
      "    At iteration 2700 -> loss: 0.09133784308027296\n",
      "    At iteration 2800 -> loss: 0.09130115025806247\n",
      "    At iteration 2900 -> loss: 0.0912747776639316\n",
      "    At iteration 3000 -> loss: 0.09152986133926254\n",
      "    At iteration 3100 -> loss: 0.0915295211456165\n",
      "    At iteration 3200 -> loss: 0.09158777481857128\n",
      "    At iteration 3300 -> loss: 0.09150402625494834\n",
      "    At iteration 3400 -> loss: 0.09150210569473315\n",
      "    At iteration 3500 -> loss: 0.09145008362528166\n",
      "    At iteration 3600 -> loss: 0.09155565063705567\n",
      "    At iteration 3700 -> loss: 0.09152380762421097\n",
      "    At iteration 3800 -> loss: 0.09147631877986076\n",
      "    At iteration 3900 -> loss: 0.09145331758216103\n",
      "    At iteration 4000 -> loss: 0.09180188337218562\n",
      "    At iteration 4100 -> loss: 0.09174030447554282\n",
      "    At iteration 4200 -> loss: 0.09177258726068309\n",
      "    At iteration 4300 -> loss: 0.09170764352443449\n",
      "    At iteration 4400 -> loss: 0.0917272909920691\n",
      "    At iteration 4500 -> loss: 0.091683233008377\n",
      "    At iteration 4600 -> loss: 0.09170741283779325\n",
      "    At iteration 4700 -> loss: 0.09169142237113419\n",
      "    At iteration 4800 -> loss: 0.09236695454286116\n",
      "    At iteration 4900 -> loss: 0.09234697228713101\n",
      "    At iteration 5000 -> loss: 0.09227943336608152\n",
      "    At iteration 5100 -> loss: 0.09221937478475384\n",
      "    At iteration 5200 -> loss: 0.0922024631967101\n",
      "    At iteration 5300 -> loss: 0.09220134958255488\n",
      "    At iteration 5400 -> loss: 0.09218443749545684\n",
      "    At iteration 5500 -> loss: 0.09215267989429224\n",
      "    At iteration 5600 -> loss: 0.09217529112141534\n",
      "    At iteration 5700 -> loss: 0.09215330417434592\n",
      "    At iteration 5800 -> loss: 0.09229305575265204\n",
      "    At iteration 5900 -> loss: 0.09229604198775053\n",
      "    At iteration 6000 -> loss: 0.0922440782594429\n",
      "    At iteration 6100 -> loss: 0.09228672018119144\n",
      "    At iteration 6200 -> loss: 0.09245257178557068\n",
      "    At iteration 6300 -> loss: 0.09241494515485184\n",
      "    At iteration 6400 -> loss: 0.09247322624886636\n",
      "    At iteration 6500 -> loss: 0.09250661985303273\n",
      "    At iteration 6600 -> loss: 0.09245853573839528\n",
      "    At iteration 6700 -> loss: 0.09245346288144064\n",
      "    At iteration 6800 -> loss: 0.09245170194788847\n",
      "    At iteration 6900 -> loss: 0.0925583220950823\n",
      "    At iteration 7000 -> loss: 0.09261106169712818\n",
      "    At iteration 7100 -> loss: 0.09257489299035797\n",
      "    At iteration 7200 -> loss: 0.092565486375403\n",
      "    At iteration 7300 -> loss: 0.09257200595492042\n",
      "    At iteration 7400 -> loss: 0.09254667447745432\n",
      "    At iteration 7500 -> loss: 0.09259485246162619\n",
      "    At iteration 7600 -> loss: 0.09260388113704651\n",
      "    At iteration 7700 -> loss: 0.09256486534118177\n",
      "    At iteration 7800 -> loss: 0.0925270014107328\n",
      "    At iteration 7900 -> loss: 0.09258143779777349\n",
      "    At iteration 8000 -> loss: 0.0926206637783788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8100 -> loss: 0.09262536110757498\n",
      "    At iteration 8200 -> loss: 0.09258798726119469\n",
      "    At iteration 8300 -> loss: 0.09254528864309255\n",
      "    At iteration 8400 -> loss: 0.09262308243624608\n",
      "    At iteration 8500 -> loss: 0.09264480311909702\n",
      "    At iteration 8600 -> loss: 0.09266980090976672\n",
      "    At iteration 8700 -> loss: 0.09277229477804291\n",
      "    At iteration 8800 -> loss: 0.092838308076229\n",
      "    At iteration 8900 -> loss: 0.0928600097689051\n",
      "    At iteration 9000 -> loss: 0.09283525671818606\n",
      "    At iteration 9100 -> loss: 0.09281208829148871\n",
      "    At iteration 9200 -> loss: 0.09282509588613733\n",
      "    At iteration 9300 -> loss: 0.0928683536767656\n",
      "    At iteration 9400 -> loss: 0.09286414203716449\n",
      "    At iteration 9500 -> loss: 0.09287156390627087\n",
      "    At iteration 9600 -> loss: 0.09287409749547541\n",
      "    At iteration 9700 -> loss: 0.09287221243901764\n",
      "    At iteration 9800 -> loss: 0.0928521261995942\n",
      "    At iteration 9900 -> loss: 0.09283536053040972\n",
      "    At iteration 10000 -> loss: 0.092806062288687\n",
      "    At iteration 10100 -> loss: 0.09284374178718294\n",
      "    At iteration 10200 -> loss: 0.09280155665675402\n",
      "    At iteration 10300 -> loss: 0.0927695663191256\n",
      "    At iteration 10400 -> loss: 0.09280901822843557\n",
      "    At iteration 10500 -> loss: 0.09277534711690875\n",
      "    At iteration 10600 -> loss: 0.09283148736760627\n",
      "    At iteration 10700 -> loss: 0.09279758539876853\n",
      "    At iteration 10800 -> loss: 0.09283791527566479\n",
      "    At iteration 10900 -> loss: 0.09292260743256613\n",
      "    At iteration 11000 -> loss: 0.09292465127397932\n",
      "    At iteration 11100 -> loss: 0.09294053228040104\n",
      "    At iteration 11200 -> loss: 0.09291050737532114\n",
      "    At iteration 11300 -> loss: 0.09288752610040009\n",
      "    At iteration 11400 -> loss: 0.09289765504883975\n",
      "    At iteration 11500 -> loss: 0.09293995424057305\n",
      "    At iteration 11600 -> loss: 0.09293846028363201\n",
      "    At iteration 11700 -> loss: 0.09307453950281595\n",
      "    At iteration 11800 -> loss: 0.09305723936576837\n",
      "    At iteration 11900 -> loss: 0.0930510992724066\n",
      "    At iteration 12000 -> loss: 0.09302394921469351\n",
      "    At iteration 12100 -> loss: 0.09301668793128155\n",
      "    At iteration 12200 -> loss: 0.09299773069077541\n",
      "    At iteration 12300 -> loss: 0.0930122618311282\n",
      "    At iteration 12400 -> loss: 0.09303226038011551\n",
      "    At iteration 12500 -> loss: 0.09303687371413473\n",
      "    At iteration 12600 -> loss: 0.09304974414599969\n",
      "    At iteration 12700 -> loss: 0.0930621770997116\n",
      "    At iteration 12800 -> loss: 0.0930356122481339\n",
      "    At iteration 12900 -> loss: 0.09303018567811763\n",
      "    At iteration 13000 -> loss: 0.09307341049384997\n",
      "    At iteration 13100 -> loss: 0.09307366309639056\n",
      "    At iteration 13200 -> loss: 0.09312568077583314\n",
      "    At iteration 13300 -> loss: 0.09310698153824898\n",
      "    At iteration 13400 -> loss: 0.09310425047115967\n",
      "    At iteration 13500 -> loss: 0.09309390868335086\n",
      "    At iteration 13600 -> loss: 0.0931156421185633\n",
      "Staring Epoch 45\n",
      "    At iteration 0 -> loss: 0.08036800382251386\n",
      "    At iteration 100 -> loss: 0.09275036609985247\n",
      "    At iteration 200 -> loss: 0.09294387451195436\n",
      "    At iteration 300 -> loss: 0.09279963558527381\n",
      "    At iteration 400 -> loss: 0.09333813460618505\n",
      "    At iteration 500 -> loss: 0.09503285258282992\n",
      "    At iteration 600 -> loss: 0.0941174777394517\n",
      "    At iteration 700 -> loss: 0.09373313689410365\n",
      "    At iteration 800 -> loss: 0.09328090590359049\n",
      "    At iteration 900 -> loss: 0.09309952722485293\n",
      "    At iteration 1000 -> loss: 0.0927571984814767\n",
      "    At iteration 1100 -> loss: 0.09250639326584707\n",
      "    At iteration 1200 -> loss: 0.09274698026751713\n",
      "    At iteration 1300 -> loss: 0.09317910798472684\n",
      "    At iteration 1400 -> loss: 0.0929852652118957\n",
      "    At iteration 1500 -> loss: 0.09290504202028103\n",
      "    At iteration 1600 -> loss: 0.0927241003760836\n",
      "    At iteration 1700 -> loss: 0.0925865454985706\n",
      "    At iteration 1800 -> loss: 0.09239820830775294\n",
      "    At iteration 1900 -> loss: 0.09231161240209226\n",
      "    At iteration 2000 -> loss: 0.09209057537523627\n",
      "    At iteration 2100 -> loss: 0.09210965806613629\n",
      "    At iteration 2200 -> loss: 0.0921342222920578\n",
      "    At iteration 2300 -> loss: 0.09205306086085097\n",
      "    At iteration 2400 -> loss: 0.09213439888219346\n",
      "    At iteration 2500 -> loss: 0.09214091899283355\n",
      "    At iteration 2600 -> loss: 0.09230465022785135\n",
      "    At iteration 2700 -> loss: 0.09221700307597439\n",
      "    At iteration 2800 -> loss: 0.09221386260413303\n",
      "    At iteration 2900 -> loss: 0.09262339318394806\n",
      "    At iteration 3000 -> loss: 0.09249132491003953\n",
      "    At iteration 3100 -> loss: 0.09242615888056956\n",
      "    At iteration 3200 -> loss: 0.0924194915888402\n",
      "    At iteration 3300 -> loss: 0.09238765558464589\n",
      "    At iteration 3400 -> loss: 0.09242293592697098\n",
      "    At iteration 3500 -> loss: 0.09239107684780833\n",
      "    At iteration 3600 -> loss: 0.09233396430512938\n",
      "    At iteration 3700 -> loss: 0.09232144846500646\n",
      "    At iteration 3800 -> loss: 0.09228801879883736\n",
      "    At iteration 3900 -> loss: 0.09222913635118438\n",
      "    At iteration 4000 -> loss: 0.0921762168838373\n",
      "    At iteration 4100 -> loss: 0.09217872586444731\n",
      "    At iteration 4200 -> loss: 0.09230516806265766\n",
      "    At iteration 4300 -> loss: 0.09222559550899424\n",
      "    At iteration 4400 -> loss: 0.09224644327013537\n",
      "    At iteration 4500 -> loss: 0.09226581761548028\n",
      "    At iteration 4600 -> loss: 0.09228661317890909\n",
      "    At iteration 4700 -> loss: 0.0922211706481427\n",
      "    At iteration 4800 -> loss: 0.09220998603969709\n",
      "    At iteration 4900 -> loss: 0.09217182688727252\n",
      "    At iteration 5000 -> loss: 0.0921504939209826\n",
      "    At iteration 5100 -> loss: 0.09232333427951471\n",
      "    At iteration 5200 -> loss: 0.09228554282234781\n",
      "    At iteration 5300 -> loss: 0.0923058570855007\n",
      "    At iteration 5400 -> loss: 0.09229219297597699\n",
      "    At iteration 5500 -> loss: 0.09233648667123603\n",
      "    At iteration 5600 -> loss: 0.09246041275499794\n",
      "    At iteration 5700 -> loss: 0.09244465411726113\n",
      "    At iteration 5800 -> loss: 0.09246191423767594\n",
      "    At iteration 5900 -> loss: 0.09249003704007938\n",
      "    At iteration 6000 -> loss: 0.09255665071643628\n",
      "    At iteration 6100 -> loss: 0.09252994105271278\n",
      "    At iteration 6200 -> loss: 0.09251766333405229\n",
      "    At iteration 6300 -> loss: 0.09275536926160763\n",
      "    At iteration 6400 -> loss: 0.09280531257248423\n",
      "    At iteration 6500 -> loss: 0.09280047629486834\n",
      "    At iteration 6600 -> loss: 0.09276913779658252\n",
      "    At iteration 6700 -> loss: 0.09271933222282577\n",
      "    At iteration 6800 -> loss: 0.09266810595988961\n",
      "    At iteration 6900 -> loss: 0.09267551460848797\n",
      "    At iteration 7000 -> loss: 0.09265068518109608\n",
      "    At iteration 7100 -> loss: 0.09261352193132084\n",
      "    At iteration 7200 -> loss: 0.09269290642380078\n",
      "    At iteration 7300 -> loss: 0.09264465323682886\n",
      "    At iteration 7400 -> loss: 0.09260335041354306\n",
      "    At iteration 7500 -> loss: 0.09258641996922483\n",
      "    At iteration 7600 -> loss: 0.09275190505130805\n",
      "    At iteration 7700 -> loss: 0.09271082166567472\n",
      "    At iteration 7800 -> loss: 0.09268671997657828\n",
      "    At iteration 7900 -> loss: 0.09274593143943954\n",
      "    At iteration 8000 -> loss: 0.09271082478797643\n",
      "    At iteration 8100 -> loss: 0.09273900930730417\n",
      "    At iteration 8200 -> loss: 0.09271890955839297\n",
      "    At iteration 8300 -> loss: 0.09269906656900936\n",
      "    At iteration 8400 -> loss: 0.09267729504880068\n",
      "    At iteration 8500 -> loss: 0.09270094896099165\n",
      "    At iteration 8600 -> loss: 0.09265887784804276\n",
      "    At iteration 8700 -> loss: 0.09264612708948314\n",
      "    At iteration 8800 -> loss: 0.09269429123835177\n",
      "    At iteration 8900 -> loss: 0.09275408659381376\n",
      "    At iteration 9000 -> loss: 0.0927497539204172\n",
      "    At iteration 9100 -> loss: 0.0927236230836358\n",
      "    At iteration 9200 -> loss: 0.09275308477276487\n",
      "    At iteration 9300 -> loss: 0.09274844571860309\n",
      "    At iteration 9400 -> loss: 0.09270420110727232\n",
      "    At iteration 9500 -> loss: 0.09268832567143952\n",
      "    At iteration 9600 -> loss: 0.09266457059464028\n",
      "    At iteration 9700 -> loss: 0.09297145031583508\n",
      "    At iteration 9800 -> loss: 0.09295382640232759\n",
      "    At iteration 9900 -> loss: 0.09293847246431254\n",
      "    At iteration 10000 -> loss: 0.0929141347605789\n",
      "    At iteration 10100 -> loss: 0.09294819868892475\n",
      "    At iteration 10200 -> loss: 0.09291572823631082\n",
      "    At iteration 10300 -> loss: 0.09296382026483635\n",
      "    At iteration 10400 -> loss: 0.09300040341141691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10500 -> loss: 0.0929944019712193\n",
      "    At iteration 10600 -> loss: 0.09297001715238533\n",
      "    At iteration 10700 -> loss: 0.09294539035485644\n",
      "    At iteration 10800 -> loss: 0.09292641456077498\n",
      "    At iteration 10900 -> loss: 0.09296198076316645\n",
      "    At iteration 11000 -> loss: 0.09297028922693765\n",
      "    At iteration 11100 -> loss: 0.09300875545572111\n",
      "    At iteration 11200 -> loss: 0.0929993977930844\n",
      "    At iteration 11300 -> loss: 0.09296314491051831\n",
      "    At iteration 11400 -> loss: 0.09299995745749151\n",
      "    At iteration 11500 -> loss: 0.09305750676872791\n",
      "    At iteration 11600 -> loss: 0.09314621309104283\n",
      "    At iteration 11700 -> loss: 0.0931765931988163\n",
      "    At iteration 11800 -> loss: 0.09315457723043748\n",
      "    At iteration 11900 -> loss: 0.0931237504111782\n",
      "    At iteration 12000 -> loss: 0.09312756886681474\n",
      "    At iteration 12100 -> loss: 0.09311999257832021\n",
      "    At iteration 12200 -> loss: 0.0930854985180249\n",
      "    At iteration 12300 -> loss: 0.09312303980203365\n",
      "    At iteration 12400 -> loss: 0.09310201773418764\n",
      "    At iteration 12500 -> loss: 0.0930889569438733\n",
      "    At iteration 12600 -> loss: 0.09312810971052513\n",
      "    At iteration 12700 -> loss: 0.09314070013436136\n",
      "    At iteration 12800 -> loss: 0.09315868529347433\n",
      "    At iteration 12900 -> loss: 0.09314597939881962\n",
      "    At iteration 13000 -> loss: 0.0931286311734997\n",
      "    At iteration 13100 -> loss: 0.09310438097543625\n",
      "    At iteration 13200 -> loss: 0.09309096711028642\n",
      "    At iteration 13300 -> loss: 0.0930801825696546\n",
      "    At iteration 13400 -> loss: 0.09307942993681663\n",
      "    At iteration 13500 -> loss: 0.09308444882032657\n",
      "    At iteration 13600 -> loss: 0.09307114208305221\n",
      "Staring Epoch 46\n",
      "    At iteration 0 -> loss: 0.08014616811851738\n",
      "    At iteration 100 -> loss: 0.09611423660556392\n",
      "    At iteration 200 -> loss: 0.09472905388766342\n",
      "    At iteration 300 -> loss: 0.09514403013174932\n",
      "    At iteration 400 -> loss: 0.09461275123971644\n",
      "    At iteration 500 -> loss: 0.09315205113168283\n",
      "    At iteration 600 -> loss: 0.09294226061945284\n",
      "    At iteration 700 -> loss: 0.09257956757577693\n",
      "    At iteration 800 -> loss: 0.09304191762655438\n",
      "    At iteration 900 -> loss: 0.0929293286760186\n",
      "    At iteration 1000 -> loss: 0.09316836005670594\n",
      "    At iteration 1100 -> loss: 0.09312607625523942\n",
      "    At iteration 1200 -> loss: 0.093049600600919\n",
      "    At iteration 1300 -> loss: 0.09298637801393334\n",
      "    At iteration 1400 -> loss: 0.09276469675688491\n",
      "    At iteration 1500 -> loss: 0.09257374220450758\n",
      "    At iteration 1600 -> loss: 0.09297660110554956\n",
      "    At iteration 1700 -> loss: 0.0927873889493289\n",
      "    At iteration 1800 -> loss: 0.09277823792454702\n",
      "    At iteration 1900 -> loss: 0.0927314167808727\n",
      "    At iteration 2000 -> loss: 0.09262541094165609\n",
      "    At iteration 2100 -> loss: 0.0939696522058447\n",
      "    At iteration 2200 -> loss: 0.09374099841641476\n",
      "    At iteration 2300 -> loss: 0.09378378775056839\n",
      "    At iteration 2400 -> loss: 0.09365280231152971\n",
      "    At iteration 2500 -> loss: 0.09377516799780763\n",
      "    At iteration 2600 -> loss: 0.09368244057493172\n",
      "    At iteration 2700 -> loss: 0.09364502825876136\n",
      "    At iteration 2800 -> loss: 0.09351412601795328\n",
      "    At iteration 2900 -> loss: 0.09348090171272913\n",
      "    At iteration 3000 -> loss: 0.0934145057093954\n",
      "    At iteration 3100 -> loss: 0.09330814460525975\n",
      "    At iteration 3200 -> loss: 0.0932357687957424\n",
      "    At iteration 3300 -> loss: 0.09327583363196269\n",
      "    At iteration 3400 -> loss: 0.09339932995109591\n",
      "    At iteration 3500 -> loss: 0.09357315352256888\n",
      "    At iteration 3600 -> loss: 0.09362791776308602\n",
      "    At iteration 3700 -> loss: 0.0936603805437891\n",
      "    At iteration 3800 -> loss: 0.09360013404072506\n",
      "    At iteration 3900 -> loss: 0.09369975058177914\n",
      "    At iteration 4000 -> loss: 0.09369339935828964\n",
      "    At iteration 4100 -> loss: 0.09366741543070206\n",
      "    At iteration 4200 -> loss: 0.09357828988969054\n",
      "    At iteration 4300 -> loss: 0.09351150130092731\n",
      "    At iteration 4400 -> loss: 0.0934291749806477\n",
      "    At iteration 4500 -> loss: 0.09339289650879667\n",
      "    At iteration 4600 -> loss: 0.09345943705928694\n",
      "    At iteration 4700 -> loss: 0.09345406546042645\n",
      "    At iteration 4800 -> loss: 0.09339624295980807\n",
      "    At iteration 4900 -> loss: 0.09365428804252149\n",
      "    At iteration 5000 -> loss: 0.0936055946793156\n",
      "    At iteration 5100 -> loss: 0.09354660128182406\n",
      "    At iteration 5200 -> loss: 0.0935269877378036\n",
      "    At iteration 5300 -> loss: 0.09346101260817279\n",
      "    At iteration 5400 -> loss: 0.0934717568658556\n",
      "    At iteration 5500 -> loss: 0.09338061711100132\n",
      "    At iteration 5600 -> loss: 0.09334552384508275\n",
      "    At iteration 5700 -> loss: 0.0934154225270647\n",
      "    At iteration 5800 -> loss: 0.09333288017335377\n",
      "    At iteration 5900 -> loss: 0.0934368469294241\n",
      "    At iteration 6000 -> loss: 0.09340444222120065\n",
      "    At iteration 6100 -> loss: 0.09336690646914432\n",
      "    At iteration 6200 -> loss: 0.09343116344344486\n",
      "    At iteration 6300 -> loss: 0.09341849239199344\n",
      "    At iteration 6400 -> loss: 0.09337228280506758\n",
      "    At iteration 6500 -> loss: 0.09332497169177056\n",
      "    At iteration 6600 -> loss: 0.09336912286347766\n",
      "    At iteration 6700 -> loss: 0.09336898981211572\n",
      "    At iteration 6800 -> loss: 0.09336283227819431\n",
      "    At iteration 6900 -> loss: 0.09335289390226717\n",
      "    At iteration 7000 -> loss: 0.09333210135767592\n",
      "    At iteration 7100 -> loss: 0.09332954907250934\n",
      "    At iteration 7200 -> loss: 0.09351473657250158\n",
      "    At iteration 7300 -> loss: 0.09346519474110726\n",
      "    At iteration 7400 -> loss: 0.09343587660642723\n",
      "    At iteration 7500 -> loss: 0.09340978324461886\n",
      "    At iteration 7600 -> loss: 0.09344105109483193\n",
      "    At iteration 7700 -> loss: 0.09341539943572687\n",
      "    At iteration 7800 -> loss: 0.09339053982249322\n",
      "    At iteration 7900 -> loss: 0.09339514503295132\n",
      "    At iteration 8000 -> loss: 0.09337606807898341\n",
      "    At iteration 8100 -> loss: 0.09335212264809374\n",
      "    At iteration 8200 -> loss: 0.09332186875354885\n",
      "    At iteration 8300 -> loss: 0.09328792846124277\n",
      "    At iteration 8400 -> loss: 0.09326716546345681\n",
      "    At iteration 8500 -> loss: 0.09325190265991817\n",
      "    At iteration 8600 -> loss: 0.09324946212033387\n",
      "    At iteration 8700 -> loss: 0.09321658690179997\n",
      "    At iteration 8800 -> loss: 0.09316902691849731\n",
      "    At iteration 8900 -> loss: 0.09315462241865033\n",
      "    At iteration 9000 -> loss: 0.09311229917639485\n",
      "    At iteration 9100 -> loss: 0.0930830611609985\n",
      "    At iteration 9200 -> loss: 0.09307650339506401\n",
      "    At iteration 9300 -> loss: 0.09307009292311214\n",
      "    At iteration 9400 -> loss: 0.09308963899761843\n",
      "    At iteration 9500 -> loss: 0.09308885584420433\n",
      "    At iteration 9600 -> loss: 0.09320612293691441\n",
      "    At iteration 9700 -> loss: 0.09317553115391781\n",
      "    At iteration 9800 -> loss: 0.09318044033403677\n",
      "    At iteration 9900 -> loss: 0.09321670560341368\n",
      "    At iteration 10000 -> loss: 0.09329472910087025\n",
      "    At iteration 10100 -> loss: 0.09328330654986124\n",
      "    At iteration 10200 -> loss: 0.09327908861511608\n",
      "    At iteration 10300 -> loss: 0.09335252247970437\n",
      "    At iteration 10400 -> loss: 0.09333264090665412\n",
      "    At iteration 10500 -> loss: 0.09337378751023208\n",
      "    At iteration 10600 -> loss: 0.09337990185736865\n",
      "    At iteration 10700 -> loss: 0.09342249385566415\n",
      "    At iteration 10800 -> loss: 0.0934146095991066\n",
      "    At iteration 10900 -> loss: 0.09341053113084452\n",
      "    At iteration 11000 -> loss: 0.09337403745692707\n",
      "    At iteration 11100 -> loss: 0.093396643086382\n",
      "    At iteration 11200 -> loss: 0.09336338564088015\n",
      "    At iteration 11300 -> loss: 0.093350446990902\n",
      "    At iteration 11400 -> loss: 0.09336162508543654\n",
      "    At iteration 11500 -> loss: 0.09334413712382594\n",
      "    At iteration 11600 -> loss: 0.09332390079293612\n",
      "    At iteration 11700 -> loss: 0.09329719496128207\n",
      "    At iteration 11800 -> loss: 0.09331047908984427\n",
      "    At iteration 11900 -> loss: 0.09330209808619983\n",
      "    At iteration 12000 -> loss: 0.0932915435587272\n",
      "    At iteration 12100 -> loss: 0.0932634768320951\n",
      "    At iteration 12200 -> loss: 0.09327338151942867\n",
      "    At iteration 12300 -> loss: 0.09326249099846014\n",
      "    At iteration 12400 -> loss: 0.09324499513774703\n",
      "    At iteration 12500 -> loss: 0.09322820961365173\n",
      "    At iteration 12600 -> loss: 0.09320130299198456\n",
      "    At iteration 12700 -> loss: 0.0932202463813409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12800 -> loss: 0.09319995979280149\n",
      "    At iteration 12900 -> loss: 0.09322905381151206\n",
      "    At iteration 13000 -> loss: 0.09322635399274058\n",
      "    At iteration 13100 -> loss: 0.09320253982036336\n",
      "    At iteration 13200 -> loss: 0.09319111897263652\n",
      "    At iteration 13300 -> loss: 0.09315798919197743\n",
      "    At iteration 13400 -> loss: 0.09313404478952356\n",
      "    At iteration 13500 -> loss: 0.09312041423662099\n",
      "    At iteration 13600 -> loss: 0.0930986613378563\n",
      "Staring Epoch 47\n",
      "    At iteration 0 -> loss: 0.08565998892299831\n",
      "    At iteration 100 -> loss: 0.09144706657708192\n",
      "    At iteration 200 -> loss: 0.09096682063724482\n",
      "    At iteration 300 -> loss: 0.09087407510125525\n",
      "    At iteration 400 -> loss: 0.09031288206322997\n",
      "    At iteration 500 -> loss: 0.09054619884692428\n",
      "    At iteration 600 -> loss: 0.09134528768164028\n",
      "    At iteration 700 -> loss: 0.09132055441802332\n",
      "    At iteration 800 -> loss: 0.09183646566655951\n",
      "    At iteration 900 -> loss: 0.092405715238046\n",
      "    At iteration 1000 -> loss: 0.09224382512816336\n",
      "    At iteration 1100 -> loss: 0.09216053891355075\n",
      "    At iteration 1200 -> loss: 0.09331063996567408\n",
      "    At iteration 1300 -> loss: 0.09306963879710038\n",
      "    At iteration 1400 -> loss: 0.09292734518057108\n",
      "    At iteration 1500 -> loss: 0.09280615739642045\n",
      "    At iteration 1600 -> loss: 0.0927455840698993\n",
      "    At iteration 1700 -> loss: 0.09333210563440614\n",
      "    At iteration 1800 -> loss: 0.09329010204554042\n",
      "    At iteration 1900 -> loss: 0.09345169865237607\n",
      "    At iteration 2000 -> loss: 0.09344855226058495\n",
      "    At iteration 2100 -> loss: 0.09349617049500648\n",
      "    At iteration 2200 -> loss: 0.09330787937818028\n",
      "    At iteration 2300 -> loss: 0.09318243459892685\n",
      "    At iteration 2400 -> loss: 0.09369858947037515\n",
      "    At iteration 2500 -> loss: 0.09348005223320201\n",
      "    At iteration 2600 -> loss: 0.0935961659326434\n",
      "    At iteration 2700 -> loss: 0.09357152030342704\n",
      "    At iteration 2800 -> loss: 0.0934589970021017\n",
      "    At iteration 2900 -> loss: 0.09351555560715492\n",
      "    At iteration 3000 -> loss: 0.09337660893238946\n",
      "    At iteration 3100 -> loss: 0.09327825082660059\n",
      "    At iteration 3200 -> loss: 0.09336741750794716\n",
      "    At iteration 3300 -> loss: 0.09343862283806569\n",
      "    At iteration 3400 -> loss: 0.09333086744541218\n",
      "    At iteration 3500 -> loss: 0.09321824189744812\n",
      "    At iteration 3600 -> loss: 0.09315732709241864\n",
      "    At iteration 3700 -> loss: 0.0931129963814488\n",
      "    At iteration 3800 -> loss: 0.09310474773849294\n",
      "    At iteration 3900 -> loss: 0.09309098836826502\n",
      "    At iteration 4000 -> loss: 0.09313285983672528\n",
      "    At iteration 4100 -> loss: 0.09334209595503715\n",
      "    At iteration 4200 -> loss: 0.0932499624084193\n",
      "    At iteration 4300 -> loss: 0.09340990480188988\n",
      "    At iteration 4400 -> loss: 0.09333829605423592\n",
      "    At iteration 4500 -> loss: 0.09330599281905834\n",
      "    At iteration 4600 -> loss: 0.09334514721106787\n",
      "    At iteration 4700 -> loss: 0.09322182721340319\n",
      "    At iteration 4800 -> loss: 0.09327106117334927\n",
      "    At iteration 4900 -> loss: 0.0932359592494331\n",
      "    At iteration 5000 -> loss: 0.09320277241552649\n",
      "    At iteration 5100 -> loss: 0.09320406327692111\n",
      "    At iteration 5200 -> loss: 0.09322295580833576\n",
      "    At iteration 5300 -> loss: 0.09314682999685238\n",
      "    At iteration 5400 -> loss: 0.09322726815564991\n",
      "    At iteration 5500 -> loss: 0.09312347573264025\n",
      "    At iteration 5600 -> loss: 0.09311095023929943\n",
      "    At iteration 5700 -> loss: 0.09309604301126073\n",
      "    At iteration 5800 -> loss: 0.09302740039437818\n",
      "    At iteration 5900 -> loss: 0.09308913907529141\n",
      "    At iteration 6000 -> loss: 0.09313134639911035\n",
      "    At iteration 6100 -> loss: 0.09313941581425007\n",
      "    At iteration 6200 -> loss: 0.0931831933660979\n",
      "    At iteration 6300 -> loss: 0.0931993419631013\n",
      "    At iteration 6400 -> loss: 0.09335136331980513\n",
      "    At iteration 6500 -> loss: 0.09336614905984533\n",
      "    At iteration 6600 -> loss: 0.093324644057098\n",
      "    At iteration 6700 -> loss: 0.09330546906580948\n",
      "    At iteration 6800 -> loss: 0.09332675123427014\n",
      "    At iteration 6900 -> loss: 0.09329770315823267\n",
      "    At iteration 7000 -> loss: 0.0932688718740599\n",
      "    At iteration 7100 -> loss: 0.09325224464281585\n",
      "    At iteration 7200 -> loss: 0.09327412616495374\n",
      "    At iteration 7300 -> loss: 0.09323729464429514\n",
      "    At iteration 7400 -> loss: 0.09324742882582612\n",
      "    At iteration 7500 -> loss: 0.09325250359177327\n",
      "    At iteration 7600 -> loss: 0.09324033236593282\n",
      "    At iteration 7700 -> loss: 0.09318137882314222\n",
      "    At iteration 7800 -> loss: 0.09320095986860079\n",
      "    At iteration 7900 -> loss: 0.09316553863697068\n",
      "    At iteration 8000 -> loss: 0.09314904319581047\n",
      "    At iteration 8100 -> loss: 0.09317897999157858\n",
      "    At iteration 8200 -> loss: 0.09315089601849859\n",
      "    At iteration 8300 -> loss: 0.09313614392735733\n",
      "    At iteration 8400 -> loss: 0.09316308264769417\n",
      "    At iteration 8500 -> loss: 0.09318448873046904\n",
      "    At iteration 8600 -> loss: 0.09328597043369728\n",
      "    At iteration 8700 -> loss: 0.09323243005179821\n",
      "    At iteration 8800 -> loss: 0.09321187104138987\n",
      "    At iteration 8900 -> loss: 0.09354022706525292\n",
      "    At iteration 9000 -> loss: 0.09349410363546613\n",
      "    At iteration 9100 -> loss: 0.09353361169190572\n",
      "    At iteration 9200 -> loss: 0.0936013841184977\n",
      "    At iteration 9300 -> loss: 0.09357189113588174\n",
      "    At iteration 9400 -> loss: 0.09353526158844516\n",
      "    At iteration 9500 -> loss: 0.09350565993650127\n",
      "    At iteration 9600 -> loss: 0.09353124251577347\n",
      "    At iteration 9700 -> loss: 0.09348355985042271\n",
      "    At iteration 9800 -> loss: 0.09350108448251446\n",
      "    At iteration 9900 -> loss: 0.09355494962890765\n",
      "    At iteration 10000 -> loss: 0.0935048656876724\n",
      "    At iteration 10100 -> loss: 0.09348575893557187\n",
      "    At iteration 10200 -> loss: 0.09345757956124111\n",
      "    At iteration 10300 -> loss: 0.09343955033374941\n",
      "    At iteration 10400 -> loss: 0.09342233391488547\n",
      "    At iteration 10500 -> loss: 0.09337222442492045\n",
      "    At iteration 10600 -> loss: 0.09340456174941905\n",
      "    At iteration 10700 -> loss: 0.09339235339677429\n",
      "    At iteration 10800 -> loss: 0.09339694269289499\n",
      "    At iteration 10900 -> loss: 0.09342594194271957\n",
      "    At iteration 11000 -> loss: 0.09341178125770544\n",
      "    At iteration 11100 -> loss: 0.09340625753859978\n",
      "    At iteration 11200 -> loss: 0.09338668171046689\n",
      "    At iteration 11300 -> loss: 0.09338874532866272\n",
      "    At iteration 11400 -> loss: 0.09337103561632505\n",
      "    At iteration 11500 -> loss: 0.09334837431767978\n",
      "    At iteration 11600 -> loss: 0.09335443798182899\n",
      "    At iteration 11700 -> loss: 0.09332117777603138\n",
      "    At iteration 11800 -> loss: 0.09328244781302247\n",
      "    At iteration 11900 -> loss: 0.09326889658203395\n",
      "    At iteration 12000 -> loss: 0.09327014153343158\n",
      "    At iteration 12100 -> loss: 0.09325237460309253\n",
      "    At iteration 12200 -> loss: 0.09325171513982641\n",
      "    At iteration 12300 -> loss: 0.09324806325098045\n",
      "    At iteration 12400 -> loss: 0.09322081831916916\n",
      "    At iteration 12500 -> loss: 0.09321625685555386\n",
      "    At iteration 12600 -> loss: 0.09319758578083122\n",
      "    At iteration 12700 -> loss: 0.09324450735861649\n",
      "    At iteration 12800 -> loss: 0.09324323101589155\n",
      "    At iteration 12900 -> loss: 0.0932406055087326\n",
      "    At iteration 13000 -> loss: 0.09321634349245256\n",
      "    At iteration 13100 -> loss: 0.09320564194187685\n",
      "    At iteration 13200 -> loss: 0.09319418600447817\n",
      "    At iteration 13300 -> loss: 0.09317876897056673\n",
      "    At iteration 13400 -> loss: 0.09315180909986762\n",
      "    At iteration 13500 -> loss: 0.09313196133998007\n",
      "    At iteration 13600 -> loss: 0.09311184070231505\n",
      "Staring Epoch 48\n",
      "    At iteration 0 -> loss: 0.0883993566967547\n",
      "    At iteration 100 -> loss: 0.08966177972699357\n",
      "    At iteration 200 -> loss: 0.09038577879180866\n",
      "    At iteration 300 -> loss: 0.09121195616945864\n",
      "    At iteration 400 -> loss: 0.0915062874815937\n",
      "    At iteration 500 -> loss: 0.09145712664488757\n",
      "    At iteration 600 -> loss: 0.09143623417936192\n",
      "    At iteration 700 -> loss: 0.0916434268848128\n",
      "    At iteration 800 -> loss: 0.09158590847610545\n",
      "    At iteration 900 -> loss: 0.09258998957774364\n",
      "    At iteration 1000 -> loss: 0.0927501120144135\n",
      "    At iteration 1100 -> loss: 0.09263420454134037\n",
      "    At iteration 1200 -> loss: 0.09265558038398218\n",
      "    At iteration 1300 -> loss: 0.09254222767573514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1400 -> loss: 0.09294981933365747\n",
      "    At iteration 1500 -> loss: 0.09307273964101315\n",
      "    At iteration 1600 -> loss: 0.09297370797581889\n",
      "    At iteration 1700 -> loss: 0.09311652868380652\n",
      "    At iteration 1800 -> loss: 0.0929416167775302\n",
      "    At iteration 1900 -> loss: 0.09307935376926289\n",
      "    At iteration 2000 -> loss: 0.092883272237567\n",
      "    At iteration 2100 -> loss: 0.09284555412531623\n",
      "    At iteration 2200 -> loss: 0.09273588191857378\n",
      "    At iteration 2300 -> loss: 0.09285001042689825\n",
      "    At iteration 2400 -> loss: 0.09426500496458683\n",
      "    At iteration 2500 -> loss: 0.0941412760330035\n",
      "    At iteration 2600 -> loss: 0.09397768692811688\n",
      "    At iteration 2700 -> loss: 0.09385945150380944\n",
      "    At iteration 2800 -> loss: 0.0938187717945127\n",
      "    At iteration 2900 -> loss: 0.09380602247528426\n",
      "    At iteration 3000 -> loss: 0.09371759492790974\n",
      "    At iteration 3100 -> loss: 0.0936532203935104\n",
      "    At iteration 3200 -> loss: 0.09361243262226102\n",
      "    At iteration 3300 -> loss: 0.09373544797991214\n",
      "    At iteration 3400 -> loss: 0.0935769269293932\n",
      "    At iteration 3500 -> loss: 0.09359923731559779\n",
      "    At iteration 3600 -> loss: 0.09366731622256376\n",
      "    At iteration 3700 -> loss: 0.09356935330769256\n",
      "    At iteration 3800 -> loss: 0.09361481560371981\n",
      "    At iteration 3900 -> loss: 0.0936146653921608\n",
      "    At iteration 4000 -> loss: 0.09384044388910613\n",
      "    At iteration 4100 -> loss: 0.0937838237688132\n",
      "    At iteration 4200 -> loss: 0.09370112206606476\n",
      "    At iteration 4300 -> loss: 0.09362777900405526\n",
      "    At iteration 4400 -> loss: 0.09351582872365599\n",
      "    At iteration 4500 -> loss: 0.09345783637711194\n",
      "    At iteration 4600 -> loss: 0.09345865445225891\n",
      "    At iteration 4700 -> loss: 0.09342387301923112\n",
      "    At iteration 4800 -> loss: 0.0933370533617435\n",
      "    At iteration 4900 -> loss: 0.09330925249463042\n",
      "    At iteration 5000 -> loss: 0.09327526548758137\n",
      "    At iteration 5100 -> loss: 0.09320178515345169\n",
      "    At iteration 5200 -> loss: 0.09324183458358572\n",
      "    At iteration 5300 -> loss: 0.0932456170854404\n",
      "    At iteration 5400 -> loss: 0.09323621189512596\n",
      "    At iteration 5500 -> loss: 0.09316644105445619\n",
      "    At iteration 5600 -> loss: 0.09317264388057302\n",
      "    At iteration 5700 -> loss: 0.09312368977071699\n",
      "    At iteration 5800 -> loss: 0.09315231146810088\n",
      "    At iteration 5900 -> loss: 0.09309962197906663\n",
      "    At iteration 6000 -> loss: 0.09321721407052054\n",
      "    At iteration 6100 -> loss: 0.0931618103813467\n",
      "    At iteration 6200 -> loss: 0.09315356643518893\n",
      "    At iteration 6300 -> loss: 0.09318383749547436\n",
      "    At iteration 6400 -> loss: 0.09315995270214748\n",
      "    At iteration 6500 -> loss: 0.09322001280469737\n",
      "    At iteration 6600 -> loss: 0.09316376617495171\n",
      "    At iteration 6700 -> loss: 0.0931443194655224\n",
      "    At iteration 6800 -> loss: 0.093065702926591\n",
      "    At iteration 6900 -> loss: 0.09304048569761401\n",
      "    At iteration 7000 -> loss: 0.09308593744397574\n",
      "    At iteration 7100 -> loss: 0.0930139156047736\n",
      "    At iteration 7200 -> loss: 0.09302768440756193\n",
      "    At iteration 7300 -> loss: 0.09296676017186298\n",
      "    At iteration 7400 -> loss: 0.09293221044112071\n",
      "    At iteration 7500 -> loss: 0.09295800373490588\n",
      "    At iteration 7600 -> loss: 0.09298552438189717\n",
      "    At iteration 7700 -> loss: 0.09295058002341018\n",
      "    At iteration 7800 -> loss: 0.09291926636292251\n",
      "    At iteration 7900 -> loss: 0.09289290270574742\n",
      "    At iteration 8000 -> loss: 0.09309759608248012\n",
      "    At iteration 8100 -> loss: 0.09308234350375338\n",
      "    At iteration 8200 -> loss: 0.09314387032669579\n",
      "    At iteration 8300 -> loss: 0.09311434990122398\n",
      "    At iteration 8400 -> loss: 0.09307597551979833\n",
      "    At iteration 8500 -> loss: 0.0930247752232358\n",
      "    At iteration 8600 -> loss: 0.09299940152784042\n",
      "    At iteration 8700 -> loss: 0.09300509962185836\n",
      "    At iteration 8800 -> loss: 0.09298473304472163\n",
      "    At iteration 8900 -> loss: 0.09295106742154029\n",
      "    At iteration 9000 -> loss: 0.09296934417132044\n",
      "    At iteration 9100 -> loss: 0.0930259547283414\n",
      "    At iteration 9200 -> loss: 0.09300355055484433\n",
      "    At iteration 9300 -> loss: 0.09298225345791598\n",
      "    At iteration 9400 -> loss: 0.09296262443195306\n",
      "    At iteration 9500 -> loss: 0.09295998070053804\n",
      "    At iteration 9600 -> loss: 0.09292199529979764\n",
      "    At iteration 9700 -> loss: 0.09289575519708464\n",
      "    At iteration 9800 -> loss: 0.0928757194865723\n",
      "    At iteration 9900 -> loss: 0.09287338743273163\n",
      "    At iteration 10000 -> loss: 0.09285756221810576\n",
      "    At iteration 10100 -> loss: 0.09284165808872395\n",
      "    At iteration 10200 -> loss: 0.09282035247611062\n",
      "    At iteration 10300 -> loss: 0.09280246519263212\n",
      "    At iteration 10400 -> loss: 0.09277320390404863\n",
      "    At iteration 10500 -> loss: 0.09286186497859858\n",
      "    At iteration 10600 -> loss: 0.09292389138851367\n",
      "    At iteration 10700 -> loss: 0.0929634327971644\n",
      "    At iteration 10800 -> loss: 0.09296917841586104\n",
      "    At iteration 10900 -> loss: 0.09295900028603608\n",
      "    At iteration 11000 -> loss: 0.0929586179363595\n",
      "    At iteration 11100 -> loss: 0.09293474329211014\n",
      "    At iteration 11200 -> loss: 0.0929527893477561\n",
      "    At iteration 11300 -> loss: 0.09291995882951344\n",
      "    At iteration 11400 -> loss: 0.09288682034658646\n",
      "    At iteration 11500 -> loss: 0.09287107860396152\n",
      "    At iteration 11600 -> loss: 0.09291241412125298\n",
      "    At iteration 11700 -> loss: 0.09289770140627315\n",
      "    At iteration 11800 -> loss: 0.09292493709497211\n",
      "    At iteration 11900 -> loss: 0.09293792240891244\n",
      "    At iteration 12000 -> loss: 0.09293380859691765\n",
      "    At iteration 12100 -> loss: 0.09291642848569288\n",
      "    At iteration 12200 -> loss: 0.09294563823606054\n",
      "    At iteration 12300 -> loss: 0.09303676336729126\n",
      "    At iteration 12400 -> loss: 0.09306837810250619\n",
      "    At iteration 12500 -> loss: 0.09306932096399519\n",
      "    At iteration 12600 -> loss: 0.09308161135905851\n",
      "    At iteration 12700 -> loss: 0.09306188034938079\n",
      "    At iteration 12800 -> loss: 0.09303708016390053\n",
      "    At iteration 12900 -> loss: 0.09302498443645818\n",
      "    At iteration 13000 -> loss: 0.09300615407590057\n",
      "    At iteration 13100 -> loss: 0.09302565871836393\n",
      "    At iteration 13200 -> loss: 0.09303324165669703\n",
      "    At iteration 13300 -> loss: 0.0930060255397857\n",
      "    At iteration 13400 -> loss: 0.09300539046827444\n",
      "    At iteration 13500 -> loss: 0.09300220019157909\n",
      "    At iteration 13600 -> loss: 0.09301380534472303\n",
      "Staring Epoch 49\n",
      "    At iteration 0 -> loss: 0.09586465964093804\n",
      "    At iteration 100 -> loss: 0.09631075217191101\n",
      "    At iteration 200 -> loss: 0.09307249505469509\n",
      "    At iteration 300 -> loss: 0.09448041645927353\n",
      "    At iteration 400 -> loss: 0.09364761222426465\n",
      "    At iteration 500 -> loss: 0.09270231869867958\n",
      "    At iteration 600 -> loss: 0.09309020663025394\n",
      "    At iteration 700 -> loss: 0.09293564921637433\n",
      "    At iteration 800 -> loss: 0.0928944404753713\n",
      "    At iteration 900 -> loss: 0.0928753266388289\n",
      "    At iteration 1000 -> loss: 0.0929551507037561\n",
      "    At iteration 1100 -> loss: 0.09303218038036601\n",
      "    At iteration 1200 -> loss: 0.09391078691371384\n",
      "    At iteration 1300 -> loss: 0.0946301080756136\n",
      "    At iteration 1400 -> loss: 0.09444367372007627\n",
      "    At iteration 1500 -> loss: 0.09421308336177335\n",
      "    At iteration 1600 -> loss: 0.09406267765488699\n",
      "    At iteration 1700 -> loss: 0.0938405989278705\n",
      "    At iteration 1800 -> loss: 0.09365185248994429\n",
      "    At iteration 1900 -> loss: 0.09358676183852224\n",
      "    At iteration 2000 -> loss: 0.0936772621297356\n",
      "    At iteration 2100 -> loss: 0.09348865394494177\n",
      "    At iteration 2200 -> loss: 0.09336395883017437\n",
      "    At iteration 2300 -> loss: 0.09381666437811381\n",
      "    At iteration 2400 -> loss: 0.09361584285504981\n",
      "    At iteration 2500 -> loss: 0.09357266355870175\n",
      "    At iteration 2600 -> loss: 0.09347485237205518\n",
      "    At iteration 2700 -> loss: 0.09349873213000853\n",
      "    At iteration 2800 -> loss: 0.09346745974422113\n",
      "    At iteration 2900 -> loss: 0.09340714403986111\n",
      "    At iteration 3000 -> loss: 0.09341149787207986\n",
      "    At iteration 3100 -> loss: 0.09346087797806625\n",
      "    At iteration 3200 -> loss: 0.09342324200104132\n",
      "    At iteration 3300 -> loss: 0.09335511618502053\n",
      "    At iteration 3400 -> loss: 0.09362570116500465\n",
      "    At iteration 3500 -> loss: 0.09353058716275667\n",
      "    At iteration 3600 -> loss: 0.09346106004496688\n",
      "    At iteration 3700 -> loss: 0.09332624682379478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3800 -> loss: 0.093313688698548\n",
      "    At iteration 3900 -> loss: 0.09340530478298019\n",
      "    At iteration 4000 -> loss: 0.09337531623217453\n",
      "    At iteration 4100 -> loss: 0.09340998413707584\n",
      "    At iteration 4200 -> loss: 0.09341545635753203\n",
      "    At iteration 4300 -> loss: 0.09343058359651776\n",
      "    At iteration 4400 -> loss: 0.09345286880144205\n",
      "    At iteration 4500 -> loss: 0.093574196008316\n",
      "    At iteration 4600 -> loss: 0.09346670947500742\n",
      "    At iteration 4700 -> loss: 0.09343611615109612\n",
      "    At iteration 4800 -> loss: 0.09342376547820933\n",
      "    At iteration 4900 -> loss: 0.09343929038052703\n",
      "    At iteration 5000 -> loss: 0.09336882262838467\n",
      "    At iteration 5100 -> loss: 0.09335424492909562\n",
      "    At iteration 5200 -> loss: 0.09330218455408572\n",
      "    At iteration 5300 -> loss: 0.09323102190738837\n",
      "    At iteration 5400 -> loss: 0.0931626338631646\n",
      "    At iteration 5500 -> loss: 0.09315980480570292\n",
      "    At iteration 5600 -> loss: 0.09315800967442266\n",
      "    At iteration 5700 -> loss: 0.0932622986954235\n",
      "    At iteration 5800 -> loss: 0.09325350128717966\n",
      "    At iteration 5900 -> loss: 0.09324578002496131\n",
      "    At iteration 6000 -> loss: 0.09320954788552051\n",
      "    At iteration 6100 -> loss: 0.09316593275950784\n",
      "    At iteration 6200 -> loss: 0.09312593259907143\n",
      "    At iteration 6300 -> loss: 0.09315227126962686\n",
      "    At iteration 6400 -> loss: 0.09307374744216274\n",
      "    At iteration 6500 -> loss: 0.0930877934471685\n",
      "    At iteration 6600 -> loss: 0.09305270064277994\n",
      "    At iteration 6700 -> loss: 0.09303731031842617\n",
      "    At iteration 6800 -> loss: 0.09313689591141666\n",
      "    At iteration 6900 -> loss: 0.09313979408181822\n",
      "    At iteration 7000 -> loss: 0.09308491518462879\n",
      "    At iteration 7100 -> loss: 0.09303714404018311\n",
      "    At iteration 7200 -> loss: 0.09298831952943755\n",
      "    At iteration 7300 -> loss: 0.09300349372818599\n",
      "    At iteration 7400 -> loss: 0.09338551490981657\n",
      "    At iteration 7500 -> loss: 0.09344494582045858\n",
      "    At iteration 7600 -> loss: 0.09343099457602524\n",
      "    At iteration 7700 -> loss: 0.09349435717340797\n",
      "    At iteration 7800 -> loss: 0.09351271150125344\n",
      "    At iteration 7900 -> loss: 0.09344563755742573\n",
      "    At iteration 8000 -> loss: 0.09340662161088589\n",
      "    At iteration 8100 -> loss: 0.093376090437847\n",
      "    At iteration 8200 -> loss: 0.09333257958415077\n",
      "    At iteration 8300 -> loss: 0.09336177698376501\n",
      "    At iteration 8400 -> loss: 0.09332284256858606\n",
      "    At iteration 8500 -> loss: 0.09341866497363192\n",
      "    At iteration 8600 -> loss: 0.09336045434556332\n",
      "    At iteration 8700 -> loss: 0.09333989583090233\n",
      "    At iteration 8800 -> loss: 0.09336010914039489\n",
      "    At iteration 8900 -> loss: 0.093317534507028\n",
      "    At iteration 9000 -> loss: 0.09337374241947452\n",
      "    At iteration 9100 -> loss: 0.09337174944576615\n",
      "    At iteration 9200 -> loss: 0.09334844937373313\n",
      "    At iteration 9300 -> loss: 0.09334221924464804\n",
      "    At iteration 9400 -> loss: 0.09330568321269905\n",
      "    At iteration 9500 -> loss: 0.09328701714506762\n",
      "    At iteration 9600 -> loss: 0.09326941259743299\n",
      "    At iteration 9700 -> loss: 0.09323837631229238\n",
      "    At iteration 9800 -> loss: 0.09320632009186201\n",
      "    At iteration 9900 -> loss: 0.09317499760331138\n",
      "    At iteration 10000 -> loss: 0.09317136825634963\n",
      "    At iteration 10100 -> loss: 0.09315051368992759\n",
      "    At iteration 10200 -> loss: 0.09312388735922571\n",
      "    At iteration 10300 -> loss: 0.09309147839684963\n",
      "    At iteration 10400 -> loss: 0.09311601986360066\n",
      "    At iteration 10500 -> loss: 0.09312660319003631\n",
      "    At iteration 10600 -> loss: 0.09309359379833783\n",
      "    At iteration 10700 -> loss: 0.09305326060773723\n",
      "    At iteration 10800 -> loss: 0.09304832504138892\n",
      "    At iteration 10900 -> loss: 0.09301041238819276\n",
      "    At iteration 11000 -> loss: 0.09298309342113226\n",
      "    At iteration 11100 -> loss: 0.09303403917313459\n",
      "    At iteration 11200 -> loss: 0.09303769609016378\n",
      "    At iteration 11300 -> loss: 0.09307259954855349\n",
      "    At iteration 11400 -> loss: 0.09305800616599198\n",
      "    At iteration 11500 -> loss: 0.09303758033230394\n",
      "    At iteration 11600 -> loss: 0.09302917292866049\n",
      "    At iteration 11700 -> loss: 0.0930573614096616\n",
      "    At iteration 11800 -> loss: 0.09310244270808983\n",
      "    At iteration 11900 -> loss: 0.09320117553207485\n",
      "    At iteration 12000 -> loss: 0.09318119840372367\n",
      "    At iteration 12100 -> loss: 0.0931633329355869\n",
      "    At iteration 12200 -> loss: 0.09313741487698479\n",
      "    At iteration 12300 -> loss: 0.09309931127161407\n",
      "    At iteration 12400 -> loss: 0.09309392349444336\n",
      "    At iteration 12500 -> loss: 0.09307943174424133\n",
      "    At iteration 12600 -> loss: 0.09306096232541856\n",
      "    At iteration 12700 -> loss: 0.09304834560729774\n",
      "    At iteration 12800 -> loss: 0.09310614840242863\n",
      "    At iteration 12900 -> loss: 0.09309477457907057\n",
      "    At iteration 13000 -> loss: 0.09307687717390781\n",
      "    At iteration 13100 -> loss: 0.09308911945236505\n",
      "    At iteration 13200 -> loss: 0.09307165082444217\n",
      "    At iteration 13300 -> loss: 0.0930505863164057\n",
      "    At iteration 13400 -> loss: 0.09306635989100591\n",
      "    At iteration 13500 -> loss: 0.09305481368623585\n",
      "    At iteration 13600 -> loss: 0.0930571458075196\n",
      "Staring Epoch 50\n",
      "    At iteration 0 -> loss: 0.08913557277992368\n",
      "    At iteration 100 -> loss: 0.09104969619539759\n",
      "    At iteration 200 -> loss: 0.09028095797316835\n",
      "    At iteration 300 -> loss: 0.09045021559575953\n",
      "    At iteration 400 -> loss: 0.0908984662521567\n",
      "    At iteration 500 -> loss: 0.09053441920481539\n",
      "    At iteration 600 -> loss: 0.09038639379229049\n",
      "    At iteration 700 -> loss: 0.09067959834046523\n",
      "    At iteration 800 -> loss: 0.0911051467602714\n",
      "    At iteration 900 -> loss: 0.09105432065700816\n",
      "    At iteration 1000 -> loss: 0.09096160422503331\n",
      "    At iteration 1100 -> loss: 0.09114429642746481\n",
      "    At iteration 1200 -> loss: 0.09110255381554253\n",
      "    At iteration 1300 -> loss: 0.09101361614650652\n",
      "    At iteration 1400 -> loss: 0.0910131099509541\n",
      "    At iteration 1500 -> loss: 0.0915007098244113\n",
      "    At iteration 1600 -> loss: 0.09143016225909259\n",
      "    At iteration 1700 -> loss: 0.09130100830230842\n",
      "    At iteration 1800 -> loss: 0.09144364942907646\n",
      "    At iteration 1900 -> loss: 0.09149298789606321\n",
      "    At iteration 2000 -> loss: 0.09134397649162683\n",
      "    At iteration 2100 -> loss: 0.09142215192035544\n",
      "    At iteration 2200 -> loss: 0.09148950363454593\n",
      "    At iteration 2300 -> loss: 0.09158273739507204\n",
      "    At iteration 2400 -> loss: 0.09169500128733268\n",
      "    At iteration 2500 -> loss: 0.09171140974280352\n",
      "    At iteration 2600 -> loss: 0.09191911999232573\n",
      "    At iteration 2700 -> loss: 0.09179502260663312\n",
      "    At iteration 2800 -> loss: 0.09184837908359814\n",
      "    At iteration 2900 -> loss: 0.09193338515548019\n",
      "    At iteration 3000 -> loss: 0.09224570998609055\n",
      "    At iteration 3100 -> loss: 0.09225584227190121\n",
      "    At iteration 3200 -> loss: 0.09226232120923496\n",
      "    At iteration 3300 -> loss: 0.09221200298272153\n",
      "    At iteration 3400 -> loss: 0.09248333803455112\n",
      "    At iteration 3500 -> loss: 0.09243875674008413\n",
      "    At iteration 3600 -> loss: 0.0924169432072117\n",
      "    At iteration 3700 -> loss: 0.09252104655526108\n",
      "    At iteration 3800 -> loss: 0.09249689720483839\n",
      "    At iteration 3900 -> loss: 0.09250211985376748\n",
      "    At iteration 4000 -> loss: 0.09246926708048743\n",
      "    At iteration 4100 -> loss: 0.09245404945214321\n",
      "    At iteration 4200 -> loss: 0.09258676250698643\n",
      "    At iteration 4300 -> loss: 0.09250854756215059\n",
      "    At iteration 4400 -> loss: 0.09264577590826152\n",
      "    At iteration 4500 -> loss: 0.09284951542924573\n",
      "    At iteration 4600 -> loss: 0.09276152495409684\n",
      "    At iteration 4700 -> loss: 0.09269025278827052\n",
      "    At iteration 4800 -> loss: 0.09260633733771907\n",
      "    At iteration 4900 -> loss: 0.09264358021090159\n",
      "    At iteration 5000 -> loss: 0.09257607102941008\n",
      "    At iteration 5100 -> loss: 0.09253606027783604\n",
      "    At iteration 5200 -> loss: 0.09257409203784743\n",
      "    At iteration 5300 -> loss: 0.09262390408432412\n",
      "    At iteration 5400 -> loss: 0.09262767345904607\n",
      "    At iteration 5500 -> loss: 0.09285942394276062\n",
      "    At iteration 5600 -> loss: 0.09289372269266331\n",
      "    At iteration 5700 -> loss: 0.09297188026920218\n",
      "    At iteration 5800 -> loss: 0.09297266957038693\n",
      "    At iteration 5900 -> loss: 0.09296523676352038\n",
      "    At iteration 6000 -> loss: 0.09291245217272881\n",
      "    At iteration 6100 -> loss: 0.09291784774320143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6200 -> loss: 0.09287202580115397\n",
      "    At iteration 6300 -> loss: 0.09292724937272538\n",
      "    At iteration 6400 -> loss: 0.09310310564075316\n",
      "    At iteration 6500 -> loss: 0.09319217816247057\n",
      "    At iteration 6600 -> loss: 0.09315759026006451\n",
      "    At iteration 6700 -> loss: 0.09309990231636206\n",
      "    At iteration 6800 -> loss: 0.09309470336965384\n",
      "    At iteration 6900 -> loss: 0.09319454602070341\n",
      "    At iteration 7000 -> loss: 0.09321879057463747\n",
      "    At iteration 7100 -> loss: 0.0931943882974391\n",
      "    At iteration 7200 -> loss: 0.09320190627402257\n",
      "    At iteration 7300 -> loss: 0.09315132655058192\n",
      "    At iteration 7400 -> loss: 0.09315931292015976\n",
      "    At iteration 7500 -> loss: 0.09312732909058549\n",
      "    At iteration 7600 -> loss: 0.09307919122024254\n",
      "    At iteration 7700 -> loss: 0.09307767033018181\n",
      "    At iteration 7800 -> loss: 0.09305267761000469\n",
      "    At iteration 7900 -> loss: 0.09304637754426114\n",
      "    At iteration 8000 -> loss: 0.0931847290066182\n",
      "    At iteration 8100 -> loss: 0.09317002820522337\n",
      "    At iteration 8200 -> loss: 0.09311821222244604\n",
      "    At iteration 8300 -> loss: 0.09307993878631503\n",
      "    At iteration 8400 -> loss: 0.09308150661145112\n",
      "    At iteration 8500 -> loss: 0.09303413232107662\n",
      "    At iteration 8600 -> loss: 0.09301673411734761\n",
      "    At iteration 8700 -> loss: 0.09299481231678076\n",
      "    At iteration 8800 -> loss: 0.09298689086980252\n",
      "    At iteration 8900 -> loss: 0.09306303421350616\n",
      "    At iteration 9000 -> loss: 0.09302722190480069\n",
      "    At iteration 9100 -> loss: 0.09300177323665408\n",
      "    At iteration 9200 -> loss: 0.09301364444178738\n",
      "    At iteration 9300 -> loss: 0.09297566805261386\n",
      "    At iteration 9400 -> loss: 0.09297770348042234\n",
      "    At iteration 9500 -> loss: 0.09295334833672961\n",
      "    At iteration 9600 -> loss: 0.09299546128836828\n",
      "    At iteration 9700 -> loss: 0.09297870487393983\n",
      "    At iteration 9800 -> loss: 0.09295324903113272\n",
      "    At iteration 9900 -> loss: 0.0929050127313817\n",
      "    At iteration 10000 -> loss: 0.09292191707288526\n",
      "    At iteration 10100 -> loss: 0.09292227984725902\n",
      "    At iteration 10200 -> loss: 0.0928887558196925\n",
      "    At iteration 10300 -> loss: 0.09290601503263173\n",
      "    At iteration 10400 -> loss: 0.0928853338404087\n",
      "    At iteration 10500 -> loss: 0.09287033015456148\n",
      "    At iteration 10600 -> loss: 0.09291035248678842\n",
      "    At iteration 10700 -> loss: 0.09288021821454398\n",
      "    At iteration 10800 -> loss: 0.09286609938220386\n",
      "    At iteration 10900 -> loss: 0.09285939159493214\n",
      "    At iteration 11000 -> loss: 0.09290020828008966\n",
      "    At iteration 11100 -> loss: 0.09288788830845779\n",
      "    At iteration 11200 -> loss: 0.09290117929663227\n",
      "    At iteration 11300 -> loss: 0.09287847360576248\n",
      "    At iteration 11400 -> loss: 0.0928566776943148\n",
      "    At iteration 11500 -> loss: 0.0929124911627273\n",
      "    At iteration 11600 -> loss: 0.09290559669323248\n",
      "    At iteration 11700 -> loss: 0.09287610614483456\n",
      "    At iteration 11800 -> loss: 0.09285252192142764\n",
      "    At iteration 11900 -> loss: 0.09289007915014941\n",
      "    At iteration 12000 -> loss: 0.09292548035926483\n",
      "    At iteration 12100 -> loss: 0.09292994878795541\n",
      "    At iteration 12200 -> loss: 0.09290894734150709\n",
      "    At iteration 12300 -> loss: 0.09289664787748496\n",
      "    At iteration 12400 -> loss: 0.09289879227328425\n",
      "    At iteration 12500 -> loss: 0.09313363725991719\n",
      "    At iteration 12600 -> loss: 0.09310425923267214\n",
      "    At iteration 12700 -> loss: 0.09309581092416147\n",
      "    At iteration 12800 -> loss: 0.09309813163307608\n",
      "    At iteration 12900 -> loss: 0.09309823446081457\n",
      "    At iteration 13000 -> loss: 0.0930634985654097\n",
      "    At iteration 13100 -> loss: 0.09305007680642956\n",
      "    At iteration 13200 -> loss: 0.09307253110301134\n",
      "    At iteration 13300 -> loss: 0.09304789086491272\n",
      "    At iteration 13400 -> loss: 0.0930414615162675\n",
      "    At iteration 13500 -> loss: 0.09305049015091747\n",
      "    At iteration 13600 -> loss: 0.09302331822062586\n",
      "Staring Epoch 51\n",
      "    At iteration 0 -> loss: 0.0800072763641424\n",
      "    At iteration 100 -> loss: 0.09109183381601195\n",
      "    At iteration 200 -> loss: 0.09040341356769228\n",
      "    At iteration 300 -> loss: 0.09226296763913172\n",
      "    At iteration 400 -> loss: 0.09302780191042809\n",
      "    At iteration 500 -> loss: 0.09334441875161573\n",
      "    At iteration 600 -> loss: 0.09354390650420097\n",
      "    At iteration 700 -> loss: 0.09332269432173923\n",
      "    At iteration 800 -> loss: 0.09377315664668986\n",
      "    At iteration 900 -> loss: 0.09348199993060038\n",
      "    At iteration 1000 -> loss: 0.09321183413960175\n",
      "    At iteration 1100 -> loss: 0.09292665365748749\n",
      "    At iteration 1200 -> loss: 0.09258547171514465\n",
      "    At iteration 1300 -> loss: 0.09260187146095901\n",
      "    At iteration 1400 -> loss: 0.09231860866343722\n",
      "    At iteration 1500 -> loss: 0.09328181358005906\n",
      "    At iteration 1600 -> loss: 0.09311020515055392\n",
      "    At iteration 1700 -> loss: 0.09344926083326029\n",
      "    At iteration 1800 -> loss: 0.09346799145177499\n",
      "    At iteration 1900 -> loss: 0.09340435133755182\n",
      "    At iteration 2000 -> loss: 0.0933527269726883\n",
      "    At iteration 2100 -> loss: 0.09329505307965326\n",
      "    At iteration 2200 -> loss: 0.09334128683182508\n",
      "    At iteration 2300 -> loss: 0.09337493366777358\n",
      "    At iteration 2400 -> loss: 0.09321019588676618\n",
      "    At iteration 2500 -> loss: 0.09318639228401142\n",
      "    At iteration 2600 -> loss: 0.09319712898652074\n",
      "    At iteration 2700 -> loss: 0.09314908278027728\n",
      "    At iteration 2800 -> loss: 0.09318715350109318\n",
      "    At iteration 2900 -> loss: 0.09319262666131095\n",
      "    At iteration 3000 -> loss: 0.09319220908321707\n",
      "    At iteration 3100 -> loss: 0.09306718760833986\n",
      "    At iteration 3200 -> loss: 0.09295722469895103\n",
      "    At iteration 3300 -> loss: 0.09291320634887623\n",
      "    At iteration 3400 -> loss: 0.09303084190088148\n",
      "    At iteration 3500 -> loss: 0.09293073937567815\n",
      "    At iteration 3600 -> loss: 0.09291522757994804\n",
      "    At iteration 3700 -> loss: 0.09283304423170824\n",
      "    At iteration 3800 -> loss: 0.09276972072553162\n",
      "    At iteration 3900 -> loss: 0.09270080110213004\n",
      "    At iteration 4000 -> loss: 0.0926210185513375\n",
      "    At iteration 4100 -> loss: 0.09255698867824932\n",
      "    At iteration 4200 -> loss: 0.09257955678112138\n",
      "    At iteration 4300 -> loss: 0.09256845899372634\n",
      "    At iteration 4400 -> loss: 0.09277561674159159\n",
      "    At iteration 4500 -> loss: 0.0927187594864024\n",
      "    At iteration 4600 -> loss: 0.09276903055342409\n",
      "    At iteration 4700 -> loss: 0.09299990470444204\n",
      "    At iteration 4800 -> loss: 0.09323786415521713\n",
      "    At iteration 4900 -> loss: 0.0933748612613981\n",
      "    At iteration 5000 -> loss: 0.09332733448419804\n",
      "    At iteration 5100 -> loss: 0.09329706806269034\n",
      "    At iteration 5200 -> loss: 0.0932662337327171\n",
      "    At iteration 5300 -> loss: 0.09319526718554774\n",
      "    At iteration 5400 -> loss: 0.09316347124467178\n",
      "    At iteration 5500 -> loss: 0.09316482901366197\n",
      "    At iteration 5600 -> loss: 0.09308975195540857\n",
      "    At iteration 5700 -> loss: 0.0931024687554465\n",
      "    At iteration 5800 -> loss: 0.09313554436173685\n",
      "    At iteration 5900 -> loss: 0.09312902101103611\n",
      "    At iteration 6000 -> loss: 0.09315530068639492\n",
      "    At iteration 6100 -> loss: 0.09311420751532168\n",
      "    At iteration 6200 -> loss: 0.0931541179467912\n",
      "    At iteration 6300 -> loss: 0.09312615551904466\n",
      "    At iteration 6400 -> loss: 0.09308183663752849\n",
      "    At iteration 6500 -> loss: 0.09306421607584149\n",
      "    At iteration 6600 -> loss: 0.09307480581964277\n",
      "    At iteration 6700 -> loss: 0.0930847391587259\n",
      "    At iteration 6800 -> loss: 0.0930515488728335\n",
      "    At iteration 6900 -> loss: 0.09301078871279875\n",
      "    At iteration 7000 -> loss: 0.09302737362710754\n",
      "    At iteration 7100 -> loss: 0.09297080777259811\n",
      "    At iteration 7200 -> loss: 0.0929746052230394\n",
      "    At iteration 7300 -> loss: 0.09290503593526073\n",
      "    At iteration 7400 -> loss: 0.09286709545967879\n",
      "    At iteration 7500 -> loss: 0.09287641270026317\n",
      "    At iteration 7600 -> loss: 0.09288171888933411\n",
      "    At iteration 7700 -> loss: 0.09288590004716829\n",
      "    At iteration 7800 -> loss: 0.09289121445180813\n",
      "    At iteration 7900 -> loss: 0.09286280290554437\n",
      "    At iteration 8000 -> loss: 0.09282706514653066\n",
      "    At iteration 8100 -> loss: 0.09282478180246272\n",
      "    At iteration 8200 -> loss: 0.09282850882405201\n",
      "    At iteration 8300 -> loss: 0.09291938111043428\n",
      "    At iteration 8400 -> loss: 0.09290487132966002\n",
      "    At iteration 8500 -> loss: 0.09288470152773841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8600 -> loss: 0.09288001116898564\n",
      "    At iteration 8700 -> loss: 0.09286738239732316\n",
      "    At iteration 8800 -> loss: 0.09283913889027476\n",
      "    At iteration 8900 -> loss: 0.09283395185440185\n",
      "    At iteration 9000 -> loss: 0.09280741692438538\n",
      "    At iteration 9100 -> loss: 0.09278593700618853\n",
      "    At iteration 9200 -> loss: 0.09275451432872871\n",
      "    At iteration 9300 -> loss: 0.09275050575456763\n",
      "    At iteration 9400 -> loss: 0.09278936602735888\n",
      "    At iteration 9500 -> loss: 0.0928614489079187\n",
      "    At iteration 9600 -> loss: 0.09287583499532642\n",
      "    At iteration 9700 -> loss: 0.0928665176023477\n",
      "    At iteration 9800 -> loss: 0.09285539097703552\n",
      "    At iteration 9900 -> loss: 0.09282438841758588\n",
      "    At iteration 10000 -> loss: 0.09286110620820358\n",
      "    At iteration 10100 -> loss: 0.09281851303970384\n",
      "    At iteration 10200 -> loss: 0.09280824026420038\n",
      "    At iteration 10300 -> loss: 0.09284491338853297\n",
      "    At iteration 10400 -> loss: 0.09282338609836589\n",
      "    At iteration 10500 -> loss: 0.09311266012515822\n",
      "    At iteration 10600 -> loss: 0.09307917481709728\n",
      "    At iteration 10700 -> loss: 0.09309016362811612\n",
      "    At iteration 10800 -> loss: 0.09306331511146015\n",
      "    At iteration 10900 -> loss: 0.09304962623566801\n",
      "    At iteration 11000 -> loss: 0.09302442436472871\n",
      "    At iteration 11100 -> loss: 0.09300438864046738\n",
      "    At iteration 11200 -> loss: 0.09298219386803168\n",
      "    At iteration 11300 -> loss: 0.09295711574700022\n",
      "    At iteration 11400 -> loss: 0.09292851355177577\n",
      "    At iteration 11500 -> loss: 0.09292008614239416\n",
      "    At iteration 11600 -> loss: 0.09288046934211133\n",
      "    At iteration 11700 -> loss: 0.0928660463199784\n",
      "    At iteration 11800 -> loss: 0.09285585052820927\n",
      "    At iteration 11900 -> loss: 0.09283470551347248\n",
      "    At iteration 12000 -> loss: 0.09284133386912026\n",
      "    At iteration 12100 -> loss: 0.09284058525993381\n",
      "    At iteration 12200 -> loss: 0.09282181641588667\n",
      "    At iteration 12300 -> loss: 0.09284571537563462\n",
      "    At iteration 12400 -> loss: 0.09288492455211851\n",
      "    At iteration 12500 -> loss: 0.09307835140610175\n",
      "    At iteration 12600 -> loss: 0.09304886608899571\n",
      "    At iteration 12700 -> loss: 0.09305518010415118\n",
      "    At iteration 12800 -> loss: 0.09304069596553341\n",
      "    At iteration 12900 -> loss: 0.09302998875150889\n",
      "    At iteration 13000 -> loss: 0.09304195882779606\n",
      "    At iteration 13100 -> loss: 0.09300666478675064\n",
      "    At iteration 13200 -> loss: 0.0930414409464484\n",
      "    At iteration 13300 -> loss: 0.09301654913846359\n",
      "    At iteration 13400 -> loss: 0.09300764346102747\n",
      "    At iteration 13500 -> loss: 0.09299250333998317\n",
      "    At iteration 13600 -> loss: 0.09303273974394513\n",
      "Staring Epoch 52\n",
      "    At iteration 0 -> loss: 0.0800352299484075\n",
      "    At iteration 100 -> loss: 0.0886024647663586\n",
      "    At iteration 200 -> loss: 0.0899713254087243\n",
      "    At iteration 300 -> loss: 0.09041657496771621\n",
      "    At iteration 400 -> loss: 0.0898435281859035\n",
      "    At iteration 500 -> loss: 0.0901188278061748\n",
      "    At iteration 600 -> loss: 0.09076167570643934\n",
      "    At iteration 700 -> loss: 0.0928500917497707\n",
      "    At iteration 800 -> loss: 0.09379192537635782\n",
      "    At iteration 900 -> loss: 0.09373849307579724\n",
      "    At iteration 1000 -> loss: 0.0934441528208771\n",
      "    At iteration 1100 -> loss: 0.0938842770243961\n",
      "    At iteration 1200 -> loss: 0.09367213632388612\n",
      "    At iteration 1300 -> loss: 0.09356250091034155\n",
      "    At iteration 1400 -> loss: 0.09374291172744426\n",
      "    At iteration 1500 -> loss: 0.09416959056662548\n",
      "    At iteration 1600 -> loss: 0.09409371515967334\n",
      "    At iteration 1700 -> loss: 0.09377003841970981\n",
      "    At iteration 1800 -> loss: 0.09366259274032031\n",
      "    At iteration 1900 -> loss: 0.09372836872235187\n",
      "    At iteration 2000 -> loss: 0.09353880967946712\n",
      "    At iteration 2100 -> loss: 0.09333188523567595\n",
      "    At iteration 2200 -> loss: 0.09330468372942206\n",
      "    At iteration 2300 -> loss: 0.09329740045736473\n",
      "    At iteration 2400 -> loss: 0.09331896773434861\n",
      "    At iteration 2500 -> loss: 0.0935715060882571\n",
      "    At iteration 2600 -> loss: 0.09340457994124583\n",
      "    At iteration 2700 -> loss: 0.09333200863640705\n",
      "    At iteration 2800 -> loss: 0.09322923495999863\n",
      "    At iteration 2900 -> loss: 0.09316104697131444\n",
      "    At iteration 3000 -> loss: 0.09313605974126253\n",
      "    At iteration 3100 -> loss: 0.09326941204450123\n",
      "    At iteration 3200 -> loss: 0.09315069512201686\n",
      "    At iteration 3300 -> loss: 0.09307758120039766\n",
      "    At iteration 3400 -> loss: 0.0933975836070715\n",
      "    At iteration 3500 -> loss: 0.09330610429246892\n",
      "    At iteration 3600 -> loss: 0.093227647222421\n",
      "    At iteration 3700 -> loss: 0.09315635832815052\n",
      "    At iteration 3800 -> loss: 0.09328369795117512\n",
      "    At iteration 3900 -> loss: 0.09328635195214247\n",
      "    At iteration 4000 -> loss: 0.09326103354442537\n",
      "    At iteration 4100 -> loss: 0.09318474572032462\n",
      "    At iteration 4200 -> loss: 0.09329468082478584\n",
      "    At iteration 4300 -> loss: 0.09317302725955576\n",
      "    At iteration 4400 -> loss: 0.09313199787493345\n",
      "    At iteration 4500 -> loss: 0.09317564370960918\n",
      "    At iteration 4600 -> loss: 0.09307898874118409\n",
      "    At iteration 4700 -> loss: 0.09299358359158584\n",
      "    At iteration 4800 -> loss: 0.09296904685583736\n",
      "    At iteration 4900 -> loss: 0.09299533005761652\n",
      "    At iteration 5000 -> loss: 0.09295343571031882\n",
      "    At iteration 5100 -> loss: 0.09291617638241119\n",
      "    At iteration 5200 -> loss: 0.09288317999491319\n",
      "    At iteration 5300 -> loss: 0.09286731144456435\n",
      "    At iteration 5400 -> loss: 0.09278377929426221\n",
      "    At iteration 5500 -> loss: 0.09289654962168141\n",
      "    At iteration 5600 -> loss: 0.0928606145902847\n",
      "    At iteration 5700 -> loss: 0.09280337488793926\n",
      "    At iteration 5800 -> loss: 0.09289207706804245\n",
      "    At iteration 5900 -> loss: 0.09285270353960265\n",
      "    At iteration 6000 -> loss: 0.09284050673902966\n",
      "    At iteration 6100 -> loss: 0.09285834155750437\n",
      "    At iteration 6200 -> loss: 0.09279655398728144\n",
      "    At iteration 6300 -> loss: 0.0928084136757209\n",
      "    At iteration 6400 -> loss: 0.09288185820323695\n",
      "    At iteration 6500 -> loss: 0.09283355565325008\n",
      "    At iteration 6600 -> loss: 0.09278137217833216\n",
      "    At iteration 6700 -> loss: 0.09276049695185067\n",
      "    At iteration 6800 -> loss: 0.09269196154442015\n",
      "    At iteration 6900 -> loss: 0.09266573935307734\n",
      "    At iteration 7000 -> loss: 0.0926315432643062\n",
      "    At iteration 7100 -> loss: 0.09261310965574639\n",
      "    At iteration 7200 -> loss: 0.09266780905487132\n",
      "    At iteration 7300 -> loss: 0.09264371547811073\n",
      "    At iteration 7400 -> loss: 0.09263081612380879\n",
      "    At iteration 7500 -> loss: 0.09262346226000882\n",
      "    At iteration 7600 -> loss: 0.09260552192811983\n",
      "    At iteration 7700 -> loss: 0.09263486515813615\n",
      "    At iteration 7800 -> loss: 0.0925874618939045\n",
      "    At iteration 7900 -> loss: 0.09265895058932719\n",
      "    At iteration 8000 -> loss: 0.09267945244157286\n",
      "    At iteration 8100 -> loss: 0.09279901886593292\n",
      "    At iteration 8200 -> loss: 0.09277731007288217\n",
      "    At iteration 8300 -> loss: 0.09280736109567862\n",
      "    At iteration 8400 -> loss: 0.09283311856468963\n",
      "    At iteration 8500 -> loss: 0.09278175291884175\n",
      "    At iteration 8600 -> loss: 0.09277747686213043\n",
      "    At iteration 8700 -> loss: 0.0927443360027393\n",
      "    At iteration 8800 -> loss: 0.092749778019036\n",
      "    At iteration 8900 -> loss: 0.09272367653719\n",
      "    At iteration 9000 -> loss: 0.09268550120786724\n",
      "    At iteration 9100 -> loss: 0.09268872678524953\n",
      "    At iteration 9200 -> loss: 0.09264024088712902\n",
      "    At iteration 9300 -> loss: 0.09262772352984491\n",
      "    At iteration 9400 -> loss: 0.09260251088503464\n",
      "    At iteration 9500 -> loss: 0.09262368274237744\n",
      "    At iteration 9600 -> loss: 0.09259097608494654\n",
      "    At iteration 9700 -> loss: 0.09254579873990482\n",
      "    At iteration 9800 -> loss: 0.09252964134980447\n",
      "    At iteration 9900 -> loss: 0.09255574012577575\n",
      "    At iteration 10000 -> loss: 0.09253487713404089\n",
      "    At iteration 10100 -> loss: 0.09262440872721943\n",
      "    At iteration 10200 -> loss: 0.092614535963974\n",
      "    At iteration 10300 -> loss: 0.09259379008681547\n",
      "    At iteration 10400 -> loss: 0.09256752885020883\n",
      "    At iteration 10500 -> loss: 0.09256320611789697\n",
      "    At iteration 10600 -> loss: 0.09259760152259673\n",
      "    At iteration 10700 -> loss: 0.09257792358420448\n",
      "    At iteration 10800 -> loss: 0.09255351962300266\n",
      "    At iteration 10900 -> loss: 0.0925434677793486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11000 -> loss: 0.0925291558827715\n",
      "    At iteration 11100 -> loss: 0.09254139479391572\n",
      "    At iteration 11200 -> loss: 0.09250693832010191\n",
      "    At iteration 11300 -> loss: 0.09277307037653229\n",
      "    At iteration 11400 -> loss: 0.0927582961460997\n",
      "    At iteration 11500 -> loss: 0.09274493451864038\n",
      "    At iteration 11600 -> loss: 0.0927314208625991\n",
      "    At iteration 11700 -> loss: 0.09269157407417536\n",
      "    At iteration 11800 -> loss: 0.09267554696525367\n",
      "    At iteration 11900 -> loss: 0.09275890252231034\n",
      "    At iteration 12000 -> loss: 0.09279913856493277\n",
      "    At iteration 12100 -> loss: 0.09283441051209886\n",
      "    At iteration 12200 -> loss: 0.0927992541973608\n",
      "    At iteration 12300 -> loss: 0.09278415850318075\n",
      "    At iteration 12400 -> loss: 0.09277415879733157\n",
      "    At iteration 12500 -> loss: 0.09281909049926798\n",
      "    At iteration 12600 -> loss: 0.09281103012591639\n",
      "    At iteration 12700 -> loss: 0.09278571754888151\n",
      "    At iteration 12800 -> loss: 0.09277538878483141\n",
      "    At iteration 12900 -> loss: 0.09277394799949551\n",
      "    At iteration 13000 -> loss: 0.09283879945765673\n",
      "    At iteration 13100 -> loss: 0.09287599845475412\n",
      "    At iteration 13200 -> loss: 0.09286068570327002\n",
      "    At iteration 13300 -> loss: 0.09290980322621584\n",
      "    At iteration 13400 -> loss: 0.09298287000102176\n",
      "    At iteration 13500 -> loss: 0.09298542340342272\n",
      "    At iteration 13600 -> loss: 0.09297153925015313\n",
      "Staring Epoch 53\n",
      "    At iteration 0 -> loss: 0.08038902869884623\n",
      "    At iteration 100 -> loss: 0.08881877706234306\n",
      "    At iteration 200 -> loss: 0.09011662060579279\n",
      "    At iteration 300 -> loss: 0.09082823162824064\n",
      "    At iteration 400 -> loss: 0.0906017404716864\n",
      "    At iteration 500 -> loss: 0.09151037169595402\n",
      "    At iteration 600 -> loss: 0.09172953523048336\n",
      "    At iteration 700 -> loss: 0.09148971107929414\n",
      "    At iteration 800 -> loss: 0.09146447077006178\n",
      "    At iteration 900 -> loss: 0.0931476586917552\n",
      "    At iteration 1000 -> loss: 0.0929335658550014\n",
      "    At iteration 1100 -> loss: 0.09290398696392548\n",
      "    At iteration 1200 -> loss: 0.09289107417290168\n",
      "    At iteration 1300 -> loss: 0.09272798488144199\n",
      "    At iteration 1400 -> loss: 0.09277525704276592\n",
      "    At iteration 1500 -> loss: 0.09250575664231725\n",
      "    At iteration 1600 -> loss: 0.09232228087036637\n",
      "    At iteration 1700 -> loss: 0.0940017899214722\n",
      "    At iteration 1800 -> loss: 0.09433977448721756\n",
      "    At iteration 1900 -> loss: 0.09461357292871272\n",
      "    At iteration 2000 -> loss: 0.09449002359749442\n",
      "    At iteration 2100 -> loss: 0.09415255668828396\n",
      "    At iteration 2200 -> loss: 0.09463112429836712\n",
      "    At iteration 2300 -> loss: 0.09470714478169572\n",
      "    At iteration 2400 -> loss: 0.0946848540312587\n",
      "    At iteration 2500 -> loss: 0.09452045572494461\n",
      "    At iteration 2600 -> loss: 0.0943019974897835\n",
      "    At iteration 2700 -> loss: 0.09421696273206494\n",
      "    At iteration 2800 -> loss: 0.09440192103958414\n",
      "    At iteration 2900 -> loss: 0.09424138676695183\n",
      "    At iteration 3000 -> loss: 0.09404912747197723\n",
      "    At iteration 3100 -> loss: 0.09392374093345784\n",
      "    At iteration 3200 -> loss: 0.09400195819644354\n",
      "    At iteration 3300 -> loss: 0.09387220716884473\n",
      "    At iteration 3400 -> loss: 0.09383003240677582\n",
      "    At iteration 3500 -> loss: 0.0937408861194897\n",
      "    At iteration 3600 -> loss: 0.09371402887859703\n",
      "    At iteration 3700 -> loss: 0.09368511460153905\n",
      "    At iteration 3800 -> loss: 0.0936535553383451\n",
      "    At iteration 3900 -> loss: 0.09368736136078405\n",
      "    At iteration 4000 -> loss: 0.09352741422932266\n",
      "    At iteration 4100 -> loss: 0.09363770136461112\n",
      "    At iteration 4200 -> loss: 0.09383781828571933\n",
      "    At iteration 4300 -> loss: 0.09388527387396546\n",
      "    At iteration 4400 -> loss: 0.09385694308862537\n",
      "    At iteration 4500 -> loss: 0.09393703461612593\n",
      "    At iteration 4600 -> loss: 0.09385921629265896\n",
      "    At iteration 4700 -> loss: 0.09376780368374017\n",
      "    At iteration 4800 -> loss: 0.09377538149867617\n",
      "    At iteration 4900 -> loss: 0.09370347695959416\n",
      "    At iteration 5000 -> loss: 0.09363733740549934\n",
      "    At iteration 5100 -> loss: 0.09365439007872535\n",
      "    At iteration 5200 -> loss: 0.09364142921769186\n",
      "    At iteration 5300 -> loss: 0.09357669970963851\n",
      "    At iteration 5400 -> loss: 0.09352069676232365\n",
      "    At iteration 5500 -> loss: 0.09342428144360249\n",
      "    At iteration 5600 -> loss: 0.09338671622055796\n",
      "    At iteration 5700 -> loss: 0.0935342834546203\n",
      "    At iteration 5800 -> loss: 0.09352956240247724\n",
      "    At iteration 5900 -> loss: 0.09351399353602433\n",
      "    At iteration 6000 -> loss: 0.09349369343597344\n",
      "    At iteration 6100 -> loss: 0.09350108745584057\n",
      "    At iteration 6200 -> loss: 0.09346280409811251\n",
      "    At iteration 6300 -> loss: 0.09358856071967757\n",
      "    At iteration 6400 -> loss: 0.093505165321528\n",
      "    At iteration 6500 -> loss: 0.0934446019971643\n",
      "    At iteration 6600 -> loss: 0.09343020234205458\n",
      "    At iteration 6700 -> loss: 0.09339769606865665\n",
      "    At iteration 6800 -> loss: 0.09339228461727192\n",
      "    At iteration 6900 -> loss: 0.09343895137680298\n",
      "    At iteration 7000 -> loss: 0.09341859500662966\n",
      "    At iteration 7100 -> loss: 0.09342519607746388\n",
      "    At iteration 7200 -> loss: 0.09333899144816139\n",
      "    At iteration 7300 -> loss: 0.09331397579339155\n",
      "    At iteration 7400 -> loss: 0.09329394837758201\n",
      "    At iteration 7500 -> loss: 0.09328364148430299\n",
      "    At iteration 7600 -> loss: 0.09323635016216825\n",
      "    At iteration 7700 -> loss: 0.09317819171005878\n",
      "    At iteration 7800 -> loss: 0.09319878376995894\n",
      "    At iteration 7900 -> loss: 0.09316920348494862\n",
      "    At iteration 8000 -> loss: 0.09314587476319905\n",
      "    At iteration 8100 -> loss: 0.09311335944509697\n",
      "    At iteration 8200 -> loss: 0.09312104764248746\n",
      "    At iteration 8300 -> loss: 0.09313284626601262\n",
      "    At iteration 8400 -> loss: 0.09313778428081092\n",
      "    At iteration 8500 -> loss: 0.0931458948782088\n",
      "    At iteration 8600 -> loss: 0.09310276966421845\n",
      "    At iteration 8700 -> loss: 0.09310309414640468\n",
      "    At iteration 8800 -> loss: 0.09307001684767245\n",
      "    At iteration 8900 -> loss: 0.09305255266084264\n",
      "    At iteration 9000 -> loss: 0.09303576952234166\n",
      "    At iteration 9100 -> loss: 0.09301769579264249\n",
      "    At iteration 9200 -> loss: 0.09302584285948858\n",
      "    At iteration 9300 -> loss: 0.0930299408334266\n",
      "    At iteration 9400 -> loss: 0.09299310655328274\n",
      "    At iteration 9500 -> loss: 0.09301244617259388\n",
      "    At iteration 9600 -> loss: 0.09298845380606163\n",
      "    At iteration 9700 -> loss: 0.09297131240838397\n",
      "    At iteration 9800 -> loss: 0.09307355854441353\n",
      "    At iteration 9900 -> loss: 0.09311867399572953\n",
      "    At iteration 10000 -> loss: 0.09309194362766646\n",
      "    At iteration 10100 -> loss: 0.09309025617878566\n",
      "    At iteration 10200 -> loss: 0.0930801632860723\n",
      "    At iteration 10300 -> loss: 0.09304540333850111\n",
      "    At iteration 10400 -> loss: 0.0930300194498423\n",
      "    At iteration 10500 -> loss: 0.09303055040357347\n",
      "    At iteration 10600 -> loss: 0.09300053020551502\n",
      "    At iteration 10700 -> loss: 0.09302481929099148\n",
      "    At iteration 10800 -> loss: 0.09300155665683081\n",
      "    At iteration 10900 -> loss: 0.09301126192159283\n",
      "    At iteration 11000 -> loss: 0.09301036898444637\n",
      "    At iteration 11100 -> loss: 0.09303140540683301\n",
      "    At iteration 11200 -> loss: 0.09304034932326163\n",
      "    At iteration 11300 -> loss: 0.09308362639202518\n",
      "    At iteration 11400 -> loss: 0.09306594055321706\n",
      "    At iteration 11500 -> loss: 0.09302613187325494\n",
      "    At iteration 11600 -> loss: 0.09301497467488423\n",
      "    At iteration 11700 -> loss: 0.0930024003167667\n",
      "    At iteration 11800 -> loss: 0.09313129548193091\n",
      "    At iteration 11900 -> loss: 0.09311178351332138\n",
      "    At iteration 12000 -> loss: 0.09308847071797839\n",
      "    At iteration 12100 -> loss: 0.09307325171493092\n",
      "    At iteration 12200 -> loss: 0.09305765169959858\n",
      "    At iteration 12300 -> loss: 0.09304790895278632\n",
      "    At iteration 12400 -> loss: 0.09303114878071116\n",
      "    At iteration 12500 -> loss: 0.09301690563941387\n",
      "    At iteration 12600 -> loss: 0.09306423734727903\n",
      "    At iteration 12700 -> loss: 0.09304105252146923\n",
      "    At iteration 12800 -> loss: 0.09302515895121212\n",
      "    At iteration 12900 -> loss: 0.09301425943914453\n",
      "    At iteration 13000 -> loss: 0.0930050375342799\n",
      "    At iteration 13100 -> loss: 0.09299043643779317\n",
      "    At iteration 13200 -> loss: 0.09297531055015723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 13300 -> loss: 0.09296515733846251\n",
      "    At iteration 13400 -> loss: 0.0929407489873948\n",
      "    At iteration 13500 -> loss: 0.09291575550977815\n",
      "    At iteration 13600 -> loss: 0.09296596278319556\n",
      "Staring Epoch 54\n",
      "    At iteration 0 -> loss: 0.09291125927120447\n",
      "    At iteration 100 -> loss: 0.08991598576412012\n",
      "    At iteration 200 -> loss: 0.0906826043777398\n",
      "    At iteration 300 -> loss: 0.09183725307628596\n",
      "    At iteration 400 -> loss: 0.09134378785402887\n",
      "    At iteration 500 -> loss: 0.09144589585381212\n",
      "    At iteration 600 -> loss: 0.09143176811402469\n",
      "    At iteration 700 -> loss: 0.09141052186810167\n",
      "    At iteration 800 -> loss: 0.09115341067870511\n",
      "    At iteration 900 -> loss: 0.09121740431697813\n",
      "    At iteration 1000 -> loss: 0.09143814814991227\n",
      "    At iteration 1100 -> loss: 0.09193257499171528\n",
      "    At iteration 1200 -> loss: 0.09182979165150228\n",
      "    At iteration 1300 -> loss: 0.09178232656652813\n",
      "    At iteration 1400 -> loss: 0.09239917538184313\n",
      "    At iteration 1500 -> loss: 0.09236885518834972\n",
      "    At iteration 1600 -> loss: 0.09246448108994575\n",
      "    At iteration 1700 -> loss: 0.09235572493502306\n",
      "    At iteration 1800 -> loss: 0.09234155335416941\n",
      "    At iteration 1900 -> loss: 0.09224161570383967\n",
      "    At iteration 2000 -> loss: 0.09252888332669135\n",
      "    At iteration 2100 -> loss: 0.09272373757659665\n",
      "    At iteration 2200 -> loss: 0.09300554351735686\n",
      "    At iteration 2300 -> loss: 0.09284494499397425\n",
      "    At iteration 2400 -> loss: 0.09273920487899717\n",
      "    At iteration 2500 -> loss: 0.09302000456869984\n",
      "    At iteration 2600 -> loss: 0.09286138560175025\n",
      "    At iteration 2700 -> loss: 0.09282205971049132\n",
      "    At iteration 2800 -> loss: 0.09281815219077351\n",
      "    At iteration 2900 -> loss: 0.09276392975563473\n",
      "    At iteration 3000 -> loss: 0.09294728629225728\n",
      "    At iteration 3100 -> loss: 0.09307251177503403\n",
      "    At iteration 3200 -> loss: 0.0929820035593477\n",
      "    At iteration 3300 -> loss: 0.09292960279702223\n",
      "    At iteration 3400 -> loss: 0.09284462508282132\n",
      "    At iteration 3500 -> loss: 0.0927700640449808\n",
      "    At iteration 3600 -> loss: 0.09275152823865956\n",
      "    At iteration 3700 -> loss: 0.09273958888679106\n",
      "    At iteration 3800 -> loss: 0.09273088148069442\n",
      "    At iteration 3900 -> loss: 0.09268169533509614\n",
      "    At iteration 4000 -> loss: 0.09262725533938462\n",
      "    At iteration 4100 -> loss: 0.09267956109428842\n",
      "    At iteration 4200 -> loss: 0.09270194896426573\n",
      "    At iteration 4300 -> loss: 0.0926087608830987\n",
      "    At iteration 4400 -> loss: 0.09262626794092364\n",
      "    At iteration 4500 -> loss: 0.09256607717680185\n",
      "    At iteration 4600 -> loss: 0.09253839885094989\n",
      "    At iteration 4700 -> loss: 0.0924986965360245\n",
      "    At iteration 4800 -> loss: 0.09243038535583006\n",
      "    At iteration 4900 -> loss: 0.09241544328060546\n",
      "    At iteration 5000 -> loss: 0.09242187372420314\n",
      "    At iteration 5100 -> loss: 0.09242866337172735\n",
      "    At iteration 5200 -> loss: 0.09255455890457394\n",
      "    At iteration 5300 -> loss: 0.09254837383192777\n",
      "    At iteration 5400 -> loss: 0.09249917249102661\n",
      "    At iteration 5500 -> loss: 0.09261132549719536\n",
      "    At iteration 5600 -> loss: 0.09255821820341086\n",
      "    At iteration 5700 -> loss: 0.09248445496846175\n",
      "    At iteration 5800 -> loss: 0.09250003135979609\n",
      "    At iteration 5900 -> loss: 0.0927053221268191\n",
      "    At iteration 6000 -> loss: 0.09266472057895389\n",
      "    At iteration 6100 -> loss: 0.09268581742178085\n",
      "    At iteration 6200 -> loss: 0.09271200706299113\n",
      "    At iteration 6300 -> loss: 0.09272148358707209\n",
      "    At iteration 6400 -> loss: 0.0926971229780753\n",
      "    At iteration 6500 -> loss: 0.09269607155353479\n",
      "    At iteration 6600 -> loss: 0.09269158602205337\n",
      "    At iteration 6700 -> loss: 0.09263640179940945\n",
      "    At iteration 6800 -> loss: 0.09266829739240351\n",
      "    At iteration 6900 -> loss: 0.09268070188650936\n",
      "    At iteration 7000 -> loss: 0.09266833428835823\n",
      "    At iteration 7100 -> loss: 0.09263821112724281\n",
      "    At iteration 7200 -> loss: 0.09259762182200787\n",
      "    At iteration 7300 -> loss: 0.09261590381851088\n",
      "    At iteration 7400 -> loss: 0.0926127909907467\n",
      "    At iteration 7500 -> loss: 0.09268380223250632\n",
      "    At iteration 7600 -> loss: 0.0926621850861474\n",
      "    At iteration 7700 -> loss: 0.09268706093880572\n",
      "    At iteration 7800 -> loss: 0.0926769580971034\n",
      "    At iteration 7900 -> loss: 0.09262563416041818\n",
      "    At iteration 8000 -> loss: 0.092676247818402\n",
      "    At iteration 8100 -> loss: 0.09274632412011075\n",
      "    At iteration 8200 -> loss: 0.09276575291394913\n",
      "    At iteration 8300 -> loss: 0.09277901962445996\n",
      "    At iteration 8400 -> loss: 0.09284129472586643\n",
      "    At iteration 8500 -> loss: 0.09284681655893073\n",
      "    At iteration 8600 -> loss: 0.0928683771400059\n",
      "    At iteration 8700 -> loss: 0.09286629062278877\n",
      "    At iteration 8800 -> loss: 0.09289153096881562\n",
      "    At iteration 8900 -> loss: 0.09289298784616065\n",
      "    At iteration 9000 -> loss: 0.0929837986847062\n",
      "    At iteration 9100 -> loss: 0.09295733817351043\n",
      "    At iteration 9200 -> loss: 0.09291475905754176\n",
      "    At iteration 9300 -> loss: 0.09287048680385819\n",
      "    At iteration 9400 -> loss: 0.09286709686778159\n",
      "    At iteration 9500 -> loss: 0.09288695912668055\n",
      "    At iteration 9600 -> loss: 0.0928709131886561\n",
      "    At iteration 9700 -> loss: 0.09287442151165955\n",
      "    At iteration 9800 -> loss: 0.09288728020184414\n",
      "    At iteration 9900 -> loss: 0.0928679151142375\n",
      "    At iteration 10000 -> loss: 0.09286612149882897\n",
      "    At iteration 10100 -> loss: 0.09282935465175046\n",
      "    At iteration 10200 -> loss: 0.09281929784099917\n",
      "    At iteration 10300 -> loss: 0.09290847016260342\n",
      "    At iteration 10400 -> loss: 0.0928772161760098\n",
      "    At iteration 10500 -> loss: 0.09282882725837684\n",
      "    At iteration 10600 -> loss: 0.09281201271616919\n",
      "    At iteration 10700 -> loss: 0.09282514252660491\n",
      "    At iteration 10800 -> loss: 0.09284492059206709\n",
      "    At iteration 10900 -> loss: 0.09281187490166243\n",
      "    At iteration 11000 -> loss: 0.0927798574492207\n",
      "    At iteration 11100 -> loss: 0.09276627719468919\n",
      "    At iteration 11200 -> loss: 0.09275054464453981\n",
      "    At iteration 11300 -> loss: 0.09276290362052426\n",
      "    At iteration 11400 -> loss: 0.09280873610884852\n",
      "    At iteration 11500 -> loss: 0.09278734917872894\n",
      "    At iteration 11600 -> loss: 0.09276229052276458\n",
      "    At iteration 11700 -> loss: 0.09277451566887472\n",
      "    At iteration 11800 -> loss: 0.09276677953501905\n",
      "    At iteration 11900 -> loss: 0.09276500265309717\n",
      "    At iteration 12000 -> loss: 0.09278031691467217\n",
      "    At iteration 12100 -> loss: 0.09303532238549984\n",
      "    At iteration 12200 -> loss: 0.09300638297693413\n",
      "    At iteration 12300 -> loss: 0.09300044473324537\n",
      "    At iteration 12400 -> loss: 0.09298085088427058\n",
      "    At iteration 12500 -> loss: 0.0929673028916606\n",
      "    At iteration 12600 -> loss: 0.09296370370102937\n",
      "    At iteration 12700 -> loss: 0.09295553564843326\n",
      "    At iteration 12800 -> loss: 0.09294540941433503\n",
      "    At iteration 12900 -> loss: 0.09299878247696912\n",
      "    At iteration 13000 -> loss: 0.09303110191737649\n",
      "    At iteration 13100 -> loss: 0.09300416365480701\n",
      "    At iteration 13200 -> loss: 0.09302276640257177\n",
      "    At iteration 13300 -> loss: 0.0929942486572733\n",
      "    At iteration 13400 -> loss: 0.0929853085592745\n",
      "    At iteration 13500 -> loss: 0.09296830035042233\n",
      "    At iteration 13600 -> loss: 0.09298252467634487\n",
      "Staring Epoch 55\n",
      "    At iteration 0 -> loss: 0.08810930512845516\n",
      "    At iteration 100 -> loss: 0.09690111494147698\n",
      "    At iteration 200 -> loss: 0.09380095299097198\n",
      "    At iteration 300 -> loss: 0.09414960307659918\n",
      "    At iteration 400 -> loss: 0.09328841954972565\n",
      "    At iteration 500 -> loss: 0.09309051229938754\n",
      "    At iteration 600 -> loss: 0.09294252427518557\n",
      "    At iteration 700 -> loss: 0.09223795421946725\n",
      "    At iteration 800 -> loss: 0.09266929749579542\n",
      "    At iteration 900 -> loss: 0.0925906632258279\n",
      "    At iteration 1000 -> loss: 0.09231942081951783\n",
      "    At iteration 1100 -> loss: 0.09212842257607924\n",
      "    At iteration 1200 -> loss: 0.09208800666740136\n",
      "    At iteration 1300 -> loss: 0.09252165346546698\n",
      "    At iteration 1400 -> loss: 0.0926095349604453\n",
      "    At iteration 1500 -> loss: 0.09266867319102612\n",
      "    At iteration 1600 -> loss: 0.0927536500661178\n",
      "    At iteration 1700 -> loss: 0.09269669066366472\n",
      "    At iteration 1800 -> loss: 0.0928493887175976\n",
      "    At iteration 1900 -> loss: 0.09273426538516455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2000 -> loss: 0.09258759444053821\n",
      "    At iteration 2100 -> loss: 0.0928479140222799\n",
      "    At iteration 2200 -> loss: 0.09280805585470209\n",
      "    At iteration 2300 -> loss: 0.09263162159873913\n",
      "    At iteration 2400 -> loss: 0.09255652788791621\n",
      "    At iteration 2500 -> loss: 0.09253239352485482\n",
      "    At iteration 2600 -> loss: 0.09242515393178927\n",
      "    At iteration 2700 -> loss: 0.09243588791943817\n",
      "    At iteration 2800 -> loss: 0.09241397884542377\n",
      "    At iteration 2900 -> loss: 0.09237489982651824\n",
      "    At iteration 3000 -> loss: 0.09232259157684462\n",
      "    At iteration 3100 -> loss: 0.09225913834204472\n",
      "    At iteration 3200 -> loss: 0.09216490990881004\n",
      "    At iteration 3300 -> loss: 0.09225503353327895\n",
      "    At iteration 3400 -> loss: 0.09228869018314212\n",
      "    At iteration 3500 -> loss: 0.09227791438598633\n",
      "    At iteration 3600 -> loss: 0.09216638673735163\n",
      "    At iteration 3700 -> loss: 0.09243258988870211\n",
      "    At iteration 3800 -> loss: 0.09254957120751434\n",
      "    At iteration 3900 -> loss: 0.09249995403192077\n",
      "    At iteration 4000 -> loss: 0.09251744087526598\n",
      "    At iteration 4100 -> loss: 0.09255739896038458\n",
      "    At iteration 4200 -> loss: 0.09257201336132041\n",
      "    At iteration 4300 -> loss: 0.09262451097678198\n",
      "    At iteration 4400 -> loss: 0.09255169359283862\n",
      "    At iteration 4500 -> loss: 0.0925073712222342\n",
      "    At iteration 4600 -> loss: 0.09248899353238808\n",
      "    At iteration 4700 -> loss: 0.09239801518108154\n",
      "    At iteration 4800 -> loss: 0.09235555793633765\n",
      "    At iteration 4900 -> loss: 0.09300166962995463\n",
      "    At iteration 5000 -> loss: 0.09292270269578168\n",
      "    At iteration 5100 -> loss: 0.09292998560917784\n",
      "    At iteration 5200 -> loss: 0.09286013747955103\n",
      "    At iteration 5300 -> loss: 0.09281315287153744\n",
      "    At iteration 5400 -> loss: 0.09287733209640234\n",
      "    At iteration 5500 -> loss: 0.09290716672047335\n",
      "    At iteration 5600 -> loss: 0.0928437448138819\n",
      "    At iteration 5700 -> loss: 0.09279078084884439\n",
      "    At iteration 5800 -> loss: 0.09274975949577621\n",
      "    At iteration 5900 -> loss: 0.09290482762812155\n",
      "    At iteration 6000 -> loss: 0.09288076771109555\n",
      "    At iteration 6100 -> loss: 0.0928395591317413\n",
      "    At iteration 6200 -> loss: 0.09279164272568109\n",
      "    At iteration 6300 -> loss: 0.09289130635510295\n",
      "    At iteration 6400 -> loss: 0.0928238221134876\n",
      "    At iteration 6500 -> loss: 0.09279554953017387\n",
      "    At iteration 6600 -> loss: 0.0928001852743941\n",
      "    At iteration 6700 -> loss: 0.09288091071490288\n",
      "    At iteration 6800 -> loss: 0.09293917596528026\n",
      "    At iteration 6900 -> loss: 0.09295885142467414\n",
      "    At iteration 7000 -> loss: 0.0929487139805359\n",
      "    At iteration 7100 -> loss: 0.09289707594025638\n",
      "    At iteration 7200 -> loss: 0.09291249261794522\n",
      "    At iteration 7300 -> loss: 0.09291528416357563\n",
      "    At iteration 7400 -> loss: 0.09289543647163498\n",
      "    At iteration 7500 -> loss: 0.09284158326076167\n",
      "    At iteration 7600 -> loss: 0.09282897821901002\n",
      "    At iteration 7700 -> loss: 0.09278295971151865\n",
      "    At iteration 7800 -> loss: 0.09273897846226993\n",
      "    At iteration 7900 -> loss: 0.09278146051591277\n",
      "    At iteration 8000 -> loss: 0.0928195040878651\n",
      "    At iteration 8100 -> loss: 0.09279166749676449\n",
      "    At iteration 8200 -> loss: 0.09277239309308614\n",
      "    At iteration 8300 -> loss: 0.09274491815690222\n",
      "    At iteration 8400 -> loss: 0.09274698138844484\n",
      "    At iteration 8500 -> loss: 0.09272920187071328\n",
      "    At iteration 8600 -> loss: 0.09269611347034414\n",
      "    At iteration 8700 -> loss: 0.09272901792986568\n",
      "    At iteration 8800 -> loss: 0.09268663050621467\n",
      "    At iteration 8900 -> loss: 0.09271426211415892\n",
      "    At iteration 9000 -> loss: 0.09274770399577689\n",
      "    At iteration 9100 -> loss: 0.09273414747585916\n",
      "    At iteration 9200 -> loss: 0.09267826414024274\n",
      "    At iteration 9300 -> loss: 0.09265540754035423\n",
      "    At iteration 9400 -> loss: 0.09267265974139972\n",
      "    At iteration 9500 -> loss: 0.09268067789474518\n",
      "    At iteration 9600 -> loss: 0.09269743727030283\n",
      "    At iteration 9700 -> loss: 0.09267939995715278\n",
      "    At iteration 9800 -> loss: 0.09266834185601704\n",
      "    At iteration 9900 -> loss: 0.09266537392493492\n",
      "    At iteration 10000 -> loss: 0.09269954117340844\n",
      "    At iteration 10100 -> loss: 0.09268515504217899\n",
      "    At iteration 10200 -> loss: 0.0926454665047349\n",
      "    At iteration 10300 -> loss: 0.09264272792260521\n",
      "    At iteration 10400 -> loss: 0.09266411464433962\n",
      "    At iteration 10500 -> loss: 0.09266311989722495\n",
      "    At iteration 10600 -> loss: 0.0926460744868665\n",
      "    At iteration 10700 -> loss: 0.09265057439720047\n",
      "    At iteration 10800 -> loss: 0.09280831788767595\n",
      "    At iteration 10900 -> loss: 0.09282652974852357\n",
      "    At iteration 11000 -> loss: 0.09278761989075066\n",
      "    At iteration 11100 -> loss: 0.09284462551362162\n",
      "    At iteration 11200 -> loss: 0.09291074009271144\n",
      "    At iteration 11300 -> loss: 0.09287917503820523\n",
      "    At iteration 11400 -> loss: 0.09288594818433119\n",
      "    At iteration 11500 -> loss: 0.09294513649272476\n",
      "    At iteration 11600 -> loss: 0.09296527877803278\n",
      "    At iteration 11700 -> loss: 0.09304181960624304\n",
      "    At iteration 11800 -> loss: 0.09303836755486473\n",
      "    At iteration 11900 -> loss: 0.093129000588179\n",
      "    At iteration 12000 -> loss: 0.09311829171413245\n",
      "    At iteration 12100 -> loss: 0.093122252416906\n",
      "    At iteration 12200 -> loss: 0.09310657062844294\n",
      "    At iteration 12300 -> loss: 0.09306789734207226\n",
      "    At iteration 12400 -> loss: 0.09311195912623545\n",
      "    At iteration 12500 -> loss: 0.09308520391744551\n",
      "    At iteration 12600 -> loss: 0.09305286889824294\n",
      "    At iteration 12700 -> loss: 0.09305756272528216\n",
      "    At iteration 12800 -> loss: 0.09304754879064228\n",
      "    At iteration 12900 -> loss: 0.09303586249629987\n",
      "    At iteration 13000 -> loss: 0.09303680039731846\n",
      "    At iteration 13100 -> loss: 0.09302726128477154\n",
      "    At iteration 13200 -> loss: 0.09301411176870286\n",
      "    At iteration 13300 -> loss: 0.09298895351307984\n",
      "    At iteration 13400 -> loss: 0.09296729289269964\n",
      "    At iteration 13500 -> loss: 0.09297831472935743\n",
      "    At iteration 13600 -> loss: 0.09301019175692427\n",
      "Staring Epoch 56\n",
      "    At iteration 0 -> loss: 0.08030183520168066\n",
      "    At iteration 100 -> loss: 0.08998394787526158\n",
      "    At iteration 200 -> loss: 0.09064794100196824\n",
      "    At iteration 300 -> loss: 0.09006008101000665\n",
      "    At iteration 400 -> loss: 0.0904120812433784\n",
      "    At iteration 500 -> loss: 0.09019437429702365\n",
      "    At iteration 600 -> loss: 0.08993945249727255\n",
      "    At iteration 700 -> loss: 0.09039887618351411\n",
      "    At iteration 800 -> loss: 0.09028489965728737\n",
      "    At iteration 900 -> loss: 0.0902565929390575\n",
      "    At iteration 1000 -> loss: 0.09053419252511187\n",
      "    At iteration 1100 -> loss: 0.09143158165903224\n",
      "    At iteration 1200 -> loss: 0.0912411696110169\n",
      "    At iteration 1300 -> loss: 0.09117095063498211\n",
      "    At iteration 1400 -> loss: 0.0912685540695745\n",
      "    At iteration 1500 -> loss: 0.09127118908026201\n",
      "    At iteration 1600 -> loss: 0.09119992072437619\n",
      "    At iteration 1700 -> loss: 0.09115342704454993\n",
      "    At iteration 1800 -> loss: 0.0911312099858245\n",
      "    At iteration 1900 -> loss: 0.0911819379744933\n",
      "    At iteration 2000 -> loss: 0.09130984874263509\n",
      "    At iteration 2100 -> loss: 0.09126828880869715\n",
      "    At iteration 2200 -> loss: 0.0918041357105609\n",
      "    At iteration 2300 -> loss: 0.09201077103135062\n",
      "    At iteration 2400 -> loss: 0.09201213785209415\n",
      "    At iteration 2500 -> loss: 0.09192527460586604\n",
      "    At iteration 2600 -> loss: 0.09188807017574885\n",
      "    At iteration 2700 -> loss: 0.09190000337988535\n",
      "    At iteration 2800 -> loss: 0.09185341509765793\n",
      "    At iteration 2900 -> loss: 0.09192944691453267\n",
      "    At iteration 3000 -> loss: 0.09207621214704771\n",
      "    At iteration 3100 -> loss: 0.09194411500140304\n",
      "    At iteration 3200 -> loss: 0.09225499324156139\n",
      "    At iteration 3300 -> loss: 0.09243527552546375\n",
      "    At iteration 3400 -> loss: 0.09235185607339591\n",
      "    At iteration 3500 -> loss: 0.0923085109159126\n",
      "    At iteration 3600 -> loss: 0.09229364968132674\n",
      "    At iteration 3700 -> loss: 0.09220242519804929\n",
      "    At iteration 3800 -> loss: 0.09226675936704734\n",
      "    At iteration 3900 -> loss: 0.09218348903990231\n",
      "    At iteration 4000 -> loss: 0.09214170702561655\n",
      "    At iteration 4100 -> loss: 0.09234432081975022\n",
      "    At iteration 4200 -> loss: 0.09236195981247608\n",
      "    At iteration 4300 -> loss: 0.0924096767755266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4400 -> loss: 0.09254505295351062\n",
      "    At iteration 4500 -> loss: 0.09256247170014195\n",
      "    At iteration 4600 -> loss: 0.09256724160406808\n",
      "    At iteration 4700 -> loss: 0.09263923870085011\n",
      "    At iteration 4800 -> loss: 0.09266778488023705\n",
      "    At iteration 4900 -> loss: 0.09273249703095848\n",
      "    At iteration 5000 -> loss: 0.09271532767270645\n",
      "    At iteration 5100 -> loss: 0.09278036462536897\n",
      "    At iteration 5200 -> loss: 0.09271959775960306\n",
      "    At iteration 5300 -> loss: 0.09268236211710047\n",
      "    At iteration 5400 -> loss: 0.09264075754058247\n",
      "    At iteration 5500 -> loss: 0.09255476100026779\n",
      "    At iteration 5600 -> loss: 0.09251311114775418\n",
      "    At iteration 5700 -> loss: 0.09251220634909235\n",
      "    At iteration 5800 -> loss: 0.09247479105203792\n",
      "    At iteration 5900 -> loss: 0.09246044449130461\n",
      "    At iteration 6000 -> loss: 0.09244277567980284\n",
      "    At iteration 6100 -> loss: 0.09253716142855839\n",
      "    At iteration 6200 -> loss: 0.09249241240735213\n",
      "    At iteration 6300 -> loss: 0.09252239915835628\n",
      "    At iteration 6400 -> loss: 0.0924909551450556\n",
      "    At iteration 6500 -> loss: 0.09259889592682255\n",
      "    At iteration 6600 -> loss: 0.09268948092042635\n",
      "    At iteration 6700 -> loss: 0.0926665046717483\n",
      "    At iteration 6800 -> loss: 0.09266509133441586\n",
      "    At iteration 6900 -> loss: 0.09266442944141984\n",
      "    At iteration 7000 -> loss: 0.09260583268411386\n",
      "    At iteration 7100 -> loss: 0.09257745266047782\n",
      "    At iteration 7200 -> loss: 0.09269847932064336\n",
      "    At iteration 7300 -> loss: 0.09266586939774672\n",
      "    At iteration 7400 -> loss: 0.09264667524030519\n",
      "    At iteration 7500 -> loss: 0.09261590230758462\n",
      "    At iteration 7600 -> loss: 0.09261602042115455\n",
      "    At iteration 7700 -> loss: 0.09255887383179423\n",
      "    At iteration 7800 -> loss: 0.09259088660038343\n",
      "    At iteration 7900 -> loss: 0.0925999833761042\n",
      "    At iteration 8000 -> loss: 0.09259475306545227\n",
      "    At iteration 8100 -> loss: 0.09259276144990451\n",
      "    At iteration 8200 -> loss: 0.09257402791059176\n",
      "    At iteration 8300 -> loss: 0.09255334113981961\n",
      "    At iteration 8400 -> loss: 0.0925154511476829\n",
      "    At iteration 8500 -> loss: 0.09249065208960021\n",
      "    At iteration 8600 -> loss: 0.0924811393345184\n",
      "    At iteration 8700 -> loss: 0.09246604780454096\n",
      "    At iteration 8800 -> loss: 0.0924785997105403\n",
      "    At iteration 8900 -> loss: 0.09244843378415449\n",
      "    At iteration 9000 -> loss: 0.09246577239128959\n",
      "    At iteration 9100 -> loss: 0.09252262132418465\n",
      "    At iteration 9200 -> loss: 0.09253122716762671\n",
      "    At iteration 9300 -> loss: 0.09250909922386917\n",
      "    At iteration 9400 -> loss: 0.0927258031847454\n",
      "    At iteration 9500 -> loss: 0.0927070983217332\n",
      "    At iteration 9600 -> loss: 0.09268367508957925\n",
      "    At iteration 9700 -> loss: 0.09266391861842738\n",
      "    At iteration 9800 -> loss: 0.0926398123021621\n",
      "    At iteration 9900 -> loss: 0.09265405048263128\n",
      "    At iteration 10000 -> loss: 0.0926336811616125\n",
      "    At iteration 10100 -> loss: 0.09270826039206471\n",
      "    At iteration 10200 -> loss: 0.09267473595296123\n",
      "    At iteration 10300 -> loss: 0.09266649513471421\n",
      "    At iteration 10400 -> loss: 0.09269093446707193\n",
      "    At iteration 10500 -> loss: 0.09270205012785479\n",
      "    At iteration 10600 -> loss: 0.09277625146693198\n",
      "    At iteration 10700 -> loss: 0.0929103094337929\n",
      "    At iteration 10800 -> loss: 0.09293057543001801\n",
      "    At iteration 10900 -> loss: 0.0929413462697986\n",
      "    At iteration 11000 -> loss: 0.09294023559909943\n",
      "    At iteration 11100 -> loss: 0.09292604642511744\n",
      "    At iteration 11200 -> loss: 0.09295100575380248\n",
      "    At iteration 11300 -> loss: 0.09297115437145045\n",
      "    At iteration 11400 -> loss: 0.09295283897426454\n",
      "    At iteration 11500 -> loss: 0.09293106187610332\n",
      "    At iteration 11600 -> loss: 0.09318154695554579\n",
      "    At iteration 11700 -> loss: 0.09317223765407114\n",
      "    At iteration 11800 -> loss: 0.09314436204244986\n",
      "    At iteration 11900 -> loss: 0.09311525261550227\n",
      "    At iteration 12000 -> loss: 0.09313918267379628\n",
      "    At iteration 12100 -> loss: 0.0931526016165447\n",
      "    At iteration 12200 -> loss: 0.09316760406527401\n",
      "    At iteration 12300 -> loss: 0.093139525711888\n",
      "    At iteration 12400 -> loss: 0.09310841867451557\n",
      "    At iteration 12500 -> loss: 0.09308518348178449\n",
      "    At iteration 12600 -> loss: 0.09306236796898748\n",
      "    At iteration 12700 -> loss: 0.09307572322764107\n",
      "    At iteration 12800 -> loss: 0.09307082648825274\n",
      "    At iteration 12900 -> loss: 0.09306016904322655\n",
      "    At iteration 13000 -> loss: 0.0930587383976529\n",
      "    At iteration 13100 -> loss: 0.09305892427680496\n",
      "    At iteration 13200 -> loss: 0.09304898179074546\n",
      "    At iteration 13300 -> loss: 0.09303078427668836\n",
      "    At iteration 13400 -> loss: 0.093001982463854\n",
      "    At iteration 13500 -> loss: 0.0930051770641868\n",
      "    At iteration 13600 -> loss: 0.09301671222502564\n",
      "Staring Epoch 57\n",
      "    At iteration 0 -> loss: 0.08026697776222136\n",
      "    At iteration 100 -> loss: 0.09203963348424637\n",
      "    At iteration 200 -> loss: 0.09336074326397302\n",
      "    At iteration 300 -> loss: 0.09236213840003708\n",
      "    At iteration 400 -> loss: 0.09159348267778447\n",
      "    At iteration 500 -> loss: 0.09151079826625551\n",
      "    At iteration 600 -> loss: 0.09157214348767563\n",
      "    At iteration 700 -> loss: 0.09161141974492429\n",
      "    At iteration 800 -> loss: 0.09162917970017266\n",
      "    At iteration 900 -> loss: 0.09159850093654481\n",
      "    At iteration 1000 -> loss: 0.09165954341674923\n",
      "    At iteration 1100 -> loss: 0.09179565977561865\n",
      "    At iteration 1200 -> loss: 0.09173899419403421\n",
      "    At iteration 1300 -> loss: 0.0917694985433387\n",
      "    At iteration 1400 -> loss: 0.0916611011139649\n",
      "    At iteration 1500 -> loss: 0.09207851576179915\n",
      "    At iteration 1600 -> loss: 0.09186069872205777\n",
      "    At iteration 1700 -> loss: 0.09195463342257765\n",
      "    At iteration 1800 -> loss: 0.09209331887927635\n",
      "    At iteration 1900 -> loss: 0.0920000607074595\n",
      "    At iteration 2000 -> loss: 0.09215289609510428\n",
      "    At iteration 2100 -> loss: 0.0922113653248767\n",
      "    At iteration 2200 -> loss: 0.09225660357421632\n",
      "    At iteration 2300 -> loss: 0.09231656996471885\n",
      "    At iteration 2400 -> loss: 0.09228861176530098\n",
      "    At iteration 2500 -> loss: 0.09214731708581536\n",
      "    At iteration 2600 -> loss: 0.09209008685186418\n",
      "    At iteration 2700 -> loss: 0.09218831157763042\n",
      "    At iteration 2800 -> loss: 0.09323066775613567\n",
      "    At iteration 2900 -> loss: 0.09328898289474553\n",
      "    At iteration 3000 -> loss: 0.09333458717960594\n",
      "    At iteration 3100 -> loss: 0.09332358910615218\n",
      "    At iteration 3200 -> loss: 0.09334159090266719\n",
      "    At iteration 3300 -> loss: 0.09331557181826733\n",
      "    At iteration 3400 -> loss: 0.0932280728394312\n",
      "    At iteration 3500 -> loss: 0.09323214664411779\n",
      "    At iteration 3600 -> loss: 0.09318829048530056\n",
      "    At iteration 3700 -> loss: 0.09312317540496155\n",
      "    At iteration 3800 -> loss: 0.09324646458409871\n",
      "    At iteration 3900 -> loss: 0.09350449708673343\n",
      "    At iteration 4000 -> loss: 0.09343508982513359\n",
      "    At iteration 4100 -> loss: 0.09335301984079694\n",
      "    At iteration 4200 -> loss: 0.09326306511137017\n",
      "    At iteration 4300 -> loss: 0.0932660858236927\n",
      "    At iteration 4400 -> loss: 0.09322395264640407\n",
      "    At iteration 4500 -> loss: 0.0932465039658603\n",
      "    At iteration 4600 -> loss: 0.0931971116323462\n",
      "    At iteration 4700 -> loss: 0.09309294684702502\n",
      "    At iteration 4800 -> loss: 0.09317396301176772\n",
      "    At iteration 4900 -> loss: 0.09312965196002249\n",
      "    At iteration 5000 -> loss: 0.09313283642892362\n",
      "    At iteration 5100 -> loss: 0.09310023341867939\n",
      "    At iteration 5200 -> loss: 0.09309937401210285\n",
      "    At iteration 5300 -> loss: 0.0930486292602432\n",
      "    At iteration 5400 -> loss: 0.0930758475013848\n",
      "    At iteration 5500 -> loss: 0.09304497604162047\n",
      "    At iteration 5600 -> loss: 0.0932211718126016\n",
      "    At iteration 5700 -> loss: 0.09318246780616483\n",
      "    At iteration 5800 -> loss: 0.09308657150812631\n",
      "    At iteration 5900 -> loss: 0.09314210653221047\n",
      "    At iteration 6000 -> loss: 0.09311503330454876\n",
      "    At iteration 6100 -> loss: 0.09310373508416948\n",
      "    At iteration 6200 -> loss: 0.09324547534228604\n",
      "    At iteration 6300 -> loss: 0.09331348665734843\n",
      "    At iteration 6400 -> loss: 0.09323370121841126\n",
      "    At iteration 6500 -> loss: 0.0932066238149964\n",
      "    At iteration 6600 -> loss: 0.09318568764438981\n",
      "    At iteration 6700 -> loss: 0.09318573887944874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6800 -> loss: 0.09314390351673336\n",
      "    At iteration 6900 -> loss: 0.09317070818648704\n",
      "    At iteration 7000 -> loss: 0.09314222874698368\n",
      "    At iteration 7100 -> loss: 0.09310077114965427\n",
      "    At iteration 7200 -> loss: 0.09307037224673817\n",
      "    At iteration 7300 -> loss: 0.09302380179361236\n",
      "    At iteration 7400 -> loss: 0.09303749730370844\n",
      "    At iteration 7500 -> loss: 0.09298174219773601\n",
      "    At iteration 7600 -> loss: 0.09297907124249351\n",
      "    At iteration 7700 -> loss: 0.09300272186457363\n",
      "    At iteration 7800 -> loss: 0.09303100248648388\n",
      "    At iteration 7900 -> loss: 0.09303029327358384\n",
      "    At iteration 8000 -> loss: 0.09302494854391628\n",
      "    At iteration 8100 -> loss: 0.09295037629551774\n",
      "    At iteration 8200 -> loss: 0.092928982160062\n",
      "    At iteration 8300 -> loss: 0.09291007257909402\n",
      "    At iteration 8400 -> loss: 0.09287841087430583\n",
      "    At iteration 8500 -> loss: 0.09283703903058514\n",
      "    At iteration 8600 -> loss: 0.09281457370154753\n",
      "    At iteration 8700 -> loss: 0.09282894045493457\n",
      "    At iteration 8800 -> loss: 0.09283374154291979\n",
      "    At iteration 8900 -> loss: 0.09292795922042404\n",
      "    At iteration 9000 -> loss: 0.09291193781808646\n",
      "    At iteration 9100 -> loss: 0.09286706172671998\n",
      "    At iteration 9200 -> loss: 0.09284553704005068\n",
      "    At iteration 9300 -> loss: 0.09280825470612089\n",
      "    At iteration 9400 -> loss: 0.09287606046591594\n",
      "    At iteration 9500 -> loss: 0.09285061630347595\n",
      "    At iteration 9600 -> loss: 0.09285049243424942\n",
      "    At iteration 9700 -> loss: 0.09282339268313673\n",
      "    At iteration 9800 -> loss: 0.09284856872848178\n",
      "    At iteration 9900 -> loss: 0.09280071511907416\n",
      "    At iteration 10000 -> loss: 0.09279277565848582\n",
      "    At iteration 10100 -> loss: 0.09283786936923306\n",
      "    At iteration 10200 -> loss: 0.09280596161342294\n",
      "    At iteration 10300 -> loss: 0.09286393351868465\n",
      "    At iteration 10400 -> loss: 0.09289235540437113\n",
      "    At iteration 10500 -> loss: 0.09286603766816381\n",
      "    At iteration 10600 -> loss: 0.09296551925021138\n",
      "    At iteration 10700 -> loss: 0.09294802890583918\n",
      "    At iteration 10800 -> loss: 0.09296172400771854\n",
      "    At iteration 10900 -> loss: 0.09295311567035838\n",
      "    At iteration 11000 -> loss: 0.09295578513907088\n",
      "    At iteration 11100 -> loss: 0.0929687634100635\n",
      "    At iteration 11200 -> loss: 0.09295466778679679\n",
      "    At iteration 11300 -> loss: 0.09293164889743397\n",
      "    At iteration 11400 -> loss: 0.09292711067837825\n",
      "    At iteration 11500 -> loss: 0.09291662751275781\n",
      "    At iteration 11600 -> loss: 0.0928960464492826\n",
      "    At iteration 11700 -> loss: 0.09290278167634644\n",
      "    At iteration 11800 -> loss: 0.09289775964443196\n",
      "    At iteration 11900 -> loss: 0.09289653625432806\n",
      "    At iteration 12000 -> loss: 0.09286839399618209\n",
      "    At iteration 12100 -> loss: 0.09284205124202319\n",
      "    At iteration 12200 -> loss: 0.09286339748389828\n",
      "    At iteration 12300 -> loss: 0.09288393730739626\n",
      "    At iteration 12400 -> loss: 0.09296259935199767\n",
      "    At iteration 12500 -> loss: 0.09296866877342558\n",
      "    At iteration 12600 -> loss: 0.09295848937368438\n",
      "    At iteration 12700 -> loss: 0.09300117541281744\n",
      "    At iteration 12800 -> loss: 0.09297425874029498\n",
      "    At iteration 12900 -> loss: 0.09296013366067157\n",
      "    At iteration 13000 -> loss: 0.09294620894832739\n",
      "    At iteration 13100 -> loss: 0.09292949489671294\n",
      "    At iteration 13200 -> loss: 0.09294237180716643\n",
      "    At iteration 13300 -> loss: 0.09291530304480257\n",
      "    At iteration 13400 -> loss: 0.09297533780833875\n",
      "    At iteration 13500 -> loss: 0.09297460409106031\n",
      "    At iteration 13600 -> loss: 0.0929740718223399\n",
      "Staring Epoch 58\n",
      "    At iteration 0 -> loss: 0.08955734362825751\n",
      "    At iteration 100 -> loss: 0.09520480871169909\n",
      "    At iteration 200 -> loss: 0.09312019208673898\n",
      "    At iteration 300 -> loss: 0.09174078467641697\n",
      "    At iteration 400 -> loss: 0.09132847818658933\n",
      "    At iteration 500 -> loss: 0.09103616865248995\n",
      "    At iteration 600 -> loss: 0.09116015049856856\n",
      "    At iteration 700 -> loss: 0.09135395522320151\n",
      "    At iteration 800 -> loss: 0.09126454476672387\n",
      "    At iteration 900 -> loss: 0.0912171166484234\n",
      "    At iteration 1000 -> loss: 0.09127175947288405\n",
      "    At iteration 1100 -> loss: 0.09142466337706934\n",
      "    At iteration 1200 -> loss: 0.09129218947975495\n",
      "    At iteration 1300 -> loss: 0.09140832864034894\n",
      "    At iteration 1400 -> loss: 0.09143890235933548\n",
      "    At iteration 1500 -> loss: 0.0919732661108183\n",
      "    At iteration 1600 -> loss: 0.09193176794004825\n",
      "    At iteration 1700 -> loss: 0.09176986346457856\n",
      "    At iteration 1800 -> loss: 0.09171386785423487\n",
      "    At iteration 1900 -> loss: 0.09212263824175207\n",
      "    At iteration 2000 -> loss: 0.09210375625493476\n",
      "    At iteration 2100 -> loss: 0.09216196340055922\n",
      "    At iteration 2200 -> loss: 0.09219555003092723\n",
      "    At iteration 2300 -> loss: 0.0922023633346113\n",
      "    At iteration 2400 -> loss: 0.09229062599068157\n",
      "    At iteration 2500 -> loss: 0.09350941607779109\n",
      "    At iteration 2600 -> loss: 0.09350907670182898\n",
      "    At iteration 2700 -> loss: 0.09331213424876549\n",
      "    At iteration 2800 -> loss: 0.09342798312231544\n",
      "    At iteration 2900 -> loss: 0.09345591494506006\n",
      "    At iteration 3000 -> loss: 0.09341707880277979\n",
      "    At iteration 3100 -> loss: 0.09326700776768118\n",
      "    At iteration 3200 -> loss: 0.09348769693395734\n",
      "    At iteration 3300 -> loss: 0.0934522666170493\n",
      "    At iteration 3400 -> loss: 0.09356469304313911\n",
      "    At iteration 3500 -> loss: 0.09350417545803602\n",
      "    At iteration 3600 -> loss: 0.09344958401806891\n",
      "    At iteration 3700 -> loss: 0.0935279164008482\n",
      "    At iteration 3800 -> loss: 0.09350610523586696\n",
      "    At iteration 3900 -> loss: 0.09339644988525364\n",
      "    At iteration 4000 -> loss: 0.09332464232263567\n",
      "    At iteration 4100 -> loss: 0.09327860529933779\n",
      "    At iteration 4200 -> loss: 0.09315251902125445\n",
      "    At iteration 4300 -> loss: 0.09365062639165875\n",
      "    At iteration 4400 -> loss: 0.09355842803588772\n",
      "    At iteration 4500 -> loss: 0.09357037140379025\n",
      "    At iteration 4600 -> loss: 0.09351814101513124\n",
      "    At iteration 4700 -> loss: 0.09348108788873681\n",
      "    At iteration 4800 -> loss: 0.09351046684916435\n",
      "    At iteration 4900 -> loss: 0.09344207143736932\n",
      "    At iteration 5000 -> loss: 0.09338707853589394\n",
      "    At iteration 5100 -> loss: 0.09341033471861732\n",
      "    At iteration 5200 -> loss: 0.09334316379458758\n",
      "    At iteration 5300 -> loss: 0.09327458304544056\n",
      "    At iteration 5400 -> loss: 0.09321288267284294\n",
      "    At iteration 5500 -> loss: 0.09320821895409964\n",
      "    At iteration 5600 -> loss: 0.09322589799186204\n",
      "    At iteration 5700 -> loss: 0.09319673784446925\n",
      "    At iteration 5800 -> loss: 0.09320909386199058\n",
      "    At iteration 5900 -> loss: 0.09316927814558112\n",
      "    At iteration 6000 -> loss: 0.09328158115820212\n",
      "    At iteration 6100 -> loss: 0.09322914552849507\n",
      "    At iteration 6200 -> loss: 0.09316853941839108\n",
      "    At iteration 6300 -> loss: 0.09314354223959585\n",
      "    At iteration 6400 -> loss: 0.09313439985231436\n",
      "    At iteration 6500 -> loss: 0.09314369508153804\n",
      "    At iteration 6600 -> loss: 0.09322998632099228\n",
      "    At iteration 6700 -> loss: 0.0932645073781728\n",
      "    At iteration 6800 -> loss: 0.09328627653095702\n",
      "    At iteration 6900 -> loss: 0.0932880586941443\n",
      "    At iteration 7000 -> loss: 0.09324398311782578\n",
      "    At iteration 7100 -> loss: 0.09317340776564145\n",
      "    At iteration 7200 -> loss: 0.09316818859595989\n",
      "    At iteration 7300 -> loss: 0.0931322937500481\n",
      "    At iteration 7400 -> loss: 0.09315043124644673\n",
      "    At iteration 7500 -> loss: 0.09312935835351992\n",
      "    At iteration 7600 -> loss: 0.09313161389041302\n",
      "    At iteration 7700 -> loss: 0.09308451610268036\n",
      "    At iteration 7800 -> loss: 0.09305972206846538\n",
      "    At iteration 7900 -> loss: 0.09306219576983779\n",
      "    At iteration 8000 -> loss: 0.09304805261299494\n",
      "    At iteration 8100 -> loss: 0.09316981795551396\n",
      "    At iteration 8200 -> loss: 0.09314770083865534\n",
      "    At iteration 8300 -> loss: 0.09313861384882899\n",
      "    At iteration 8400 -> loss: 0.09312597830342521\n",
      "    At iteration 8500 -> loss: 0.09315096290354058\n",
      "    At iteration 8600 -> loss: 0.09311151485970147\n",
      "    At iteration 8700 -> loss: 0.09307429003803826\n",
      "    At iteration 8800 -> loss: 0.09302083957982146\n",
      "    At iteration 8900 -> loss: 0.0930500721020825\n",
      "    At iteration 9000 -> loss: 0.0930776765181044\n",
      "    At iteration 9100 -> loss: 0.09309570963315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9200 -> loss: 0.09307273218751588\n",
      "    At iteration 9300 -> loss: 0.09305951423191518\n",
      "    At iteration 9400 -> loss: 0.09305652874978156\n",
      "    At iteration 9500 -> loss: 0.09304353628583723\n",
      "    At iteration 9600 -> loss: 0.09302163031916906\n",
      "    At iteration 9700 -> loss: 0.09300830854710465\n",
      "    At iteration 9800 -> loss: 0.0929913393250388\n",
      "    At iteration 9900 -> loss: 0.09298972271046779\n",
      "    At iteration 10000 -> loss: 0.09299546884926542\n",
      "    At iteration 10100 -> loss: 0.09296073324814184\n",
      "    At iteration 10200 -> loss: 0.09297673129000718\n",
      "    At iteration 10300 -> loss: 0.09297425242178768\n",
      "    At iteration 10400 -> loss: 0.09294955646075434\n",
      "    At iteration 10500 -> loss: 0.09293203366671776\n",
      "    At iteration 10600 -> loss: 0.09291616482038949\n",
      "    At iteration 10700 -> loss: 0.09288503998602485\n",
      "    At iteration 10800 -> loss: 0.09286794361322472\n",
      "    At iteration 10900 -> loss: 0.09283286526191191\n",
      "    At iteration 11000 -> loss: 0.09290083572186501\n",
      "    At iteration 11100 -> loss: 0.09287751304420588\n",
      "    At iteration 11200 -> loss: 0.0928652459777356\n",
      "    At iteration 11300 -> loss: 0.09284188595204877\n",
      "    At iteration 11400 -> loss: 0.0928091750516712\n",
      "    At iteration 11500 -> loss: 0.09281948753672097\n",
      "    At iteration 11600 -> loss: 0.09288321667245691\n",
      "    At iteration 11700 -> loss: 0.09297722685459302\n",
      "    At iteration 11800 -> loss: 0.09301857960332424\n",
      "    At iteration 11900 -> loss: 0.09307846508544841\n",
      "    At iteration 12000 -> loss: 0.0930356683549825\n",
      "    At iteration 12100 -> loss: 0.09306498729976816\n",
      "    At iteration 12200 -> loss: 0.0930949511362825\n",
      "    At iteration 12300 -> loss: 0.09308078180767151\n",
      "    At iteration 12400 -> loss: 0.09307082890879544\n",
      "    At iteration 12500 -> loss: 0.09305318483284454\n",
      "    At iteration 12600 -> loss: 0.09302090400922707\n",
      "    At iteration 12700 -> loss: 0.09301372283164193\n",
      "    At iteration 12800 -> loss: 0.09300720167760076\n",
      "    At iteration 12900 -> loss: 0.09297721818229891\n",
      "    At iteration 13000 -> loss: 0.0929604711466876\n",
      "    At iteration 13100 -> loss: 0.0929418941604334\n",
      "    At iteration 13200 -> loss: 0.0929201117639118\n",
      "    At iteration 13300 -> loss: 0.09292363004761522\n",
      "    At iteration 13400 -> loss: 0.09296643229125719\n",
      "    At iteration 13500 -> loss: 0.09298471109536977\n",
      "    At iteration 13600 -> loss: 0.09300949758459358\n",
      "Staring Epoch 59\n",
      "    At iteration 0 -> loss: 0.0958097418770194\n",
      "    At iteration 100 -> loss: 0.09236757866903987\n",
      "    At iteration 200 -> loss: 0.09266036902730516\n",
      "    At iteration 300 -> loss: 0.09182660507663915\n",
      "    At iteration 400 -> loss: 0.09177687794400496\n",
      "    At iteration 500 -> loss: 0.09225708402920807\n",
      "    At iteration 600 -> loss: 0.09232901249233398\n",
      "    At iteration 700 -> loss: 0.09227234401769524\n",
      "    At iteration 800 -> loss: 0.09299595515269986\n",
      "    At iteration 900 -> loss: 0.09371171804773017\n",
      "    At iteration 1000 -> loss: 0.09322710976966661\n",
      "    At iteration 1100 -> loss: 0.09339197673023945\n",
      "    At iteration 1200 -> loss: 0.09338276736990683\n",
      "    At iteration 1300 -> loss: 0.09344519069600822\n",
      "    At iteration 1400 -> loss: 0.09318131384885463\n",
      "    At iteration 1500 -> loss: 0.09295047768873828\n",
      "    At iteration 1600 -> loss: 0.09267668517843063\n",
      "    At iteration 1700 -> loss: 0.0926492353319206\n",
      "    At iteration 1800 -> loss: 0.09419921145509802\n",
      "    At iteration 1900 -> loss: 0.09401323039036642\n",
      "    At iteration 2000 -> loss: 0.09388615111410242\n",
      "    At iteration 2100 -> loss: 0.09382810139754223\n",
      "    At iteration 2200 -> loss: 0.09369815590922839\n",
      "    At iteration 2300 -> loss: 0.09352026580995539\n",
      "    At iteration 2400 -> loss: 0.09350018907445051\n",
      "    At iteration 2500 -> loss: 0.09352509892658592\n",
      "    At iteration 2600 -> loss: 0.09357497388563206\n",
      "    At iteration 2700 -> loss: 0.09346852713306071\n",
      "    At iteration 2800 -> loss: 0.09337211788477898\n",
      "    At iteration 2900 -> loss: 0.09333735933474162\n",
      "    At iteration 3000 -> loss: 0.09330246561220867\n",
      "    At iteration 3100 -> loss: 0.0932608864077054\n",
      "    At iteration 3200 -> loss: 0.0932299212941238\n",
      "    At iteration 3300 -> loss: 0.09339476791448136\n",
      "    At iteration 3400 -> loss: 0.09349330381632377\n",
      "    At iteration 3500 -> loss: 0.0934348798252926\n",
      "    At iteration 3600 -> loss: 0.09341901397396975\n",
      "    At iteration 3700 -> loss: 0.09331213119155478\n",
      "    At iteration 3800 -> loss: 0.09331526267525087\n",
      "    At iteration 3900 -> loss: 0.09324703841297681\n",
      "    At iteration 4000 -> loss: 0.09313775685548\n",
      "    At iteration 4100 -> loss: 0.09310666909478779\n",
      "    At iteration 4200 -> loss: 0.09308458420910196\n",
      "    At iteration 4300 -> loss: 0.09314918872942578\n",
      "    At iteration 4400 -> loss: 0.09342313261384567\n",
      "    At iteration 4500 -> loss: 0.09340350658032849\n",
      "    At iteration 4600 -> loss: 0.09334938494204548\n",
      "    At iteration 4700 -> loss: 0.09343557946920805\n",
      "    At iteration 4800 -> loss: 0.09342341265923097\n",
      "    At iteration 4900 -> loss: 0.09358609994663836\n",
      "    At iteration 5000 -> loss: 0.09349528394226461\n",
      "    At iteration 5100 -> loss: 0.09340554261446272\n",
      "    At iteration 5200 -> loss: 0.0934868315639537\n",
      "    At iteration 5300 -> loss: 0.09342677407712828\n",
      "    At iteration 5400 -> loss: 0.09337095224059341\n",
      "    At iteration 5500 -> loss: 0.09328182059747028\n",
      "    At iteration 5600 -> loss: 0.09324065899124513\n",
      "    At iteration 5700 -> loss: 0.09320601243093109\n",
      "    At iteration 5800 -> loss: 0.09334312219501918\n",
      "    At iteration 5900 -> loss: 0.09328776253768684\n",
      "    At iteration 6000 -> loss: 0.09325413441375723\n",
      "    At iteration 6100 -> loss: 0.09325125728277053\n",
      "    At iteration 6200 -> loss: 0.09332278735632647\n",
      "    At iteration 6300 -> loss: 0.09324477074638077\n",
      "    At iteration 6400 -> loss: 0.09320368058790929\n",
      "    At iteration 6500 -> loss: 0.09329058033772905\n",
      "    At iteration 6600 -> loss: 0.09325526749607509\n",
      "    At iteration 6700 -> loss: 0.09323242981081145\n",
      "    At iteration 6800 -> loss: 0.09317027127651158\n",
      "    At iteration 6900 -> loss: 0.09312405639178578\n",
      "    At iteration 7000 -> loss: 0.09307978642418958\n",
      "    At iteration 7100 -> loss: 0.09310739278188386\n",
      "    At iteration 7200 -> loss: 0.09315599196768293\n",
      "    At iteration 7300 -> loss: 0.09312626739710633\n",
      "    At iteration 7400 -> loss: 0.09313866950622467\n",
      "    At iteration 7500 -> loss: 0.09310772559983557\n",
      "    At iteration 7600 -> loss: 0.09308871272454282\n",
      "    At iteration 7700 -> loss: 0.09304315693989783\n",
      "    At iteration 7800 -> loss: 0.0930210839679097\n",
      "    At iteration 7900 -> loss: 0.0930786300866583\n",
      "    At iteration 8000 -> loss: 0.09306972678980924\n",
      "    At iteration 8100 -> loss: 0.09303600020627029\n",
      "    At iteration 8200 -> loss: 0.0929892103039537\n",
      "    At iteration 8300 -> loss: 0.09299145603628102\n",
      "    At iteration 8400 -> loss: 0.09298541093739249\n",
      "    At iteration 8500 -> loss: 0.09301892934337465\n",
      "    At iteration 8600 -> loss: 0.09296737052193846\n",
      "    At iteration 8700 -> loss: 0.09298214446271799\n",
      "    At iteration 8800 -> loss: 0.09302015279940665\n",
      "    At iteration 8900 -> loss: 0.09297689111809435\n",
      "    At iteration 9000 -> loss: 0.09294389750561993\n",
      "    At iteration 9100 -> loss: 0.092914409015321\n",
      "    At iteration 9200 -> loss: 0.09291272895627041\n",
      "    At iteration 9300 -> loss: 0.09295227154590927\n",
      "    At iteration 9400 -> loss: 0.09293196979425494\n",
      "    At iteration 9500 -> loss: 0.0930406039316444\n",
      "    At iteration 9600 -> loss: 0.093035099064811\n",
      "    At iteration 9700 -> loss: 0.09302331005515513\n",
      "    At iteration 9800 -> loss: 0.09302736056511524\n",
      "    At iteration 9900 -> loss: 0.09303159547656574\n",
      "    At iteration 10000 -> loss: 0.09303201653303675\n",
      "    At iteration 10100 -> loss: 0.09303739919988999\n",
      "    At iteration 10200 -> loss: 0.09309488448428874\n",
      "    At iteration 10300 -> loss: 0.09307559305919479\n",
      "    At iteration 10400 -> loss: 0.09312107580400253\n",
      "    At iteration 10500 -> loss: 0.09310903679631322\n",
      "    At iteration 10600 -> loss: 0.0930870016351572\n",
      "    At iteration 10700 -> loss: 0.09316273511224211\n",
      "    At iteration 10800 -> loss: 0.09314920893499612\n",
      "    At iteration 10900 -> loss: 0.09318979655141606\n",
      "    At iteration 11000 -> loss: 0.09314958154539621\n",
      "    At iteration 11100 -> loss: 0.09311537675085915\n",
      "    At iteration 11200 -> loss: 0.0931208898054293\n",
      "    At iteration 11300 -> loss: 0.09311744587319258\n",
      "    At iteration 11400 -> loss: 0.09312460987575148\n",
      "    At iteration 11500 -> loss: 0.09320464074553926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11600 -> loss: 0.09318108924514003\n",
      "    At iteration 11700 -> loss: 0.09314994333295534\n",
      "    At iteration 11800 -> loss: 0.09311191474849341\n",
      "    At iteration 11900 -> loss: 0.09313599595346903\n",
      "    At iteration 12000 -> loss: 0.09310998355657182\n",
      "    At iteration 12100 -> loss: 0.09308259788465141\n",
      "    At iteration 12200 -> loss: 0.09305563212074247\n",
      "    At iteration 12300 -> loss: 0.09306596947210777\n",
      "    At iteration 12400 -> loss: 0.09309527256127939\n",
      "    At iteration 12500 -> loss: 0.09310614157711795\n",
      "    At iteration 12600 -> loss: 0.0930984025909941\n",
      "    At iteration 12700 -> loss: 0.09310123415675674\n",
      "    At iteration 12800 -> loss: 0.09306918557656567\n",
      "    At iteration 12900 -> loss: 0.09309940210125957\n",
      "    At iteration 13000 -> loss: 0.09308137536004328\n",
      "    At iteration 13100 -> loss: 0.09306979909912418\n",
      "    At iteration 13200 -> loss: 0.09305487370799392\n",
      "    At iteration 13300 -> loss: 0.09302429940630201\n",
      "    At iteration 13400 -> loss: 0.09300162353150636\n",
      "    At iteration 13500 -> loss: 0.0929948550739819\n",
      "    At iteration 13600 -> loss: 0.09298488333164212\n",
      "Staring Epoch 60\n",
      "    At iteration 0 -> loss: 0.08080019187764265\n",
      "    At iteration 100 -> loss: 0.09098436156869615\n",
      "    At iteration 200 -> loss: 0.0923099900981277\n",
      "    At iteration 300 -> loss: 0.09189608127029655\n",
      "    At iteration 400 -> loss: 0.091061264428099\n",
      "    At iteration 500 -> loss: 0.091831365751498\n",
      "    At iteration 600 -> loss: 0.09201796986829457\n",
      "    At iteration 700 -> loss: 0.09179749355868055\n",
      "    At iteration 800 -> loss: 0.09192331081657903\n",
      "    At iteration 900 -> loss: 0.09193593835242943\n",
      "    At iteration 1000 -> loss: 0.09220941051381117\n",
      "    At iteration 1100 -> loss: 0.09205371135768682\n",
      "    At iteration 1200 -> loss: 0.09189282392934917\n",
      "    At iteration 1300 -> loss: 0.0920703609542468\n",
      "    At iteration 1400 -> loss: 0.0917124745113182\n",
      "    At iteration 1500 -> loss: 0.091761635950983\n",
      "    At iteration 1600 -> loss: 0.09169068225136394\n",
      "    At iteration 1700 -> loss: 0.09179254044468012\n",
      "    At iteration 1800 -> loss: 0.09172930699966571\n",
      "    At iteration 1900 -> loss: 0.09166651298000185\n",
      "    At iteration 2000 -> loss: 0.09203964932683772\n",
      "    At iteration 2100 -> loss: 0.09201267245694945\n",
      "    At iteration 2200 -> loss: 0.09199814693762237\n",
      "    At iteration 2300 -> loss: 0.0919828146203406\n",
      "    At iteration 2400 -> loss: 0.09288002891173767\n",
      "    At iteration 2500 -> loss: 0.09291858640131731\n",
      "    At iteration 2600 -> loss: 0.09279103597384308\n",
      "    At iteration 2700 -> loss: 0.09265805716467948\n",
      "    At iteration 2800 -> loss: 0.09265148234304085\n",
      "    At iteration 2900 -> loss: 0.09257203319733659\n",
      "    At iteration 3000 -> loss: 0.09354320563247161\n",
      "    At iteration 3100 -> loss: 0.09338986535835458\n",
      "    At iteration 3200 -> loss: 0.09333824742651312\n",
      "    At iteration 3300 -> loss: 0.09323668167874612\n",
      "    At iteration 3400 -> loss: 0.09319452334644983\n",
      "    At iteration 3500 -> loss: 0.09309074076243354\n",
      "    At iteration 3600 -> loss: 0.09320417749892383\n",
      "    At iteration 3700 -> loss: 0.0933620317622458\n",
      "    At iteration 3800 -> loss: 0.09334369697376113\n",
      "    At iteration 3900 -> loss: 0.09328293749028223\n",
      "    At iteration 4000 -> loss: 0.09327882950091938\n",
      "    At iteration 4100 -> loss: 0.09316978951034632\n",
      "    At iteration 4200 -> loss: 0.09310274604808745\n",
      "    At iteration 4300 -> loss: 0.0930908423747458\n",
      "    At iteration 4400 -> loss: 0.09302296981686511\n",
      "    At iteration 4500 -> loss: 0.09299654997238829\n",
      "    At iteration 4600 -> loss: 0.09298651418655456\n",
      "    At iteration 4700 -> loss: 0.09306259944247497\n",
      "    At iteration 4800 -> loss: 0.09299327531615308\n",
      "    At iteration 4900 -> loss: 0.09302416551527563\n",
      "    At iteration 5000 -> loss: 0.09301798376479296\n",
      "    At iteration 5100 -> loss: 0.09304700794373158\n",
      "    At iteration 5200 -> loss: 0.09301532552807873\n",
      "    At iteration 5300 -> loss: 0.09301293997743647\n",
      "    At iteration 5400 -> loss: 0.09295318098020287\n",
      "    At iteration 5500 -> loss: 0.09295230825078958\n",
      "    At iteration 5600 -> loss: 0.09289452006613746\n",
      "    At iteration 5700 -> loss: 0.09288101633205807\n",
      "    At iteration 5800 -> loss: 0.092885732526922\n",
      "    At iteration 5900 -> loss: 0.09284933909592168\n",
      "    At iteration 6000 -> loss: 0.0928352586124158\n",
      "    At iteration 6100 -> loss: 0.09277090135819659\n",
      "    At iteration 6200 -> loss: 0.09285723486050934\n",
      "    At iteration 6300 -> loss: 0.09279519844845917\n",
      "    At iteration 6400 -> loss: 0.09294116971824677\n",
      "    At iteration 6500 -> loss: 0.09303645478755852\n",
      "    At iteration 6600 -> loss: 0.09300228133246323\n",
      "    At iteration 6700 -> loss: 0.09297607164149436\n",
      "    At iteration 6800 -> loss: 0.0929371141423698\n",
      "    At iteration 6900 -> loss: 0.0929278705202159\n",
      "    At iteration 7000 -> loss: 0.09291410663619842\n",
      "    At iteration 7100 -> loss: 0.09291624928899249\n",
      "    At iteration 7200 -> loss: 0.09289834920717072\n",
      "    At iteration 7300 -> loss: 0.09285501334497057\n",
      "    At iteration 7400 -> loss: 0.09291163700066231\n",
      "    At iteration 7500 -> loss: 0.0928850609159994\n",
      "    At iteration 7600 -> loss: 0.09287941875290691\n",
      "    At iteration 7700 -> loss: 0.09284267118709814\n",
      "    At iteration 7800 -> loss: 0.09281543246968414\n",
      "    At iteration 7900 -> loss: 0.09284126389104311\n",
      "    At iteration 8000 -> loss: 0.09284317399364883\n",
      "    At iteration 8100 -> loss: 0.09279312880886732\n",
      "    At iteration 8200 -> loss: 0.0927735090022831\n",
      "    At iteration 8300 -> loss: 0.09286332195895214\n",
      "    At iteration 8400 -> loss: 0.09297868934226146\n",
      "    At iteration 8500 -> loss: 0.0929361096667736\n",
      "    At iteration 8600 -> loss: 0.09300634291048819\n",
      "    At iteration 8700 -> loss: 0.09302471000724033\n",
      "    At iteration 8800 -> loss: 0.09303868996823489\n",
      "    At iteration 8900 -> loss: 0.09302777706130867\n",
      "    At iteration 9000 -> loss: 0.09304930765866544\n",
      "    At iteration 9100 -> loss: 0.0931310182497828\n",
      "    At iteration 9200 -> loss: 0.09310424917813985\n",
      "    At iteration 9300 -> loss: 0.09306819162588179\n",
      "    At iteration 9400 -> loss: 0.09314471020816696\n",
      "    At iteration 9500 -> loss: 0.09309430680032939\n",
      "    At iteration 9600 -> loss: 0.09308337081003085\n",
      "    At iteration 9700 -> loss: 0.09304140897401711\n",
      "    At iteration 9800 -> loss: 0.09310681823201072\n",
      "    At iteration 9900 -> loss: 0.09309652697845154\n",
      "    At iteration 10000 -> loss: 0.09308563109509087\n",
      "    At iteration 10100 -> loss: 0.09307346892788569\n",
      "    At iteration 10200 -> loss: 0.0931095513555421\n",
      "    At iteration 10300 -> loss: 0.09309258709186613\n",
      "    At iteration 10400 -> loss: 0.09307991533720032\n",
      "    At iteration 10500 -> loss: 0.09304641510665466\n",
      "    At iteration 10600 -> loss: 0.09301592933602441\n",
      "    At iteration 10700 -> loss: 0.09299188570223273\n",
      "    At iteration 10800 -> loss: 0.09297731959768013\n",
      "    At iteration 10900 -> loss: 0.09296684009965445\n",
      "    At iteration 11000 -> loss: 0.0929665933075893\n",
      "    At iteration 11100 -> loss: 0.09294396659060544\n",
      "    At iteration 11200 -> loss: 0.09291574472165828\n",
      "    At iteration 11300 -> loss: 0.09290287993038722\n",
      "    At iteration 11400 -> loss: 0.09288293249156694\n",
      "    At iteration 11500 -> loss: 0.09290856580319387\n",
      "    At iteration 11600 -> loss: 0.0929053087894081\n",
      "    At iteration 11700 -> loss: 0.0928830952103069\n",
      "    At iteration 11800 -> loss: 0.09284878429979009\n",
      "    At iteration 11900 -> loss: 0.0928678767149638\n",
      "    At iteration 12000 -> loss: 0.09287269420956128\n",
      "    At iteration 12100 -> loss: 0.09288210351321675\n",
      "    At iteration 12200 -> loss: 0.09289687497482936\n",
      "    At iteration 12300 -> loss: 0.09291400686316387\n",
      "    At iteration 12400 -> loss: 0.09288779171271287\n",
      "    At iteration 12500 -> loss: 0.09292041671219856\n",
      "    At iteration 12600 -> loss: 0.0929186946881175\n",
      "    At iteration 12700 -> loss: 0.09291277656147565\n",
      "    At iteration 12800 -> loss: 0.09291156275722777\n",
      "    At iteration 12900 -> loss: 0.09306071161378988\n",
      "    At iteration 13000 -> loss: 0.09304701527107309\n",
      "    At iteration 13100 -> loss: 0.09304396823407451\n",
      "    At iteration 13200 -> loss: 0.09304116670868662\n",
      "    At iteration 13300 -> loss: 0.09304768046930745\n",
      "    At iteration 13400 -> loss: 0.0930253785218331\n",
      "    At iteration 13500 -> loss: 0.09300808068945804\n",
      "    At iteration 13600 -> loss: 0.09301581444073254\n",
      "Staring Epoch 61\n",
      "    At iteration 0 -> loss: 0.11316340789198875\n",
      "    At iteration 100 -> loss: 0.09198376284279595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 200 -> loss: 0.09113571937450912\n",
      "    At iteration 300 -> loss: 0.09089527770130532\n",
      "    At iteration 400 -> loss: 0.09253523609910545\n",
      "    At iteration 500 -> loss: 0.09299111252895749\n",
      "    At iteration 600 -> loss: 0.09290564011135016\n",
      "    At iteration 700 -> loss: 0.0925530769130506\n",
      "    At iteration 800 -> loss: 0.09223029857063786\n",
      "    At iteration 900 -> loss: 0.09226989276180879\n",
      "    At iteration 1000 -> loss: 0.09515576181007734\n",
      "    At iteration 1100 -> loss: 0.09458644372434044\n",
      "    At iteration 1200 -> loss: 0.09448266961398699\n",
      "    At iteration 1300 -> loss: 0.09456619155797406\n",
      "    At iteration 1400 -> loss: 0.0943446036560857\n",
      "    At iteration 1500 -> loss: 0.09445782628696087\n",
      "    At iteration 1600 -> loss: 0.09424127683196466\n",
      "    At iteration 1700 -> loss: 0.09409396765750794\n",
      "    At iteration 1800 -> loss: 0.09403603285460604\n",
      "    At iteration 1900 -> loss: 0.09384329835921426\n",
      "    At iteration 2000 -> loss: 0.09368084866231885\n",
      "    At iteration 2100 -> loss: 0.09345697582530808\n",
      "    At iteration 2200 -> loss: 0.09334053448724448\n",
      "    At iteration 2300 -> loss: 0.09322675286452405\n",
      "    At iteration 2400 -> loss: 0.09312114712470566\n",
      "    At iteration 2500 -> loss: 0.09292202969075318\n",
      "    At iteration 2600 -> loss: 0.09299973123368624\n",
      "    At iteration 2700 -> loss: 0.09278489213287926\n",
      "    At iteration 2800 -> loss: 0.0927446892768464\n",
      "    At iteration 2900 -> loss: 0.09277112436409053\n",
      "    At iteration 3000 -> loss: 0.09268446947167945\n",
      "    At iteration 3100 -> loss: 0.09261074483766468\n",
      "    At iteration 3200 -> loss: 0.09255462008350879\n",
      "    At iteration 3300 -> loss: 0.09276175653433855\n",
      "    At iteration 3400 -> loss: 0.09272145398549717\n",
      "    At iteration 3500 -> loss: 0.09266598236170431\n",
      "    At iteration 3600 -> loss: 0.09268805271004175\n",
      "    At iteration 3700 -> loss: 0.09263622819403583\n",
      "    At iteration 3800 -> loss: 0.09268956404032201\n",
      "    At iteration 3900 -> loss: 0.09261685291804546\n",
      "    At iteration 4000 -> loss: 0.09258305407463119\n",
      "    At iteration 4100 -> loss: 0.0926416025198718\n",
      "    At iteration 4200 -> loss: 0.09275757360636552\n",
      "    At iteration 4300 -> loss: 0.0927290864341691\n",
      "    At iteration 4400 -> loss: 0.09272026613690867\n",
      "    At iteration 4500 -> loss: 0.09269507033570881\n",
      "    At iteration 4600 -> loss: 0.09274306630192486\n",
      "    At iteration 4700 -> loss: 0.09271081507979256\n",
      "    At iteration 4800 -> loss: 0.09266155156199644\n",
      "    At iteration 4900 -> loss: 0.09261276527214445\n",
      "    At iteration 5000 -> loss: 0.09258953702236652\n",
      "    At iteration 5100 -> loss: 0.09260414237180037\n",
      "    At iteration 5200 -> loss: 0.09258113816471632\n",
      "    At iteration 5300 -> loss: 0.09256425012326872\n",
      "    At iteration 5400 -> loss: 0.09251373078968825\n",
      "    At iteration 5500 -> loss: 0.09252806063100612\n",
      "    At iteration 5600 -> loss: 0.0925147922966397\n",
      "    At iteration 5700 -> loss: 0.09247743273881864\n",
      "    At iteration 5800 -> loss: 0.09255501219065565\n",
      "    At iteration 5900 -> loss: 0.09269125715018761\n",
      "    At iteration 6000 -> loss: 0.09266485948338717\n",
      "    At iteration 6100 -> loss: 0.09272046840297693\n",
      "    At iteration 6200 -> loss: 0.0926606963377811\n",
      "    At iteration 6300 -> loss: 0.0926909570000739\n",
      "    At iteration 6400 -> loss: 0.09265275052933868\n",
      "    At iteration 6500 -> loss: 0.09259488297542891\n",
      "    At iteration 6600 -> loss: 0.09258965816545871\n",
      "    At iteration 6700 -> loss: 0.09255775634547675\n",
      "    At iteration 6800 -> loss: 0.09254175073436455\n",
      "    At iteration 6900 -> loss: 0.09249144578487668\n",
      "    At iteration 7000 -> loss: 0.09249144365667109\n",
      "    At iteration 7100 -> loss: 0.09252825238190403\n",
      "    At iteration 7200 -> loss: 0.09251196369381084\n",
      "    At iteration 7300 -> loss: 0.09252358265430925\n",
      "    At iteration 7400 -> loss: 0.09255116414971412\n",
      "    At iteration 7500 -> loss: 0.09254995511823348\n",
      "    At iteration 7600 -> loss: 0.09256369935739113\n",
      "    At iteration 7700 -> loss: 0.09257071459925803\n",
      "    At iteration 7800 -> loss: 0.09252658355035638\n",
      "    At iteration 7900 -> loss: 0.0925456227478346\n",
      "    At iteration 8000 -> loss: 0.09253353593849022\n",
      "    At iteration 8100 -> loss: 0.09255684134788089\n",
      "    At iteration 8200 -> loss: 0.09254110921959528\n",
      "    At iteration 8300 -> loss: 0.09253347078716624\n",
      "    At iteration 8400 -> loss: 0.09254380646610685\n",
      "    At iteration 8500 -> loss: 0.09260260818688558\n",
      "    At iteration 8600 -> loss: 0.09260670215093851\n",
      "    At iteration 8700 -> loss: 0.09260027750941612\n",
      "    At iteration 8800 -> loss: 0.09258348632581703\n",
      "    At iteration 8900 -> loss: 0.09263371776453445\n",
      "    At iteration 9000 -> loss: 0.09261919603771739\n",
      "    At iteration 9100 -> loss: 0.09258175125481428\n",
      "    At iteration 9200 -> loss: 0.0925612401626634\n",
      "    At iteration 9300 -> loss: 0.09252316149168416\n",
      "    At iteration 9400 -> loss: 0.0925137870592976\n",
      "    At iteration 9500 -> loss: 0.09259613351235554\n",
      "    At iteration 9600 -> loss: 0.09257739024437182\n",
      "    At iteration 9700 -> loss: 0.09260822963718543\n",
      "    At iteration 9800 -> loss: 0.09267964058694364\n",
      "    At iteration 9900 -> loss: 0.09269614261391901\n",
      "    At iteration 10000 -> loss: 0.09268724637274507\n",
      "    At iteration 10100 -> loss: 0.09272046116661965\n",
      "    At iteration 10200 -> loss: 0.09273844969652364\n",
      "    At iteration 10300 -> loss: 0.09277793965288364\n",
      "    At iteration 10400 -> loss: 0.09293380016639695\n",
      "    At iteration 10500 -> loss: 0.09303513863145926\n",
      "    At iteration 10600 -> loss: 0.09300224846775222\n",
      "    At iteration 10700 -> loss: 0.09300727318045551\n",
      "    At iteration 10800 -> loss: 0.09299670980795466\n",
      "    At iteration 10900 -> loss: 0.09295968968614661\n",
      "    At iteration 11000 -> loss: 0.09293681621456792\n",
      "    At iteration 11100 -> loss: 0.09292032759826857\n",
      "    At iteration 11200 -> loss: 0.09293515575121788\n",
      "    At iteration 11300 -> loss: 0.09295028397602952\n",
      "    At iteration 11400 -> loss: 0.0929121060250629\n",
      "    At iteration 11500 -> loss: 0.09289644014680923\n",
      "    At iteration 11600 -> loss: 0.09292812693611179\n",
      "    At iteration 11700 -> loss: 0.09291396661734894\n",
      "    At iteration 11800 -> loss: 0.09295251867113653\n",
      "    At iteration 11900 -> loss: 0.09293838257221639\n",
      "    At iteration 12000 -> loss: 0.09292001670775275\n",
      "    At iteration 12100 -> loss: 0.09291348278412374\n",
      "    At iteration 12200 -> loss: 0.09293495276157565\n",
      "    At iteration 12300 -> loss: 0.09290734921669097\n",
      "    At iteration 12400 -> loss: 0.0929068820163046\n",
      "    At iteration 12500 -> loss: 0.09302259379351609\n",
      "    At iteration 12600 -> loss: 0.09299835695174455\n",
      "    At iteration 12700 -> loss: 0.0929963822600712\n",
      "    At iteration 12800 -> loss: 0.09299384724312754\n",
      "    At iteration 12900 -> loss: 0.09298309874388165\n",
      "    At iteration 13000 -> loss: 0.09303391375902784\n",
      "    At iteration 13100 -> loss: 0.09302479978169836\n",
      "    At iteration 13200 -> loss: 0.09298750274996856\n",
      "    At iteration 13300 -> loss: 0.09308208153468045\n",
      "    At iteration 13400 -> loss: 0.09306195251274031\n",
      "    At iteration 13500 -> loss: 0.09304739789863573\n",
      "    At iteration 13600 -> loss: 0.09303652845452084\n",
      "Staring Epoch 62\n",
      "    At iteration 0 -> loss: 0.08239118300843984\n",
      "    At iteration 100 -> loss: 0.090503786955419\n",
      "    At iteration 200 -> loss: 0.0904434907350567\n",
      "    At iteration 300 -> loss: 0.09082097288998899\n",
      "    At iteration 400 -> loss: 0.09112685637411315\n",
      "    At iteration 500 -> loss: 0.09197379355435\n",
      "    At iteration 600 -> loss: 0.09144043433122485\n",
      "    At iteration 700 -> loss: 0.09155178255070366\n",
      "    At iteration 800 -> loss: 0.09148135671397017\n",
      "    At iteration 900 -> loss: 0.09202100785454002\n",
      "    At iteration 1000 -> loss: 0.09205072161120487\n",
      "    At iteration 1100 -> loss: 0.09207002089650568\n",
      "    At iteration 1200 -> loss: 0.09363843156433603\n",
      "    At iteration 1300 -> loss: 0.09358695875972144\n",
      "    At iteration 1400 -> loss: 0.09341885260898129\n",
      "    At iteration 1500 -> loss: 0.09338374506964213\n",
      "    At iteration 1600 -> loss: 0.09323273919128938\n",
      "    At iteration 1700 -> loss: 0.093272651017215\n",
      "    At iteration 1800 -> loss: 0.09321276536690584\n",
      "    At iteration 1900 -> loss: 0.09313353889427957\n",
      "    At iteration 2000 -> loss: 0.09298870082010091\n",
      "    At iteration 2100 -> loss: 0.0928724813635823\n",
      "    At iteration 2200 -> loss: 0.09279943119230726\n",
      "    At iteration 2300 -> loss: 0.09280622594670623\n",
      "    At iteration 2400 -> loss: 0.09280110446098531\n",
      "    At iteration 2500 -> loss: 0.09264378905697628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2600 -> loss: 0.09266022289507117\n",
      "    At iteration 2700 -> loss: 0.09302492466457947\n",
      "    At iteration 2800 -> loss: 0.09290513602515922\n",
      "    At iteration 2900 -> loss: 0.09295691073757695\n",
      "    At iteration 3000 -> loss: 0.09285651645109375\n",
      "    At iteration 3100 -> loss: 0.09273725694365276\n",
      "    At iteration 3200 -> loss: 0.0927546364225143\n",
      "    At iteration 3300 -> loss: 0.0926687820097367\n",
      "    At iteration 3400 -> loss: 0.09257280421452928\n",
      "    At iteration 3500 -> loss: 0.09252051354075376\n",
      "    At iteration 3600 -> loss: 0.09240156638432076\n",
      "    At iteration 3700 -> loss: 0.09240784414262466\n",
      "    At iteration 3800 -> loss: 0.09245090302427944\n",
      "    At iteration 3900 -> loss: 0.09246707826284026\n",
      "    At iteration 4000 -> loss: 0.09243312419993115\n",
      "    At iteration 4100 -> loss: 0.09267218989188983\n",
      "    At iteration 4200 -> loss: 0.09264188122440882\n",
      "    At iteration 4300 -> loss: 0.09271507104521932\n",
      "    At iteration 4400 -> loss: 0.09264954025356771\n",
      "    At iteration 4500 -> loss: 0.09270066875490067\n",
      "    At iteration 4600 -> loss: 0.09273550495331259\n",
      "    At iteration 4700 -> loss: 0.09273297868861473\n",
      "    At iteration 4800 -> loss: 0.0926741295479405\n",
      "    At iteration 4900 -> loss: 0.09268490925812044\n",
      "    At iteration 5000 -> loss: 0.09263048544725835\n",
      "    At iteration 5100 -> loss: 0.09258275445633236\n",
      "    At iteration 5200 -> loss: 0.0926632739250511\n",
      "    At iteration 5300 -> loss: 0.09264287650265117\n",
      "    At iteration 5400 -> loss: 0.09256977571259466\n",
      "    At iteration 5500 -> loss: 0.09253820534208634\n",
      "    At iteration 5600 -> loss: 0.09249499073616721\n",
      "    At iteration 5700 -> loss: 0.09243865493497372\n",
      "    At iteration 5800 -> loss: 0.09241140735843001\n",
      "    At iteration 5900 -> loss: 0.09237014068826094\n",
      "    At iteration 6000 -> loss: 0.0923321647710539\n",
      "    At iteration 6100 -> loss: 0.09231652867040321\n",
      "    At iteration 6200 -> loss: 0.09231453757590576\n",
      "    At iteration 6300 -> loss: 0.09236689807828037\n",
      "    At iteration 6400 -> loss: 0.0924895969949181\n",
      "    At iteration 6500 -> loss: 0.09246792087897242\n",
      "    At iteration 6600 -> loss: 0.09239982130970927\n",
      "    At iteration 6700 -> loss: 0.0923935107472127\n",
      "    At iteration 6800 -> loss: 0.09242141879290086\n",
      "    At iteration 6900 -> loss: 0.0923738160825445\n",
      "    At iteration 7000 -> loss: 0.09246586522082585\n",
      "    At iteration 7100 -> loss: 0.09241365145201473\n",
      "    At iteration 7200 -> loss: 0.09238277460028828\n",
      "    At iteration 7300 -> loss: 0.09238436961025245\n",
      "    At iteration 7400 -> loss: 0.09239111895195465\n",
      "    At iteration 7500 -> loss: 0.0923889611949323\n",
      "    At iteration 7600 -> loss: 0.09240419949466352\n",
      "    At iteration 7700 -> loss: 0.09238004480606724\n",
      "    At iteration 7800 -> loss: 0.09238028901458636\n",
      "    At iteration 7900 -> loss: 0.09238689384942415\n",
      "    At iteration 8000 -> loss: 0.09239003062847924\n",
      "    At iteration 8100 -> loss: 0.09239768705556867\n",
      "    At iteration 8200 -> loss: 0.09239098081339216\n",
      "    At iteration 8300 -> loss: 0.09260166407971888\n",
      "    At iteration 8400 -> loss: 0.09255382236434692\n",
      "    At iteration 8500 -> loss: 0.09255632993120555\n",
      "    At iteration 8600 -> loss: 0.09250696677651936\n",
      "    At iteration 8700 -> loss: 0.0924969681024957\n",
      "    At iteration 8800 -> loss: 0.09247686329832579\n",
      "    At iteration 8900 -> loss: 0.09244751942846842\n",
      "    At iteration 9000 -> loss: 0.09240751068343062\n",
      "    At iteration 9100 -> loss: 0.09241695010623796\n",
      "    At iteration 9200 -> loss: 0.09240275061052995\n",
      "    At iteration 9300 -> loss: 0.0925693102361709\n",
      "    At iteration 9400 -> loss: 0.09256012836687212\n",
      "    At iteration 9500 -> loss: 0.09252566760935736\n",
      "    At iteration 9600 -> loss: 0.09251721561444627\n",
      "    At iteration 9700 -> loss: 0.09254054246607077\n",
      "    At iteration 9800 -> loss: 0.09251426253704882\n",
      "    At iteration 9900 -> loss: 0.09247714032862778\n",
      "    At iteration 10000 -> loss: 0.0924804549735215\n",
      "    At iteration 10100 -> loss: 0.09245143527440332\n",
      "    At iteration 10200 -> loss: 0.09242013411190463\n",
      "    At iteration 10300 -> loss: 0.0925142691838831\n",
      "    At iteration 10400 -> loss: 0.09254468485343151\n",
      "    At iteration 10500 -> loss: 0.09252131369745006\n",
      "    At iteration 10600 -> loss: 0.09257449327164341\n",
      "    At iteration 10700 -> loss: 0.0925587710253145\n",
      "    At iteration 10800 -> loss: 0.09252864521068423\n",
      "    At iteration 10900 -> loss: 0.09257610543486806\n",
      "    At iteration 11000 -> loss: 0.09255014949401698\n",
      "    At iteration 11100 -> loss: 0.0925324308736804\n",
      "    At iteration 11200 -> loss: 0.09253163209880949\n",
      "    At iteration 11300 -> loss: 0.09250076696155018\n",
      "    At iteration 11400 -> loss: 0.0925192118284825\n",
      "    At iteration 11500 -> loss: 0.0924965987404699\n",
      "    At iteration 11600 -> loss: 0.09248108473549922\n",
      "    At iteration 11700 -> loss: 0.09245740384017223\n",
      "    At iteration 11800 -> loss: 0.09248453226120504\n",
      "    At iteration 11900 -> loss: 0.09253247011284883\n",
      "    At iteration 12000 -> loss: 0.09261861657678298\n",
      "    At iteration 12100 -> loss: 0.09271637639946621\n",
      "    At iteration 12200 -> loss: 0.09276780140440098\n",
      "    At iteration 12300 -> loss: 0.09276188629509469\n",
      "    At iteration 12400 -> loss: 0.09274997173325578\n",
      "    At iteration 12500 -> loss: 0.09273687521036275\n",
      "    At iteration 12600 -> loss: 0.09271787563084953\n",
      "    At iteration 12700 -> loss: 0.0927173662295926\n",
      "    At iteration 12800 -> loss: 0.09275189064694783\n",
      "    At iteration 12900 -> loss: 0.09276723406872774\n",
      "    At iteration 13000 -> loss: 0.09281146316273305\n",
      "    At iteration 13100 -> loss: 0.09281332331842526\n",
      "    At iteration 13200 -> loss: 0.0930235682658705\n",
      "    At iteration 13300 -> loss: 0.09299178152338895\n",
      "    At iteration 13400 -> loss: 0.09296122630263845\n",
      "    At iteration 13500 -> loss: 0.09294571808543543\n",
      "    At iteration 13600 -> loss: 0.09296907353350772\n",
      "Staring Epoch 63\n",
      "    At iteration 0 -> loss: 0.08136004500556737\n",
      "    At iteration 100 -> loss: 0.09217346880761919\n",
      "    At iteration 200 -> loss: 0.0915173943735927\n",
      "    At iteration 300 -> loss: 0.09136370809038219\n",
      "    At iteration 400 -> loss: 0.09159218138908627\n",
      "    At iteration 500 -> loss: 0.09151568827159244\n",
      "    At iteration 600 -> loss: 0.09229719264450045\n",
      "    At iteration 700 -> loss: 0.0923669410115742\n",
      "    At iteration 800 -> loss: 0.09276788854470298\n",
      "    At iteration 900 -> loss: 0.0925519462758987\n",
      "    At iteration 1000 -> loss: 0.09249881958457228\n",
      "    At iteration 1100 -> loss: 0.09237706811487932\n",
      "    At iteration 1200 -> loss: 0.09234910526618316\n",
      "    At iteration 1300 -> loss: 0.09236777868340326\n",
      "    At iteration 1400 -> loss: 0.09250093167988113\n",
      "    At iteration 1500 -> loss: 0.09324253341310525\n",
      "    At iteration 1600 -> loss: 0.09287769349748835\n",
      "    At iteration 1700 -> loss: 0.0928739877198494\n",
      "    At iteration 1800 -> loss: 0.09287009076032851\n",
      "    At iteration 1900 -> loss: 0.09310991477779286\n",
      "    At iteration 2000 -> loss: 0.09296896262507265\n",
      "    At iteration 2100 -> loss: 0.09328921417213404\n",
      "    At iteration 2200 -> loss: 0.09316634296620586\n",
      "    At iteration 2300 -> loss: 0.09305741453144686\n",
      "    At iteration 2400 -> loss: 0.09304978055403189\n",
      "    At iteration 2500 -> loss: 0.09296168450107005\n",
      "    At iteration 2600 -> loss: 0.09338138050727268\n",
      "    At iteration 2700 -> loss: 0.09356433518858855\n",
      "    At iteration 2800 -> loss: 0.09356733292292144\n",
      "    At iteration 2900 -> loss: 0.09387480321843124\n",
      "    At iteration 3000 -> loss: 0.09381515042789171\n",
      "    At iteration 3100 -> loss: 0.09374653199309274\n",
      "    At iteration 3200 -> loss: 0.09365434540680896\n",
      "    At iteration 3300 -> loss: 0.09364183576862474\n",
      "    At iteration 3400 -> loss: 0.09351516626542943\n",
      "    At iteration 3500 -> loss: 0.09377274740226181\n",
      "    At iteration 3600 -> loss: 0.09362672224702669\n",
      "    At iteration 3700 -> loss: 0.09376143921612116\n",
      "    At iteration 3800 -> loss: 0.09381818978732052\n",
      "    At iteration 3900 -> loss: 0.0937000396626188\n",
      "    At iteration 4000 -> loss: 0.09381626142340206\n",
      "    At iteration 4100 -> loss: 0.09397345189910543\n",
      "    At iteration 4200 -> loss: 0.0939366539501066\n",
      "    At iteration 4300 -> loss: 0.0938804189964071\n",
      "    At iteration 4400 -> loss: 0.09394177029663635\n",
      "    At iteration 4500 -> loss: 0.09389966088671561\n",
      "    At iteration 4600 -> loss: 0.09382046519610725\n",
      "    At iteration 4700 -> loss: 0.09380202971638871\n",
      "    At iteration 4800 -> loss: 0.09388918648975011\n",
      "    At iteration 4900 -> loss: 0.09384281938816036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5000 -> loss: 0.09375964550126883\n",
      "    At iteration 5100 -> loss: 0.09374446086156919\n",
      "    At iteration 5200 -> loss: 0.09382017559943605\n",
      "    At iteration 5300 -> loss: 0.09376432546745203\n",
      "    At iteration 5400 -> loss: 0.0937431373133043\n",
      "    At iteration 5500 -> loss: 0.09366007224454082\n",
      "    At iteration 5600 -> loss: 0.09362112591684325\n",
      "    At iteration 5700 -> loss: 0.0935789207288825\n",
      "    At iteration 5800 -> loss: 0.09351845529952853\n",
      "    At iteration 5900 -> loss: 0.09346107864354208\n",
      "    At iteration 6000 -> loss: 0.09347421100876743\n",
      "    At iteration 6100 -> loss: 0.09344800977934772\n",
      "    At iteration 6200 -> loss: 0.09336293476128346\n",
      "    At iteration 6300 -> loss: 0.09348706487152424\n",
      "    At iteration 6400 -> loss: 0.09349104901283561\n",
      "    At iteration 6500 -> loss: 0.09347239110187186\n",
      "    At iteration 6600 -> loss: 0.09346214790156063\n",
      "    At iteration 6700 -> loss: 0.09338111612950432\n",
      "    At iteration 6800 -> loss: 0.09333596011501709\n",
      "    At iteration 6900 -> loss: 0.09330998882079364\n",
      "    At iteration 7000 -> loss: 0.09328567446426231\n",
      "    At iteration 7100 -> loss: 0.09323886023199152\n",
      "    At iteration 7200 -> loss: 0.09320556140863512\n",
      "    At iteration 7300 -> loss: 0.09315627064584779\n",
      "    At iteration 7400 -> loss: 0.09318317003871744\n",
      "    At iteration 7500 -> loss: 0.09319423279990159\n",
      "    At iteration 7600 -> loss: 0.09318366844453899\n",
      "    At iteration 7700 -> loss: 0.09316870895051013\n",
      "    At iteration 7800 -> loss: 0.0931579008436833\n",
      "    At iteration 7900 -> loss: 0.09313714328624487\n",
      "    At iteration 8000 -> loss: 0.09310172741408182\n",
      "    At iteration 8100 -> loss: 0.09309757425900833\n",
      "    At iteration 8200 -> loss: 0.09307834577251554\n",
      "    At iteration 8300 -> loss: 0.09308526782240097\n",
      "    At iteration 8400 -> loss: 0.09309473323928442\n",
      "    At iteration 8500 -> loss: 0.09310599624830683\n",
      "    At iteration 8600 -> loss: 0.09308687277439012\n",
      "    At iteration 8700 -> loss: 0.09311078110106513\n",
      "    At iteration 8800 -> loss: 0.09312447229621706\n",
      "    At iteration 8900 -> loss: 0.09306653756217423\n",
      "    At iteration 9000 -> loss: 0.0930685554519853\n",
      "    At iteration 9100 -> loss: 0.09305123110341013\n",
      "    At iteration 9200 -> loss: 0.09303581239280062\n",
      "    At iteration 9300 -> loss: 0.09301450981919057\n",
      "    At iteration 9400 -> loss: 0.09298514930499105\n",
      "    At iteration 9500 -> loss: 0.09295977505580273\n",
      "    At iteration 9600 -> loss: 0.0929973286461048\n",
      "    At iteration 9700 -> loss: 0.09295459216183051\n",
      "    At iteration 9800 -> loss: 0.0929607693593648\n",
      "    At iteration 9900 -> loss: 0.09296401558524968\n",
      "    At iteration 10000 -> loss: 0.0931792206834734\n",
      "    At iteration 10100 -> loss: 0.09318376902514663\n",
      "    At iteration 10200 -> loss: 0.09313046658377667\n",
      "    At iteration 10300 -> loss: 0.09313974791990645\n",
      "    At iteration 10400 -> loss: 0.09311521634262956\n",
      "    At iteration 10500 -> loss: 0.09307838539089207\n",
      "    At iteration 10600 -> loss: 0.09306913314774927\n",
      "    At iteration 10700 -> loss: 0.09304930048745863\n",
      "    At iteration 10800 -> loss: 0.09306395826556373\n",
      "    At iteration 10900 -> loss: 0.09307453706127367\n",
      "    At iteration 11000 -> loss: 0.09305927197697618\n",
      "    At iteration 11100 -> loss: 0.09303699251047252\n",
      "    At iteration 11200 -> loss: 0.09301273958646922\n",
      "    At iteration 11300 -> loss: 0.0930181046163973\n",
      "    At iteration 11400 -> loss: 0.09301152880806945\n",
      "    At iteration 11500 -> loss: 0.09299677860982705\n",
      "    At iteration 11600 -> loss: 0.0929899428702278\n",
      "    At iteration 11700 -> loss: 0.09328258709956347\n",
      "    At iteration 11800 -> loss: 0.09324864415662763\n",
      "    At iteration 11900 -> loss: 0.0932162551633526\n",
      "    At iteration 12000 -> loss: 0.09323504337726511\n",
      "    At iteration 12100 -> loss: 0.09323872107538281\n",
      "    At iteration 12200 -> loss: 0.09321632447956098\n",
      "    At iteration 12300 -> loss: 0.09319195354843912\n",
      "    At iteration 12400 -> loss: 0.09315482942593656\n",
      "    At iteration 12500 -> loss: 0.09314534886460712\n",
      "    At iteration 12600 -> loss: 0.09311990401042623\n",
      "    At iteration 12700 -> loss: 0.09313448169412251\n",
      "    At iteration 12800 -> loss: 0.0931043537049008\n",
      "    At iteration 12900 -> loss: 0.09313424567668296\n",
      "    At iteration 13000 -> loss: 0.09313679685452125\n",
      "    At iteration 13100 -> loss: 0.09310989469234957\n",
      "    At iteration 13200 -> loss: 0.09310524395883149\n",
      "    At iteration 13300 -> loss: 0.09307598522877254\n",
      "    At iteration 13400 -> loss: 0.09305534952636745\n",
      "    At iteration 13500 -> loss: 0.09303684893874803\n",
      "    At iteration 13600 -> loss: 0.09301470000680113\n",
      "Staring Epoch 64\n",
      "    At iteration 0 -> loss: 0.08074869657866657\n",
      "    At iteration 100 -> loss: 0.09233976416566611\n",
      "    At iteration 200 -> loss: 0.09005361092086112\n",
      "    At iteration 300 -> loss: 0.09138492841903743\n",
      "    At iteration 400 -> loss: 0.0908050143788776\n",
      "    At iteration 500 -> loss: 0.09197913514071517\n",
      "    At iteration 600 -> loss: 0.09242401987539348\n",
      "    At iteration 700 -> loss: 0.09203596383688231\n",
      "    At iteration 800 -> loss: 0.09183517115201263\n",
      "    At iteration 900 -> loss: 0.0922909152988979\n",
      "    At iteration 1000 -> loss: 0.09212756793674091\n",
      "    At iteration 1100 -> loss: 0.09200486087800082\n",
      "    At iteration 1200 -> loss: 0.09203969031077666\n",
      "    At iteration 1300 -> loss: 0.09206676678151396\n",
      "    At iteration 1400 -> loss: 0.09220523498581908\n",
      "    At iteration 1500 -> loss: 0.0919409020679297\n",
      "    At iteration 1600 -> loss: 0.09197853120918294\n",
      "    At iteration 1700 -> loss: 0.09218586864421718\n",
      "    At iteration 1800 -> loss: 0.09209955185059265\n",
      "    At iteration 1900 -> loss: 0.09198123507523052\n",
      "    At iteration 2000 -> loss: 0.09182225162300599\n",
      "    At iteration 2100 -> loss: 0.0918700276352018\n",
      "    At iteration 2200 -> loss: 0.09190810385014607\n",
      "    At iteration 2300 -> loss: 0.09182598905991204\n",
      "    At iteration 2400 -> loss: 0.09177844073456803\n",
      "    At iteration 2500 -> loss: 0.09162771117915366\n",
      "    At iteration 2600 -> loss: 0.09174084044238379\n",
      "    At iteration 2700 -> loss: 0.0916782472613586\n",
      "    At iteration 2800 -> loss: 0.09198612930466558\n",
      "    At iteration 2900 -> loss: 0.09194490193227023\n",
      "    At iteration 3000 -> loss: 0.09190173820468328\n",
      "    At iteration 3100 -> loss: 0.09188858922333647\n",
      "    At iteration 3200 -> loss: 0.0918785059913068\n",
      "    At iteration 3300 -> loss: 0.09192126510494712\n",
      "    At iteration 3400 -> loss: 0.0919014088896384\n",
      "    At iteration 3500 -> loss: 0.09187084594293411\n",
      "    At iteration 3600 -> loss: 0.09270714381294283\n",
      "    At iteration 3700 -> loss: 0.09265918416586741\n",
      "    At iteration 3800 -> loss: 0.0925642755551189\n",
      "    At iteration 3900 -> loss: 0.09261780932640451\n",
      "    At iteration 4000 -> loss: 0.09260089453526461\n",
      "    At iteration 4100 -> loss: 0.09261893200992244\n",
      "    At iteration 4200 -> loss: 0.09259729829197809\n",
      "    At iteration 4300 -> loss: 0.09263731323714695\n",
      "    At iteration 4400 -> loss: 0.09260155719448317\n",
      "    At iteration 4500 -> loss: 0.09273020431191054\n",
      "    At iteration 4600 -> loss: 0.09265065309360793\n",
      "    At iteration 4700 -> loss: 0.09268527461191685\n",
      "    At iteration 4800 -> loss: 0.09260530387141451\n",
      "    At iteration 4900 -> loss: 0.0926188914514783\n",
      "    At iteration 5000 -> loss: 0.09268987125021878\n",
      "    At iteration 5100 -> loss: 0.09263787370856798\n",
      "    At iteration 5200 -> loss: 0.09258242638742521\n",
      "    At iteration 5300 -> loss: 0.09256487930882329\n",
      "    At iteration 5400 -> loss: 0.09254329245286697\n",
      "    At iteration 5500 -> loss: 0.09257206790755047\n",
      "    At iteration 5600 -> loss: 0.09256187263203372\n",
      "    At iteration 5700 -> loss: 0.09259828120757642\n",
      "    At iteration 5800 -> loss: 0.09261573268692312\n",
      "    At iteration 5900 -> loss: 0.09264077721763704\n",
      "    At iteration 6000 -> loss: 0.09262112702028268\n",
      "    At iteration 6100 -> loss: 0.09271904194657338\n",
      "    At iteration 6200 -> loss: 0.09273236888743451\n",
      "    At iteration 6300 -> loss: 0.09274016577007178\n",
      "    At iteration 6400 -> loss: 0.09291702162617153\n",
      "    At iteration 6500 -> loss: 0.09288212975458235\n",
      "    At iteration 6600 -> loss: 0.09290900582276217\n",
      "    At iteration 6700 -> loss: 0.09286017965573792\n",
      "    At iteration 6800 -> loss: 0.09285481716601342\n",
      "    At iteration 6900 -> loss: 0.09285770731507766\n",
      "    At iteration 7000 -> loss: 0.09299684753742754\n",
      "    At iteration 7100 -> loss: 0.09294398293820273\n",
      "    At iteration 7200 -> loss: 0.09292420694142701\n",
      "    At iteration 7300 -> loss: 0.09285710946197542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7400 -> loss: 0.09280344499652707\n",
      "    At iteration 7500 -> loss: 0.09276906688198282\n",
      "    At iteration 7600 -> loss: 0.09274384043753586\n",
      "    At iteration 7700 -> loss: 0.09271185262752098\n",
      "    At iteration 7800 -> loss: 0.09281899667631885\n",
      "    At iteration 7900 -> loss: 0.09279977752359397\n",
      "    At iteration 8000 -> loss: 0.09278937855228772\n",
      "    At iteration 8100 -> loss: 0.09278357099860172\n",
      "    At iteration 8200 -> loss: 0.09275701493667543\n",
      "    At iteration 8300 -> loss: 0.09271404590753449\n",
      "    At iteration 8400 -> loss: 0.09268492263983398\n",
      "    At iteration 8500 -> loss: 0.09268040936546583\n",
      "    At iteration 8600 -> loss: 0.09273603354948012\n",
      "    At iteration 8700 -> loss: 0.09268425960174519\n",
      "    At iteration 8800 -> loss: 0.09266308719412582\n",
      "    At iteration 8900 -> loss: 0.09272748063712384\n",
      "    At iteration 9000 -> loss: 0.09270251746937846\n",
      "    At iteration 9100 -> loss: 0.09267862656521077\n",
      "    At iteration 9200 -> loss: 0.09270734795645318\n",
      "    At iteration 9300 -> loss: 0.09271021273722008\n",
      "    At iteration 9400 -> loss: 0.09283789526437995\n",
      "    At iteration 9500 -> loss: 0.09280265896719585\n",
      "    At iteration 9600 -> loss: 0.09280307765076816\n",
      "    At iteration 9700 -> loss: 0.0927636565448455\n",
      "    At iteration 9800 -> loss: 0.09274157808306288\n",
      "    At iteration 9900 -> loss: 0.09274126446783666\n",
      "    At iteration 10000 -> loss: 0.09272776280035074\n",
      "    At iteration 10100 -> loss: 0.09279500607382762\n",
      "    At iteration 10200 -> loss: 0.09278568861497591\n",
      "    At iteration 10300 -> loss: 0.09280953002965525\n",
      "    At iteration 10400 -> loss: 0.09279350069215983\n",
      "    At iteration 10500 -> loss: 0.09281006807643093\n",
      "    At iteration 10600 -> loss: 0.09290111726832645\n",
      "    At iteration 10700 -> loss: 0.09288169228591592\n",
      "    At iteration 10800 -> loss: 0.09288529044687945\n",
      "    At iteration 10900 -> loss: 0.09291721410707413\n",
      "    At iteration 11000 -> loss: 0.09304645014675218\n",
      "    At iteration 11100 -> loss: 0.09310328058227242\n",
      "    At iteration 11200 -> loss: 0.09310848253376398\n",
      "    At iteration 11300 -> loss: 0.09322000526542676\n",
      "    At iteration 11400 -> loss: 0.09322912625467816\n",
      "    At iteration 11500 -> loss: 0.0932080724789863\n",
      "    At iteration 11600 -> loss: 0.09320460447449636\n",
      "    At iteration 11700 -> loss: 0.09324311528407955\n",
      "    At iteration 11800 -> loss: 0.0932089111084552\n",
      "    At iteration 11900 -> loss: 0.09319186286106022\n",
      "    At iteration 12000 -> loss: 0.09317635451586088\n",
      "    At iteration 12100 -> loss: 0.09317964425400879\n",
      "    At iteration 12200 -> loss: 0.09315512034905882\n",
      "    At iteration 12300 -> loss: 0.09316261333487642\n",
      "    At iteration 12400 -> loss: 0.09316227446113633\n",
      "    At iteration 12500 -> loss: 0.09315268530595053\n",
      "    At iteration 12600 -> loss: 0.09313179968371735\n",
      "    At iteration 12700 -> loss: 0.09311033425114727\n",
      "    At iteration 12800 -> loss: 0.09309476995480602\n",
      "    At iteration 12900 -> loss: 0.09307214275149474\n",
      "    At iteration 13000 -> loss: 0.09309132375421081\n",
      "    At iteration 13100 -> loss: 0.0930728923005221\n",
      "    At iteration 13200 -> loss: 0.09304939861158343\n",
      "    At iteration 13300 -> loss: 0.09305288850023803\n",
      "    At iteration 13400 -> loss: 0.0930353850833638\n",
      "    At iteration 13500 -> loss: 0.09301959221580815\n",
      "    At iteration 13600 -> loss: 0.0930318292205253\n",
      "Staring Epoch 65\n",
      "    At iteration 0 -> loss: 0.08197681425372139\n",
      "    At iteration 100 -> loss: 0.10048143991148076\n",
      "    At iteration 200 -> loss: 0.09734950815585904\n",
      "    At iteration 300 -> loss: 0.09565445709304354\n",
      "    At iteration 400 -> loss: 0.09459996912729102\n",
      "    At iteration 500 -> loss: 0.09550481604462636\n",
      "    At iteration 600 -> loss: 0.09458090253345575\n",
      "    At iteration 700 -> loss: 0.09831327553039\n",
      "    At iteration 800 -> loss: 0.09707548017754115\n",
      "    At iteration 900 -> loss: 0.09718448374541777\n",
      "    At iteration 1000 -> loss: 0.09650933051679518\n",
      "    At iteration 1100 -> loss: 0.09615855903193665\n",
      "    At iteration 1200 -> loss: 0.0957460930653366\n",
      "    At iteration 1300 -> loss: 0.09550421181474572\n",
      "    At iteration 1400 -> loss: 0.0951308774701716\n",
      "    At iteration 1500 -> loss: 0.09506280397968533\n",
      "    At iteration 1600 -> loss: 0.09545995683709595\n",
      "    At iteration 1700 -> loss: 0.09516919525378609\n",
      "    At iteration 1800 -> loss: 0.09484881135839501\n",
      "    At iteration 1900 -> loss: 0.09471454480259177\n",
      "    At iteration 2000 -> loss: 0.09490518579275949\n",
      "    At iteration 2100 -> loss: 0.09473457718072179\n",
      "    At iteration 2200 -> loss: 0.09486216463302057\n",
      "    At iteration 2300 -> loss: 0.09468585960220338\n",
      "    At iteration 2400 -> loss: 0.09447336789889377\n",
      "    At iteration 2500 -> loss: 0.09429910159175393\n",
      "    At iteration 2600 -> loss: 0.09425520221011087\n",
      "    At iteration 2700 -> loss: 0.0940304125253551\n",
      "    At iteration 2800 -> loss: 0.09387703878265063\n",
      "    At iteration 2900 -> loss: 0.09387947678741916\n",
      "    At iteration 3000 -> loss: 0.09392779456862775\n",
      "    At iteration 3100 -> loss: 0.09381341904001786\n",
      "    At iteration 3200 -> loss: 0.09380812232983969\n",
      "    At iteration 3300 -> loss: 0.09371556543446515\n",
      "    At iteration 3400 -> loss: 0.09367513726610678\n",
      "    At iteration 3500 -> loss: 0.09363024549320621\n",
      "    At iteration 3600 -> loss: 0.09354613364176974\n",
      "    At iteration 3700 -> loss: 0.09348932030815173\n",
      "    At iteration 3800 -> loss: 0.09340164149297006\n",
      "    At iteration 3900 -> loss: 0.09340393368376919\n",
      "    At iteration 4000 -> loss: 0.09344713043116006\n",
      "    At iteration 4100 -> loss: 0.09347727769643435\n",
      "    At iteration 4200 -> loss: 0.09339805670402115\n",
      "    At iteration 4300 -> loss: 0.09335662823172106\n",
      "    At iteration 4400 -> loss: 0.09328477187995846\n",
      "    At iteration 4500 -> loss: 0.09325102303336832\n",
      "    At iteration 4600 -> loss: 0.09333338096555514\n",
      "    At iteration 4700 -> loss: 0.0932688127643649\n",
      "    At iteration 4800 -> loss: 0.09316713127267168\n",
      "    At iteration 4900 -> loss: 0.09317478399222279\n",
      "    At iteration 5000 -> loss: 0.09316360296927699\n",
      "    At iteration 5100 -> loss: 0.09309809261630997\n",
      "    At iteration 5200 -> loss: 0.09307138620154161\n",
      "    At iteration 5300 -> loss: 0.09303505116841997\n",
      "    At iteration 5400 -> loss: 0.09309653299731867\n",
      "    At iteration 5500 -> loss: 0.09321417219573112\n",
      "    At iteration 5600 -> loss: 0.09324098643207065\n",
      "    At iteration 5700 -> loss: 0.09321191798036248\n",
      "    At iteration 5800 -> loss: 0.0931519882833554\n",
      "    At iteration 5900 -> loss: 0.09314549085579608\n",
      "    At iteration 6000 -> loss: 0.09313802587211971\n",
      "    At iteration 6100 -> loss: 0.09311161063169997\n",
      "    At iteration 6200 -> loss: 0.09306617621832478\n",
      "    At iteration 6300 -> loss: 0.0930564923631981\n",
      "    At iteration 6400 -> loss: 0.09304703886587852\n",
      "    At iteration 6500 -> loss: 0.09310612660784715\n",
      "    At iteration 6600 -> loss: 0.09307106772706694\n",
      "    At iteration 6700 -> loss: 0.09305624210152522\n",
      "    At iteration 6800 -> loss: 0.09309142462818358\n",
      "    At iteration 6900 -> loss: 0.09308586891081254\n",
      "    At iteration 7000 -> loss: 0.09310759981307438\n",
      "    At iteration 7100 -> loss: 0.09306863343494501\n",
      "    At iteration 7200 -> loss: 0.0930145038058939\n",
      "    At iteration 7300 -> loss: 0.09296273977353507\n",
      "    At iteration 7400 -> loss: 0.09294714523133246\n",
      "    At iteration 7500 -> loss: 0.09291812682523541\n",
      "    At iteration 7600 -> loss: 0.0929511677560143\n",
      "    At iteration 7700 -> loss: 0.09290612448456348\n",
      "    At iteration 7800 -> loss: 0.0928621241200152\n",
      "    At iteration 7900 -> loss: 0.0928877624593379\n",
      "    At iteration 8000 -> loss: 0.09285758066718097\n",
      "    At iteration 8100 -> loss: 0.09287338965725667\n",
      "    At iteration 8200 -> loss: 0.09287825872027634\n",
      "    At iteration 8300 -> loss: 0.09285443352788056\n",
      "    At iteration 8400 -> loss: 0.09286743190407892\n",
      "    At iteration 8500 -> loss: 0.09284889861823467\n",
      "    At iteration 8600 -> loss: 0.0928337465557838\n",
      "    At iteration 8700 -> loss: 0.092797484113632\n",
      "    At iteration 8800 -> loss: 0.09279629776501468\n",
      "    At iteration 8900 -> loss: 0.09281144965032026\n",
      "    At iteration 9000 -> loss: 0.09277033068448114\n",
      "    At iteration 9100 -> loss: 0.0928370552702427\n",
      "    At iteration 9200 -> loss: 0.09281729832068582\n",
      "    At iteration 9300 -> loss: 0.09284737543641118\n",
      "    At iteration 9400 -> loss: 0.0928525632150434\n",
      "    At iteration 9500 -> loss: 0.09286722185798114\n",
      "    At iteration 9600 -> loss: 0.09293265325197575\n",
      "    At iteration 9700 -> loss: 0.0929672153622929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9800 -> loss: 0.09297188037155832\n",
      "    At iteration 9900 -> loss: 0.09306125341921619\n",
      "    At iteration 10000 -> loss: 0.0930504900159626\n",
      "    At iteration 10100 -> loss: 0.09300989167771806\n",
      "    At iteration 10200 -> loss: 0.0930122531604676\n",
      "    At iteration 10300 -> loss: 0.09301575271823125\n",
      "    At iteration 10400 -> loss: 0.09296903551573683\n",
      "    At iteration 10500 -> loss: 0.09294193291296424\n",
      "    At iteration 10600 -> loss: 0.09294666134827773\n",
      "    At iteration 10700 -> loss: 0.0929382552279393\n",
      "    At iteration 10800 -> loss: 0.09292694941742945\n",
      "    At iteration 10900 -> loss: 0.09295147207441795\n",
      "    At iteration 11000 -> loss: 0.09298208588424108\n",
      "    At iteration 11100 -> loss: 0.09295797506477096\n",
      "    At iteration 11200 -> loss: 0.09295216488368498\n",
      "    At iteration 11300 -> loss: 0.09295698642753514\n",
      "    At iteration 11400 -> loss: 0.09293784529797405\n",
      "    At iteration 11500 -> loss: 0.092938872404999\n",
      "    At iteration 11600 -> loss: 0.09293340886059571\n",
      "    At iteration 11700 -> loss: 0.09290006518606572\n",
      "    At iteration 11800 -> loss: 0.09286939023961031\n",
      "    At iteration 11900 -> loss: 0.09285172382814696\n",
      "    At iteration 12000 -> loss: 0.09283476706757927\n",
      "    At iteration 12100 -> loss: 0.09280426126804044\n",
      "    At iteration 12200 -> loss: 0.0928037001837045\n",
      "    At iteration 12300 -> loss: 0.09279351519017169\n",
      "    At iteration 12400 -> loss: 0.09282709347763177\n",
      "    At iteration 12500 -> loss: 0.09279326029626764\n",
      "    At iteration 12600 -> loss: 0.09277467136246519\n",
      "    At iteration 12700 -> loss: 0.09275810872280224\n",
      "    At iteration 12800 -> loss: 0.09280705829909919\n",
      "    At iteration 12900 -> loss: 0.09284758018792727\n",
      "    At iteration 13000 -> loss: 0.09284534522672744\n",
      "    At iteration 13100 -> loss: 0.09282294737084171\n",
      "    At iteration 13200 -> loss: 0.09288266716652349\n",
      "    At iteration 13300 -> loss: 0.09295134270575905\n",
      "    At iteration 13400 -> loss: 0.09295941076958197\n",
      "    At iteration 13500 -> loss: 0.09301253415755016\n",
      "    At iteration 13600 -> loss: 0.09302186370855503\n",
      "Staring Epoch 66\n",
      "    At iteration 0 -> loss: 0.0804714835685445\n",
      "    At iteration 100 -> loss: 0.0917742079189784\n",
      "    At iteration 200 -> loss: 0.09156133950158206\n",
      "    At iteration 300 -> loss: 0.09098410209350356\n",
      "    At iteration 400 -> loss: 0.09172383683778142\n",
      "    At iteration 500 -> loss: 0.09145479140037804\n",
      "    At iteration 600 -> loss: 0.0912272718652185\n",
      "    At iteration 700 -> loss: 0.09211845863507852\n",
      "    At iteration 800 -> loss: 0.09210526601725552\n",
      "    At iteration 900 -> loss: 0.09297577797324127\n",
      "    At iteration 1000 -> loss: 0.09274467149273456\n",
      "    At iteration 1100 -> loss: 0.09267668604159573\n",
      "    At iteration 1200 -> loss: 0.09533667611095872\n",
      "    At iteration 1300 -> loss: 0.09492869214040361\n",
      "    At iteration 1400 -> loss: 0.09470234941593682\n",
      "    At iteration 1500 -> loss: 0.09464107685942966\n",
      "    At iteration 1600 -> loss: 0.094373672708341\n",
      "    At iteration 1700 -> loss: 0.09437135470706895\n",
      "    At iteration 1800 -> loss: 0.09421413272159034\n",
      "    At iteration 1900 -> loss: 0.0942373604940584\n",
      "    At iteration 2000 -> loss: 0.09439176642445252\n",
      "    At iteration 2100 -> loss: 0.09459341929866151\n",
      "    At iteration 2200 -> loss: 0.09433069255620546\n",
      "    At iteration 2300 -> loss: 0.0940707256627114\n",
      "    At iteration 2400 -> loss: 0.09392187297653092\n",
      "    At iteration 2500 -> loss: 0.09380880584356109\n",
      "    At iteration 2600 -> loss: 0.09391973989558502\n",
      "    At iteration 2700 -> loss: 0.0939114615359783\n",
      "    At iteration 2800 -> loss: 0.09383195052503009\n",
      "    At iteration 2900 -> loss: 0.09369409322350905\n",
      "    At iteration 3000 -> loss: 0.09383716028908994\n",
      "    At iteration 3100 -> loss: 0.09383938516400907\n",
      "    At iteration 3200 -> loss: 0.09382376507030712\n",
      "    At iteration 3300 -> loss: 0.09363158051812004\n",
      "    At iteration 3400 -> loss: 0.09361111787510411\n",
      "    At iteration 3500 -> loss: 0.09350053883689707\n",
      "    At iteration 3600 -> loss: 0.09350876690015089\n",
      "    At iteration 3700 -> loss: 0.09338361903031117\n",
      "    At iteration 3800 -> loss: 0.09327050107888489\n",
      "    At iteration 3900 -> loss: 0.09326799728037037\n",
      "    At iteration 4000 -> loss: 0.0931707655738084\n",
      "    At iteration 4100 -> loss: 0.09320742468868155\n",
      "    At iteration 4200 -> loss: 0.09311140691724994\n",
      "    At iteration 4300 -> loss: 0.09307018356905006\n",
      "    At iteration 4400 -> loss: 0.09298720574823033\n",
      "    At iteration 4500 -> loss: 0.09323512765067664\n",
      "    At iteration 4600 -> loss: 0.09323191825970621\n",
      "    At iteration 4700 -> loss: 0.09320304595659823\n",
      "    At iteration 4800 -> loss: 0.09327887920461721\n",
      "    At iteration 4900 -> loss: 0.09321411225223199\n",
      "    At iteration 5000 -> loss: 0.09322332093354614\n",
      "    At iteration 5100 -> loss: 0.0931914098399615\n",
      "    At iteration 5200 -> loss: 0.09316497867876361\n",
      "    At iteration 5300 -> loss: 0.09312000649882579\n",
      "    At iteration 5400 -> loss: 0.09304706796403327\n",
      "    At iteration 5500 -> loss: 0.09300490132068664\n",
      "    At iteration 5600 -> loss: 0.0931054058558822\n",
      "    At iteration 5700 -> loss: 0.09305433033364412\n",
      "    At iteration 5800 -> loss: 0.09299282761364841\n",
      "    At iteration 5900 -> loss: 0.09297623625619487\n",
      "    At iteration 6000 -> loss: 0.09298214673568278\n",
      "    At iteration 6100 -> loss: 0.09309670748401949\n",
      "    At iteration 6200 -> loss: 0.09306787685221808\n",
      "    At iteration 6300 -> loss: 0.09311617833366394\n",
      "    At iteration 6400 -> loss: 0.09302143755487657\n",
      "    At iteration 6500 -> loss: 0.09304106792360729\n",
      "    At iteration 6600 -> loss: 0.09312975577496746\n",
      "    At iteration 6700 -> loss: 0.09311117548673291\n",
      "    At iteration 6800 -> loss: 0.0931141436305453\n",
      "    At iteration 6900 -> loss: 0.09305155566275997\n",
      "    At iteration 7000 -> loss: 0.0930298123317241\n",
      "    At iteration 7100 -> loss: 0.09303513072909607\n",
      "    At iteration 7200 -> loss: 0.0930292076990704\n",
      "    At iteration 7300 -> loss: 0.09298822999365712\n",
      "    At iteration 7400 -> loss: 0.09296499897129662\n",
      "    At iteration 7500 -> loss: 0.09290615963854879\n",
      "    At iteration 7600 -> loss: 0.0928996687762314\n",
      "    At iteration 7700 -> loss: 0.09291040503415572\n",
      "    At iteration 7800 -> loss: 0.09291095610446452\n",
      "    At iteration 7900 -> loss: 0.09286110010893361\n",
      "    At iteration 8000 -> loss: 0.09287187062159434\n",
      "    At iteration 8100 -> loss: 0.09285126411783295\n",
      "    At iteration 8200 -> loss: 0.09284079523799604\n",
      "    At iteration 8300 -> loss: 0.09281615102590493\n",
      "    At iteration 8400 -> loss: 0.09297311077437256\n",
      "    At iteration 8500 -> loss: 0.09297997041186488\n",
      "    At iteration 8600 -> loss: 0.09297507598070866\n",
      "    At iteration 8700 -> loss: 0.09300010666121687\n",
      "    At iteration 8800 -> loss: 0.09299183408629547\n",
      "    At iteration 8900 -> loss: 0.0929910464838814\n",
      "    At iteration 9000 -> loss: 0.09299733468853903\n",
      "    At iteration 9100 -> loss: 0.09299202977693924\n",
      "    At iteration 9200 -> loss: 0.0929496221433096\n",
      "    At iteration 9300 -> loss: 0.09294333931608839\n",
      "    At iteration 9400 -> loss: 0.09302797434666873\n",
      "    At iteration 9500 -> loss: 0.09301181301209648\n",
      "    At iteration 9600 -> loss: 0.09303562499857562\n",
      "    At iteration 9700 -> loss: 0.0930206830280209\n",
      "    At iteration 9800 -> loss: 0.09298607765329611\n",
      "    At iteration 9900 -> loss: 0.09301822086817885\n",
      "    At iteration 10000 -> loss: 0.0929883310698286\n",
      "    At iteration 10100 -> loss: 0.09303944520281858\n",
      "    At iteration 10200 -> loss: 0.09301573363043782\n",
      "    At iteration 10300 -> loss: 0.09297329178969774\n",
      "    At iteration 10400 -> loss: 0.09295740580580115\n",
      "    At iteration 10500 -> loss: 0.0929479766966681\n",
      "    At iteration 10600 -> loss: 0.09305017589364195\n",
      "    At iteration 10700 -> loss: 0.09306003378156923\n",
      "    At iteration 10800 -> loss: 0.09303608766540072\n",
      "    At iteration 10900 -> loss: 0.09298800389563777\n",
      "    At iteration 11000 -> loss: 0.0930065490952035\n",
      "    At iteration 11100 -> loss: 0.09300770902662864\n",
      "    At iteration 11200 -> loss: 0.09314785078671534\n",
      "    At iteration 11300 -> loss: 0.09310503766174422\n",
      "    At iteration 11400 -> loss: 0.09310269660110737\n",
      "    At iteration 11500 -> loss: 0.09311009317871177\n",
      "    At iteration 11600 -> loss: 0.09308275199128363\n",
      "    At iteration 11700 -> loss: 0.0930619796283925\n",
      "    At iteration 11800 -> loss: 0.09303222789750969\n",
      "    At iteration 11900 -> loss: 0.0930388202728254\n",
      "    At iteration 12000 -> loss: 0.09302901084620376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12100 -> loss: 0.09304484351252243\n",
      "    At iteration 12200 -> loss: 0.0930244726160504\n",
      "    At iteration 12300 -> loss: 0.09303209900906814\n",
      "    At iteration 12400 -> loss: 0.09309719560421442\n",
      "    At iteration 12500 -> loss: 0.09309121765233178\n",
      "    At iteration 12600 -> loss: 0.09312072470347876\n",
      "    At iteration 12700 -> loss: 0.09311403438365214\n",
      "    At iteration 12800 -> loss: 0.09310661956645308\n",
      "    At iteration 12900 -> loss: 0.09308062525256126\n",
      "    At iteration 13000 -> loss: 0.09309188671975205\n",
      "    At iteration 13100 -> loss: 0.09306351641969751\n",
      "    At iteration 13200 -> loss: 0.0930318825935212\n",
      "    At iteration 13300 -> loss: 0.09302495850388179\n",
      "    At iteration 13400 -> loss: 0.09301492993027753\n",
      "    At iteration 13500 -> loss: 0.09298497310723083\n",
      "    At iteration 13600 -> loss: 0.09298193322393698\n",
      "Staring Epoch 67\n",
      "    At iteration 0 -> loss: 0.08222377672791481\n",
      "    At iteration 100 -> loss: 0.09644870685479355\n",
      "    At iteration 200 -> loss: 0.0945780885723074\n",
      "    At iteration 300 -> loss: 0.09374305557814974\n",
      "    At iteration 400 -> loss: 0.09302877313444692\n",
      "    At iteration 500 -> loss: 0.09208986882448711\n",
      "    At iteration 600 -> loss: 0.09213687675813025\n",
      "    At iteration 700 -> loss: 0.09149154352484216\n",
      "    At iteration 800 -> loss: 0.09211173492820972\n",
      "    At iteration 900 -> loss: 0.09169394711579394\n",
      "    At iteration 1000 -> loss: 0.0922113702946801\n",
      "    At iteration 1100 -> loss: 0.09204172115642976\n",
      "    At iteration 1200 -> loss: 0.0920638781616238\n",
      "    At iteration 1300 -> loss: 0.09225358049926342\n",
      "    At iteration 1400 -> loss: 0.09240853514712777\n",
      "    At iteration 1500 -> loss: 0.09233578754029721\n",
      "    At iteration 1600 -> loss: 0.09221738681790065\n",
      "    At iteration 1700 -> loss: 0.09236341184796944\n",
      "    At iteration 1800 -> loss: 0.09245900511047729\n",
      "    At iteration 1900 -> loss: 0.0928546094448629\n",
      "    At iteration 2000 -> loss: 0.09270729627213474\n",
      "    At iteration 2100 -> loss: 0.09265047538231526\n",
      "    At iteration 2200 -> loss: 0.09281803190495105\n",
      "    At iteration 2300 -> loss: 0.09278765852037511\n",
      "    At iteration 2400 -> loss: 0.09291620840271131\n",
      "    At iteration 2500 -> loss: 0.09282344522970898\n",
      "    At iteration 2600 -> loss: 0.0932696192797781\n",
      "    At iteration 2700 -> loss: 0.09322611384222067\n",
      "    At iteration 2800 -> loss: 0.0932634914054676\n",
      "    At iteration 2900 -> loss: 0.09315712841635838\n",
      "    At iteration 3000 -> loss: 0.09313731047027876\n",
      "    At iteration 3100 -> loss: 0.09413191846660761\n",
      "    At iteration 3200 -> loss: 0.09421730621916191\n",
      "    At iteration 3300 -> loss: 0.09418699666561905\n",
      "    At iteration 3400 -> loss: 0.09407051448414896\n",
      "    At iteration 3500 -> loss: 0.09430590041018672\n",
      "    At iteration 3600 -> loss: 0.09446377668664174\n",
      "    At iteration 3700 -> loss: 0.09441851224190025\n",
      "    At iteration 3800 -> loss: 0.09436808192107082\n",
      "    At iteration 3900 -> loss: 0.09425710973410498\n",
      "    At iteration 4000 -> loss: 0.09442211312452638\n",
      "    At iteration 4100 -> loss: 0.09435587780285494\n",
      "    At iteration 4200 -> loss: 0.09427843262171996\n",
      "    At iteration 4300 -> loss: 0.09415744260597277\n",
      "    At iteration 4400 -> loss: 0.09420996855648381\n",
      "    At iteration 4500 -> loss: 0.09412046322272025\n",
      "    At iteration 4600 -> loss: 0.09408289731247248\n",
      "    At iteration 4700 -> loss: 0.09405970593926918\n",
      "    At iteration 4800 -> loss: 0.09401604505258218\n",
      "    At iteration 4900 -> loss: 0.09393861901911922\n",
      "    At iteration 5000 -> loss: 0.0939942742945134\n",
      "    At iteration 5100 -> loss: 0.09405273950482268\n",
      "    At iteration 5200 -> loss: 0.09394329509582279\n",
      "    At iteration 5300 -> loss: 0.09388310551208488\n",
      "    At iteration 5400 -> loss: 0.09385892427660791\n",
      "    At iteration 5500 -> loss: 0.09384879428598344\n",
      "    At iteration 5600 -> loss: 0.09386055334772622\n",
      "    At iteration 5700 -> loss: 0.09379600345902743\n",
      "    At iteration 5800 -> loss: 0.09373797161601222\n",
      "    At iteration 5900 -> loss: 0.09374125487669305\n",
      "    At iteration 6000 -> loss: 0.09368913051868469\n",
      "    At iteration 6100 -> loss: 0.09366170988213078\n",
      "    At iteration 6200 -> loss: 0.0937112063071778\n",
      "    At iteration 6300 -> loss: 0.09370547230707936\n",
      "    At iteration 6400 -> loss: 0.09367958432445263\n",
      "    At iteration 6500 -> loss: 0.09363437622656402\n",
      "    At iteration 6600 -> loss: 0.09361631188195584\n",
      "    At iteration 6700 -> loss: 0.09360102307151484\n",
      "    At iteration 6800 -> loss: 0.09362259789920536\n",
      "    At iteration 6900 -> loss: 0.09359206534592159\n",
      "    At iteration 7000 -> loss: 0.09354498569944783\n",
      "    At iteration 7100 -> loss: 0.09349316839521973\n",
      "    At iteration 7200 -> loss: 0.09362039102175393\n",
      "    At iteration 7300 -> loss: 0.09363601815750265\n",
      "    At iteration 7400 -> loss: 0.09361612021560542\n",
      "    At iteration 7500 -> loss: 0.09353504609685037\n",
      "    At iteration 7600 -> loss: 0.09358823816753575\n",
      "    At iteration 7700 -> loss: 0.09354705195411829\n",
      "    At iteration 7800 -> loss: 0.09351502883114776\n",
      "    At iteration 7900 -> loss: 0.09347128393699805\n",
      "    At iteration 8000 -> loss: 0.09350156506748653\n",
      "    At iteration 8100 -> loss: 0.09350390662386475\n",
      "    At iteration 8200 -> loss: 0.09351489465548196\n",
      "    At iteration 8300 -> loss: 0.0934827458645344\n",
      "    At iteration 8400 -> loss: 0.09346533686075219\n",
      "    At iteration 8500 -> loss: 0.0934900062236553\n",
      "    At iteration 8600 -> loss: 0.09341126805858153\n",
      "    At iteration 8700 -> loss: 0.09342709054096737\n",
      "    At iteration 8800 -> loss: 0.09346564222349683\n",
      "    At iteration 8900 -> loss: 0.09348477064128977\n",
      "    At iteration 9000 -> loss: 0.09348943040080535\n",
      "    At iteration 9100 -> loss: 0.09353141506499955\n",
      "    At iteration 9200 -> loss: 0.0935195525633513\n",
      "    At iteration 9300 -> loss: 0.09362824048364927\n",
      "    At iteration 9400 -> loss: 0.09357393510998459\n",
      "    At iteration 9500 -> loss: 0.09358011996439217\n",
      "    At iteration 9600 -> loss: 0.09352681382157803\n",
      "    At iteration 9700 -> loss: 0.09355862109687998\n",
      "    At iteration 9800 -> loss: 0.09356197693508596\n",
      "    At iteration 9900 -> loss: 0.09354269162552238\n",
      "    At iteration 10000 -> loss: 0.09350840401122885\n",
      "    At iteration 10100 -> loss: 0.09350618989465863\n",
      "    At iteration 10200 -> loss: 0.09348385399236785\n",
      "    At iteration 10300 -> loss: 0.09349715447295566\n",
      "    At iteration 10400 -> loss: 0.09348954030223292\n",
      "    At iteration 10500 -> loss: 0.09348690219783903\n",
      "    At iteration 10600 -> loss: 0.09347439781487442\n",
      "    At iteration 10700 -> loss: 0.09344454197161566\n",
      "    At iteration 10800 -> loss: 0.09339868885428497\n",
      "    At iteration 10900 -> loss: 0.0933623254160332\n",
      "    At iteration 11000 -> loss: 0.09334754019278553\n",
      "    At iteration 11100 -> loss: 0.09335441812975548\n",
      "    At iteration 11200 -> loss: 0.09332017841653463\n",
      "    At iteration 11300 -> loss: 0.09331169732396291\n",
      "    At iteration 11400 -> loss: 0.09329157479725271\n",
      "    At iteration 11500 -> loss: 0.09326514389143792\n",
      "    At iteration 11600 -> loss: 0.09324933856164005\n",
      "    At iteration 11700 -> loss: 0.09322524659504114\n",
      "    At iteration 11800 -> loss: 0.09321669044234278\n",
      "    At iteration 11900 -> loss: 0.09325631868743547\n",
      "    At iteration 12000 -> loss: 0.09328063138262822\n",
      "    At iteration 12100 -> loss: 0.09327717370349982\n",
      "    At iteration 12200 -> loss: 0.09327198534700253\n",
      "    At iteration 12300 -> loss: 0.09325191059729487\n",
      "    At iteration 12400 -> loss: 0.09322216236324707\n",
      "    At iteration 12500 -> loss: 0.09320170055770205\n",
      "    At iteration 12600 -> loss: 0.09317954183476229\n",
      "    At iteration 12700 -> loss: 0.09316053390807894\n",
      "    At iteration 12800 -> loss: 0.09316130836028949\n",
      "    At iteration 12900 -> loss: 0.09313762084023613\n",
      "    At iteration 13000 -> loss: 0.09309976667579618\n",
      "    At iteration 13100 -> loss: 0.09311884811618405\n",
      "    At iteration 13200 -> loss: 0.09310777749754393\n",
      "    At iteration 13300 -> loss: 0.0930911184632373\n",
      "    At iteration 13400 -> loss: 0.09306868883638694\n",
      "    At iteration 13500 -> loss: 0.09303626255839545\n",
      "    At iteration 13600 -> loss: 0.09302974824009164\n",
      "Staring Epoch 68\n",
      "    At iteration 0 -> loss: 0.08013860319624655\n",
      "    At iteration 100 -> loss: 0.09104421367826625\n",
      "    At iteration 200 -> loss: 0.0904705307083033\n",
      "    At iteration 300 -> loss: 0.09106354250887955\n",
      "    At iteration 400 -> loss: 0.09098353101473887\n",
      "    At iteration 500 -> loss: 0.09323136429048645\n",
      "    At iteration 600 -> loss: 0.09257706399411884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 700 -> loss: 0.09359070751577685\n",
      "    At iteration 800 -> loss: 0.09319151137576875\n",
      "    At iteration 900 -> loss: 0.09284994546461996\n",
      "    At iteration 1000 -> loss: 0.09279454943305189\n",
      "    At iteration 1100 -> loss: 0.09248114641579991\n",
      "    At iteration 1200 -> loss: 0.09247353028210674\n",
      "    At iteration 1300 -> loss: 0.09249595898765092\n",
      "    At iteration 1400 -> loss: 0.0936176085496883\n",
      "    At iteration 1500 -> loss: 0.09350155387137518\n",
      "    At iteration 1600 -> loss: 0.09340547470560409\n",
      "    At iteration 1700 -> loss: 0.09398858523635818\n",
      "    At iteration 1800 -> loss: 0.09371380502426505\n",
      "    At iteration 1900 -> loss: 0.09371825779366597\n",
      "    At iteration 2000 -> loss: 0.09346764725641266\n",
      "    At iteration 2100 -> loss: 0.09337320372565322\n",
      "    At iteration 2200 -> loss: 0.09328015297921566\n",
      "    At iteration 2300 -> loss: 0.09310914290218066\n",
      "    At iteration 2400 -> loss: 0.09318541984167096\n",
      "    At iteration 2500 -> loss: 0.09359904029918165\n",
      "    At iteration 2600 -> loss: 0.0934959679213427\n",
      "    At iteration 2700 -> loss: 0.0934903456767625\n",
      "    At iteration 2800 -> loss: 0.09344584941551283\n",
      "    At iteration 2900 -> loss: 0.09336280299665015\n",
      "    At iteration 3000 -> loss: 0.09324151843860125\n",
      "    At iteration 3100 -> loss: 0.0931635470276871\n",
      "    At iteration 3200 -> loss: 0.09311426555090725\n",
      "    At iteration 3300 -> loss: 0.093048378701847\n",
      "    At iteration 3400 -> loss: 0.09299238484573329\n",
      "    At iteration 3500 -> loss: 0.09290452705589607\n",
      "    At iteration 3600 -> loss: 0.09282133723397998\n",
      "    At iteration 3700 -> loss: 0.0927135225784899\n",
      "    At iteration 3800 -> loss: 0.09267880342842043\n",
      "    At iteration 3900 -> loss: 0.09269014435617998\n",
      "    At iteration 4000 -> loss: 0.0928857445781121\n",
      "    At iteration 4100 -> loss: 0.09292399377978239\n",
      "    At iteration 4200 -> loss: 0.09294380700698338\n",
      "    At iteration 4300 -> loss: 0.09286026174824084\n",
      "    At iteration 4400 -> loss: 0.0928041214995743\n",
      "    At iteration 4500 -> loss: 0.0927690812366612\n",
      "    At iteration 4600 -> loss: 0.09270829831086418\n",
      "    At iteration 4700 -> loss: 0.09292564264672826\n",
      "    At iteration 4800 -> loss: 0.09291599594662737\n",
      "    At iteration 4900 -> loss: 0.09297839554443482\n",
      "    At iteration 5000 -> loss: 0.0929399927340661\n",
      "    At iteration 5100 -> loss: 0.09285559630808907\n",
      "    At iteration 5200 -> loss: 0.09286203056393734\n",
      "    At iteration 5300 -> loss: 0.09282987017893583\n",
      "    At iteration 5400 -> loss: 0.09281215995871663\n",
      "    At iteration 5500 -> loss: 0.0927845092025063\n",
      "    At iteration 5600 -> loss: 0.09303071960549802\n",
      "    At iteration 5700 -> loss: 0.09297858346577553\n",
      "    At iteration 5800 -> loss: 0.09297458366212767\n",
      "    At iteration 5900 -> loss: 0.09296961482294672\n",
      "    At iteration 6000 -> loss: 0.09295772456723764\n",
      "    At iteration 6100 -> loss: 0.0929454371646017\n",
      "    At iteration 6200 -> loss: 0.09292586493474098\n",
      "    At iteration 6300 -> loss: 0.0929440514165769\n",
      "    At iteration 6400 -> loss: 0.0930140055094949\n",
      "    At iteration 6500 -> loss: 0.09302229777966733\n",
      "    At iteration 6600 -> loss: 0.0929847236946193\n",
      "    At iteration 6700 -> loss: 0.09293939877781245\n",
      "    At iteration 6800 -> loss: 0.09294135983230223\n",
      "    At iteration 6900 -> loss: 0.09300301997237166\n",
      "    At iteration 7000 -> loss: 0.09298189578302056\n",
      "    At iteration 7100 -> loss: 0.09296867952711327\n",
      "    At iteration 7200 -> loss: 0.09301805676878194\n",
      "    At iteration 7300 -> loss: 0.09299767665006017\n",
      "    At iteration 7400 -> loss: 0.09305559360224663\n",
      "    At iteration 7500 -> loss: 0.09301003695134455\n",
      "    At iteration 7600 -> loss: 0.09301258892947109\n",
      "    At iteration 7700 -> loss: 0.0932168512221291\n",
      "    At iteration 7800 -> loss: 0.09316732159018366\n",
      "    At iteration 7900 -> loss: 0.093152721497447\n",
      "    At iteration 8000 -> loss: 0.09314230292954508\n",
      "    At iteration 8100 -> loss: 0.09310734120695421\n",
      "    At iteration 8200 -> loss: 0.09307427844538461\n",
      "    At iteration 8300 -> loss: 0.09307698374317926\n",
      "    At iteration 8400 -> loss: 0.09306231558537784\n",
      "    At iteration 8500 -> loss: 0.09303098555675907\n",
      "    At iteration 8600 -> loss: 0.09299395172895417\n",
      "    At iteration 8700 -> loss: 0.09296479955576047\n",
      "    At iteration 8800 -> loss: 0.09292281887565516\n",
      "    At iteration 8900 -> loss: 0.09292312113075224\n",
      "    At iteration 9000 -> loss: 0.09291352352429182\n",
      "    At iteration 9100 -> loss: 0.09288980953449183\n",
      "    At iteration 9200 -> loss: 0.09286426710791174\n",
      "    At iteration 9300 -> loss: 0.09286841516919653\n",
      "    At iteration 9400 -> loss: 0.09292698623982017\n",
      "    At iteration 9500 -> loss: 0.0928738877555886\n",
      "    At iteration 9600 -> loss: 0.09287881903922053\n",
      "    At iteration 9700 -> loss: 0.09291058552661403\n",
      "    At iteration 9800 -> loss: 0.09290403737930522\n",
      "    At iteration 9900 -> loss: 0.09293204856434374\n",
      "    At iteration 10000 -> loss: 0.09290484340176447\n",
      "    At iteration 10100 -> loss: 0.0928898297151997\n",
      "    At iteration 10200 -> loss: 0.09287445093857778\n",
      "    At iteration 10300 -> loss: 0.09284926888507182\n",
      "    At iteration 10400 -> loss: 0.0928699336995767\n",
      "    At iteration 10500 -> loss: 0.09287352015269722\n",
      "    At iteration 10600 -> loss: 0.09289010398800164\n",
      "    At iteration 10700 -> loss: 0.0931481059656584\n",
      "    At iteration 10800 -> loss: 0.09318150280269648\n",
      "    At iteration 10900 -> loss: 0.09316083250870769\n",
      "    At iteration 11000 -> loss: 0.09314231262118258\n",
      "    At iteration 11100 -> loss: 0.09312379325503042\n",
      "    At iteration 11200 -> loss: 0.0931128703837181\n",
      "    At iteration 11300 -> loss: 0.09313320077891546\n",
      "    At iteration 11400 -> loss: 0.0931165705922858\n",
      "    At iteration 11500 -> loss: 0.09308160364364435\n",
      "    At iteration 11600 -> loss: 0.09306372701595914\n",
      "    At iteration 11700 -> loss: 0.0930733315802851\n",
      "    At iteration 11800 -> loss: 0.09308951964256343\n",
      "    At iteration 11900 -> loss: 0.0931051300669\n",
      "    At iteration 12000 -> loss: 0.09308395354513918\n",
      "    At iteration 12100 -> loss: 0.09308505434189239\n",
      "    At iteration 12200 -> loss: 0.09310191298369169\n",
      "    At iteration 12300 -> loss: 0.0931420116399827\n",
      "    At iteration 12400 -> loss: 0.09310798586128431\n",
      "    At iteration 12500 -> loss: 0.09313355206387977\n",
      "    At iteration 12600 -> loss: 0.09310187168626034\n",
      "    At iteration 12700 -> loss: 0.09314073903707058\n",
      "    At iteration 12800 -> loss: 0.09312515625759286\n",
      "    At iteration 12900 -> loss: 0.09314406424018867\n",
      "    At iteration 13000 -> loss: 0.09311113382572854\n",
      "    At iteration 13100 -> loss: 0.09307975310413799\n",
      "    At iteration 13200 -> loss: 0.09306335230533025\n",
      "    At iteration 13300 -> loss: 0.09305259694707275\n",
      "    At iteration 13400 -> loss: 0.09304390213419715\n",
      "    At iteration 13500 -> loss: 0.09302479445814152\n",
      "    At iteration 13600 -> loss: 0.09299889560559034\n",
      "Staring Epoch 69\n",
      "    At iteration 0 -> loss: 0.08369007357396185\n",
      "    At iteration 100 -> loss: 0.0995234991812856\n",
      "    At iteration 200 -> loss: 0.09657402074321196\n",
      "    At iteration 300 -> loss: 0.09415322265267786\n",
      "    At iteration 400 -> loss: 0.0940858672506359\n",
      "    At iteration 500 -> loss: 0.0931821980847684\n",
      "    At iteration 600 -> loss: 0.09389352423128713\n",
      "    At iteration 700 -> loss: 0.09355999902209562\n",
      "    At iteration 800 -> loss: 0.09312936955852244\n",
      "    At iteration 900 -> loss: 0.0929065570473304\n",
      "    At iteration 1000 -> loss: 0.0925963274000076\n",
      "    At iteration 1100 -> loss: 0.09248097485240711\n",
      "    At iteration 1200 -> loss: 0.09229307192496379\n",
      "    At iteration 1300 -> loss: 0.09219819353533946\n",
      "    At iteration 1400 -> loss: 0.09264536392025323\n",
      "    At iteration 1500 -> loss: 0.09266786191710766\n",
      "    At iteration 1600 -> loss: 0.09247695652596367\n",
      "    At iteration 1700 -> loss: 0.09247590151147936\n",
      "    At iteration 1800 -> loss: 0.092375990823095\n",
      "    At iteration 1900 -> loss: 0.09226111552861237\n",
      "    At iteration 2000 -> loss: 0.09209121126747304\n",
      "    At iteration 2100 -> loss: 0.09199579875983747\n",
      "    At iteration 2200 -> loss: 0.09182866281840257\n",
      "    At iteration 2300 -> loss: 0.09182801317873876\n",
      "    At iteration 2400 -> loss: 0.09206439638506032\n",
      "    At iteration 2500 -> loss: 0.09202564758885598\n",
      "    At iteration 2600 -> loss: 0.09212475812695635\n",
      "    At iteration 2700 -> loss: 0.09203659670748089\n",
      "    At iteration 2800 -> loss: 0.09208256893324794\n",
      "    At iteration 2900 -> loss: 0.09205776877245767\n",
      "    At iteration 3000 -> loss: 0.09200781599461973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3100 -> loss: 0.09214708570351061\n",
      "    At iteration 3200 -> loss: 0.09207560359483004\n",
      "    At iteration 3300 -> loss: 0.09203497496697141\n",
      "    At iteration 3400 -> loss: 0.09193201321221758\n",
      "    At iteration 3500 -> loss: 0.09200263746352191\n",
      "    At iteration 3600 -> loss: 0.09195682949638777\n",
      "    At iteration 3700 -> loss: 0.09195623521818037\n",
      "    At iteration 3800 -> loss: 0.09198645190772964\n",
      "    At iteration 3900 -> loss: 0.0920059396571673\n",
      "    At iteration 4000 -> loss: 0.09190991548628356\n",
      "    At iteration 4100 -> loss: 0.09199544523456163\n",
      "    At iteration 4200 -> loss: 0.09194070912377261\n",
      "    At iteration 4300 -> loss: 0.09192998796428495\n",
      "    At iteration 4400 -> loss: 0.09191311524554606\n",
      "    At iteration 4500 -> loss: 0.09194646345924504\n",
      "    At iteration 4600 -> loss: 0.09190162290709697\n",
      "    At iteration 4700 -> loss: 0.09184503251921164\n",
      "    At iteration 4800 -> loss: 0.09182135205136935\n",
      "    At iteration 4900 -> loss: 0.09181494557666489\n",
      "    At iteration 5000 -> loss: 0.091846044198083\n",
      "    At iteration 5100 -> loss: 0.0919407054895345\n",
      "    At iteration 5200 -> loss: 0.09193159693929985\n",
      "    At iteration 5300 -> loss: 0.09205160941091335\n",
      "    At iteration 5400 -> loss: 0.0921058035219523\n",
      "    At iteration 5500 -> loss: 0.09213556962754126\n",
      "    At iteration 5600 -> loss: 0.09209626655081388\n",
      "    At iteration 5700 -> loss: 0.09207722557326983\n",
      "    At iteration 5800 -> loss: 0.09211649316747267\n",
      "    At iteration 5900 -> loss: 0.09229077603262867\n",
      "    At iteration 6000 -> loss: 0.09225533727668737\n",
      "    At iteration 6100 -> loss: 0.09223566356925397\n",
      "    At iteration 6200 -> loss: 0.092181584691714\n",
      "    At iteration 6300 -> loss: 0.09216358619030665\n",
      "    At iteration 6400 -> loss: 0.09219357287821828\n",
      "    At iteration 6500 -> loss: 0.09226031322690224\n",
      "    At iteration 6600 -> loss: 0.0922281582131088\n",
      "    At iteration 6700 -> loss: 0.09218519084888672\n",
      "    At iteration 6800 -> loss: 0.09221643725319537\n",
      "    At iteration 6900 -> loss: 0.0922413024834269\n",
      "    At iteration 7000 -> loss: 0.09230162578195338\n",
      "    At iteration 7100 -> loss: 0.09228961007984504\n",
      "    At iteration 7200 -> loss: 0.09249436810129988\n",
      "    At iteration 7300 -> loss: 0.09254231913098801\n",
      "    At iteration 7400 -> loss: 0.0925164913742088\n",
      "    At iteration 7500 -> loss: 0.0926005995715732\n",
      "    At iteration 7600 -> loss: 0.0925840036524749\n",
      "    At iteration 7700 -> loss: 0.09257854932265722\n",
      "    At iteration 7800 -> loss: 0.09254713768627687\n",
      "    At iteration 7900 -> loss: 0.09253685391006383\n",
      "    At iteration 8000 -> loss: 0.09259872994642251\n",
      "    At iteration 8100 -> loss: 0.09260290845080799\n",
      "    At iteration 8200 -> loss: 0.09264924497001095\n",
      "    At iteration 8300 -> loss: 0.09267228363207634\n",
      "    At iteration 8400 -> loss: 0.09267589272860519\n",
      "    At iteration 8500 -> loss: 0.09300245632546565\n",
      "    At iteration 8600 -> loss: 0.09307842914297845\n",
      "    At iteration 8700 -> loss: 0.09304895788013444\n",
      "    At iteration 8800 -> loss: 0.09305192306362703\n",
      "    At iteration 8900 -> loss: 0.09301539619705676\n",
      "    At iteration 9000 -> loss: 0.09304105832959432\n",
      "    At iteration 9100 -> loss: 0.09304344488990725\n",
      "    At iteration 9200 -> loss: 0.09316982853684916\n",
      "    At iteration 9300 -> loss: 0.09315031692542418\n",
      "    At iteration 9400 -> loss: 0.09310484795630916\n",
      "    At iteration 9500 -> loss: 0.09308941780760312\n",
      "    At iteration 9600 -> loss: 0.09307600794212587\n",
      "    At iteration 9700 -> loss: 0.09306939903388083\n",
      "    At iteration 9800 -> loss: 0.09305266607780628\n",
      "    At iteration 9900 -> loss: 0.0930271135539272\n",
      "    At iteration 10000 -> loss: 0.09300096340254393\n",
      "    At iteration 10100 -> loss: 0.09298940141868287\n",
      "    At iteration 10200 -> loss: 0.09301828304792467\n",
      "    At iteration 10300 -> loss: 0.09297069629043118\n",
      "    At iteration 10400 -> loss: 0.09301016347779155\n",
      "    At iteration 10500 -> loss: 0.0930140915351455\n",
      "    At iteration 10600 -> loss: 0.0930097732718429\n",
      "    At iteration 10700 -> loss: 0.0929963784106092\n",
      "    At iteration 10800 -> loss: 0.0929861157376152\n",
      "    At iteration 10900 -> loss: 0.09298524190364588\n",
      "    At iteration 11000 -> loss: 0.09303607201252158\n",
      "    At iteration 11100 -> loss: 0.09299385066505353\n",
      "    At iteration 11200 -> loss: 0.09311710255966313\n",
      "    At iteration 11300 -> loss: 0.0930789554006846\n",
      "    At iteration 11400 -> loss: 0.09306841828294192\n",
      "    At iteration 11500 -> loss: 0.09305176263989362\n",
      "    At iteration 11600 -> loss: 0.09305357696236043\n",
      "    At iteration 11700 -> loss: 0.0930436273170074\n",
      "    At iteration 11800 -> loss: 0.09311793005384827\n",
      "    At iteration 11900 -> loss: 0.09309770207538974\n",
      "    At iteration 12000 -> loss: 0.09306955793838936\n",
      "    At iteration 12100 -> loss: 0.09305202851594709\n",
      "    At iteration 12200 -> loss: 0.0930825047703086\n",
      "    At iteration 12300 -> loss: 0.093085825711862\n",
      "    At iteration 12400 -> loss: 0.09310070911469193\n",
      "    At iteration 12500 -> loss: 0.09308907108448047\n",
      "    At iteration 12600 -> loss: 0.09307727229739463\n",
      "    At iteration 12700 -> loss: 0.09304252129108626\n",
      "    At iteration 12800 -> loss: 0.09306657499219129\n",
      "    At iteration 12900 -> loss: 0.0930338352393876\n",
      "    At iteration 13000 -> loss: 0.09301888610355034\n",
      "    At iteration 13100 -> loss: 0.09303994130595972\n",
      "    At iteration 13200 -> loss: 0.09301960453708928\n",
      "    At iteration 13300 -> loss: 0.09302184967266272\n",
      "    At iteration 13400 -> loss: 0.0930459824179361\n",
      "    At iteration 13500 -> loss: 0.09302422172567291\n",
      "    At iteration 13600 -> loss: 0.093038887187763\n",
      "Staring Epoch 70\n",
      "    At iteration 0 -> loss: 0.08048953238176182\n",
      "    At iteration 100 -> loss: 0.12298901823395465\n",
      "    At iteration 200 -> loss: 0.10740261640960488\n",
      "    At iteration 300 -> loss: 0.10159829172084753\n",
      "    At iteration 400 -> loss: 0.0992626204987717\n",
      "    At iteration 500 -> loss: 0.09834694546368186\n",
      "    At iteration 600 -> loss: 0.09685238485728302\n",
      "    At iteration 700 -> loss: 0.09613222705569956\n",
      "    At iteration 800 -> loss: 0.09525817792543437\n",
      "    At iteration 900 -> loss: 0.0946979198999733\n",
      "    At iteration 1000 -> loss: 0.09438078368618603\n",
      "    At iteration 1100 -> loss: 0.09443317033435983\n",
      "    At iteration 1200 -> loss: 0.09439141368596499\n",
      "    At iteration 1300 -> loss: 0.09402023373505669\n",
      "    At iteration 1400 -> loss: 0.09373808668631126\n",
      "    At iteration 1500 -> loss: 0.09357755492066258\n",
      "    At iteration 1600 -> loss: 0.09352681834288049\n",
      "    At iteration 1700 -> loss: 0.0933765317003921\n",
      "    At iteration 1800 -> loss: 0.09341164179435081\n",
      "    At iteration 1900 -> loss: 0.09357441981930188\n",
      "    At iteration 2000 -> loss: 0.09365064928470426\n",
      "    At iteration 2100 -> loss: 0.09364019256709978\n",
      "    At iteration 2200 -> loss: 0.09360807552013284\n",
      "    At iteration 2300 -> loss: 0.09351271619895997\n",
      "    At iteration 2400 -> loss: 0.09357675793400927\n",
      "    At iteration 2500 -> loss: 0.09355829701975521\n",
      "    At iteration 2600 -> loss: 0.09391125705031654\n",
      "    At iteration 2700 -> loss: 0.09386234385858712\n",
      "    At iteration 2800 -> loss: 0.09376541478732973\n",
      "    At iteration 2900 -> loss: 0.09364740983744069\n",
      "    At iteration 3000 -> loss: 0.09369383803000174\n",
      "    At iteration 3100 -> loss: 0.0935921193585843\n",
      "    At iteration 3200 -> loss: 0.0937385040472793\n",
      "    At iteration 3300 -> loss: 0.09362169865835243\n",
      "    At iteration 3400 -> loss: 0.09353515077279347\n",
      "    At iteration 3500 -> loss: 0.09348596111796152\n",
      "    At iteration 3600 -> loss: 0.09365051503963306\n",
      "    At iteration 3700 -> loss: 0.09364504503002487\n",
      "    At iteration 3800 -> loss: 0.09354850218159103\n",
      "    At iteration 3900 -> loss: 0.09341278723291298\n",
      "    At iteration 4000 -> loss: 0.09339118120671892\n",
      "    At iteration 4100 -> loss: 0.09327538649570591\n",
      "    At iteration 4200 -> loss: 0.09324599755466427\n",
      "    At iteration 4300 -> loss: 0.09350125508456347\n",
      "    At iteration 4400 -> loss: 0.09342792669230104\n",
      "    At iteration 4500 -> loss: 0.09340713371259288\n",
      "    At iteration 4600 -> loss: 0.09333515997429463\n",
      "    At iteration 4700 -> loss: 0.09332608327900487\n",
      "    At iteration 4800 -> loss: 0.09327408365239545\n",
      "    At iteration 4900 -> loss: 0.09326589313724204\n",
      "    At iteration 5000 -> loss: 0.09333536942989833\n",
      "    At iteration 5100 -> loss: 0.09356671695885838\n",
      "    At iteration 5200 -> loss: 0.09352459753366911\n",
      "    At iteration 5300 -> loss: 0.09348666992053566\n",
      "    At iteration 5400 -> loss: 0.09350426867645957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5500 -> loss: 0.09342655797222213\n",
      "    At iteration 5600 -> loss: 0.09336374972692803\n",
      "    At iteration 5700 -> loss: 0.09333956382427709\n",
      "    At iteration 5800 -> loss: 0.09325775584583507\n",
      "    At iteration 5900 -> loss: 0.09327132617378697\n",
      "    At iteration 6000 -> loss: 0.09319754203785403\n",
      "    At iteration 6100 -> loss: 0.09321568302738666\n",
      "    At iteration 6200 -> loss: 0.093170566854019\n",
      "    At iteration 6300 -> loss: 0.09330473289148057\n",
      "    At iteration 6400 -> loss: 0.0932281309304064\n",
      "    At iteration 6500 -> loss: 0.09323003976397919\n",
      "    At iteration 6600 -> loss: 0.09323184232514144\n",
      "    At iteration 6700 -> loss: 0.09325196373838239\n",
      "    At iteration 6800 -> loss: 0.09320685610729283\n",
      "    At iteration 6900 -> loss: 0.09316764947128259\n",
      "    At iteration 7000 -> loss: 0.09311136119629854\n",
      "    At iteration 7100 -> loss: 0.09309290350470951\n",
      "    At iteration 7200 -> loss: 0.09309879574203567\n",
      "    At iteration 7300 -> loss: 0.09311693249491995\n",
      "    At iteration 7400 -> loss: 0.09324184287757667\n",
      "    At iteration 7500 -> loss: 0.09319855391965602\n",
      "    At iteration 7600 -> loss: 0.0932480389304591\n",
      "    At iteration 7700 -> loss: 0.09324424001898214\n",
      "    At iteration 7800 -> loss: 0.0932111642161383\n",
      "    At iteration 7900 -> loss: 0.0932005067033378\n",
      "    At iteration 8000 -> loss: 0.09329343353863122\n",
      "    At iteration 8100 -> loss: 0.09324766329504176\n",
      "    At iteration 8200 -> loss: 0.09329901692663044\n",
      "    At iteration 8300 -> loss: 0.09326974145826877\n",
      "    At iteration 8400 -> loss: 0.09325995403564782\n",
      "    At iteration 8500 -> loss: 0.09325222989723421\n",
      "    At iteration 8600 -> loss: 0.09322888675274467\n",
      "    At iteration 8700 -> loss: 0.0932414100399119\n",
      "    At iteration 8800 -> loss: 0.09321063043531269\n",
      "    At iteration 8900 -> loss: 0.09320067081762909\n",
      "    At iteration 9000 -> loss: 0.09319037603157887\n",
      "    At iteration 9100 -> loss: 0.09317661017371225\n",
      "    At iteration 9200 -> loss: 0.0931737745478383\n",
      "    At iteration 9300 -> loss: 0.09314839564119172\n",
      "    At iteration 9400 -> loss: 0.09311128343861994\n",
      "    At iteration 9500 -> loss: 0.09311553212053669\n",
      "    At iteration 9600 -> loss: 0.09313292066382722\n",
      "    At iteration 9700 -> loss: 0.09309772821029463\n",
      "    At iteration 9800 -> loss: 0.09307034139405859\n",
      "    At iteration 9900 -> loss: 0.09306809485346391\n",
      "    At iteration 10000 -> loss: 0.09311368371560463\n",
      "    At iteration 10100 -> loss: 0.09307522323480699\n",
      "    At iteration 10200 -> loss: 0.09305902681239916\n",
      "    At iteration 10300 -> loss: 0.09301843596166412\n",
      "    At iteration 10400 -> loss: 0.09304680934845244\n",
      "    At iteration 10500 -> loss: 0.09305729094611108\n",
      "    At iteration 10600 -> loss: 0.09309428333589366\n",
      "    At iteration 10700 -> loss: 0.09307787941294342\n",
      "    At iteration 10800 -> loss: 0.09305087588987294\n",
      "    At iteration 10900 -> loss: 0.0930352781435407\n",
      "    At iteration 11000 -> loss: 0.09306384420103932\n",
      "    At iteration 11100 -> loss: 0.09305476969863817\n",
      "    At iteration 11200 -> loss: 0.09301698886272117\n",
      "    At iteration 11300 -> loss: 0.09302278198124438\n",
      "    At iteration 11400 -> loss: 0.09298507963307469\n",
      "    At iteration 11500 -> loss: 0.09298330825306293\n",
      "    At iteration 11600 -> loss: 0.09296825678618977\n",
      "    At iteration 11700 -> loss: 0.09295178112608067\n",
      "    At iteration 11800 -> loss: 0.09296100478728148\n",
      "    At iteration 11900 -> loss: 0.09294193134809221\n",
      "    At iteration 12000 -> loss: 0.09298478145027672\n",
      "    At iteration 12100 -> loss: 0.09299819447114072\n",
      "    At iteration 12200 -> loss: 0.09308607705159949\n",
      "    At iteration 12300 -> loss: 0.09307861566926212\n",
      "    At iteration 12400 -> loss: 0.0930978244091049\n",
      "    At iteration 12500 -> loss: 0.09306754182991907\n",
      "    At iteration 12600 -> loss: 0.09306952655867122\n",
      "    At iteration 12700 -> loss: 0.09304198226190179\n",
      "    At iteration 12800 -> loss: 0.09303095831888694\n",
      "    At iteration 12900 -> loss: 0.09307860664844601\n",
      "    At iteration 13000 -> loss: 0.09309642912473402\n",
      "    At iteration 13100 -> loss: 0.09306464602543585\n",
      "    At iteration 13200 -> loss: 0.0930837724030187\n",
      "    At iteration 13300 -> loss: 0.0930618386522354\n",
      "    At iteration 13400 -> loss: 0.09302953395926752\n",
      "    At iteration 13500 -> loss: 0.09301383811978202\n",
      "    At iteration 13600 -> loss: 0.09302638749339628\n",
      "Staring Epoch 71\n",
      "    At iteration 0 -> loss: 0.07997571381156376\n",
      "    At iteration 100 -> loss: 0.09006762710992862\n",
      "    At iteration 200 -> loss: 0.09417555677316136\n",
      "    At iteration 300 -> loss: 0.09329937515829757\n",
      "    At iteration 400 -> loss: 0.09290073786699075\n",
      "    At iteration 500 -> loss: 0.09319528464425396\n",
      "    At iteration 600 -> loss: 0.09285041149322361\n",
      "    At iteration 700 -> loss: 0.09456743039928454\n",
      "    At iteration 800 -> loss: 0.09453096325890097\n",
      "    At iteration 900 -> loss: 0.09400409292598405\n",
      "    At iteration 1000 -> loss: 0.09386832400647502\n",
      "    At iteration 1100 -> loss: 0.09420379066050798\n",
      "    At iteration 1200 -> loss: 0.09382550237768025\n",
      "    At iteration 1300 -> loss: 0.0935116832951599\n",
      "    At iteration 1400 -> loss: 0.09356433310242629\n",
      "    At iteration 1500 -> loss: 0.09328976537537768\n",
      "    At iteration 1600 -> loss: 0.0930353755823537\n",
      "    At iteration 1700 -> loss: 0.09327640982446245\n",
      "    At iteration 1800 -> loss: 0.09304016321807333\n",
      "    At iteration 1900 -> loss: 0.09337174854478326\n",
      "    At iteration 2000 -> loss: 0.09313479317468798\n",
      "    At iteration 2100 -> loss: 0.0930738842812693\n",
      "    At iteration 2200 -> loss: 0.0929874158343964\n",
      "    At iteration 2300 -> loss: 0.09334244269982718\n",
      "    At iteration 2400 -> loss: 0.09321014289973384\n",
      "    At iteration 2500 -> loss: 0.09307396399164021\n",
      "    At iteration 2600 -> loss: 0.09301277595140647\n",
      "    At iteration 2700 -> loss: 0.093111855841854\n",
      "    At iteration 2800 -> loss: 0.09310396990683056\n",
      "    At iteration 2900 -> loss: 0.09347450981927281\n",
      "    At iteration 3000 -> loss: 0.09338011171464697\n",
      "    At iteration 3100 -> loss: 0.09335810991555754\n",
      "    At iteration 3200 -> loss: 0.09326608593612594\n",
      "    At iteration 3300 -> loss: 0.09338141983333766\n",
      "    At iteration 3400 -> loss: 0.09333615571732091\n",
      "    At iteration 3500 -> loss: 0.09338522568555127\n",
      "    At iteration 3600 -> loss: 0.09337480075759992\n",
      "    At iteration 3700 -> loss: 0.09339127633659533\n",
      "    At iteration 3800 -> loss: 0.09337407460803943\n",
      "    At iteration 3900 -> loss: 0.09335219453793381\n",
      "    At iteration 4000 -> loss: 0.09331950249366701\n",
      "    At iteration 4100 -> loss: 0.09325097033287504\n",
      "    At iteration 4200 -> loss: 0.09324061339704694\n",
      "    At iteration 4300 -> loss: 0.09320027242520461\n",
      "    At iteration 4400 -> loss: 0.09334717291357887\n",
      "    At iteration 4500 -> loss: 0.09333586748693598\n",
      "    At iteration 4600 -> loss: 0.09323250104128307\n",
      "    At iteration 4700 -> loss: 0.0933512004575901\n",
      "    At iteration 4800 -> loss: 0.09333285878451242\n",
      "    At iteration 4900 -> loss: 0.093392488091294\n",
      "    At iteration 5000 -> loss: 0.09331792154164566\n",
      "    At iteration 5100 -> loss: 0.09329346371943267\n",
      "    At iteration 5200 -> loss: 0.09320899161338825\n",
      "    At iteration 5300 -> loss: 0.09323157988464796\n",
      "    At iteration 5400 -> loss: 0.093155898061574\n",
      "    At iteration 5500 -> loss: 0.09320587188950842\n",
      "    At iteration 5600 -> loss: 0.09312998405709176\n",
      "    At iteration 5700 -> loss: 0.09309671038763803\n",
      "    At iteration 5800 -> loss: 0.09301600251259184\n",
      "    At iteration 5900 -> loss: 0.09297841026847445\n",
      "    At iteration 6000 -> loss: 0.09298779839980692\n",
      "    At iteration 6100 -> loss: 0.09299726400144534\n",
      "    At iteration 6200 -> loss: 0.09295409633155788\n",
      "    At iteration 6300 -> loss: 0.09289756477047514\n",
      "    At iteration 6400 -> loss: 0.09286635928206978\n",
      "    At iteration 6500 -> loss: 0.092825728020166\n",
      "    At iteration 6600 -> loss: 0.09284400246279823\n",
      "    At iteration 6700 -> loss: 0.09282939593060034\n",
      "    At iteration 6800 -> loss: 0.09281444225699977\n",
      "    At iteration 6900 -> loss: 0.09280369771635172\n",
      "    At iteration 7000 -> loss: 0.09278406886664115\n",
      "    At iteration 7100 -> loss: 0.0927385233455275\n",
      "    At iteration 7200 -> loss: 0.09278475469202437\n",
      "    At iteration 7300 -> loss: 0.09277624393055409\n",
      "    At iteration 7400 -> loss: 0.09273098243980558\n",
      "    At iteration 7500 -> loss: 0.09270470239948368\n",
      "    At iteration 7600 -> loss: 0.09265656586487224\n",
      "    At iteration 7700 -> loss: 0.09261441030927216\n",
      "    At iteration 7800 -> loss: 0.09262865769792561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7900 -> loss: 0.09267434081913029\n",
      "    At iteration 8000 -> loss: 0.09267692149370638\n",
      "    At iteration 8100 -> loss: 0.092733145536073\n",
      "    At iteration 8200 -> loss: 0.09271088323601182\n",
      "    At iteration 8300 -> loss: 0.09269727743910817\n",
      "    At iteration 8400 -> loss: 0.09266529574444812\n",
      "    At iteration 8500 -> loss: 0.09264982921808913\n",
      "    At iteration 8600 -> loss: 0.09266086391020101\n",
      "    At iteration 8700 -> loss: 0.09269187366007554\n",
      "    At iteration 8800 -> loss: 0.09281662805194299\n",
      "    At iteration 8900 -> loss: 0.09277602416089074\n",
      "    At iteration 9000 -> loss: 0.09275068976507753\n",
      "    At iteration 9100 -> loss: 0.09278050154199617\n",
      "    At iteration 9200 -> loss: 0.09287883355137026\n",
      "    At iteration 9300 -> loss: 0.09283736599813651\n",
      "    At iteration 9400 -> loss: 0.0928469864223184\n",
      "    At iteration 9500 -> loss: 0.09282821565316919\n",
      "    At iteration 9600 -> loss: 0.0927996580860478\n",
      "    At iteration 9700 -> loss: 0.09278722841881412\n",
      "    At iteration 9800 -> loss: 0.09279867422439864\n",
      "    At iteration 9900 -> loss: 0.09277394056294752\n",
      "    At iteration 10000 -> loss: 0.09274495103767039\n",
      "    At iteration 10100 -> loss: 0.09276319827664341\n",
      "    At iteration 10200 -> loss: 0.09275952523583411\n",
      "    At iteration 10300 -> loss: 0.09276982953791829\n",
      "    At iteration 10400 -> loss: 0.09283341411606406\n",
      "    At iteration 10500 -> loss: 0.09292247014408361\n",
      "    At iteration 10600 -> loss: 0.09290824325944332\n",
      "    At iteration 10700 -> loss: 0.0929044540237137\n",
      "    At iteration 10800 -> loss: 0.092870027227133\n",
      "    At iteration 10900 -> loss: 0.09289216238939196\n",
      "    At iteration 11000 -> loss: 0.09284951063263427\n",
      "    At iteration 11100 -> loss: 0.09286034347394702\n",
      "    At iteration 11200 -> loss: 0.09286630185828978\n",
      "    At iteration 11300 -> loss: 0.09291891096026027\n",
      "    At iteration 11400 -> loss: 0.09290510551152871\n",
      "    At iteration 11500 -> loss: 0.09287511126765871\n",
      "    At iteration 11600 -> loss: 0.09290889640842603\n",
      "    At iteration 11700 -> loss: 0.09289837929468098\n",
      "    At iteration 11800 -> loss: 0.09288544488744375\n",
      "    At iteration 11900 -> loss: 0.09286090674072152\n",
      "    At iteration 12000 -> loss: 0.09284243892153089\n",
      "    At iteration 12100 -> loss: 0.09282912465275507\n",
      "    At iteration 12200 -> loss: 0.09305811442974059\n",
      "    At iteration 12300 -> loss: 0.09302624700760295\n",
      "    At iteration 12400 -> loss: 0.0930065574393723\n",
      "    At iteration 12500 -> loss: 0.09299641856496486\n",
      "    At iteration 12600 -> loss: 0.09296105106663186\n",
      "    At iteration 12700 -> loss: 0.0929534054802695\n",
      "    At iteration 12800 -> loss: 0.09292693406396896\n",
      "    At iteration 12900 -> loss: 0.09297472681928647\n",
      "    At iteration 13000 -> loss: 0.09296434132608393\n",
      "    At iteration 13100 -> loss: 0.09298297252664209\n",
      "    At iteration 13200 -> loss: 0.09296297806205968\n",
      "    At iteration 13300 -> loss: 0.09298567586076216\n",
      "    At iteration 13400 -> loss: 0.09298872150377799\n",
      "    At iteration 13500 -> loss: 0.09299049839030346\n",
      "    At iteration 13600 -> loss: 0.09303708902035132\n",
      "Staring Epoch 72\n",
      "    At iteration 0 -> loss: 0.08184918510960415\n",
      "    At iteration 100 -> loss: 0.08962267801372674\n",
      "    At iteration 200 -> loss: 0.0912790971692255\n",
      "    At iteration 300 -> loss: 0.09164635863176873\n",
      "    At iteration 400 -> loss: 0.09191866509499526\n",
      "    At iteration 500 -> loss: 0.09261409174132615\n",
      "    At iteration 600 -> loss: 0.09310948167122728\n",
      "    At iteration 700 -> loss: 0.09300200588550069\n",
      "    At iteration 800 -> loss: 0.09290567335498962\n",
      "    At iteration 900 -> loss: 0.09300657508269383\n",
      "    At iteration 1000 -> loss: 0.09292529302680425\n",
      "    At iteration 1100 -> loss: 0.09297859841639039\n",
      "    At iteration 1200 -> loss: 0.09276237285337235\n",
      "    At iteration 1300 -> loss: 0.09255760832738752\n",
      "    At iteration 1400 -> loss: 0.09254324930495206\n",
      "    At iteration 1500 -> loss: 0.09239488619380763\n",
      "    At iteration 1600 -> loss: 0.09244689765882194\n",
      "    At iteration 1700 -> loss: 0.0923568880808807\n",
      "    At iteration 1800 -> loss: 0.09235646561124157\n",
      "    At iteration 1900 -> loss: 0.09252857559817992\n",
      "    At iteration 2000 -> loss: 0.09245713740689745\n",
      "    At iteration 2100 -> loss: 0.09266923335784565\n",
      "    At iteration 2200 -> loss: 0.09265994772939941\n",
      "    At iteration 2300 -> loss: 0.0925986578017825\n",
      "    At iteration 2400 -> loss: 0.0926273470980967\n",
      "    At iteration 2500 -> loss: 0.0925387276444868\n",
      "    At iteration 2600 -> loss: 0.09248152455714932\n",
      "    At iteration 2700 -> loss: 0.09388458542311977\n",
      "    At iteration 2800 -> loss: 0.09366185337425356\n",
      "    At iteration 2900 -> loss: 0.09363771050347383\n",
      "    At iteration 3000 -> loss: 0.0936413028875406\n",
      "    At iteration 3100 -> loss: 0.09349108226569483\n",
      "    At iteration 3200 -> loss: 0.09344991182148302\n",
      "    At iteration 3300 -> loss: 0.09344004348125505\n",
      "    At iteration 3400 -> loss: 0.09335205300433352\n",
      "    At iteration 3500 -> loss: 0.09336744900999287\n",
      "    At iteration 3600 -> loss: 0.09335156763969554\n",
      "    At iteration 3700 -> loss: 0.09333828931471985\n",
      "    At iteration 3800 -> loss: 0.09329269931453413\n",
      "    At iteration 3900 -> loss: 0.09341029567530364\n",
      "    At iteration 4000 -> loss: 0.09336445355921855\n",
      "    At iteration 4100 -> loss: 0.09324976678834661\n",
      "    At iteration 4200 -> loss: 0.09316948637382402\n",
      "    At iteration 4300 -> loss: 0.0931336350211864\n",
      "    At iteration 4400 -> loss: 0.09328010228640161\n",
      "    At iteration 4500 -> loss: 0.09317647957817467\n",
      "    At iteration 4600 -> loss: 0.09309655437496621\n",
      "    At iteration 4700 -> loss: 0.09303722011114866\n",
      "    At iteration 4800 -> loss: 0.09302640971073753\n",
      "    At iteration 4900 -> loss: 0.09291200522134926\n",
      "    At iteration 5000 -> loss: 0.09291701417655042\n",
      "    At iteration 5100 -> loss: 0.0928763154706006\n",
      "    At iteration 5200 -> loss: 0.09289085049978746\n",
      "    At iteration 5300 -> loss: 0.09291087611551983\n",
      "    At iteration 5400 -> loss: 0.09284969753543826\n",
      "    At iteration 5500 -> loss: 0.09282282256766064\n",
      "    At iteration 5600 -> loss: 0.09281094883127747\n",
      "    At iteration 5700 -> loss: 0.09278000509768121\n",
      "    At iteration 5800 -> loss: 0.09278226577947114\n",
      "    At iteration 5900 -> loss: 0.09281993028625661\n",
      "    At iteration 6000 -> loss: 0.09279605737168467\n",
      "    At iteration 6100 -> loss: 0.09286663271503513\n",
      "    At iteration 6200 -> loss: 0.09286372770540341\n",
      "    At iteration 6300 -> loss: 0.09281917946318823\n",
      "    At iteration 6400 -> loss: 0.09281528756364632\n",
      "    At iteration 6500 -> loss: 0.09278963540064826\n",
      "    At iteration 6600 -> loss: 0.09280933820185448\n",
      "    At iteration 6700 -> loss: 0.09278232011672741\n",
      "    At iteration 6800 -> loss: 0.09279532184361547\n",
      "    At iteration 6900 -> loss: 0.09276140043736332\n",
      "    At iteration 7000 -> loss: 0.09283165280216071\n",
      "    At iteration 7100 -> loss: 0.09295196380690941\n",
      "    At iteration 7200 -> loss: 0.0929237157107713\n",
      "    At iteration 7300 -> loss: 0.09297498426253589\n",
      "    At iteration 7400 -> loss: 0.09298967502817883\n",
      "    At iteration 7500 -> loss: 0.09293868146433293\n",
      "    At iteration 7600 -> loss: 0.09297040267334682\n",
      "    At iteration 7700 -> loss: 0.09292861516484006\n",
      "    At iteration 7800 -> loss: 0.09293807677967787\n",
      "    At iteration 7900 -> loss: 0.09289268836400322\n",
      "    At iteration 8000 -> loss: 0.09287239761078274\n",
      "    At iteration 8100 -> loss: 0.09300396632748319\n",
      "    At iteration 8200 -> loss: 0.09300076737689349\n",
      "    At iteration 8300 -> loss: 0.09302628812314728\n",
      "    At iteration 8400 -> loss: 0.0929700382750469\n",
      "    At iteration 8500 -> loss: 0.09291774025960012\n",
      "    At iteration 8600 -> loss: 0.09295348859570016\n",
      "    At iteration 8700 -> loss: 0.09293388398015609\n",
      "    At iteration 8800 -> loss: 0.09290855112391094\n",
      "    At iteration 8900 -> loss: 0.09293923536859194\n",
      "    At iteration 9000 -> loss: 0.09296073900815237\n",
      "    At iteration 9100 -> loss: 0.09293976474716491\n",
      "    At iteration 9200 -> loss: 0.09293353357979352\n",
      "    At iteration 9300 -> loss: 0.09293468061460095\n",
      "    At iteration 9400 -> loss: 0.09290244914656401\n",
      "    At iteration 9500 -> loss: 0.0928746098498539\n",
      "    At iteration 9600 -> loss: 0.09284863770102843\n",
      "    At iteration 9700 -> loss: 0.09282571639174034\n",
      "    At iteration 9800 -> loss: 0.09279926128386831\n",
      "    At iteration 9900 -> loss: 0.09275574218522976\n",
      "    At iteration 10000 -> loss: 0.0927149692569683\n",
      "    At iteration 10100 -> loss: 0.09268659490425818\n",
      "    At iteration 10200 -> loss: 0.09266041922883088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10300 -> loss: 0.09267773518840529\n",
      "    At iteration 10400 -> loss: 0.09291014672427265\n",
      "    At iteration 10500 -> loss: 0.09287673136463855\n",
      "    At iteration 10600 -> loss: 0.09287981856784765\n",
      "    At iteration 10700 -> loss: 0.09287156599635528\n",
      "    At iteration 10800 -> loss: 0.09288263093563516\n",
      "    At iteration 10900 -> loss: 0.09289037738670025\n",
      "    At iteration 11000 -> loss: 0.09290343071680966\n",
      "    At iteration 11100 -> loss: 0.0928828062956897\n",
      "    At iteration 11200 -> loss: 0.09291680560877306\n",
      "    At iteration 11300 -> loss: 0.09301064563848539\n",
      "    At iteration 11400 -> loss: 0.09301525605143725\n",
      "    At iteration 11500 -> loss: 0.09298924723901517\n",
      "    At iteration 11600 -> loss: 0.09298160355119464\n",
      "    At iteration 11700 -> loss: 0.09298897164295421\n",
      "    At iteration 11800 -> loss: 0.09297481619101847\n",
      "    At iteration 11900 -> loss: 0.09298285070617977\n",
      "    At iteration 12000 -> loss: 0.09306953228991022\n",
      "    At iteration 12100 -> loss: 0.0931256899749048\n",
      "    At iteration 12200 -> loss: 0.093092376603493\n",
      "    At iteration 12300 -> loss: 0.09313303309478432\n",
      "    At iteration 12400 -> loss: 0.09311986576360726\n",
      "    At iteration 12500 -> loss: 0.09310999920808881\n",
      "    At iteration 12600 -> loss: 0.09310743115427393\n",
      "    At iteration 12700 -> loss: 0.09308052957063917\n",
      "    At iteration 12800 -> loss: 0.09306238896402598\n",
      "    At iteration 12900 -> loss: 0.09306075049393826\n",
      "    At iteration 13000 -> loss: 0.09304231793229807\n",
      "    At iteration 13100 -> loss: 0.09300598149413954\n",
      "    At iteration 13200 -> loss: 0.09306902104757671\n",
      "    At iteration 13300 -> loss: 0.09306487519735908\n",
      "    At iteration 13400 -> loss: 0.09303064488333922\n",
      "    At iteration 13500 -> loss: 0.09305427157078801\n",
      "    At iteration 13600 -> loss: 0.093044493076178\n",
      "Staring Epoch 73\n",
      "    At iteration 0 -> loss: 0.09207011898979545\n",
      "    At iteration 100 -> loss: 0.09073121048367551\n",
      "    At iteration 200 -> loss: 0.09415576429202065\n",
      "    At iteration 300 -> loss: 0.0922932001115623\n",
      "    At iteration 400 -> loss: 0.09373934149265567\n",
      "    At iteration 500 -> loss: 0.09333903712820131\n",
      "    At iteration 600 -> loss: 0.09237672902969474\n",
      "    At iteration 700 -> loss: 0.09248176372614875\n",
      "    At iteration 800 -> loss: 0.0922069953587966\n",
      "    At iteration 900 -> loss: 0.09252604976873759\n",
      "    At iteration 1000 -> loss: 0.09239669994836097\n",
      "    At iteration 1100 -> loss: 0.09225478804292547\n",
      "    At iteration 1200 -> loss: 0.0920206386804948\n",
      "    At iteration 1300 -> loss: 0.0920795161361517\n",
      "    At iteration 1400 -> loss: 0.09189731044804847\n",
      "    At iteration 1500 -> loss: 0.09203386302581366\n",
      "    At iteration 1600 -> loss: 0.09237965157097494\n",
      "    At iteration 1700 -> loss: 0.09227609622260673\n",
      "    At iteration 1800 -> loss: 0.0921602190937391\n",
      "    At iteration 1900 -> loss: 0.09209643903482971\n",
      "    At iteration 2000 -> loss: 0.09198517272253771\n",
      "    At iteration 2100 -> loss: 0.09212390274020538\n",
      "    At iteration 2200 -> loss: 0.09209161251204753\n",
      "    At iteration 2300 -> loss: 0.09219995979220517\n",
      "    At iteration 2400 -> loss: 0.0923365196653496\n",
      "    At iteration 2500 -> loss: 0.09242623459054441\n",
      "    At iteration 2600 -> loss: 0.09231967758952069\n",
      "    At iteration 2700 -> loss: 0.09222775253118519\n",
      "    At iteration 2800 -> loss: 0.09222700870214048\n",
      "    At iteration 2900 -> loss: 0.09227240534924289\n",
      "    At iteration 3000 -> loss: 0.09226755868299448\n",
      "    At iteration 3100 -> loss: 0.09223166494418968\n",
      "    At iteration 3200 -> loss: 0.0921984740118092\n",
      "    At iteration 3300 -> loss: 0.09209604062248648\n",
      "    At iteration 3400 -> loss: 0.09207998984766783\n",
      "    At iteration 3500 -> loss: 0.09208312289125138\n",
      "    At iteration 3600 -> loss: 0.0920342211432434\n",
      "    At iteration 3700 -> loss: 0.09203428319914\n",
      "    At iteration 3800 -> loss: 0.09193584060837864\n",
      "    At iteration 3900 -> loss: 0.09214069459260307\n",
      "    At iteration 4000 -> loss: 0.09224928321868041\n",
      "    At iteration 4100 -> loss: 0.0923406317992783\n",
      "    At iteration 4200 -> loss: 0.0923511541526744\n",
      "    At iteration 4300 -> loss: 0.09237582290567006\n",
      "    At iteration 4400 -> loss: 0.09245260912300488\n",
      "    At iteration 4500 -> loss: 0.0924043432708219\n",
      "    At iteration 4600 -> loss: 0.09236640950506587\n",
      "    At iteration 4700 -> loss: 0.09235112323051747\n",
      "    At iteration 4800 -> loss: 0.09229181463892032\n",
      "    At iteration 4900 -> loss: 0.09248439937160491\n",
      "    At iteration 5000 -> loss: 0.09250821326180533\n",
      "    At iteration 5100 -> loss: 0.09258281433409248\n",
      "    At iteration 5200 -> loss: 0.09260203996752171\n",
      "    At iteration 5300 -> loss: 0.09253191786836326\n",
      "    At iteration 5400 -> loss: 0.09262529114887394\n",
      "    At iteration 5500 -> loss: 0.09256437196388434\n",
      "    At iteration 5600 -> loss: 0.09261019932561669\n",
      "    At iteration 5700 -> loss: 0.09263858750806822\n",
      "    At iteration 5800 -> loss: 0.09263595186190185\n",
      "    At iteration 5900 -> loss: 0.09258950050732768\n",
      "    At iteration 6000 -> loss: 0.09308999351398595\n",
      "    At iteration 6100 -> loss: 0.09306839962616421\n",
      "    At iteration 6200 -> loss: 0.09303819900915412\n",
      "    At iteration 6300 -> loss: 0.09305995756960128\n",
      "    At iteration 6400 -> loss: 0.09308846412008512\n",
      "    At iteration 6500 -> loss: 0.09307618223856122\n",
      "    At iteration 6600 -> loss: 0.0929968583679469\n",
      "    At iteration 6700 -> loss: 0.09297804838841775\n",
      "    At iteration 6800 -> loss: 0.09295517301316432\n",
      "    At iteration 6900 -> loss: 0.09305599251704977\n",
      "    At iteration 7000 -> loss: 0.09307532909931962\n",
      "    At iteration 7100 -> loss: 0.09303805233683865\n",
      "    At iteration 7200 -> loss: 0.09308451512388577\n",
      "    At iteration 7300 -> loss: 0.09303825361613428\n",
      "    At iteration 7400 -> loss: 0.09299859389709232\n",
      "    At iteration 7500 -> loss: 0.09294712284149653\n",
      "    At iteration 7600 -> loss: 0.0930424726776649\n",
      "    At iteration 7700 -> loss: 0.09306784986721857\n",
      "    At iteration 7800 -> loss: 0.09308836335087897\n",
      "    At iteration 7900 -> loss: 0.09306632225701876\n",
      "    At iteration 8000 -> loss: 0.09303774974782733\n",
      "    At iteration 8100 -> loss: 0.09304822178919382\n",
      "    At iteration 8200 -> loss: 0.09304129303091203\n",
      "    At iteration 8300 -> loss: 0.09310910759315011\n",
      "    At iteration 8400 -> loss: 0.09308113654828838\n",
      "    At iteration 8500 -> loss: 0.09306701640065436\n",
      "    At iteration 8600 -> loss: 0.09300389439127471\n",
      "    At iteration 8700 -> loss: 0.09301141693997193\n",
      "    At iteration 8800 -> loss: 0.09297706270283519\n",
      "    At iteration 8900 -> loss: 0.09298890223564321\n",
      "    At iteration 9000 -> loss: 0.0929990564102564\n",
      "    At iteration 9100 -> loss: 0.0929977653759293\n",
      "    At iteration 9200 -> loss: 0.09308343371239612\n",
      "    At iteration 9300 -> loss: 0.0930767859274922\n",
      "    At iteration 9400 -> loss: 0.0930669288688724\n",
      "    At iteration 9500 -> loss: 0.09304879367774865\n",
      "    At iteration 9600 -> loss: 0.09300181030387226\n",
      "    At iteration 9700 -> loss: 0.0929626141591876\n",
      "    At iteration 9800 -> loss: 0.09296567836175357\n",
      "    At iteration 9900 -> loss: 0.0929339219997024\n",
      "    At iteration 10000 -> loss: 0.0929476967539876\n",
      "    At iteration 10100 -> loss: 0.09300685957079016\n",
      "    At iteration 10200 -> loss: 0.09298045706517016\n",
      "    At iteration 10300 -> loss: 0.09297850617409031\n",
      "    At iteration 10400 -> loss: 0.09295666476870759\n",
      "    At iteration 10500 -> loss: 0.092934413098303\n",
      "    At iteration 10600 -> loss: 0.0930080361944575\n",
      "    At iteration 10700 -> loss: 0.0929824752740538\n",
      "    At iteration 10800 -> loss: 0.09300793320935727\n",
      "    At iteration 10900 -> loss: 0.09299104180041037\n",
      "    At iteration 11000 -> loss: 0.0929850375128896\n",
      "    At iteration 11100 -> loss: 0.09299792668142141\n",
      "    At iteration 11200 -> loss: 0.09299231991068707\n",
      "    At iteration 11300 -> loss: 0.09297044675678032\n",
      "    At iteration 11400 -> loss: 0.09296205194449171\n",
      "    At iteration 11500 -> loss: 0.09298745121541575\n",
      "    At iteration 11600 -> loss: 0.09295464027530055\n",
      "    At iteration 11700 -> loss: 0.09300615521929384\n",
      "    At iteration 11800 -> loss: 0.09299941434155252\n",
      "    At iteration 11900 -> loss: 0.09298341233702404\n",
      "    At iteration 12000 -> loss: 0.09299094421593435\n",
      "    At iteration 12100 -> loss: 0.09298462095160663\n",
      "    At iteration 12200 -> loss: 0.09297654267681342\n",
      "    At iteration 12300 -> loss: 0.0929717048410787\n",
      "    At iteration 12400 -> loss: 0.09296558812759972\n",
      "    At iteration 12500 -> loss: 0.09298963410715942\n",
      "    At iteration 12600 -> loss: 0.09303465894166024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12700 -> loss: 0.09303141308025091\n",
      "    At iteration 12800 -> loss: 0.09300118897209278\n",
      "    At iteration 12900 -> loss: 0.09297037342428806\n",
      "    At iteration 13000 -> loss: 0.09294901150010379\n",
      "    At iteration 13100 -> loss: 0.09295512191447533\n",
      "    At iteration 13200 -> loss: 0.09294547975739059\n",
      "    At iteration 13300 -> loss: 0.09296584788463888\n",
      "    At iteration 13400 -> loss: 0.09296381069129286\n",
      "    At iteration 13500 -> loss: 0.09303294565817584\n",
      "    At iteration 13600 -> loss: 0.0930343816050008\n",
      "Staring Epoch 74\n",
      "    At iteration 0 -> loss: 0.08292206370970234\n",
      "    At iteration 100 -> loss: 0.09248731826733429\n",
      "    At iteration 200 -> loss: 0.09313618346607426\n",
      "    At iteration 300 -> loss: 0.0929630961095491\n",
      "    At iteration 400 -> loss: 0.09403082687031436\n",
      "    At iteration 500 -> loss: 0.09367067783586756\n",
      "    At iteration 600 -> loss: 0.09377582340669377\n",
      "    At iteration 700 -> loss: 0.0941831290260629\n",
      "    At iteration 800 -> loss: 0.09375502740638936\n",
      "    At iteration 900 -> loss: 0.09411210896503129\n",
      "    At iteration 1000 -> loss: 0.09402787665191976\n",
      "    At iteration 1100 -> loss: 0.09391729972622985\n",
      "    At iteration 1200 -> loss: 0.09361137908985241\n",
      "    At iteration 1300 -> loss: 0.09339459580485451\n",
      "    At iteration 1400 -> loss: 0.09340474102727048\n",
      "    At iteration 1500 -> loss: 0.09338802556089733\n",
      "    At iteration 1600 -> loss: 0.09323344937774301\n",
      "    At iteration 1700 -> loss: 0.09331678092236027\n",
      "    At iteration 1800 -> loss: 0.09307047556384668\n",
      "    At iteration 1900 -> loss: 0.0928206634351747\n",
      "    At iteration 2000 -> loss: 0.09280121621131057\n",
      "    At iteration 2100 -> loss: 0.09309261775948838\n",
      "    At iteration 2200 -> loss: 0.09301963851422725\n",
      "    At iteration 2300 -> loss: 0.09295536191075351\n",
      "    At iteration 2400 -> loss: 0.09280155875865066\n",
      "    At iteration 2500 -> loss: 0.09282275429206881\n",
      "    At iteration 2600 -> loss: 0.09271425516129146\n",
      "    At iteration 2700 -> loss: 0.09309530475907775\n",
      "    At iteration 2800 -> loss: 0.09301319998582136\n",
      "    At iteration 2900 -> loss: 0.09404482585579683\n",
      "    At iteration 3000 -> loss: 0.09415199586256472\n",
      "    At iteration 3100 -> loss: 0.0942344313345856\n",
      "    At iteration 3200 -> loss: 0.09426501126907907\n",
      "    At iteration 3300 -> loss: 0.09415679961357853\n",
      "    At iteration 3400 -> loss: 0.09402671640317312\n",
      "    At iteration 3500 -> loss: 0.09402244131444426\n",
      "    At iteration 3600 -> loss: 0.09384175213491555\n",
      "    At iteration 3700 -> loss: 0.09397262926975651\n",
      "    At iteration 3800 -> loss: 0.09387607956208234\n",
      "    At iteration 3900 -> loss: 0.09396997201151182\n",
      "    At iteration 4000 -> loss: 0.09388260055953974\n",
      "    At iteration 4100 -> loss: 0.0939244679884323\n",
      "    At iteration 4200 -> loss: 0.0941044209617923\n",
      "    At iteration 4300 -> loss: 0.09400462950353694\n",
      "    At iteration 4400 -> loss: 0.09427937024842489\n",
      "    At iteration 4500 -> loss: 0.09417206897957096\n",
      "    At iteration 4600 -> loss: 0.09410103606972851\n",
      "    At iteration 4700 -> loss: 0.09410604357568755\n",
      "    At iteration 4800 -> loss: 0.09415083232861952\n",
      "    At iteration 4900 -> loss: 0.09405783684815239\n",
      "    At iteration 5000 -> loss: 0.09401470641771656\n",
      "    At iteration 5100 -> loss: 0.09393064018728524\n",
      "    At iteration 5200 -> loss: 0.09387302078124023\n",
      "    At iteration 5300 -> loss: 0.09378543493555512\n",
      "    At iteration 5400 -> loss: 0.09376867102254377\n",
      "    At iteration 5500 -> loss: 0.09372201095906872\n",
      "    At iteration 5600 -> loss: 0.09369435924927626\n",
      "    At iteration 5700 -> loss: 0.09365459244930774\n",
      "    At iteration 5800 -> loss: 0.09358860518805998\n",
      "    At iteration 5900 -> loss: 0.09352192835335384\n",
      "    At iteration 6000 -> loss: 0.09350811142667671\n",
      "    At iteration 6100 -> loss: 0.09344141605698227\n",
      "    At iteration 6200 -> loss: 0.0934480600035193\n",
      "    At iteration 6300 -> loss: 0.09345049470584386\n",
      "    At iteration 6400 -> loss: 0.09343157008005237\n",
      "    At iteration 6500 -> loss: 0.09345881505836898\n",
      "    At iteration 6600 -> loss: 0.09350222612577556\n",
      "    At iteration 6700 -> loss: 0.09346802947248667\n",
      "    At iteration 6800 -> loss: 0.09345589073869447\n",
      "    At iteration 6900 -> loss: 0.09348748677771802\n",
      "    At iteration 7000 -> loss: 0.09354619553000493\n",
      "    At iteration 7100 -> loss: 0.09351916844803652\n",
      "    At iteration 7200 -> loss: 0.09353075757916234\n",
      "    At iteration 7300 -> loss: 0.09347116813951775\n",
      "    At iteration 7400 -> loss: 0.0934443927016628\n",
      "    At iteration 7500 -> loss: 0.09342031034586096\n",
      "    At iteration 7600 -> loss: 0.09339164873838288\n",
      "    At iteration 7700 -> loss: 0.09334814958026955\n",
      "    At iteration 7800 -> loss: 0.09335336055190599\n",
      "    At iteration 7900 -> loss: 0.09332340071489217\n",
      "    At iteration 8000 -> loss: 0.09329440085254238\n",
      "    At iteration 8100 -> loss: 0.09327345036257347\n",
      "    At iteration 8200 -> loss: 0.09326109838723538\n",
      "    At iteration 8300 -> loss: 0.09332985689438061\n",
      "    At iteration 8400 -> loss: 0.09352931173062137\n",
      "    At iteration 8500 -> loss: 0.09350412519953535\n",
      "    At iteration 8600 -> loss: 0.0934670154105461\n",
      "    At iteration 8700 -> loss: 0.09352304430245616\n",
      "    At iteration 8800 -> loss: 0.09349983922721451\n",
      "    At iteration 8900 -> loss: 0.09344306509299276\n",
      "    At iteration 9000 -> loss: 0.09348644254257556\n",
      "    At iteration 9100 -> loss: 0.0934794525779509\n",
      "    At iteration 9200 -> loss: 0.09345918620410723\n",
      "    At iteration 9300 -> loss: 0.09352146659360333\n",
      "    At iteration 9400 -> loss: 0.09356638139531373\n",
      "    At iteration 9500 -> loss: 0.0935566228837457\n",
      "    At iteration 9600 -> loss: 0.09351800510228818\n",
      "    At iteration 9700 -> loss: 0.09350089200780458\n",
      "    At iteration 9800 -> loss: 0.0934747494472815\n",
      "    At iteration 9900 -> loss: 0.09343326210261124\n",
      "    At iteration 10000 -> loss: 0.09344845779203168\n",
      "    At iteration 10100 -> loss: 0.09344368519204099\n",
      "    At iteration 10200 -> loss: 0.09342572281274321\n",
      "    At iteration 10300 -> loss: 0.09338775666638682\n",
      "    At iteration 10400 -> loss: 0.09335085824668812\n",
      "    At iteration 10500 -> loss: 0.09338381929323572\n",
      "    At iteration 10600 -> loss: 0.09337448592885782\n",
      "    At iteration 10700 -> loss: 0.09335351353765517\n",
      "    At iteration 10800 -> loss: 0.093310542659017\n",
      "    At iteration 10900 -> loss: 0.09329325433048843\n",
      "    At iteration 11000 -> loss: 0.09325833736142687\n",
      "    At iteration 11100 -> loss: 0.09322009222586385\n",
      "    At iteration 11200 -> loss: 0.09320848541881512\n",
      "    At iteration 11300 -> loss: 0.09315645501001013\n",
      "    At iteration 11400 -> loss: 0.09315902618872601\n",
      "    At iteration 11500 -> loss: 0.09313001195512247\n",
      "    At iteration 11600 -> loss: 0.09312564067995314\n",
      "    At iteration 11700 -> loss: 0.09309189587208716\n",
      "    At iteration 11800 -> loss: 0.09309415746975219\n",
      "    At iteration 11900 -> loss: 0.09307872302894098\n",
      "    At iteration 12000 -> loss: 0.09305507840007639\n",
      "    At iteration 12100 -> loss: 0.09304608892436579\n",
      "    At iteration 12200 -> loss: 0.09308447834886568\n",
      "    At iteration 12300 -> loss: 0.09305034084100951\n",
      "    At iteration 12400 -> loss: 0.09304544680125669\n",
      "    At iteration 12500 -> loss: 0.09301326936480618\n",
      "    At iteration 12600 -> loss: 0.09301995410293318\n",
      "    At iteration 12700 -> loss: 0.09300146920465524\n",
      "    At iteration 12800 -> loss: 0.09297992115705729\n",
      "    At iteration 12900 -> loss: 0.09297873471566297\n",
      "    At iteration 13000 -> loss: 0.0929714747952118\n",
      "    At iteration 13100 -> loss: 0.09295943101225546\n",
      "    At iteration 13200 -> loss: 0.09294419705154207\n",
      "    At iteration 13300 -> loss: 0.09294218038238083\n",
      "    At iteration 13400 -> loss: 0.0929507439734691\n",
      "    At iteration 13500 -> loss: 0.092927379339268\n",
      "    At iteration 13600 -> loss: 0.09300609639032067\n",
      "Staring Epoch 75\n",
      "    At iteration 0 -> loss: 0.0950186443515122\n",
      "    At iteration 100 -> loss: 0.09710420163649602\n",
      "    At iteration 200 -> loss: 0.09933792711073813\n",
      "    At iteration 300 -> loss: 0.09597325898940605\n",
      "    At iteration 400 -> loss: 0.09440383124431756\n",
      "    At iteration 500 -> loss: 0.09371976851426934\n",
      "    At iteration 600 -> loss: 0.09269838595055963\n",
      "    At iteration 700 -> loss: 0.09351464075739292\n",
      "    At iteration 800 -> loss: 0.09343288655730705\n",
      "    At iteration 900 -> loss: 0.09401701328725075\n",
      "    At iteration 1000 -> loss: 0.09374770773980555\n",
      "    At iteration 1100 -> loss: 0.09348229659922606\n",
      "    At iteration 1200 -> loss: 0.09388557591377458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1300 -> loss: 0.09374421063729096\n",
      "    At iteration 1400 -> loss: 0.09337106616920571\n",
      "    At iteration 1500 -> loss: 0.0932810992637837\n",
      "    At iteration 1600 -> loss: 0.09322287686777611\n",
      "    At iteration 1700 -> loss: 0.09314266852163722\n",
      "    At iteration 1800 -> loss: 0.09305153673782743\n",
      "    At iteration 1900 -> loss: 0.09305391847490793\n",
      "    At iteration 2000 -> loss: 0.09300524217318017\n",
      "    At iteration 2100 -> loss: 0.09331300130589185\n",
      "    At iteration 2200 -> loss: 0.0931502728011031\n",
      "    At iteration 2300 -> loss: 0.09299620680761589\n",
      "    At iteration 2400 -> loss: 0.09288400732766175\n",
      "    At iteration 2500 -> loss: 0.09281961764581438\n",
      "    At iteration 2600 -> loss: 0.09272258686260097\n",
      "    At iteration 2700 -> loss: 0.0927552119483851\n",
      "    At iteration 2800 -> loss: 0.09272963595251851\n",
      "    At iteration 2900 -> loss: 0.09266858423516369\n",
      "    At iteration 3000 -> loss: 0.09261899306488881\n",
      "    At iteration 3100 -> loss: 0.09268968343889339\n",
      "    At iteration 3200 -> loss: 0.09263870692830833\n",
      "    At iteration 3300 -> loss: 0.09262421975306918\n",
      "    At iteration 3400 -> loss: 0.09255203050841719\n",
      "    At iteration 3500 -> loss: 0.09258647040992218\n",
      "    At iteration 3600 -> loss: 0.09254767521524039\n",
      "    At iteration 3700 -> loss: 0.09261917964528929\n",
      "    At iteration 3800 -> loss: 0.09272639511118022\n",
      "    At iteration 3900 -> loss: 0.09275800969188379\n",
      "    At iteration 4000 -> loss: 0.09273643579120543\n",
      "    At iteration 4100 -> loss: 0.0928185120147102\n",
      "    At iteration 4200 -> loss: 0.09282898551308474\n",
      "    At iteration 4300 -> loss: 0.09277519193649987\n",
      "    At iteration 4400 -> loss: 0.092734459814595\n",
      "    At iteration 4500 -> loss: 0.09274681705911542\n",
      "    At iteration 4600 -> loss: 0.09333933787025227\n",
      "    At iteration 4700 -> loss: 0.09325712037624644\n",
      "    At iteration 4800 -> loss: 0.09321809489009818\n",
      "    At iteration 4900 -> loss: 0.09319619577090983\n",
      "    At iteration 5000 -> loss: 0.09313149027738768\n",
      "    At iteration 5100 -> loss: 0.09306191310210017\n",
      "    At iteration 5200 -> loss: 0.09307816872340158\n",
      "    At iteration 5300 -> loss: 0.09309666768884317\n",
      "    At iteration 5400 -> loss: 0.0930785685580139\n",
      "    At iteration 5500 -> loss: 0.09326472710792516\n",
      "    At iteration 5600 -> loss: 0.09320468333299407\n",
      "    At iteration 5700 -> loss: 0.0931910413368388\n",
      "    At iteration 5800 -> loss: 0.09341792791905196\n",
      "    At iteration 5900 -> loss: 0.09335340884214796\n",
      "    At iteration 6000 -> loss: 0.0933059168580965\n",
      "    At iteration 6100 -> loss: 0.09326213959363003\n",
      "    At iteration 6200 -> loss: 0.09324587249394185\n",
      "    At iteration 6300 -> loss: 0.09324270397914682\n",
      "    At iteration 6400 -> loss: 0.09318329308041311\n",
      "    At iteration 6500 -> loss: 0.09313391227131644\n",
      "    At iteration 6600 -> loss: 0.09311432781434643\n",
      "    At iteration 6700 -> loss: 0.09310518863278593\n",
      "    At iteration 6800 -> loss: 0.09306635460889812\n",
      "    At iteration 6900 -> loss: 0.09305668618427751\n",
      "    At iteration 7000 -> loss: 0.09300900838389858\n",
      "    At iteration 7100 -> loss: 0.09299134548087017\n",
      "    At iteration 7200 -> loss: 0.09298018976549104\n",
      "    At iteration 7300 -> loss: 0.0929364361577061\n",
      "    At iteration 7400 -> loss: 0.09290734873098161\n",
      "    At iteration 7500 -> loss: 0.09298986385453617\n",
      "    At iteration 7600 -> loss: 0.09302167320499773\n",
      "    At iteration 7700 -> loss: 0.0930787683133182\n",
      "    At iteration 7800 -> loss: 0.09303096081033026\n",
      "    At iteration 7900 -> loss: 0.09301812356619928\n",
      "    At iteration 8000 -> loss: 0.09315647509999427\n",
      "    At iteration 8100 -> loss: 0.09313782892126868\n",
      "    At iteration 8200 -> loss: 0.09308220334066489\n",
      "    At iteration 8300 -> loss: 0.09308897898676098\n",
      "    At iteration 8400 -> loss: 0.0931622261519726\n",
      "    At iteration 8500 -> loss: 0.09323353636083519\n",
      "    At iteration 8600 -> loss: 0.09324797699661536\n",
      "    At iteration 8700 -> loss: 0.09321751668386205\n",
      "    At iteration 8800 -> loss: 0.0932041522556623\n",
      "    At iteration 8900 -> loss: 0.09320802150468001\n",
      "    At iteration 9000 -> loss: 0.09318681764048817\n",
      "    At iteration 9100 -> loss: 0.0931990532652263\n",
      "    At iteration 9200 -> loss: 0.09316781044179488\n",
      "    At iteration 9300 -> loss: 0.09316217159441813\n",
      "    At iteration 9400 -> loss: 0.09314772198572943\n",
      "    At iteration 9500 -> loss: 0.09312088288594814\n",
      "    At iteration 9600 -> loss: 0.09310812952058578\n",
      "    At iteration 9700 -> loss: 0.09308936723188202\n",
      "    At iteration 9800 -> loss: 0.09304249696172837\n",
      "    At iteration 9900 -> loss: 0.0930565463838894\n",
      "    At iteration 10000 -> loss: 0.09305903850873158\n",
      "    At iteration 10100 -> loss: 0.09303864874473054\n",
      "    At iteration 10200 -> loss: 0.09314010778888235\n",
      "    At iteration 10300 -> loss: 0.09310467449194998\n",
      "    At iteration 10400 -> loss: 0.09310974273946852\n",
      "    At iteration 10500 -> loss: 0.09307554567332804\n",
      "    At iteration 10600 -> loss: 0.09306568791128789\n",
      "    At iteration 10700 -> loss: 0.09304027541381586\n",
      "    At iteration 10800 -> loss: 0.09305551523870303\n",
      "    At iteration 10900 -> loss: 0.09305041007729989\n",
      "    At iteration 11000 -> loss: 0.09302246687716695\n",
      "    At iteration 11100 -> loss: 0.09301209471962178\n",
      "    At iteration 11200 -> loss: 0.09303906722019666\n",
      "    At iteration 11300 -> loss: 0.09308902743046718\n",
      "    At iteration 11400 -> loss: 0.09306244338584993\n",
      "    At iteration 11500 -> loss: 0.0931008387946461\n",
      "    At iteration 11600 -> loss: 0.09307893752989621\n",
      "    At iteration 11700 -> loss: 0.09309986049217378\n",
      "    At iteration 11800 -> loss: 0.09311618230984131\n",
      "    At iteration 11900 -> loss: 0.09309113972283334\n",
      "    At iteration 12000 -> loss: 0.09310025574326711\n",
      "    At iteration 12100 -> loss: 0.09307020445974887\n",
      "    At iteration 12200 -> loss: 0.09309387205789586\n",
      "    At iteration 12300 -> loss: 0.0930638980579532\n",
      "    At iteration 12400 -> loss: 0.0930756902608218\n",
      "    At iteration 12500 -> loss: 0.09307382592767408\n",
      "    At iteration 12600 -> loss: 0.09304203233866196\n",
      "    At iteration 12700 -> loss: 0.09304800713722873\n",
      "    At iteration 12800 -> loss: 0.09302282940621767\n",
      "    At iteration 12900 -> loss: 0.09299703188037403\n",
      "    At iteration 13000 -> loss: 0.09298552949429398\n",
      "    At iteration 13100 -> loss: 0.09300155350352374\n",
      "    At iteration 13200 -> loss: 0.09299272521303759\n",
      "    At iteration 13300 -> loss: 0.09299930409696584\n",
      "    At iteration 13400 -> loss: 0.0929581795154336\n",
      "    At iteration 13500 -> loss: 0.092957427137292\n",
      "    At iteration 13600 -> loss: 0.09297657011308239\n",
      "Staring Epoch 76\n",
      "    At iteration 0 -> loss: 0.08280610700603575\n",
      "    At iteration 100 -> loss: 0.09015978679628046\n",
      "    At iteration 200 -> loss: 0.09271623792081543\n",
      "    At iteration 300 -> loss: 0.0952386232027655\n",
      "    At iteration 400 -> loss: 0.0937787725260798\n",
      "    At iteration 500 -> loss: 0.0928346656582748\n",
      "    At iteration 600 -> loss: 0.09255752512789746\n",
      "    At iteration 700 -> loss: 0.09358328729904725\n",
      "    At iteration 800 -> loss: 0.09388301796646502\n",
      "    At iteration 900 -> loss: 0.09328958310239019\n",
      "    At iteration 1000 -> loss: 0.09281335061542921\n",
      "    At iteration 1100 -> loss: 0.09329053166560686\n",
      "    At iteration 1200 -> loss: 0.09334694291426718\n",
      "    At iteration 1300 -> loss: 0.0936419575083468\n",
      "    At iteration 1400 -> loss: 0.09350143930764422\n",
      "    At iteration 1500 -> loss: 0.09328542150247424\n",
      "    At iteration 1600 -> loss: 0.0934929288922653\n",
      "    At iteration 1700 -> loss: 0.09352590145990765\n",
      "    At iteration 1800 -> loss: 0.09339411200099683\n",
      "    At iteration 1900 -> loss: 0.09336826722751176\n",
      "    At iteration 2000 -> loss: 0.09345965428413526\n",
      "    At iteration 2100 -> loss: 0.09336560125113723\n",
      "    At iteration 2200 -> loss: 0.09322622478142888\n",
      "    At iteration 2300 -> loss: 0.09321756340133582\n",
      "    At iteration 2400 -> loss: 0.09309301092957559\n",
      "    At iteration 2500 -> loss: 0.093053694291396\n",
      "    At iteration 2600 -> loss: 0.09297705286738238\n",
      "    At iteration 2700 -> loss: 0.09284484398348272\n",
      "    At iteration 2800 -> loss: 0.09282959724933779\n",
      "    At iteration 2900 -> loss: 0.09282285628499248\n",
      "    At iteration 3000 -> loss: 0.09277153028982991\n",
      "    At iteration 3100 -> loss: 0.09285493319136896\n",
      "    At iteration 3200 -> loss: 0.09298809994377014\n",
      "    At iteration 3300 -> loss: 0.09297158194129153\n",
      "    At iteration 3400 -> loss: 0.0930736562190119\n",
      "    At iteration 3500 -> loss: 0.09306413734502737\n",
      "    At iteration 3600 -> loss: 0.09303329058657647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3700 -> loss: 0.09296506623538174\n",
      "    At iteration 3800 -> loss: 0.09293781061918036\n",
      "    At iteration 3900 -> loss: 0.0928373082188892\n",
      "    At iteration 4000 -> loss: 0.09285307926880491\n",
      "    At iteration 4100 -> loss: 0.09288836611431442\n",
      "    At iteration 4200 -> loss: 0.09281032178104252\n",
      "    At iteration 4300 -> loss: 0.09279018008245267\n",
      "    At iteration 4400 -> loss: 0.09278934717474001\n",
      "    At iteration 4500 -> loss: 0.09298884545245042\n",
      "    At iteration 4600 -> loss: 0.09290675279096876\n",
      "    At iteration 4700 -> loss: 0.09292557875815964\n",
      "    At iteration 4800 -> loss: 0.0928937650059669\n",
      "    At iteration 4900 -> loss: 0.09283078717008068\n",
      "    At iteration 5000 -> loss: 0.09284194493371847\n",
      "    At iteration 5100 -> loss: 0.09274131500092848\n",
      "    At iteration 5200 -> loss: 0.09278476102975111\n",
      "    At iteration 5300 -> loss: 0.09286743398145074\n",
      "    At iteration 5400 -> loss: 0.09286814306665522\n",
      "    At iteration 5500 -> loss: 0.09295476369991368\n",
      "    At iteration 5600 -> loss: 0.09308242074748167\n",
      "    At iteration 5700 -> loss: 0.09313195809136514\n",
      "    At iteration 5800 -> loss: 0.09314312061282115\n",
      "    At iteration 5900 -> loss: 0.09318421396294665\n",
      "    At iteration 6000 -> loss: 0.09312991903256636\n",
      "    At iteration 6100 -> loss: 0.09315968782694234\n",
      "    At iteration 6200 -> loss: 0.09318613160756889\n",
      "    At iteration 6300 -> loss: 0.09318938111679734\n",
      "    At iteration 6400 -> loss: 0.09312483832234038\n",
      "    At iteration 6500 -> loss: 0.09307709020738905\n",
      "    At iteration 6600 -> loss: 0.09302751086004081\n",
      "    At iteration 6700 -> loss: 0.09300600443337811\n",
      "    At iteration 6800 -> loss: 0.09309060419722567\n",
      "    At iteration 6900 -> loss: 0.09306437439160932\n",
      "    At iteration 7000 -> loss: 0.0930573855315431\n",
      "    At iteration 7100 -> loss: 0.09307957446168373\n",
      "    At iteration 7200 -> loss: 0.09306157375429017\n",
      "    At iteration 7300 -> loss: 0.09303788942038634\n",
      "    At iteration 7400 -> loss: 0.09303460215494137\n",
      "    At iteration 7500 -> loss: 0.09305094243937438\n",
      "    At iteration 7600 -> loss: 0.09303319517950219\n",
      "    At iteration 7700 -> loss: 0.09299852111267605\n",
      "    At iteration 7800 -> loss: 0.09296260095065738\n",
      "    At iteration 7900 -> loss: 0.09303831728507271\n",
      "    At iteration 8000 -> loss: 0.09301174781408664\n",
      "    At iteration 8100 -> loss: 0.09300595741375293\n",
      "    At iteration 8200 -> loss: 0.09298471809700298\n",
      "    At iteration 8300 -> loss: 0.09295900409317993\n",
      "    At iteration 8400 -> loss: 0.0930497635875687\n",
      "    At iteration 8500 -> loss: 0.0930491789940381\n",
      "    At iteration 8600 -> loss: 0.09302720120714597\n",
      "    At iteration 8700 -> loss: 0.09301293035872851\n",
      "    At iteration 8800 -> loss: 0.09303700585756908\n",
      "    At iteration 8900 -> loss: 0.0930315900654287\n",
      "    At iteration 9000 -> loss: 0.093034733507755\n",
      "    At iteration 9100 -> loss: 0.09304444108989368\n",
      "    At iteration 9200 -> loss: 0.09303508828107251\n",
      "    At iteration 9300 -> loss: 0.09298988301492384\n",
      "    At iteration 9400 -> loss: 0.09298076516624784\n",
      "    At iteration 9500 -> loss: 0.09298619543408115\n",
      "    At iteration 9600 -> loss: 0.09328125154341974\n",
      "    At iteration 9700 -> loss: 0.09327069477297834\n",
      "    At iteration 9800 -> loss: 0.09323081929839042\n",
      "    At iteration 9900 -> loss: 0.09322301878627527\n",
      "    At iteration 10000 -> loss: 0.09321067589219442\n",
      "    At iteration 10100 -> loss: 0.09321703228379205\n",
      "    At iteration 10200 -> loss: 0.0932779259439411\n",
      "    At iteration 10300 -> loss: 0.0932746454447689\n",
      "    At iteration 10400 -> loss: 0.09326571529780231\n",
      "    At iteration 10500 -> loss: 0.09324212686840841\n",
      "    At iteration 10600 -> loss: 0.0932034307150466\n",
      "    At iteration 10700 -> loss: 0.09320205829508257\n",
      "    At iteration 10800 -> loss: 0.09318976266262648\n",
      "    At iteration 10900 -> loss: 0.09318253410331076\n",
      "    At iteration 11000 -> loss: 0.09317629375376318\n",
      "    At iteration 11100 -> loss: 0.09313119760911213\n",
      "    At iteration 11200 -> loss: 0.09312535582337758\n",
      "    At iteration 11300 -> loss: 0.09312399069006783\n",
      "    At iteration 11400 -> loss: 0.09312496616534473\n",
      "    At iteration 11500 -> loss: 0.09314279411867309\n",
      "    At iteration 11600 -> loss: 0.09310344543626733\n",
      "    At iteration 11700 -> loss: 0.09309955955509944\n",
      "    At iteration 11800 -> loss: 0.09309864719388679\n",
      "    At iteration 11900 -> loss: 0.0930943933215507\n",
      "    At iteration 12000 -> loss: 0.09312619642878099\n",
      "    At iteration 12100 -> loss: 0.0931257132854744\n",
      "    At iteration 12200 -> loss: 0.09310730897490181\n",
      "    At iteration 12300 -> loss: 0.09306997490066023\n",
      "    At iteration 12400 -> loss: 0.0930833982793012\n",
      "    At iteration 12500 -> loss: 0.09304523563534459\n",
      "    At iteration 12600 -> loss: 0.0930270208291828\n",
      "    At iteration 12700 -> loss: 0.0930268683580904\n",
      "    At iteration 12800 -> loss: 0.09304396241240705\n",
      "    At iteration 12900 -> loss: 0.09301956296344024\n",
      "    At iteration 13000 -> loss: 0.09300291533651124\n",
      "    At iteration 13100 -> loss: 0.09298528607336887\n",
      "    At iteration 13200 -> loss: 0.09298048653534537\n",
      "    At iteration 13300 -> loss: 0.0929720115551663\n",
      "    At iteration 13400 -> loss: 0.09299143513572465\n",
      "    At iteration 13500 -> loss: 0.09300208178371837\n",
      "    At iteration 13600 -> loss: 0.0929925925682853\n",
      "Staring Epoch 77\n",
      "    At iteration 0 -> loss: 0.10113085340708494\n",
      "    At iteration 100 -> loss: 0.09666318394504017\n",
      "    At iteration 200 -> loss: 0.09391233043671887\n",
      "    At iteration 300 -> loss: 0.09376655699926044\n",
      "    At iteration 400 -> loss: 0.09246253004779477\n",
      "    At iteration 500 -> loss: 0.09291957102610078\n",
      "    At iteration 600 -> loss: 0.09288462369335937\n",
      "    At iteration 700 -> loss: 0.09455739029663095\n",
      "    At iteration 800 -> loss: 0.09377022060430991\n",
      "    At iteration 900 -> loss: 0.09351033768643728\n",
      "    At iteration 1000 -> loss: 0.09372189399463376\n",
      "    At iteration 1100 -> loss: 0.09340826431803735\n",
      "    At iteration 1200 -> loss: 0.09340756475546892\n",
      "    At iteration 1300 -> loss: 0.09318171780788527\n",
      "    At iteration 1400 -> loss: 0.09298868944590942\n",
      "    At iteration 1500 -> loss: 0.09313441331719091\n",
      "    At iteration 1600 -> loss: 0.09297431082993421\n",
      "    At iteration 1700 -> loss: 0.09308207431593286\n",
      "    At iteration 1800 -> loss: 0.09294941637657557\n",
      "    At iteration 1900 -> loss: 0.09299965967907244\n",
      "    At iteration 2000 -> loss: 0.09323265711034011\n",
      "    At iteration 2100 -> loss: 0.09330961631755022\n",
      "    At iteration 2200 -> loss: 0.0932722510023443\n",
      "    At iteration 2300 -> loss: 0.09333938065503385\n",
      "    At iteration 2400 -> loss: 0.09324354858843698\n",
      "    At iteration 2500 -> loss: 0.09309436506253692\n",
      "    At iteration 2600 -> loss: 0.09321570579334619\n",
      "    At iteration 2700 -> loss: 0.09320222235357098\n",
      "    At iteration 2800 -> loss: 0.09312154071860403\n",
      "    At iteration 2900 -> loss: 0.09305463805213533\n",
      "    At iteration 3000 -> loss: 0.0930732370399294\n",
      "    At iteration 3100 -> loss: 0.09299002930016735\n",
      "    At iteration 3200 -> loss: 0.09312751685949182\n",
      "    At iteration 3300 -> loss: 0.0934030180101599\n",
      "    At iteration 3400 -> loss: 0.09359282157604208\n",
      "    At iteration 3500 -> loss: 0.09354306835783968\n",
      "    At iteration 3600 -> loss: 0.09357743517507706\n",
      "    At iteration 3700 -> loss: 0.09357911287545694\n",
      "    At iteration 3800 -> loss: 0.09352382931257396\n",
      "    At iteration 3900 -> loss: 0.09351561630546174\n",
      "    At iteration 4000 -> loss: 0.0934672981007762\n",
      "    At iteration 4100 -> loss: 0.0934000112314867\n",
      "    At iteration 4200 -> loss: 0.0933843639892833\n",
      "    At iteration 4300 -> loss: 0.09341873573713026\n",
      "    At iteration 4400 -> loss: 0.09337212179152683\n",
      "    At iteration 4500 -> loss: 0.09330795205458586\n",
      "    At iteration 4600 -> loss: 0.09326752511752563\n",
      "    At iteration 4700 -> loss: 0.0932104916514\n",
      "    At iteration 4800 -> loss: 0.09314231048147431\n",
      "    At iteration 4900 -> loss: 0.09306750825110457\n",
      "    At iteration 5000 -> loss: 0.09317198373034499\n",
      "    At iteration 5100 -> loss: 0.09322886096575156\n",
      "    At iteration 5200 -> loss: 0.09319053964457598\n",
      "    At iteration 5300 -> loss: 0.09314997462849042\n",
      "    At iteration 5400 -> loss: 0.09316686226008371\n",
      "    At iteration 5500 -> loss: 0.09319931854766761\n",
      "    At iteration 5600 -> loss: 0.09329029351019297\n",
      "    At iteration 5700 -> loss: 0.093486827890451\n",
      "    At iteration 5800 -> loss: 0.09343105355776882\n",
      "    At iteration 5900 -> loss: 0.09336002690231392\n",
      "    At iteration 6000 -> loss: 0.09329505494544332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6100 -> loss: 0.09328082230859772\n",
      "    At iteration 6200 -> loss: 0.09326167185713288\n",
      "    At iteration 6300 -> loss: 0.09325573950690977\n",
      "    At iteration 6400 -> loss: 0.09330096572340904\n",
      "    At iteration 6500 -> loss: 0.09322553517836285\n",
      "    At iteration 6600 -> loss: 0.09323426469485595\n",
      "    At iteration 6700 -> loss: 0.09325105345818793\n",
      "    At iteration 6800 -> loss: 0.09321259075572279\n",
      "    At iteration 6900 -> loss: 0.09315679120071362\n",
      "    At iteration 7000 -> loss: 0.0931665690193881\n",
      "    At iteration 7100 -> loss: 0.09314441135379592\n",
      "    At iteration 7200 -> loss: 0.09307910155719903\n",
      "    At iteration 7300 -> loss: 0.0930466639456819\n",
      "    At iteration 7400 -> loss: 0.09301782554006112\n",
      "    At iteration 7500 -> loss: 0.09296992962500325\n",
      "    At iteration 7600 -> loss: 0.09292581258780301\n",
      "    At iteration 7700 -> loss: 0.09293890332910901\n",
      "    At iteration 7800 -> loss: 0.09291386041508948\n",
      "    At iteration 7900 -> loss: 0.09290550546301155\n",
      "    At iteration 8000 -> loss: 0.09294914267397353\n",
      "    At iteration 8100 -> loss: 0.09291461713230612\n",
      "    At iteration 8200 -> loss: 0.09293785727016018\n",
      "    At iteration 8300 -> loss: 0.09288692626265127\n",
      "    At iteration 8400 -> loss: 0.09286199265831961\n",
      "    At iteration 8500 -> loss: 0.09282697280109453\n",
      "    At iteration 8600 -> loss: 0.0928128259274629\n",
      "    At iteration 8700 -> loss: 0.09279508570931815\n",
      "    At iteration 8800 -> loss: 0.09277129311506875\n",
      "    At iteration 8900 -> loss: 0.09275607679488396\n",
      "    At iteration 9000 -> loss: 0.0927416843973181\n",
      "    At iteration 9100 -> loss: 0.09275336654107183\n",
      "    At iteration 9200 -> loss: 0.0927700559179277\n",
      "    At iteration 9300 -> loss: 0.09274113759065135\n",
      "    At iteration 9400 -> loss: 0.09270384101929097\n",
      "    At iteration 9500 -> loss: 0.09273669163039737\n",
      "    At iteration 9600 -> loss: 0.09275912351913315\n",
      "    At iteration 9700 -> loss: 0.09272461231456013\n",
      "    At iteration 9800 -> loss: 0.09271101248742576\n",
      "    At iteration 9900 -> loss: 0.0927123086859325\n",
      "    At iteration 10000 -> loss: 0.09271327341456295\n",
      "    At iteration 10100 -> loss: 0.09270054992151629\n",
      "    At iteration 10200 -> loss: 0.09270354269734869\n",
      "    At iteration 10300 -> loss: 0.09268005955372502\n",
      "    At iteration 10400 -> loss: 0.09265692479353597\n",
      "    At iteration 10500 -> loss: 0.09266603281431897\n",
      "    At iteration 10600 -> loss: 0.09265997096625866\n",
      "    At iteration 10700 -> loss: 0.09267056031293745\n",
      "    At iteration 10800 -> loss: 0.09274804355010544\n",
      "    At iteration 10900 -> loss: 0.09275176774036982\n",
      "    At iteration 11000 -> loss: 0.09278065100110439\n",
      "    At iteration 11100 -> loss: 0.09276568101352768\n",
      "    At iteration 11200 -> loss: 0.09275671796875591\n",
      "    At iteration 11300 -> loss: 0.09274042940931125\n",
      "    At iteration 11400 -> loss: 0.09272648761443371\n",
      "    At iteration 11500 -> loss: 0.09269579640322796\n",
      "    At iteration 11600 -> loss: 0.09268660086328506\n",
      "    At iteration 11700 -> loss: 0.09269332375762324\n",
      "    At iteration 11800 -> loss: 0.09294295573064944\n",
      "    At iteration 11900 -> loss: 0.09296124888059801\n",
      "    At iteration 12000 -> loss: 0.09297429142884601\n",
      "    At iteration 12100 -> loss: 0.09295796096901053\n",
      "    At iteration 12200 -> loss: 0.0929618671216305\n",
      "    At iteration 12300 -> loss: 0.09305470632481647\n",
      "    At iteration 12400 -> loss: 0.09302579644602296\n",
      "    At iteration 12500 -> loss: 0.09300920851313703\n",
      "    At iteration 12600 -> loss: 0.09298102568237575\n",
      "    At iteration 12700 -> loss: 0.09298111644497217\n",
      "    At iteration 12800 -> loss: 0.09298071884657717\n",
      "    At iteration 12900 -> loss: 0.09297234101734583\n",
      "    At iteration 13000 -> loss: 0.09293613680045819\n",
      "    At iteration 13100 -> loss: 0.0929388463695478\n",
      "    At iteration 13200 -> loss: 0.09292754735044569\n",
      "    At iteration 13300 -> loss: 0.09304806693909577\n",
      "    At iteration 13400 -> loss: 0.09301914789735556\n",
      "    At iteration 13500 -> loss: 0.09301805577184799\n",
      "    At iteration 13600 -> loss: 0.09301821040382072\n",
      "Staring Epoch 78\n",
      "    At iteration 0 -> loss: 0.08953922451473773\n",
      "    At iteration 100 -> loss: 0.09328797889287696\n",
      "    At iteration 200 -> loss: 0.09089860478239527\n",
      "    At iteration 300 -> loss: 0.0930068001573339\n",
      "    At iteration 400 -> loss: 0.09282832516102253\n",
      "    At iteration 500 -> loss: 0.09258701986129472\n",
      "    At iteration 600 -> loss: 0.094109013556821\n",
      "    At iteration 700 -> loss: 0.09359933633170209\n",
      "    At iteration 800 -> loss: 0.0933807382234122\n",
      "    At iteration 900 -> loss: 0.09312038966350535\n",
      "    At iteration 1000 -> loss: 0.0928442760966031\n",
      "    At iteration 1100 -> loss: 0.09270671379020481\n",
      "    At iteration 1200 -> loss: 0.09323237192998834\n",
      "    At iteration 1300 -> loss: 0.09309222205839637\n",
      "    At iteration 1400 -> loss: 0.09307505325568911\n",
      "    At iteration 1500 -> loss: 0.09299691964418351\n",
      "    At iteration 1600 -> loss: 0.09298762456109483\n",
      "    At iteration 1700 -> loss: 0.09297536777262694\n",
      "    At iteration 1800 -> loss: 0.09310051895922448\n",
      "    At iteration 1900 -> loss: 0.09323411001305854\n",
      "    At iteration 2000 -> loss: 0.09330203825866458\n",
      "    At iteration 2100 -> loss: 0.09320962107820371\n",
      "    At iteration 2200 -> loss: 0.09318779346175725\n",
      "    At iteration 2300 -> loss: 0.09298018791817923\n",
      "    At iteration 2400 -> loss: 0.09310007513118126\n",
      "    At iteration 2500 -> loss: 0.09291135911982341\n",
      "    At iteration 2600 -> loss: 0.09293093867729052\n",
      "    At iteration 2700 -> loss: 0.09313665356012005\n",
      "    At iteration 2800 -> loss: 0.09313424848212953\n",
      "    At iteration 2900 -> loss: 0.09308726950711804\n",
      "    At iteration 3000 -> loss: 0.09302639525216232\n",
      "    At iteration 3100 -> loss: 0.09305152406030844\n",
      "    At iteration 3200 -> loss: 0.0931621369318956\n",
      "    At iteration 3300 -> loss: 0.09323722622963136\n",
      "    At iteration 3400 -> loss: 0.09406477083177706\n",
      "    At iteration 3500 -> loss: 0.09398583989503134\n",
      "    At iteration 3600 -> loss: 0.09389828155917032\n",
      "    At iteration 3700 -> loss: 0.09384660974047802\n",
      "    At iteration 3800 -> loss: 0.0937376810066889\n",
      "    At iteration 3900 -> loss: 0.09379109240464509\n",
      "    At iteration 4000 -> loss: 0.09370367833152003\n",
      "    At iteration 4100 -> loss: 0.09380946536786108\n",
      "    At iteration 4200 -> loss: 0.09377202079478093\n",
      "    At iteration 4300 -> loss: 0.09372910156559218\n",
      "    At iteration 4400 -> loss: 0.09390095824939335\n",
      "    At iteration 4500 -> loss: 0.09380967827985864\n",
      "    At iteration 4600 -> loss: 0.0938042188487621\n",
      "    At iteration 4700 -> loss: 0.09375426315224937\n",
      "    At iteration 4800 -> loss: 0.09369054145744851\n",
      "    At iteration 4900 -> loss: 0.09362947786506508\n",
      "    At iteration 5000 -> loss: 0.09363393476432716\n",
      "    At iteration 5100 -> loss: 0.09354355859635026\n",
      "    At iteration 5200 -> loss: 0.09356815517156386\n",
      "    At iteration 5300 -> loss: 0.09349236234519977\n",
      "    At iteration 5400 -> loss: 0.09340558243182824\n",
      "    At iteration 5500 -> loss: 0.09335781941567753\n",
      "    At iteration 5600 -> loss: 0.0933196415200326\n",
      "    At iteration 5700 -> loss: 0.0932932171635774\n",
      "    At iteration 5800 -> loss: 0.09326448360696318\n",
      "    At iteration 5900 -> loss: 0.09318213149027046\n",
      "    At iteration 6000 -> loss: 0.09313883102714784\n",
      "    At iteration 6100 -> loss: 0.09327995410338945\n",
      "    At iteration 6200 -> loss: 0.09346755551062026\n",
      "    At iteration 6300 -> loss: 0.09345919941331715\n",
      "    At iteration 6400 -> loss: 0.0934597722707321\n",
      "    At iteration 6500 -> loss: 0.09344447796517678\n",
      "    At iteration 6600 -> loss: 0.09341325424757699\n",
      "    At iteration 6700 -> loss: 0.09344153086280156\n",
      "    At iteration 6800 -> loss: 0.09340755733514398\n",
      "    At iteration 6900 -> loss: 0.09337359292673432\n",
      "    At iteration 7000 -> loss: 0.09332545192433489\n",
      "    At iteration 7100 -> loss: 0.09338527937246743\n",
      "    At iteration 7200 -> loss: 0.09340835560769138\n",
      "    At iteration 7300 -> loss: 0.09336000747377557\n",
      "    At iteration 7400 -> loss: 0.09357178717391434\n",
      "    At iteration 7500 -> loss: 0.09351250926978193\n",
      "    At iteration 7600 -> loss: 0.0934531708800906\n",
      "    At iteration 7700 -> loss: 0.09342834501301435\n",
      "    At iteration 7800 -> loss: 0.09345291063061131\n",
      "    At iteration 7900 -> loss: 0.09342971088172539\n",
      "    At iteration 8000 -> loss: 0.09343596771767458\n",
      "    At iteration 8100 -> loss: 0.09345709108954364\n",
      "    At iteration 8200 -> loss: 0.09341597283400092\n",
      "    At iteration 8300 -> loss: 0.09341626344394202\n",
      "    At iteration 8400 -> loss: 0.09343205006767133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8500 -> loss: 0.09341728101725474\n",
      "    At iteration 8600 -> loss: 0.09339159222129632\n",
      "    At iteration 8700 -> loss: 0.09336363382271466\n",
      "    At iteration 8800 -> loss: 0.09334769822574522\n",
      "    At iteration 8900 -> loss: 0.09336354660238998\n",
      "    At iteration 9000 -> loss: 0.09335050028723003\n",
      "    At iteration 9100 -> loss: 0.09327715740943576\n",
      "    At iteration 9200 -> loss: 0.09324694158689655\n",
      "    At iteration 9300 -> loss: 0.09322819765815366\n",
      "    At iteration 9400 -> loss: 0.09321385020411338\n",
      "    At iteration 9500 -> loss: 0.09320579379729164\n",
      "    At iteration 9600 -> loss: 0.09321500305181062\n",
      "    At iteration 9700 -> loss: 0.09318210090916762\n",
      "    At iteration 9800 -> loss: 0.09313105498720542\n",
      "    At iteration 9900 -> loss: 0.09310285668117599\n",
      "    At iteration 10000 -> loss: 0.09312073252795276\n",
      "    At iteration 10100 -> loss: 0.09313218581462841\n",
      "    At iteration 10200 -> loss: 0.09313192859711239\n",
      "    At iteration 10300 -> loss: 0.09317654045620709\n",
      "    At iteration 10400 -> loss: 0.09318374936060765\n",
      "    At iteration 10500 -> loss: 0.09315612453723983\n",
      "    At iteration 10600 -> loss: 0.09313510228498795\n",
      "    At iteration 10700 -> loss: 0.09313067996197756\n",
      "    At iteration 10800 -> loss: 0.09311297784442346\n",
      "    At iteration 10900 -> loss: 0.09308825352293909\n",
      "    At iteration 11000 -> loss: 0.09305816253779427\n",
      "    At iteration 11100 -> loss: 0.09304667649638731\n",
      "    At iteration 11200 -> loss: 0.09312784965000324\n",
      "    At iteration 11300 -> loss: 0.09312630455537924\n",
      "    At iteration 11400 -> loss: 0.09312233739048159\n",
      "    At iteration 11500 -> loss: 0.09312750254177404\n",
      "    At iteration 11600 -> loss: 0.09312106093114711\n",
      "    At iteration 11700 -> loss: 0.09313369648424999\n",
      "    At iteration 11800 -> loss: 0.09313573786521266\n",
      "    At iteration 11900 -> loss: 0.09311896074907391\n",
      "    At iteration 12000 -> loss: 0.09310328737308711\n",
      "    At iteration 12100 -> loss: 0.09307439758652904\n",
      "    At iteration 12200 -> loss: 0.09308106446746214\n",
      "    At iteration 12300 -> loss: 0.0930782092529446\n",
      "    At iteration 12400 -> loss: 0.09307840279422182\n",
      "    At iteration 12500 -> loss: 0.09305696540504788\n",
      "    At iteration 12600 -> loss: 0.09303484500077738\n",
      "    At iteration 12700 -> loss: 0.09302973388967832\n",
      "    At iteration 12800 -> loss: 0.0930227317442019\n",
      "    At iteration 12900 -> loss: 0.0930089324542631\n",
      "    At iteration 13000 -> loss: 0.09299985916757941\n",
      "    At iteration 13100 -> loss: 0.09296008847727402\n",
      "    At iteration 13200 -> loss: 0.09292890656191263\n",
      "    At iteration 13300 -> loss: 0.09290428378119307\n",
      "    At iteration 13400 -> loss: 0.09293911704149448\n",
      "    At iteration 13500 -> loss: 0.09297337538121885\n",
      "    At iteration 13600 -> loss: 0.09299607098953802\n",
      "Staring Epoch 79\n",
      "    At iteration 0 -> loss: 0.0800995690997297\n",
      "    At iteration 100 -> loss: 0.08911442977082577\n",
      "    At iteration 200 -> loss: 0.09204146235147256\n",
      "    At iteration 300 -> loss: 0.09221619712526723\n",
      "    At iteration 400 -> loss: 0.09227548455991097\n",
      "    At iteration 500 -> loss: 0.09188145428334706\n",
      "    At iteration 600 -> loss: 0.09168322973791383\n",
      "    At iteration 700 -> loss: 0.09191684124373065\n",
      "    At iteration 800 -> loss: 0.09304217894719907\n",
      "    At iteration 900 -> loss: 0.09257063976470065\n",
      "    At iteration 1000 -> loss: 0.09225256620771342\n",
      "    At iteration 1100 -> loss: 0.09215119355049703\n",
      "    At iteration 1200 -> loss: 0.09182550469192904\n",
      "    At iteration 1300 -> loss: 0.09184787599841641\n",
      "    At iteration 1400 -> loss: 0.09175719062017655\n",
      "    At iteration 1500 -> loss: 0.09160313621245299\n",
      "    At iteration 1600 -> loss: 0.09181418602814821\n",
      "    At iteration 1700 -> loss: 0.0917352926743207\n",
      "    At iteration 1800 -> loss: 0.09158139418428475\n",
      "    At iteration 1900 -> loss: 0.09155200346059278\n",
      "    At iteration 2000 -> loss: 0.09159606083082845\n",
      "    At iteration 2100 -> loss: 0.0917105924305312\n",
      "    At iteration 2200 -> loss: 0.09159391523444611\n",
      "    At iteration 2300 -> loss: 0.09176656214952142\n",
      "    At iteration 2400 -> loss: 0.09236571828416114\n",
      "    At iteration 2500 -> loss: 0.09229332838498473\n",
      "    At iteration 2600 -> loss: 0.09239998662029208\n",
      "    At iteration 2700 -> loss: 0.09242224949385035\n",
      "    At iteration 2800 -> loss: 0.09249073509026136\n",
      "    At iteration 2900 -> loss: 0.09242505375194854\n",
      "    At iteration 3000 -> loss: 0.09242459372449421\n",
      "    At iteration 3100 -> loss: 0.0926892254453384\n",
      "    At iteration 3200 -> loss: 0.09284216431057825\n",
      "    At iteration 3300 -> loss: 0.09283402627370727\n",
      "    At iteration 3400 -> loss: 0.09278482877000875\n",
      "    At iteration 3500 -> loss: 0.09266006879489357\n",
      "    At iteration 3600 -> loss: 0.09259478390380022\n",
      "    At iteration 3700 -> loss: 0.09256551556607422\n",
      "    At iteration 3800 -> loss: 0.09253190787889691\n",
      "    At iteration 3900 -> loss: 0.0925334934724393\n",
      "    At iteration 4000 -> loss: 0.09253834950204151\n",
      "    At iteration 4100 -> loss: 0.09254205071229364\n",
      "    At iteration 4200 -> loss: 0.09248158979791028\n",
      "    At iteration 4300 -> loss: 0.09239462357824138\n",
      "    At iteration 4400 -> loss: 0.09240532402990986\n",
      "    At iteration 4500 -> loss: 0.09236646723481819\n",
      "    At iteration 4600 -> loss: 0.09264055216047318\n",
      "    At iteration 4700 -> loss: 0.09273713418795827\n",
      "    At iteration 4800 -> loss: 0.09277170326593326\n",
      "    At iteration 4900 -> loss: 0.09280059956891441\n",
      "    At iteration 5000 -> loss: 0.09275546372817775\n",
      "    At iteration 5100 -> loss: 0.09278076862893488\n",
      "    At iteration 5200 -> loss: 0.09273534979861571\n",
      "    At iteration 5300 -> loss: 0.09281618903258099\n",
      "    At iteration 5400 -> loss: 0.09273097025234167\n",
      "    At iteration 5500 -> loss: 0.09267894956830561\n",
      "    At iteration 5600 -> loss: 0.09266404995658648\n",
      "    At iteration 5700 -> loss: 0.09268209535691614\n",
      "    At iteration 5800 -> loss: 0.09259283440968709\n",
      "    At iteration 5900 -> loss: 0.09263047095119982\n",
      "    At iteration 6000 -> loss: 0.0926716337613873\n",
      "    At iteration 6100 -> loss: 0.09267663339016245\n",
      "    At iteration 6200 -> loss: 0.09261811573940058\n",
      "    At iteration 6300 -> loss: 0.09264649073750017\n",
      "    At iteration 6400 -> loss: 0.09263208034624519\n",
      "    At iteration 6500 -> loss: 0.09261504099157265\n",
      "    At iteration 6600 -> loss: 0.092581639921234\n",
      "    At iteration 6700 -> loss: 0.09307336498017219\n",
      "    At iteration 6800 -> loss: 0.09302784865762603\n",
      "    At iteration 6900 -> loss: 0.09306038717845005\n",
      "    At iteration 7000 -> loss: 0.09304007964491906\n",
      "    At iteration 7100 -> loss: 0.0929872977821651\n",
      "    At iteration 7200 -> loss: 0.09295122004816428\n",
      "    At iteration 7300 -> loss: 0.09294962493428037\n",
      "    At iteration 7400 -> loss: 0.09293818722949199\n",
      "    At iteration 7500 -> loss: 0.09289845810006594\n",
      "    At iteration 7600 -> loss: 0.09288301209376842\n",
      "    At iteration 7700 -> loss: 0.09291202286993996\n",
      "    At iteration 7800 -> loss: 0.09290324448191675\n",
      "    At iteration 7900 -> loss: 0.09287582895597432\n",
      "    At iteration 8000 -> loss: 0.0928730829727748\n",
      "    At iteration 8100 -> loss: 0.09301822618432663\n",
      "    At iteration 8200 -> loss: 0.09301050122571139\n",
      "    At iteration 8300 -> loss: 0.09310450915950728\n",
      "    At iteration 8400 -> loss: 0.09310775745439227\n",
      "    At iteration 8500 -> loss: 0.0931132866078809\n",
      "    At iteration 8600 -> loss: 0.09310784693848999\n",
      "    At iteration 8700 -> loss: 0.09310429948619248\n",
      "    At iteration 8800 -> loss: 0.09305918995324111\n",
      "    At iteration 8900 -> loss: 0.09308016279327729\n",
      "    At iteration 9000 -> loss: 0.09309935654190461\n",
      "    At iteration 9100 -> loss: 0.09308715079483648\n",
      "    At iteration 9200 -> loss: 0.09307534422541641\n",
      "    At iteration 9300 -> loss: 0.09309819869723066\n",
      "    At iteration 9400 -> loss: 0.0930960667096441\n",
      "    At iteration 9500 -> loss: 0.09307380458667308\n",
      "    At iteration 9600 -> loss: 0.09313927680694192\n",
      "    At iteration 9700 -> loss: 0.09311695406336193\n",
      "    At iteration 9800 -> loss: 0.09312089345274979\n",
      "    At iteration 9900 -> loss: 0.09313277484094212\n",
      "    At iteration 10000 -> loss: 0.09326842818073962\n",
      "    At iteration 10100 -> loss: 0.09322229183698155\n",
      "    At iteration 10200 -> loss: 0.09322636686188729\n",
      "    At iteration 10300 -> loss: 0.09322896686657665\n",
      "    At iteration 10400 -> loss: 0.09319508755457706\n",
      "    At iteration 10500 -> loss: 0.09319518121085113\n",
      "    At iteration 10600 -> loss: 0.09317641056086011\n",
      "    At iteration 10700 -> loss: 0.09317245644180865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10800 -> loss: 0.09315229050606326\n",
      "    At iteration 10900 -> loss: 0.09316455825162381\n",
      "    At iteration 11000 -> loss: 0.09314060344406822\n",
      "    At iteration 11100 -> loss: 0.09312152738835719\n",
      "    At iteration 11200 -> loss: 0.09309909778875614\n",
      "    At iteration 11300 -> loss: 0.09306373660872647\n",
      "    At iteration 11400 -> loss: 0.09303456256064085\n",
      "    At iteration 11500 -> loss: 0.09306235735880294\n",
      "    At iteration 11600 -> loss: 0.09304406387041746\n",
      "    At iteration 11700 -> loss: 0.0930372103922732\n",
      "    At iteration 11800 -> loss: 0.09302888382562603\n",
      "    At iteration 11900 -> loss: 0.09305850110717001\n",
      "    At iteration 12000 -> loss: 0.09308423120173484\n",
      "    At iteration 12100 -> loss: 0.0930485184074798\n",
      "    At iteration 12200 -> loss: 0.0930632450785152\n",
      "    At iteration 12300 -> loss: 0.09308109392810762\n",
      "    At iteration 12400 -> loss: 0.09306085754818248\n",
      "    At iteration 12500 -> loss: 0.09303008477624743\n",
      "    At iteration 12600 -> loss: 0.09299982224381828\n",
      "    At iteration 12700 -> loss: 0.09298517994755501\n",
      "    At iteration 12800 -> loss: 0.0929978185269362\n",
      "    At iteration 12900 -> loss: 0.09296290019426577\n",
      "    At iteration 13000 -> loss: 0.09295126776619225\n",
      "    At iteration 13100 -> loss: 0.0929600878899865\n",
      "    At iteration 13200 -> loss: 0.09293166323718016\n",
      "    At iteration 13300 -> loss: 0.09298154378009214\n",
      "    At iteration 13400 -> loss: 0.09300502519657756\n",
      "    At iteration 13500 -> loss: 0.09299249151852264\n",
      "    At iteration 13600 -> loss: 0.09302253214154203\n",
      "Staring Epoch 80\n",
      "    At iteration 0 -> loss: 0.09946055943146348\n",
      "    At iteration 100 -> loss: 0.09205905360556924\n",
      "    At iteration 200 -> loss: 0.09001836864664807\n",
      "    At iteration 300 -> loss: 0.1029204598369562\n",
      "    At iteration 400 -> loss: 0.09905073314968522\n",
      "    At iteration 500 -> loss: 0.09792932899766153\n",
      "    At iteration 600 -> loss: 0.09864047189281913\n",
      "    At iteration 700 -> loss: 0.09911536005376473\n",
      "    At iteration 800 -> loss: 0.0979478866913553\n",
      "    At iteration 900 -> loss: 0.09759810095615758\n",
      "    At iteration 1000 -> loss: 0.09687554345961776\n",
      "    At iteration 1100 -> loss: 0.09639897440039434\n",
      "    At iteration 1200 -> loss: 0.09582196211356071\n",
      "    At iteration 1300 -> loss: 0.09581407428071563\n",
      "    At iteration 1400 -> loss: 0.09547857415600193\n",
      "    At iteration 1500 -> loss: 0.09514904587678803\n",
      "    At iteration 1600 -> loss: 0.09491930251796592\n",
      "    At iteration 1700 -> loss: 0.09485231716773211\n",
      "    At iteration 1800 -> loss: 0.09493060851288466\n",
      "    At iteration 1900 -> loss: 0.09472118327013243\n",
      "    At iteration 2000 -> loss: 0.09517724545650237\n",
      "    At iteration 2100 -> loss: 0.0949385603438783\n",
      "    At iteration 2200 -> loss: 0.09497421238476488\n",
      "    At iteration 2300 -> loss: 0.094750626214434\n",
      "    At iteration 2400 -> loss: 0.09459710877112748\n",
      "    At iteration 2500 -> loss: 0.09442886930494299\n",
      "    At iteration 2600 -> loss: 0.09448749757888232\n",
      "    At iteration 2700 -> loss: 0.09433548649470504\n",
      "    At iteration 2800 -> loss: 0.09422145699152844\n",
      "    At iteration 2900 -> loss: 0.09423042953064897\n",
      "    At iteration 3000 -> loss: 0.09403184811994428\n",
      "    At iteration 3100 -> loss: 0.09399626478383928\n",
      "    At iteration 3200 -> loss: 0.09404193185300823\n",
      "    At iteration 3300 -> loss: 0.09393244280473989\n",
      "    At iteration 3400 -> loss: 0.09394420269656129\n",
      "    At iteration 3500 -> loss: 0.09389937671649067\n",
      "    At iteration 3600 -> loss: 0.09376556356750451\n",
      "    At iteration 3700 -> loss: 0.093674597109186\n",
      "    At iteration 3800 -> loss: 0.09361938365523208\n",
      "    At iteration 3900 -> loss: 0.09362172421882585\n",
      "    At iteration 4000 -> loss: 0.0938374198837629\n",
      "    At iteration 4100 -> loss: 0.09378810265080063\n",
      "    At iteration 4200 -> loss: 0.09370619392643935\n",
      "    At iteration 4300 -> loss: 0.09363499018898362\n",
      "    At iteration 4400 -> loss: 0.09361454707133868\n",
      "    At iteration 4500 -> loss: 0.09352324735400241\n",
      "    At iteration 4600 -> loss: 0.09341830714937566\n",
      "    At iteration 4700 -> loss: 0.09331367988783361\n",
      "    At iteration 4800 -> loss: 0.0932740628904804\n",
      "    At iteration 4900 -> loss: 0.09338696655880498\n",
      "    At iteration 5000 -> loss: 0.0933380950595106\n",
      "    At iteration 5100 -> loss: 0.09328098950134343\n",
      "    At iteration 5200 -> loss: 0.09334496988749347\n",
      "    At iteration 5300 -> loss: 0.09332452349244508\n",
      "    At iteration 5400 -> loss: 0.09329976593924048\n",
      "    At iteration 5500 -> loss: 0.09341724635998187\n",
      "    At iteration 5600 -> loss: 0.09337322384017971\n",
      "    At iteration 5700 -> loss: 0.09332152633492513\n",
      "    At iteration 5800 -> loss: 0.09325094237679236\n",
      "    At iteration 5900 -> loss: 0.0932116889751266\n",
      "    At iteration 6000 -> loss: 0.09318281457067154\n",
      "    At iteration 6100 -> loss: 0.09315505957235853\n",
      "    At iteration 6200 -> loss: 0.09315309148381498\n",
      "    At iteration 6300 -> loss: 0.09310891773994857\n",
      "    At iteration 6400 -> loss: 0.09308990634664892\n",
      "    At iteration 6500 -> loss: 0.09305499281563552\n",
      "    At iteration 6600 -> loss: 0.09313911304839846\n",
      "    At iteration 6700 -> loss: 0.09322806601198033\n",
      "    At iteration 6800 -> loss: 0.09322384872916531\n",
      "    At iteration 6900 -> loss: 0.09322657539643055\n",
      "    At iteration 7000 -> loss: 0.09319598916893186\n",
      "    At iteration 7100 -> loss: 0.09318830285789817\n",
      "    At iteration 7200 -> loss: 0.09312644211523856\n",
      "    At iteration 7300 -> loss: 0.09310896518826123\n",
      "    At iteration 7400 -> loss: 0.09314084214015043\n",
      "    At iteration 7500 -> loss: 0.09314558862132913\n",
      "    At iteration 7600 -> loss: 0.09315736537907696\n",
      "    At iteration 7700 -> loss: 0.09312734743600357\n",
      "    At iteration 7800 -> loss: 0.09305797492984139\n",
      "    At iteration 7900 -> loss: 0.09305052980890482\n",
      "    At iteration 8000 -> loss: 0.09304293930730373\n",
      "    At iteration 8100 -> loss: 0.09301179196516864\n",
      "    At iteration 8200 -> loss: 0.0929865902215327\n",
      "    At iteration 8300 -> loss: 0.0929662511845219\n",
      "    At iteration 8400 -> loss: 0.09297840143372194\n",
      "    At iteration 8500 -> loss: 0.09306413482105821\n",
      "    At iteration 8600 -> loss: 0.09303236302547481\n",
      "    At iteration 8700 -> loss: 0.09302817948423023\n",
      "    At iteration 8800 -> loss: 0.09302191730958013\n",
      "    At iteration 8900 -> loss: 0.09301418801494621\n",
      "    At iteration 9000 -> loss: 0.09301745907235667\n",
      "    At iteration 9100 -> loss: 0.09299164858291448\n",
      "    At iteration 9200 -> loss: 0.09296141563744481\n",
      "    At iteration 9300 -> loss: 0.09298557280681911\n",
      "    At iteration 9400 -> loss: 0.09301499282538533\n",
      "    At iteration 9500 -> loss: 0.09298223814140863\n",
      "    At iteration 9600 -> loss: 0.09305326652491226\n",
      "    At iteration 9700 -> loss: 0.09311208886298683\n",
      "    At iteration 9800 -> loss: 0.09309660255729207\n",
      "    At iteration 9900 -> loss: 0.09307824685627313\n",
      "    At iteration 10000 -> loss: 0.09303752544811907\n",
      "    At iteration 10100 -> loss: 0.09306107341362072\n",
      "    At iteration 10200 -> loss: 0.09304801453120766\n",
      "    At iteration 10300 -> loss: 0.09307738603683516\n",
      "    At iteration 10400 -> loss: 0.09305643805278821\n",
      "    At iteration 10500 -> loss: 0.09302917775330001\n",
      "    At iteration 10600 -> loss: 0.09302696335117466\n",
      "    At iteration 10700 -> loss: 0.0929965130676723\n",
      "    At iteration 10800 -> loss: 0.09304903069330757\n",
      "    At iteration 10900 -> loss: 0.09302512179188395\n",
      "    At iteration 11000 -> loss: 0.09300713073696117\n",
      "    At iteration 11100 -> loss: 0.0929868657434925\n",
      "    At iteration 11200 -> loss: 0.09298013293539689\n",
      "    At iteration 11300 -> loss: 0.0929488277437261\n",
      "    At iteration 11400 -> loss: 0.09294906113174893\n",
      "    At iteration 11500 -> loss: 0.09293704914120246\n",
      "    At iteration 11600 -> loss: 0.09290932426042929\n",
      "    At iteration 11700 -> loss: 0.09289144889656702\n",
      "    At iteration 11800 -> loss: 0.0928856648299874\n",
      "    At iteration 11900 -> loss: 0.09291467090169642\n",
      "    At iteration 12000 -> loss: 0.09289540490255689\n",
      "    At iteration 12100 -> loss: 0.09289280696341806\n",
      "    At iteration 12200 -> loss: 0.09288104977967054\n",
      "    At iteration 12300 -> loss: 0.09286713048575639\n",
      "    At iteration 12400 -> loss: 0.09288271928285907\n",
      "    At iteration 12500 -> loss: 0.09286400416154375\n",
      "    At iteration 12600 -> loss: 0.09285381832776532\n",
      "    At iteration 12700 -> loss: 0.09289603037963162\n",
      "    At iteration 12800 -> loss: 0.09285923396266228\n",
      "    At iteration 12900 -> loss: 0.09283843302905496\n",
      "    At iteration 13000 -> loss: 0.09284492424978164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 13100 -> loss: 0.09284839909645912\n",
      "    At iteration 13200 -> loss: 0.09287130866411927\n",
      "    At iteration 13300 -> loss: 0.09289790342048311\n",
      "    At iteration 13400 -> loss: 0.0928990265169317\n",
      "    At iteration 13500 -> loss: 0.0928928825250626\n",
      "    At iteration 13600 -> loss: 0.09303047623406294\n",
      "Staring Epoch 81\n",
      "    At iteration 0 -> loss: 0.08058958257606719\n",
      "    At iteration 100 -> loss: 0.08732008051146996\n",
      "    At iteration 200 -> loss: 0.09058928481738052\n",
      "    At iteration 300 -> loss: 0.0939445556636615\n",
      "    At iteration 400 -> loss: 0.09485863295230183\n",
      "    At iteration 500 -> loss: 0.09375919364043983\n",
      "    At iteration 600 -> loss: 0.09388404879787338\n",
      "    At iteration 700 -> loss: 0.0933050259903785\n",
      "    At iteration 800 -> loss: 0.09428113377088734\n",
      "    At iteration 900 -> loss: 0.0938847082436786\n",
      "    At iteration 1000 -> loss: 0.09401750774958788\n",
      "    At iteration 1100 -> loss: 0.09357050791281063\n",
      "    At iteration 1200 -> loss: 0.09376816509594522\n",
      "    At iteration 1300 -> loss: 0.09357228031585649\n",
      "    At iteration 1400 -> loss: 0.0934244828059702\n",
      "    At iteration 1500 -> loss: 0.09426590240373454\n",
      "    At iteration 1600 -> loss: 0.09411104091194446\n",
      "    At iteration 1700 -> loss: 0.09384349648868293\n",
      "    At iteration 1800 -> loss: 0.09387385533593819\n",
      "    At iteration 1900 -> loss: 0.09378022842764985\n",
      "    At iteration 2000 -> loss: 0.0936243476997822\n",
      "    At iteration 2100 -> loss: 0.09354772834311874\n",
      "    At iteration 2200 -> loss: 0.09340993390693847\n",
      "    At iteration 2300 -> loss: 0.09349897930827494\n",
      "    At iteration 2400 -> loss: 0.09344281395550753\n",
      "    At iteration 2500 -> loss: 0.09342062065347076\n",
      "    At iteration 2600 -> loss: 0.09328723594129028\n",
      "    At iteration 2700 -> loss: 0.09321287225275447\n",
      "    At iteration 2800 -> loss: 0.09323751688695435\n",
      "    At iteration 2900 -> loss: 0.09316036054591902\n",
      "    At iteration 3000 -> loss: 0.09342139325640965\n",
      "    At iteration 3100 -> loss: 0.09332952886313585\n",
      "    At iteration 3200 -> loss: 0.09352287994688298\n",
      "    At iteration 3300 -> loss: 0.09367438157083016\n",
      "    At iteration 3400 -> loss: 0.0935954660432775\n",
      "    At iteration 3500 -> loss: 0.0935985278357332\n",
      "    At iteration 3600 -> loss: 0.0943532393287171\n",
      "    At iteration 3700 -> loss: 0.09423443566491956\n",
      "    At iteration 3800 -> loss: 0.0941605502827297\n",
      "    At iteration 3900 -> loss: 0.09415944065150433\n",
      "    At iteration 4000 -> loss: 0.09412403243777195\n",
      "    At iteration 4100 -> loss: 0.09405953496779346\n",
      "    At iteration 4200 -> loss: 0.09399396225613836\n",
      "    At iteration 4300 -> loss: 0.09391604498001122\n",
      "    At iteration 4400 -> loss: 0.093896677074617\n",
      "    At iteration 4500 -> loss: 0.09380607210002945\n",
      "    At iteration 4600 -> loss: 0.09372215016725052\n",
      "    At iteration 4700 -> loss: 0.09373195623538062\n",
      "    At iteration 4800 -> loss: 0.09371536489218428\n",
      "    At iteration 4900 -> loss: 0.09359830912319339\n",
      "    At iteration 5000 -> loss: 0.09358521715350153\n",
      "    At iteration 5100 -> loss: 0.0935235989076736\n",
      "    At iteration 5200 -> loss: 0.09355837283783143\n",
      "    At iteration 5300 -> loss: 0.09351434180475848\n",
      "    At iteration 5400 -> loss: 0.09350065760917926\n",
      "    At iteration 5500 -> loss: 0.09348026207289284\n",
      "    At iteration 5600 -> loss: 0.0934225277839888\n",
      "    At iteration 5700 -> loss: 0.0933846152292083\n",
      "    At iteration 5800 -> loss: 0.0933737047252772\n",
      "    At iteration 5900 -> loss: 0.09339589207612163\n",
      "    At iteration 6000 -> loss: 0.09345018231266108\n",
      "    At iteration 6100 -> loss: 0.09339743021403907\n",
      "    At iteration 6200 -> loss: 0.09332256216841964\n",
      "    At iteration 6300 -> loss: 0.09327614812372019\n",
      "    At iteration 6400 -> loss: 0.09329417216924543\n",
      "    At iteration 6500 -> loss: 0.09325140466706279\n",
      "    At iteration 6600 -> loss: 0.09322407865399888\n",
      "    At iteration 6700 -> loss: 0.09317826265311997\n",
      "    At iteration 6800 -> loss: 0.0932158703535861\n",
      "    At iteration 6900 -> loss: 0.09323736391058261\n",
      "    At iteration 7000 -> loss: 0.093191101860433\n",
      "    At iteration 7100 -> loss: 0.09321803581453669\n",
      "    At iteration 7200 -> loss: 0.09316955048916775\n",
      "    At iteration 7300 -> loss: 0.09315482720959262\n",
      "    At iteration 7400 -> loss: 0.09325769246558853\n",
      "    At iteration 7500 -> loss: 0.09322967433817878\n",
      "    At iteration 7600 -> loss: 0.09321969124404395\n",
      "    At iteration 7700 -> loss: 0.09318800662782362\n",
      "    At iteration 7800 -> loss: 0.09319822466183178\n",
      "    At iteration 7900 -> loss: 0.09314404239504251\n",
      "    At iteration 8000 -> loss: 0.09310902699556602\n",
      "    At iteration 8100 -> loss: 0.09310712912734337\n",
      "    At iteration 8200 -> loss: 0.0930669977875329\n",
      "    At iteration 8300 -> loss: 0.09304899576570615\n",
      "    At iteration 8400 -> loss: 0.09304015649514126\n",
      "    At iteration 8500 -> loss: 0.09303054159564704\n",
      "    At iteration 8600 -> loss: 0.09302586185997107\n",
      "    At iteration 8700 -> loss: 0.09297834041861158\n",
      "    At iteration 8800 -> loss: 0.09297864967275722\n",
      "    At iteration 8900 -> loss: 0.09303025264221036\n",
      "    At iteration 9000 -> loss: 0.09302207477649\n",
      "    At iteration 9100 -> loss: 0.09302286217707412\n",
      "    At iteration 9200 -> loss: 0.09297071394309107\n",
      "    At iteration 9300 -> loss: 0.09292686773014727\n",
      "    At iteration 9400 -> loss: 0.09293837761002995\n",
      "    At iteration 9500 -> loss: 0.09298771700497359\n",
      "    At iteration 9600 -> loss: 0.09304805380126695\n",
      "    At iteration 9700 -> loss: 0.09302743919428769\n",
      "    At iteration 9800 -> loss: 0.09300288176165879\n",
      "    At iteration 9900 -> loss: 0.09309323973739937\n",
      "    At iteration 10000 -> loss: 0.09311849376329268\n",
      "    At iteration 10100 -> loss: 0.09308608139614855\n",
      "    At iteration 10200 -> loss: 0.09306101825124083\n",
      "    At iteration 10300 -> loss: 0.09302823104626536\n",
      "    At iteration 10400 -> loss: 0.09300668362745367\n",
      "    At iteration 10500 -> loss: 0.09305148535506025\n",
      "    At iteration 10600 -> loss: 0.09306469387763465\n",
      "    At iteration 10700 -> loss: 0.0930388554326648\n",
      "    At iteration 10800 -> loss: 0.09300573990570217\n",
      "    At iteration 10900 -> loss: 0.09298241167659226\n",
      "    At iteration 11000 -> loss: 0.0929511605464672\n",
      "    At iteration 11100 -> loss: 0.09292520458912948\n",
      "    At iteration 11200 -> loss: 0.0929372361865694\n",
      "    At iteration 11300 -> loss: 0.09305456115382696\n",
      "    At iteration 11400 -> loss: 0.09304224113284759\n",
      "    At iteration 11500 -> loss: 0.09304379115196271\n",
      "    At iteration 11600 -> loss: 0.09302019369717102\n",
      "    At iteration 11700 -> loss: 0.09299505714541513\n",
      "    At iteration 11800 -> loss: 0.0929747087803697\n",
      "    At iteration 11900 -> loss: 0.09296433651218923\n",
      "    At iteration 12000 -> loss: 0.0929602594987704\n",
      "    At iteration 12100 -> loss: 0.09302378113741193\n",
      "    At iteration 12200 -> loss: 0.09300976449876394\n",
      "    At iteration 12300 -> loss: 0.09298633863452153\n",
      "    At iteration 12400 -> loss: 0.09295299207706385\n",
      "    At iteration 12500 -> loss: 0.0929374635940706\n",
      "    At iteration 12600 -> loss: 0.09299288273912218\n",
      "    At iteration 12700 -> loss: 0.09306167989096845\n",
      "    At iteration 12800 -> loss: 0.09307676992811804\n",
      "    At iteration 12900 -> loss: 0.09306473446422162\n",
      "    At iteration 13000 -> loss: 0.09302031478206974\n",
      "    At iteration 13100 -> loss: 0.0930214014312746\n",
      "    At iteration 13200 -> loss: 0.09304959236047868\n",
      "    At iteration 13300 -> loss: 0.09303903622654426\n",
      "    At iteration 13400 -> loss: 0.09301499208372045\n",
      "    At iteration 13500 -> loss: 0.09300210767772034\n",
      "    At iteration 13600 -> loss: 0.09300704653633099\n",
      "Staring Epoch 82\n",
      "    At iteration 0 -> loss: 0.10548965074121952\n",
      "    At iteration 100 -> loss: 0.09111410305063551\n",
      "    At iteration 200 -> loss: 0.09150826803896697\n",
      "    At iteration 300 -> loss: 0.0910582042867591\n",
      "    At iteration 400 -> loss: 0.09109481219776733\n",
      "    At iteration 500 -> loss: 0.09232654691106129\n",
      "    At iteration 600 -> loss: 0.09222242068087623\n",
      "    At iteration 700 -> loss: 0.09175040341339817\n",
      "    At iteration 800 -> loss: 0.09144650399322782\n",
      "    At iteration 900 -> loss: 0.09194192552591852\n",
      "    At iteration 1000 -> loss: 0.09198686612343852\n",
      "    At iteration 1100 -> loss: 0.09198288599637783\n",
      "    At iteration 1200 -> loss: 0.09209933527300836\n",
      "    At iteration 1300 -> loss: 0.09271999785130003\n",
      "    At iteration 1400 -> loss: 0.09261910149035074\n",
      "    At iteration 1500 -> loss: 0.09261598719161097\n",
      "    At iteration 1600 -> loss: 0.09239206277443267\n",
      "    At iteration 1700 -> loss: 0.09236239356466668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1800 -> loss: 0.09225457125374897\n",
      "    At iteration 1900 -> loss: 0.09225931699388279\n",
      "    At iteration 2000 -> loss: 0.09219974312326423\n",
      "    At iteration 2100 -> loss: 0.09252047265763144\n",
      "    At iteration 2200 -> loss: 0.0924838455705164\n",
      "    At iteration 2300 -> loss: 0.09240111966266992\n",
      "    At iteration 2400 -> loss: 0.09224168523319395\n",
      "    At iteration 2500 -> loss: 0.0921666664663114\n",
      "    At iteration 2600 -> loss: 0.09229022151581366\n",
      "    At iteration 2700 -> loss: 0.09241526239611776\n",
      "    At iteration 2800 -> loss: 0.09232461964404817\n",
      "    At iteration 2900 -> loss: 0.09231143860016869\n",
      "    At iteration 3000 -> loss: 0.09229012726968204\n",
      "    At iteration 3100 -> loss: 0.09222669639503384\n",
      "    At iteration 3200 -> loss: 0.09236497314649127\n",
      "    At iteration 3300 -> loss: 0.09228565968585065\n",
      "    At iteration 3400 -> loss: 0.09237800847447496\n",
      "    At iteration 3500 -> loss: 0.09240720401041733\n",
      "    At iteration 3600 -> loss: 0.09259159825045067\n",
      "    At iteration 3700 -> loss: 0.09249707207417585\n",
      "    At iteration 3800 -> loss: 0.09271881966337404\n",
      "    At iteration 3900 -> loss: 0.09285276298630887\n",
      "    At iteration 4000 -> loss: 0.09280942772410257\n",
      "    At iteration 4100 -> loss: 0.09278676345042233\n",
      "    At iteration 4200 -> loss: 0.09300226938406093\n",
      "    At iteration 4300 -> loss: 0.09295295796366775\n",
      "    At iteration 4400 -> loss: 0.09290789064019651\n",
      "    At iteration 4500 -> loss: 0.09288190808294275\n",
      "    At iteration 4600 -> loss: 0.09290173097441443\n",
      "    At iteration 4700 -> loss: 0.09291478141360812\n",
      "    At iteration 4800 -> loss: 0.09300135486209657\n",
      "    At iteration 4900 -> loss: 0.09289897403215401\n",
      "    At iteration 5000 -> loss: 0.09289685259203793\n",
      "    At iteration 5100 -> loss: 0.09288229815692636\n",
      "    At iteration 5200 -> loss: 0.09289380373345994\n",
      "    At iteration 5300 -> loss: 0.09297342220304397\n",
      "    At iteration 5400 -> loss: 0.09294636045322797\n",
      "    At iteration 5500 -> loss: 0.09300406689397814\n",
      "    At iteration 5600 -> loss: 0.093032000667437\n",
      "    At iteration 5700 -> loss: 0.0929840567229429\n",
      "    At iteration 5800 -> loss: 0.09300478897638988\n",
      "    At iteration 5900 -> loss: 0.09296745408028981\n",
      "    At iteration 6000 -> loss: 0.092968153099707\n",
      "    At iteration 6100 -> loss: 0.09293144820074244\n",
      "    At iteration 6200 -> loss: 0.09290191084009931\n",
      "    At iteration 6300 -> loss: 0.0928741195615505\n",
      "    At iteration 6400 -> loss: 0.09284888009350971\n",
      "    At iteration 6500 -> loss: 0.09294392976342322\n",
      "    At iteration 6600 -> loss: 0.09288082699349522\n",
      "    At iteration 6700 -> loss: 0.09288281222776819\n",
      "    At iteration 6800 -> loss: 0.09287513328209752\n",
      "    At iteration 6900 -> loss: 0.09288109375190465\n",
      "    At iteration 7000 -> loss: 0.09287106814137258\n",
      "    At iteration 7100 -> loss: 0.09282500163607135\n",
      "    At iteration 7200 -> loss: 0.09281027479288706\n",
      "    At iteration 7300 -> loss: 0.0929248680747959\n",
      "    At iteration 7400 -> loss: 0.09289004867797174\n",
      "    At iteration 7500 -> loss: 0.0929226307376613\n",
      "    At iteration 7600 -> loss: 0.09284936432299208\n",
      "    At iteration 7700 -> loss: 0.09281403522039884\n",
      "    At iteration 7800 -> loss: 0.0927499027078218\n",
      "    At iteration 7900 -> loss: 0.09273113910721185\n",
      "    At iteration 8000 -> loss: 0.09272830852708776\n",
      "    At iteration 8100 -> loss: 0.09293267029735135\n",
      "    At iteration 8200 -> loss: 0.09295048468272234\n",
      "    At iteration 8300 -> loss: 0.09301162317005213\n",
      "    At iteration 8400 -> loss: 0.0929966687789418\n",
      "    At iteration 8500 -> loss: 0.0930576115505724\n",
      "    At iteration 8600 -> loss: 0.09308406609907553\n",
      "    At iteration 8700 -> loss: 0.09303798635489746\n",
      "    At iteration 8800 -> loss: 0.093059503787711\n",
      "    At iteration 8900 -> loss: 0.09304248035424435\n",
      "    At iteration 9000 -> loss: 0.09301470263650154\n",
      "    At iteration 9100 -> loss: 0.09297122164410657\n",
      "    At iteration 9200 -> loss: 0.09292190636308618\n",
      "    At iteration 9300 -> loss: 0.09303980144706789\n",
      "    At iteration 9400 -> loss: 0.09302006950274742\n",
      "    At iteration 9500 -> loss: 0.09299618482474353\n",
      "    At iteration 9600 -> loss: 0.09295923591534834\n",
      "    At iteration 9700 -> loss: 0.0929669394712939\n",
      "    At iteration 9800 -> loss: 0.09294770943234862\n",
      "    At iteration 9900 -> loss: 0.09293180372930629\n",
      "    At iteration 10000 -> loss: 0.0928977248519227\n",
      "    At iteration 10100 -> loss: 0.09286805882354808\n",
      "    At iteration 10200 -> loss: 0.09283965309676105\n",
      "    At iteration 10300 -> loss: 0.0928480122768517\n",
      "    At iteration 10400 -> loss: 0.09316845154172361\n",
      "    At iteration 10500 -> loss: 0.0931633639230705\n",
      "    At iteration 10600 -> loss: 0.09316899502100319\n",
      "    At iteration 10700 -> loss: 0.09314868112645748\n",
      "    At iteration 10800 -> loss: 0.09315728621977781\n",
      "    At iteration 10900 -> loss: 0.09313916644973115\n",
      "    At iteration 11000 -> loss: 0.09312709297893774\n",
      "    At iteration 11100 -> loss: 0.09310511426048708\n",
      "    At iteration 11200 -> loss: 0.09306700349510362\n",
      "    At iteration 11300 -> loss: 0.0930611258029899\n",
      "    At iteration 11400 -> loss: 0.09306421110975413\n",
      "    At iteration 11500 -> loss: 0.09308480356678679\n",
      "    At iteration 11600 -> loss: 0.0931042561531514\n",
      "    At iteration 11700 -> loss: 0.09308646292238407\n",
      "    At iteration 11800 -> loss: 0.0930750676174266\n",
      "    At iteration 11900 -> loss: 0.09307313352451214\n",
      "    At iteration 12000 -> loss: 0.09306945831327557\n",
      "    At iteration 12100 -> loss: 0.09306981051756875\n",
      "    At iteration 12200 -> loss: 0.09305358039648905\n",
      "    At iteration 12300 -> loss: 0.09304723340224935\n",
      "    At iteration 12400 -> loss: 0.09304737244851621\n",
      "    At iteration 12500 -> loss: 0.09303451883269209\n",
      "    At iteration 12600 -> loss: 0.09302383702982092\n",
      "    At iteration 12700 -> loss: 0.09310636042381068\n",
      "    At iteration 12800 -> loss: 0.09307250190586802\n",
      "    At iteration 12900 -> loss: 0.09309363340266781\n",
      "    At iteration 13000 -> loss: 0.09306125349905381\n",
      "    At iteration 13100 -> loss: 0.09305796816170239\n",
      "    At iteration 13200 -> loss: 0.0930592215576096\n",
      "    At iteration 13300 -> loss: 0.09305060691359762\n",
      "    At iteration 13400 -> loss: 0.09304053237732679\n",
      "    At iteration 13500 -> loss: 0.09302958209008999\n",
      "    At iteration 13600 -> loss: 0.09301761228319912\n",
      "Staring Epoch 83\n",
      "    At iteration 0 -> loss: 0.08766262233257294\n",
      "    At iteration 100 -> loss: 0.09020484820897974\n",
      "    At iteration 200 -> loss: 0.09182826292053019\n",
      "    At iteration 300 -> loss: 0.09052605279422829\n",
      "    At iteration 400 -> loss: 0.09063286646770576\n",
      "    At iteration 500 -> loss: 0.09271797338509054\n",
      "    At iteration 600 -> loss: 0.09201556024155451\n",
      "    At iteration 700 -> loss: 0.09341477151196001\n",
      "    At iteration 800 -> loss: 0.09337981790413609\n",
      "    At iteration 900 -> loss: 0.09301668693728095\n",
      "    At iteration 1000 -> loss: 0.0928375105755073\n",
      "    At iteration 1100 -> loss: 0.09289240837462047\n",
      "    At iteration 1200 -> loss: 0.09266751826252123\n",
      "    At iteration 1300 -> loss: 0.0930225977245426\n",
      "    At iteration 1400 -> loss: 0.0930711161087438\n",
      "    At iteration 1500 -> loss: 0.09334381010227985\n",
      "    At iteration 1600 -> loss: 0.09308116156560355\n",
      "    At iteration 1700 -> loss: 0.09285404886639045\n",
      "    At iteration 1800 -> loss: 0.09268940557200474\n",
      "    At iteration 1900 -> loss: 0.09270300561756761\n",
      "    At iteration 2000 -> loss: 0.0926702922600402\n",
      "    At iteration 2100 -> loss: 0.09262535657954518\n",
      "    At iteration 2200 -> loss: 0.09271841644452179\n",
      "    At iteration 2300 -> loss: 0.09264473909903685\n",
      "    At iteration 2400 -> loss: 0.0926470112272116\n",
      "    At iteration 2500 -> loss: 0.09256564275095822\n",
      "    At iteration 2600 -> loss: 0.09266139610355363\n",
      "    At iteration 2700 -> loss: 0.09264264789756887\n",
      "    At iteration 2800 -> loss: 0.09269116158637312\n",
      "    At iteration 2900 -> loss: 0.09258479275833144\n",
      "    At iteration 3000 -> loss: 0.09282656307102884\n",
      "    At iteration 3100 -> loss: 0.09273601878012093\n",
      "    At iteration 3200 -> loss: 0.09262659517923934\n",
      "    At iteration 3300 -> loss: 0.09253697093199764\n",
      "    At iteration 3400 -> loss: 0.09256929532390729\n",
      "    At iteration 3500 -> loss: 0.09268259613759885\n",
      "    At iteration 3600 -> loss: 0.0926071791170895\n",
      "    At iteration 3700 -> loss: 0.09263158961146976\n",
      "    At iteration 3800 -> loss: 0.09264888804668855\n",
      "    At iteration 3900 -> loss: 0.09299429310706103\n",
      "    At iteration 4000 -> loss: 0.09301917345979897\n",
      "    At iteration 4100 -> loss: 0.09298570149866155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4200 -> loss: 0.09292207345523505\n",
      "    At iteration 4300 -> loss: 0.09296764242543247\n",
      "    At iteration 4400 -> loss: 0.09295011596347531\n",
      "    At iteration 4500 -> loss: 0.09301010164175896\n",
      "    At iteration 4600 -> loss: 0.09311833087169452\n",
      "    At iteration 4700 -> loss: 0.09320408891630522\n",
      "    At iteration 4800 -> loss: 0.09392085217317149\n",
      "    At iteration 4900 -> loss: 0.09384712362671621\n",
      "    At iteration 5000 -> loss: 0.09385867405650739\n",
      "    At iteration 5100 -> loss: 0.09383430004384731\n",
      "    At iteration 5200 -> loss: 0.09377974955206088\n",
      "    At iteration 5300 -> loss: 0.0937107796178335\n",
      "    At iteration 5400 -> loss: 0.09383912848148517\n",
      "    At iteration 5500 -> loss: 0.09384914452247983\n",
      "    At iteration 5600 -> loss: 0.09379027199403002\n",
      "    At iteration 5700 -> loss: 0.0938251135743833\n",
      "    At iteration 5800 -> loss: 0.09377839002496094\n",
      "    At iteration 5900 -> loss: 0.09373917960117406\n",
      "    At iteration 6000 -> loss: 0.09370876410606113\n",
      "    At iteration 6100 -> loss: 0.09379380507944117\n",
      "    At iteration 6200 -> loss: 0.0937432622506773\n",
      "    At iteration 6300 -> loss: 0.09394862809158624\n",
      "    At iteration 6400 -> loss: 0.09390657546732843\n",
      "    At iteration 6500 -> loss: 0.09388820032727095\n",
      "    At iteration 6600 -> loss: 0.09383628838396169\n",
      "    At iteration 6700 -> loss: 0.09381454419835253\n",
      "    At iteration 6800 -> loss: 0.09383322988333728\n",
      "    At iteration 6900 -> loss: 0.09384406839246288\n",
      "    At iteration 7000 -> loss: 0.0937879366439954\n",
      "    At iteration 7100 -> loss: 0.09374085452024274\n",
      "    At iteration 7200 -> loss: 0.09369960841801317\n",
      "    At iteration 7300 -> loss: 0.0936593044137778\n",
      "    At iteration 7400 -> loss: 0.09360950459702921\n",
      "    At iteration 7500 -> loss: 0.09355418472929332\n",
      "    At iteration 7600 -> loss: 0.09356523488618043\n",
      "    At iteration 7700 -> loss: 0.09366239921072238\n",
      "    At iteration 7800 -> loss: 0.09359510121980978\n",
      "    At iteration 7900 -> loss: 0.0935502765294385\n",
      "    At iteration 8000 -> loss: 0.09351993578681372\n",
      "    At iteration 8100 -> loss: 0.0934810927298114\n",
      "    At iteration 8200 -> loss: 0.09347100124558119\n",
      "    At iteration 8300 -> loss: 0.09341836156655245\n",
      "    At iteration 8400 -> loss: 0.09345343342251551\n",
      "    At iteration 8500 -> loss: 0.09344166749503574\n",
      "    At iteration 8600 -> loss: 0.09341604407832238\n",
      "    At iteration 8700 -> loss: 0.09342429735161398\n",
      "    At iteration 8800 -> loss: 0.0933867341532646\n",
      "    At iteration 8900 -> loss: 0.09337884392520443\n",
      "    At iteration 9000 -> loss: 0.0933320826567967\n",
      "    At iteration 9100 -> loss: 0.09336101160156798\n",
      "    At iteration 9200 -> loss: 0.09334928297141104\n",
      "    At iteration 9300 -> loss: 0.09331120770010154\n",
      "    At iteration 9400 -> loss: 0.09329242633605703\n",
      "    At iteration 9500 -> loss: 0.0932658964726004\n",
      "    At iteration 9600 -> loss: 0.09328580283657435\n",
      "    At iteration 9700 -> loss: 0.09332936543763297\n",
      "    At iteration 9800 -> loss: 0.09332711499485764\n",
      "    At iteration 9900 -> loss: 0.09330043777174823\n",
      "    At iteration 10000 -> loss: 0.09335300679661787\n",
      "    At iteration 10100 -> loss: 0.09332627701696408\n",
      "    At iteration 10200 -> loss: 0.09328570227441543\n",
      "    At iteration 10300 -> loss: 0.09328365196983904\n",
      "    At iteration 10400 -> loss: 0.09326529702747502\n",
      "    At iteration 10500 -> loss: 0.09324164453115462\n",
      "    At iteration 10600 -> loss: 0.09321639422491977\n",
      "    At iteration 10700 -> loss: 0.09320149974924806\n",
      "    At iteration 10800 -> loss: 0.09315500197754256\n",
      "    At iteration 10900 -> loss: 0.09315436466642642\n",
      "    At iteration 11000 -> loss: 0.0931394407954961\n",
      "    At iteration 11100 -> loss: 0.09311173632788448\n",
      "    At iteration 11200 -> loss: 0.09310934801298196\n",
      "    At iteration 11300 -> loss: 0.09310561545340447\n",
      "    At iteration 11400 -> loss: 0.09305744609995813\n",
      "    At iteration 11500 -> loss: 0.093072708279456\n",
      "    At iteration 11600 -> loss: 0.09303474802962629\n",
      "    At iteration 11700 -> loss: 0.09307794828899837\n",
      "    At iteration 11800 -> loss: 0.09304042053908296\n",
      "    At iteration 11900 -> loss: 0.09301918413501525\n",
      "    At iteration 12000 -> loss: 0.09300346840748575\n",
      "    At iteration 12100 -> loss: 0.09300978459771359\n",
      "    At iteration 12200 -> loss: 0.09300776617144103\n",
      "    At iteration 12300 -> loss: 0.092984909789041\n",
      "    At iteration 12400 -> loss: 0.09302648267654133\n",
      "    At iteration 12500 -> loss: 0.09303527086737379\n",
      "    At iteration 12600 -> loss: 0.09301968960341087\n",
      "    At iteration 12700 -> loss: 0.09315514606864189\n",
      "    At iteration 12800 -> loss: 0.09314322135447499\n",
      "    At iteration 12900 -> loss: 0.09312148350447313\n",
      "    At iteration 13000 -> loss: 0.09311556897355695\n",
      "    At iteration 13100 -> loss: 0.09308875507227105\n",
      "    At iteration 13200 -> loss: 0.09305290532665987\n",
      "    At iteration 13300 -> loss: 0.09305170772414782\n",
      "    At iteration 13400 -> loss: 0.09304945738941317\n",
      "    At iteration 13500 -> loss: 0.09301749947072328\n",
      "    At iteration 13600 -> loss: 0.09301402140972444\n",
      "Staring Epoch 84\n",
      "    At iteration 0 -> loss: 0.08988408325240016\n",
      "    At iteration 100 -> loss: 0.08943178326844348\n",
      "    At iteration 200 -> loss: 0.09105410659222005\n",
      "    At iteration 300 -> loss: 0.09145535327069851\n",
      "    At iteration 400 -> loss: 0.09073384657253578\n",
      "    At iteration 500 -> loss: 0.09231036433618756\n",
      "    At iteration 600 -> loss: 0.09212109284490463\n",
      "    At iteration 700 -> loss: 0.09230361808528596\n",
      "    At iteration 800 -> loss: 0.09212529422822772\n",
      "    At iteration 900 -> loss: 0.09177129670884768\n",
      "    At iteration 1000 -> loss: 0.09235797083197529\n",
      "    At iteration 1100 -> loss: 0.09204383496914002\n",
      "    At iteration 1200 -> loss: 0.09226105660304464\n",
      "    At iteration 1300 -> loss: 0.09235449240236444\n",
      "    At iteration 1400 -> loss: 0.09246964126518563\n",
      "    At iteration 1500 -> loss: 0.0923918407915615\n",
      "    At iteration 1600 -> loss: 0.09275070470560363\n",
      "    At iteration 1700 -> loss: 0.09251983613229242\n",
      "    At iteration 1800 -> loss: 0.09245893691092612\n",
      "    At iteration 1900 -> loss: 0.09244542674172943\n",
      "    At iteration 2000 -> loss: 0.09231961488150507\n",
      "    At iteration 2100 -> loss: 0.09215079007522158\n",
      "    At iteration 2200 -> loss: 0.09199845549338687\n",
      "    At iteration 2300 -> loss: 0.09258770588192972\n",
      "    At iteration 2400 -> loss: 0.09253200685287682\n",
      "    At iteration 2500 -> loss: 0.09246773504003689\n",
      "    At iteration 2600 -> loss: 0.092554184398971\n",
      "    At iteration 2700 -> loss: 0.09262418640233146\n",
      "    At iteration 2800 -> loss: 0.09254511660258784\n",
      "    At iteration 2900 -> loss: 0.09243539195430713\n",
      "    At iteration 3000 -> loss: 0.09245101295920784\n",
      "    At iteration 3100 -> loss: 0.09234950140692103\n",
      "    At iteration 3200 -> loss: 0.0924455849861477\n",
      "    At iteration 3300 -> loss: 0.09230550686319462\n",
      "    At iteration 3400 -> loss: 0.09227870195856863\n",
      "    At iteration 3500 -> loss: 0.09223819737084933\n",
      "    At iteration 3600 -> loss: 0.09228208029850703\n",
      "    At iteration 3700 -> loss: 0.09229663709840467\n",
      "    At iteration 3800 -> loss: 0.09229100434201878\n",
      "    At iteration 3900 -> loss: 0.0922416724609725\n",
      "    At iteration 4000 -> loss: 0.09220902135682374\n",
      "    At iteration 4100 -> loss: 0.09233838283789111\n",
      "    At iteration 4200 -> loss: 0.0924933173214665\n",
      "    At iteration 4300 -> loss: 0.09262225568418764\n",
      "    At iteration 4400 -> loss: 0.09271169275189842\n",
      "    At iteration 4500 -> loss: 0.09268316404678156\n",
      "    At iteration 4600 -> loss: 0.0926173429738891\n",
      "    At iteration 4700 -> loss: 0.09261959042034086\n",
      "    At iteration 4800 -> loss: 0.09262941203423451\n",
      "    At iteration 4900 -> loss: 0.09258962725706445\n",
      "    At iteration 5000 -> loss: 0.09260463268716967\n",
      "    At iteration 5100 -> loss: 0.09274198872644347\n",
      "    At iteration 5200 -> loss: 0.09276906169855406\n",
      "    At iteration 5300 -> loss: 0.09267058009372917\n",
      "    At iteration 5400 -> loss: 0.09258567681260986\n",
      "    At iteration 5500 -> loss: 0.09257081564644387\n",
      "    At iteration 5600 -> loss: 0.09252298513749548\n",
      "    At iteration 5700 -> loss: 0.09247511857103047\n",
      "    At iteration 5800 -> loss: 0.09244479496490571\n",
      "    At iteration 5900 -> loss: 0.09242290125193936\n",
      "    At iteration 6000 -> loss: 0.09244640399488584\n",
      "    At iteration 6100 -> loss: 0.09240830805804197\n",
      "    At iteration 6200 -> loss: 0.09241388956660188\n",
      "    At iteration 6300 -> loss: 0.09239505678425707\n",
      "    At iteration 6400 -> loss: 0.09235580991740149\n",
      "    At iteration 6500 -> loss: 0.09252858345062935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6600 -> loss: 0.09248280187263243\n",
      "    At iteration 6700 -> loss: 0.09249408475078484\n",
      "    At iteration 6800 -> loss: 0.09250015676296866\n",
      "    At iteration 6900 -> loss: 0.09249448579780727\n",
      "    At iteration 7000 -> loss: 0.09245514759651033\n",
      "    At iteration 7100 -> loss: 0.09246654737544652\n",
      "    At iteration 7200 -> loss: 0.0924618509735683\n",
      "    At iteration 7300 -> loss: 0.09251929737553823\n",
      "    At iteration 7400 -> loss: 0.09275860626440928\n",
      "    At iteration 7500 -> loss: 0.09269436652415039\n",
      "    At iteration 7600 -> loss: 0.09268753443166086\n",
      "    At iteration 7700 -> loss: 0.09267942676448011\n",
      "    At iteration 7800 -> loss: 0.09269754675756782\n",
      "    At iteration 7900 -> loss: 0.09271261320877972\n",
      "    At iteration 8000 -> loss: 0.09273229721530947\n",
      "    At iteration 8100 -> loss: 0.09272570737953263\n",
      "    At iteration 8200 -> loss: 0.09267355503886172\n",
      "    At iteration 8300 -> loss: 0.09265985251405881\n",
      "    At iteration 8400 -> loss: 0.09263927691805232\n",
      "    At iteration 8500 -> loss: 0.09267498652131988\n",
      "    At iteration 8600 -> loss: 0.0926745841261616\n",
      "    At iteration 8700 -> loss: 0.0926774393932368\n",
      "    At iteration 8800 -> loss: 0.09275431796532971\n",
      "    At iteration 8900 -> loss: 0.09273961192552034\n",
      "    At iteration 9000 -> loss: 0.09280853672697202\n",
      "    At iteration 9100 -> loss: 0.0927685729639727\n",
      "    At iteration 9200 -> loss: 0.09276319387957305\n",
      "    At iteration 9300 -> loss: 0.09284224806617383\n",
      "    At iteration 9400 -> loss: 0.09284391263538473\n",
      "    At iteration 9500 -> loss: 0.09280527611544948\n",
      "    At iteration 9600 -> loss: 0.0928575268283124\n",
      "    At iteration 9700 -> loss: 0.09283820494816683\n",
      "    At iteration 9800 -> loss: 0.09281649445379009\n",
      "    At iteration 9900 -> loss: 0.09282183312619911\n",
      "    At iteration 10000 -> loss: 0.09280982739463153\n",
      "    At iteration 10100 -> loss: 0.09280931980518112\n",
      "    At iteration 10200 -> loss: 0.09288420124145205\n",
      "    At iteration 10300 -> loss: 0.09286327296311217\n",
      "    At iteration 10400 -> loss: 0.092843578765476\n",
      "    At iteration 10500 -> loss: 0.09281722307728171\n",
      "    At iteration 10600 -> loss: 0.09281501040288566\n",
      "    At iteration 10700 -> loss: 0.09281155840734852\n",
      "    At iteration 10800 -> loss: 0.09288967109790372\n",
      "    At iteration 10900 -> loss: 0.09285572375454938\n",
      "    At iteration 11000 -> loss: 0.0928348681692871\n",
      "    At iteration 11100 -> loss: 0.09281923721705358\n",
      "    At iteration 11200 -> loss: 0.09287526240597356\n",
      "    At iteration 11300 -> loss: 0.09287737744215833\n",
      "    At iteration 11400 -> loss: 0.09284796068748909\n",
      "    At iteration 11500 -> loss: 0.09282109717181523\n",
      "    At iteration 11600 -> loss: 0.09281139952655376\n",
      "    At iteration 11700 -> loss: 0.09307966056380644\n",
      "    At iteration 11800 -> loss: 0.09310987019818591\n",
      "    At iteration 11900 -> loss: 0.09311651094566868\n",
      "    At iteration 12000 -> loss: 0.09308174690414343\n",
      "    At iteration 12100 -> loss: 0.09307613637720556\n",
      "    At iteration 12200 -> loss: 0.09305739402107034\n",
      "    At iteration 12300 -> loss: 0.0930661883780494\n",
      "    At iteration 12400 -> loss: 0.09305279496995218\n",
      "    At iteration 12500 -> loss: 0.0930710498909402\n",
      "    At iteration 12600 -> loss: 0.09308131761324853\n",
      "    At iteration 12700 -> loss: 0.09306192370636608\n",
      "    At iteration 12800 -> loss: 0.09304034033381468\n",
      "    At iteration 12900 -> loss: 0.09301231981213211\n",
      "    At iteration 13000 -> loss: 0.09306085052291227\n",
      "    At iteration 13100 -> loss: 0.09303914754945927\n",
      "    At iteration 13200 -> loss: 0.09302366228138104\n",
      "    At iteration 13300 -> loss: 0.0930225099297886\n",
      "    At iteration 13400 -> loss: 0.09300181082399472\n",
      "    At iteration 13500 -> loss: 0.09304373975630634\n",
      "    At iteration 13600 -> loss: 0.09305089996354261\n",
      "Staring Epoch 85\n",
      "    At iteration 0 -> loss: 0.08404997386969626\n",
      "    At iteration 100 -> loss: 0.09086776550045282\n",
      "    At iteration 200 -> loss: 0.09177807538032291\n",
      "    At iteration 300 -> loss: 0.09112186681870663\n",
      "    At iteration 400 -> loss: 0.09554203331935036\n",
      "    At iteration 500 -> loss: 0.10051884440052793\n",
      "    At iteration 600 -> loss: 0.09948259715870554\n",
      "    At iteration 700 -> loss: 0.09826901254424542\n",
      "    At iteration 800 -> loss: 0.09739466523559712\n",
      "    At iteration 900 -> loss: 0.09676296498659014\n",
      "    At iteration 1000 -> loss: 0.09613341476018385\n",
      "    At iteration 1100 -> loss: 0.09544645646237075\n",
      "    At iteration 1200 -> loss: 0.09522941999657313\n",
      "    At iteration 1300 -> loss: 0.09515500763671508\n",
      "    At iteration 1400 -> loss: 0.09597645887333016\n",
      "    At iteration 1500 -> loss: 0.09553108947908567\n",
      "    At iteration 1600 -> loss: 0.09522070472668517\n",
      "    At iteration 1700 -> loss: 0.09513555276481342\n",
      "    At iteration 1800 -> loss: 0.09512070935204149\n",
      "    At iteration 1900 -> loss: 0.09482995559256642\n",
      "    At iteration 2000 -> loss: 0.09472978417671099\n",
      "    At iteration 2100 -> loss: 0.09451557618988628\n",
      "    At iteration 2200 -> loss: 0.09436933387024048\n",
      "    At iteration 2300 -> loss: 0.09422168058015981\n",
      "    At iteration 2400 -> loss: 0.09401527719774953\n",
      "    At iteration 2500 -> loss: 0.09393164450629153\n",
      "    At iteration 2600 -> loss: 0.0937720070739046\n",
      "    At iteration 2700 -> loss: 0.09365063272437296\n",
      "    At iteration 2800 -> loss: 0.09346822244954844\n",
      "    At iteration 2900 -> loss: 0.09338106366713173\n",
      "    At iteration 3000 -> loss: 0.09344600711324524\n",
      "    At iteration 3100 -> loss: 0.09341451761872259\n",
      "    At iteration 3200 -> loss: 0.09362491207425987\n",
      "    At iteration 3300 -> loss: 0.09376189981577884\n",
      "    At iteration 3400 -> loss: 0.09368762506363279\n",
      "    At iteration 3500 -> loss: 0.09362645969275941\n",
      "    At iteration 3600 -> loss: 0.0935365616778419\n",
      "    At iteration 3700 -> loss: 0.09350554149878293\n",
      "    At iteration 3800 -> loss: 0.09346902402350842\n",
      "    At iteration 3900 -> loss: 0.09339216178451579\n",
      "    At iteration 4000 -> loss: 0.09331467530847908\n",
      "    At iteration 4100 -> loss: 0.09333460311292922\n",
      "    At iteration 4200 -> loss: 0.09323206222785141\n",
      "    At iteration 4300 -> loss: 0.09323619978083815\n",
      "    At iteration 4400 -> loss: 0.0931822428460667\n",
      "    At iteration 4500 -> loss: 0.09314874815811335\n",
      "    At iteration 4600 -> loss: 0.0931391345406816\n",
      "    At iteration 4700 -> loss: 0.09306673019780771\n",
      "    At iteration 4800 -> loss: 0.09339111283304359\n",
      "    At iteration 4900 -> loss: 0.0933511030801357\n",
      "    At iteration 5000 -> loss: 0.09333756257645644\n",
      "    At iteration 5100 -> loss: 0.09332190144718597\n",
      "    At iteration 5200 -> loss: 0.09331957000333106\n",
      "    At iteration 5300 -> loss: 0.0932757063057667\n",
      "    At iteration 5400 -> loss: 0.09327181728130458\n",
      "    At iteration 5500 -> loss: 0.09327590286011164\n",
      "    At iteration 5600 -> loss: 0.09330553374001842\n",
      "    At iteration 5700 -> loss: 0.09337315282600175\n",
      "    At iteration 5800 -> loss: 0.09330637090854098\n",
      "    At iteration 5900 -> loss: 0.0933286277510514\n",
      "    At iteration 6000 -> loss: 0.09333858950541817\n",
      "    At iteration 6100 -> loss: 0.09333764790148096\n",
      "    At iteration 6200 -> loss: 0.09328354052898903\n",
      "    At iteration 6300 -> loss: 0.09320923284842518\n",
      "    At iteration 6400 -> loss: 0.0931996004453893\n",
      "    At iteration 6500 -> loss: 0.0932469564609636\n",
      "    At iteration 6600 -> loss: 0.09323949245246618\n",
      "    At iteration 6700 -> loss: 0.09330023781728013\n",
      "    At iteration 6800 -> loss: 0.09327521412989616\n",
      "    At iteration 6900 -> loss: 0.09321105059826211\n",
      "    At iteration 7000 -> loss: 0.09318064809726961\n",
      "    At iteration 7100 -> loss: 0.0931786856423131\n",
      "    At iteration 7200 -> loss: 0.0931796441299677\n",
      "    At iteration 7300 -> loss: 0.09320037371878571\n",
      "    At iteration 7400 -> loss: 0.09326972830726052\n",
      "    At iteration 7500 -> loss: 0.09343005361913902\n",
      "    At iteration 7600 -> loss: 0.09337678172481761\n",
      "    At iteration 7700 -> loss: 0.09331935434012956\n",
      "    At iteration 7800 -> loss: 0.09341125588067145\n",
      "    At iteration 7900 -> loss: 0.0933789385166473\n",
      "    At iteration 8000 -> loss: 0.09335456145641903\n",
      "    At iteration 8100 -> loss: 0.09339474120347893\n",
      "    At iteration 8200 -> loss: 0.09348664382573228\n",
      "    At iteration 8300 -> loss: 0.09347473643069733\n",
      "    At iteration 8400 -> loss: 0.09343004077780519\n",
      "    At iteration 8500 -> loss: 0.0933595610633331\n",
      "    At iteration 8600 -> loss: 0.09337440286892318\n",
      "    At iteration 8700 -> loss: 0.09334895470103023\n",
      "    At iteration 8800 -> loss: 0.09332955339827967\n",
      "    At iteration 8900 -> loss: 0.0932940766160098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9000 -> loss: 0.09338968276106623\n",
      "    At iteration 9100 -> loss: 0.09340901998473221\n",
      "    At iteration 9200 -> loss: 0.09335646783130049\n",
      "    At iteration 9300 -> loss: 0.09334500304845876\n",
      "    At iteration 9400 -> loss: 0.09335807088289139\n",
      "    At iteration 9500 -> loss: 0.09334304147145385\n",
      "    At iteration 9600 -> loss: 0.09329108359724145\n",
      "    At iteration 9700 -> loss: 0.09326365531609918\n",
      "    At iteration 9800 -> loss: 0.09323332695322617\n",
      "    At iteration 9900 -> loss: 0.09323088648201329\n",
      "    At iteration 10000 -> loss: 0.09329616719757962\n",
      "    At iteration 10100 -> loss: 0.09325896820806115\n",
      "    At iteration 10200 -> loss: 0.09333311456630447\n",
      "    At iteration 10300 -> loss: 0.09330893934240975\n",
      "    At iteration 10400 -> loss: 0.09329627902832115\n",
      "    At iteration 10500 -> loss: 0.09334793822037249\n",
      "    At iteration 10600 -> loss: 0.09333676268716606\n",
      "    At iteration 10700 -> loss: 0.0933232367671258\n",
      "    At iteration 10800 -> loss: 0.0933174543821375\n",
      "    At iteration 10900 -> loss: 0.09326668305939531\n",
      "    At iteration 11000 -> loss: 0.09326365307067844\n",
      "    At iteration 11100 -> loss: 0.09322525216232103\n",
      "    At iteration 11200 -> loss: 0.09325301658735739\n",
      "    At iteration 11300 -> loss: 0.09320766607045561\n",
      "    At iteration 11400 -> loss: 0.0932711467136094\n",
      "    At iteration 11500 -> loss: 0.09323784687621732\n",
      "    At iteration 11600 -> loss: 0.09321125229784108\n",
      "    At iteration 11700 -> loss: 0.09322911591489355\n",
      "    At iteration 11800 -> loss: 0.0932099117505306\n",
      "    At iteration 11900 -> loss: 0.09318803298148\n",
      "    At iteration 12000 -> loss: 0.09317477232149905\n",
      "    At iteration 12100 -> loss: 0.09314447188389506\n",
      "    At iteration 12200 -> loss: 0.09311887263820613\n",
      "    At iteration 12300 -> loss: 0.09311404536253359\n",
      "    At iteration 12400 -> loss: 0.09311513143249611\n",
      "    At iteration 12500 -> loss: 0.09308781203568586\n",
      "    At iteration 12600 -> loss: 0.09305201555606653\n",
      "    At iteration 12700 -> loss: 0.09306587010942592\n",
      "    At iteration 12800 -> loss: 0.09306376371808815\n",
      "    At iteration 12900 -> loss: 0.09309181567660715\n",
      "    At iteration 13000 -> loss: 0.09305849080576428\n",
      "    At iteration 13100 -> loss: 0.09303562296960362\n",
      "    At iteration 13200 -> loss: 0.093025429079968\n",
      "    At iteration 13300 -> loss: 0.09302323671091227\n",
      "    At iteration 13400 -> loss: 0.09301608606450046\n",
      "    At iteration 13500 -> loss: 0.09302260604045533\n",
      "    At iteration 13600 -> loss: 0.09300311831058868\n",
      "Staring Epoch 86\n",
      "    At iteration 0 -> loss: 0.08564318157732487\n",
      "    At iteration 100 -> loss: 0.09569553770323087\n",
      "    At iteration 200 -> loss: 0.09748903546603652\n",
      "    At iteration 300 -> loss: 0.09573258679604814\n",
      "    At iteration 400 -> loss: 0.09504793580762178\n",
      "    At iteration 500 -> loss: 0.09501806106260205\n",
      "    At iteration 600 -> loss: 0.09519548832130294\n",
      "    At iteration 700 -> loss: 0.09576622160447461\n",
      "    At iteration 800 -> loss: 0.09481891638423513\n",
      "    At iteration 900 -> loss: 0.09441243038062055\n",
      "    At iteration 1000 -> loss: 0.09500331943509183\n",
      "    At iteration 1100 -> loss: 0.09497365347702807\n",
      "    At iteration 1200 -> loss: 0.09494584185208942\n",
      "    At iteration 1300 -> loss: 0.09460208814641792\n",
      "    At iteration 1400 -> loss: 0.0947563248527285\n",
      "    At iteration 1500 -> loss: 0.09453760875666244\n",
      "    At iteration 1600 -> loss: 0.09436693938959843\n",
      "    At iteration 1700 -> loss: 0.09471232775121495\n",
      "    At iteration 1800 -> loss: 0.09460825909873595\n",
      "    At iteration 1900 -> loss: 0.0943036862096333\n",
      "    At iteration 2000 -> loss: 0.09403798287870895\n",
      "    At iteration 2100 -> loss: 0.09382079629487791\n",
      "    At iteration 2200 -> loss: 0.09358893351423542\n",
      "    At iteration 2300 -> loss: 0.09355422717936165\n",
      "    At iteration 2400 -> loss: 0.09338597817697933\n",
      "    At iteration 2500 -> loss: 0.09338391908291784\n",
      "    At iteration 2600 -> loss: 0.09369703386700262\n",
      "    At iteration 2700 -> loss: 0.0936208573969435\n",
      "    At iteration 2800 -> loss: 0.0934293274134886\n",
      "    At iteration 2900 -> loss: 0.09359146787628932\n",
      "    At iteration 3000 -> loss: 0.09344378337522931\n",
      "    At iteration 3100 -> loss: 0.093368259003797\n",
      "    At iteration 3200 -> loss: 0.09324109938572611\n",
      "    At iteration 3300 -> loss: 0.09327562563196097\n",
      "    At iteration 3400 -> loss: 0.09328722191294396\n",
      "    At iteration 3500 -> loss: 0.0931324456493505\n",
      "    At iteration 3600 -> loss: 0.0930474518178452\n",
      "    At iteration 3700 -> loss: 0.09297251623208144\n",
      "    At iteration 3800 -> loss: 0.09297564027693067\n",
      "    At iteration 3900 -> loss: 0.09307165089918709\n",
      "    At iteration 4000 -> loss: 0.09303569806817967\n",
      "    At iteration 4100 -> loss: 0.09294166055399378\n",
      "    At iteration 4200 -> loss: 0.09307842432846967\n",
      "    At iteration 4300 -> loss: 0.09298938682271791\n",
      "    At iteration 4400 -> loss: 0.09292106273612522\n",
      "    At iteration 4500 -> loss: 0.0928930182589894\n",
      "    At iteration 4600 -> loss: 0.09287101222690239\n",
      "    At iteration 4700 -> loss: 0.09284568817399247\n",
      "    At iteration 4800 -> loss: 0.09280698851476538\n",
      "    At iteration 4900 -> loss: 0.09275749570103566\n",
      "    At iteration 5000 -> loss: 0.09275662626647992\n",
      "    At iteration 5100 -> loss: 0.09281413943774496\n",
      "    At iteration 5200 -> loss: 0.09275916801060205\n",
      "    At iteration 5300 -> loss: 0.09271968098063856\n",
      "    At iteration 5400 -> loss: 0.09279939159759029\n",
      "    At iteration 5500 -> loss: 0.0929654268235356\n",
      "    At iteration 5600 -> loss: 0.09295257759709352\n",
      "    At iteration 5700 -> loss: 0.09293961257878659\n",
      "    At iteration 5800 -> loss: 0.09301264135768773\n",
      "    At iteration 5900 -> loss: 0.09297390780374835\n",
      "    At iteration 6000 -> loss: 0.09291780612148784\n",
      "    At iteration 6100 -> loss: 0.092899623515833\n",
      "    At iteration 6200 -> loss: 0.09282437125554358\n",
      "    At iteration 6300 -> loss: 0.0928554477493992\n",
      "    At iteration 6400 -> loss: 0.09278002483122699\n",
      "    At iteration 6500 -> loss: 0.09281149471824737\n",
      "    At iteration 6600 -> loss: 0.09281138039954558\n",
      "    At iteration 6700 -> loss: 0.09276269301341673\n",
      "    At iteration 6800 -> loss: 0.09289708193796002\n",
      "    At iteration 6900 -> loss: 0.09291803659328955\n",
      "    At iteration 7000 -> loss: 0.09287447807684866\n",
      "    At iteration 7100 -> loss: 0.09285058817578197\n",
      "    At iteration 7200 -> loss: 0.09289643358173118\n",
      "    At iteration 7300 -> loss: 0.09291593346534824\n",
      "    At iteration 7400 -> loss: 0.09298566718179149\n",
      "    At iteration 7500 -> loss: 0.09292913588335637\n",
      "    At iteration 7600 -> loss: 0.0929470159237845\n",
      "    At iteration 7700 -> loss: 0.09290111667177901\n",
      "    At iteration 7800 -> loss: 0.09292211679385333\n",
      "    At iteration 7900 -> loss: 0.09289262770414879\n",
      "    At iteration 8000 -> loss: 0.09287839926067436\n",
      "    At iteration 8100 -> loss: 0.09282298325287994\n",
      "    At iteration 8200 -> loss: 0.09278505536933905\n",
      "    At iteration 8300 -> loss: 0.09278484402472668\n",
      "    At iteration 8400 -> loss: 0.09274586240135102\n",
      "    At iteration 8500 -> loss: 0.09278314570797491\n",
      "    At iteration 8600 -> loss: 0.09280628961908328\n",
      "    At iteration 8700 -> loss: 0.09279639124067189\n",
      "    At iteration 8800 -> loss: 0.09285843124232597\n",
      "    At iteration 8900 -> loss: 0.09286310720697125\n",
      "    At iteration 9000 -> loss: 0.09284413808671786\n",
      "    At iteration 9100 -> loss: 0.092830480945467\n",
      "    At iteration 9200 -> loss: 0.09279285836112203\n",
      "    At iteration 9300 -> loss: 0.09285473787632062\n",
      "    At iteration 9400 -> loss: 0.09281891262707037\n",
      "    At iteration 9500 -> loss: 0.09279815751074777\n",
      "    At iteration 9600 -> loss: 0.0927808981441489\n",
      "    At iteration 9700 -> loss: 0.09284923039813714\n",
      "    At iteration 9800 -> loss: 0.09284845545564986\n",
      "    At iteration 9900 -> loss: 0.09281495721670961\n",
      "    At iteration 10000 -> loss: 0.09279195781635051\n",
      "    At iteration 10100 -> loss: 0.0927620558985791\n",
      "    At iteration 10200 -> loss: 0.0927149643219963\n",
      "    At iteration 10300 -> loss: 0.09269847954384432\n",
      "    At iteration 10400 -> loss: 0.09269212972215302\n",
      "    At iteration 10500 -> loss: 0.09264734994765096\n",
      "    At iteration 10600 -> loss: 0.09268719849901415\n",
      "    At iteration 10700 -> loss: 0.09266027939271756\n",
      "    At iteration 10800 -> loss: 0.09266749149167113\n",
      "    At iteration 10900 -> loss: 0.09265932324409613\n",
      "    At iteration 11000 -> loss: 0.09263600180300882\n",
      "    At iteration 11100 -> loss: 0.09268720173470471\n",
      "    At iteration 11200 -> loss: 0.09268428652775919\n",
      "    At iteration 11300 -> loss: 0.09277151019769743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11400 -> loss: 0.09275749701295963\n",
      "    At iteration 11500 -> loss: 0.09275173610899531\n",
      "    At iteration 11600 -> loss: 0.09276749974125477\n",
      "    At iteration 11700 -> loss: 0.09300617527692813\n",
      "    At iteration 11800 -> loss: 0.09300428499097452\n",
      "    At iteration 11900 -> loss: 0.09306097547250917\n",
      "    At iteration 12000 -> loss: 0.09308987998738669\n",
      "    At iteration 12100 -> loss: 0.09307849007097344\n",
      "    At iteration 12200 -> loss: 0.09306353707671325\n",
      "    At iteration 12300 -> loss: 0.09304354699112397\n",
      "    At iteration 12400 -> loss: 0.09302051931316985\n",
      "    At iteration 12500 -> loss: 0.09300562018160224\n",
      "    At iteration 12600 -> loss: 0.09299030233314291\n",
      "    At iteration 12700 -> loss: 0.09301883785033821\n",
      "    At iteration 12800 -> loss: 0.09305014305176944\n",
      "    At iteration 12900 -> loss: 0.09302470529290947\n",
      "    At iteration 13000 -> loss: 0.09305828368722217\n",
      "    At iteration 13100 -> loss: 0.09305082926837639\n",
      "    At iteration 13200 -> loss: 0.09306654780881746\n",
      "    At iteration 13300 -> loss: 0.09304923083500921\n",
      "    At iteration 13400 -> loss: 0.0930415402379423\n",
      "    At iteration 13500 -> loss: 0.09303386549565208\n",
      "    At iteration 13600 -> loss: 0.09302625080937972\n",
      "Staring Epoch 87\n",
      "    At iteration 0 -> loss: 0.08283905091229826\n",
      "    At iteration 100 -> loss: 0.09172178674476743\n",
      "    At iteration 200 -> loss: 0.09094804268768623\n",
      "    At iteration 300 -> loss: 0.09074177120139654\n",
      "    At iteration 400 -> loss: 0.09138290635460981\n",
      "    At iteration 500 -> loss: 0.09075337771429563\n",
      "    At iteration 600 -> loss: 0.09126174751498352\n",
      "    At iteration 700 -> loss: 0.09168605442001378\n",
      "    At iteration 800 -> loss: 0.09162793681301455\n",
      "    At iteration 900 -> loss: 0.09160171279120592\n",
      "    At iteration 1000 -> loss: 0.09161646361770158\n",
      "    At iteration 1100 -> loss: 0.09207092959047693\n",
      "    At iteration 1200 -> loss: 0.09213274560717398\n",
      "    At iteration 1300 -> loss: 0.09190048625402766\n",
      "    At iteration 1400 -> loss: 0.09168654052022664\n",
      "    At iteration 1500 -> loss: 0.09163686716955696\n",
      "    At iteration 1600 -> loss: 0.09152901638474652\n",
      "    At iteration 1700 -> loss: 0.0921434365784265\n",
      "    At iteration 1800 -> loss: 0.09199676134470358\n",
      "    At iteration 1900 -> loss: 0.09200391085448015\n",
      "    At iteration 2000 -> loss: 0.09202866598725402\n",
      "    At iteration 2100 -> loss: 0.09212689903730736\n",
      "    At iteration 2200 -> loss: 0.09230676882704072\n",
      "    At iteration 2300 -> loss: 0.0922067960096891\n",
      "    At iteration 2400 -> loss: 0.09246794132199429\n",
      "    At iteration 2500 -> loss: 0.09248092638309229\n",
      "    At iteration 2600 -> loss: 0.09249620173115221\n",
      "    At iteration 2700 -> loss: 0.09238085606514894\n",
      "    At iteration 2800 -> loss: 0.09229497742210334\n",
      "    At iteration 2900 -> loss: 0.09258815617121315\n",
      "    At iteration 3000 -> loss: 0.09265366617494548\n",
      "    At iteration 3100 -> loss: 0.09261640751119288\n",
      "    At iteration 3200 -> loss: 0.09253788778896219\n",
      "    At iteration 3300 -> loss: 0.09244789198484242\n",
      "    At iteration 3400 -> loss: 0.09243630098628995\n",
      "    At iteration 3500 -> loss: 0.09237749734628864\n",
      "    At iteration 3600 -> loss: 0.09254916710520081\n",
      "    At iteration 3700 -> loss: 0.09243233265780694\n",
      "    At iteration 3800 -> loss: 0.09242862278451545\n",
      "    At iteration 3900 -> loss: 0.0925016718753292\n",
      "    At iteration 4000 -> loss: 0.09246718359652378\n",
      "    At iteration 4100 -> loss: 0.09244520073317193\n",
      "    At iteration 4200 -> loss: 0.09239657459781685\n",
      "    At iteration 4300 -> loss: 0.09233990611029777\n",
      "    At iteration 4400 -> loss: 0.09232799285184079\n",
      "    At iteration 4500 -> loss: 0.09228702944045133\n",
      "    At iteration 4600 -> loss: 0.09229336823871136\n",
      "    At iteration 4700 -> loss: 0.0922421426487139\n",
      "    At iteration 4800 -> loss: 0.09238135132025438\n",
      "    At iteration 4900 -> loss: 0.09231758842985134\n",
      "    At iteration 5000 -> loss: 0.09235816943410423\n",
      "    At iteration 5100 -> loss: 0.09236672642638014\n",
      "    At iteration 5200 -> loss: 0.0924707667292528\n",
      "    At iteration 5300 -> loss: 0.09246661653881183\n",
      "    At iteration 5400 -> loss: 0.09250907268096112\n",
      "    At iteration 5500 -> loss: 0.09256753690122631\n",
      "    At iteration 5600 -> loss: 0.0925250808036977\n",
      "    At iteration 5700 -> loss: 0.09255950074038176\n",
      "    At iteration 5800 -> loss: 0.0925615005361528\n",
      "    At iteration 5900 -> loss: 0.09250398138017116\n",
      "    At iteration 6000 -> loss: 0.09248892170061948\n",
      "    At iteration 6100 -> loss: 0.09245958629728465\n",
      "    At iteration 6200 -> loss: 0.09240089075647973\n",
      "    At iteration 6300 -> loss: 0.09243416003065781\n",
      "    At iteration 6400 -> loss: 0.0924124510124437\n",
      "    At iteration 6500 -> loss: 0.09241737061577997\n",
      "    At iteration 6600 -> loss: 0.09246258038177617\n",
      "    At iteration 6700 -> loss: 0.09244782223369655\n",
      "    At iteration 6800 -> loss: 0.09249420766679436\n",
      "    At iteration 6900 -> loss: 0.09265825774674247\n",
      "    At iteration 7000 -> loss: 0.09265152135696562\n",
      "    At iteration 7100 -> loss: 0.0926342545096054\n",
      "    At iteration 7200 -> loss: 0.09267467914986553\n",
      "    At iteration 7300 -> loss: 0.09267642562196761\n",
      "    At iteration 7400 -> loss: 0.09265994457372859\n",
      "    At iteration 7500 -> loss: 0.09263171580736022\n",
      "    At iteration 7600 -> loss: 0.09268897555695871\n",
      "    At iteration 7700 -> loss: 0.09266233169783566\n",
      "    At iteration 7800 -> loss: 0.09267984094276482\n",
      "    At iteration 7900 -> loss: 0.09269152986686587\n",
      "    At iteration 8000 -> loss: 0.09265848050507297\n",
      "    At iteration 8100 -> loss: 0.09276269711101189\n",
      "    At iteration 8200 -> loss: 0.09274119260817747\n",
      "    At iteration 8300 -> loss: 0.09270621886161204\n",
      "    At iteration 8400 -> loss: 0.09265273716229268\n",
      "    At iteration 8500 -> loss: 0.09262351980335856\n",
      "    At iteration 8600 -> loss: 0.09260373499908195\n",
      "    At iteration 8700 -> loss: 0.09264651572784395\n",
      "    At iteration 8800 -> loss: 0.09260155512988709\n",
      "    At iteration 8900 -> loss: 0.09257685920792527\n",
      "    At iteration 9000 -> loss: 0.09259332334950492\n",
      "    At iteration 9100 -> loss: 0.09262903737538085\n",
      "    At iteration 9200 -> loss: 0.09260146769089309\n",
      "    At iteration 9300 -> loss: 0.09261912408712542\n",
      "    At iteration 9400 -> loss: 0.09260540483571826\n",
      "    At iteration 9500 -> loss: 0.09261859933170617\n",
      "    At iteration 9600 -> loss: 0.09261136906312607\n",
      "    At iteration 9700 -> loss: 0.09262119421464948\n",
      "    At iteration 9800 -> loss: 0.09260444922885093\n",
      "    At iteration 9900 -> loss: 0.09258257102035843\n",
      "    At iteration 10000 -> loss: 0.09256853961590411\n",
      "    At iteration 10100 -> loss: 0.09253846628681124\n",
      "    At iteration 10200 -> loss: 0.09261937835259515\n",
      "    At iteration 10300 -> loss: 0.09259727274471893\n",
      "    At iteration 10400 -> loss: 0.09258484836399268\n",
      "    At iteration 10500 -> loss: 0.09263455310232445\n",
      "    At iteration 10600 -> loss: 0.0926120787660536\n",
      "    At iteration 10700 -> loss: 0.09260317294788359\n",
      "    At iteration 10800 -> loss: 0.0925849347368246\n",
      "    At iteration 10900 -> loss: 0.09264947266466456\n",
      "    At iteration 11000 -> loss: 0.09264332292017474\n",
      "    At iteration 11100 -> loss: 0.09263490063159195\n",
      "    At iteration 11200 -> loss: 0.09262643251324207\n",
      "    At iteration 11300 -> loss: 0.09266426436074156\n",
      "    At iteration 11400 -> loss: 0.09264294441898019\n",
      "    At iteration 11500 -> loss: 0.09263008722291761\n",
      "    At iteration 11600 -> loss: 0.09259559354790209\n",
      "    At iteration 11700 -> loss: 0.09294001024198391\n",
      "    At iteration 11800 -> loss: 0.09294008091751271\n",
      "    At iteration 11900 -> loss: 0.09292400500843777\n",
      "    At iteration 12000 -> loss: 0.09289652520817523\n",
      "    At iteration 12100 -> loss: 0.09292451476676411\n",
      "    At iteration 12200 -> loss: 0.09290603633889653\n",
      "    At iteration 12300 -> loss: 0.09296659266432468\n",
      "    At iteration 12400 -> loss: 0.09295732349899118\n",
      "    At iteration 12500 -> loss: 0.09297141887298711\n",
      "    At iteration 12600 -> loss: 0.09294490524360112\n",
      "    At iteration 12700 -> loss: 0.09294899380090832\n",
      "    At iteration 12800 -> loss: 0.09295723379585707\n",
      "    At iteration 12900 -> loss: 0.09295796267529795\n",
      "    At iteration 13000 -> loss: 0.09298363154119989\n",
      "    At iteration 13100 -> loss: 0.09297209553145384\n",
      "    At iteration 13200 -> loss: 0.09300272878587398\n",
      "    At iteration 13300 -> loss: 0.09308503502086869\n",
      "    At iteration 13400 -> loss: 0.09305704469087732\n",
      "    At iteration 13500 -> loss: 0.09304922416581883\n",
      "    At iteration 13600 -> loss: 0.09304179529098854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring Epoch 88\n",
      "    At iteration 0 -> loss: 0.08068017920595594\n",
      "    At iteration 100 -> loss: 0.09190833454129045\n",
      "    At iteration 200 -> loss: 0.09238435253735458\n",
      "    At iteration 300 -> loss: 0.09258342042092375\n",
      "    At iteration 400 -> loss: 0.09190469728725749\n",
      "    At iteration 500 -> loss: 0.09160725643501987\n",
      "    At iteration 600 -> loss: 0.09148473274755281\n",
      "    At iteration 700 -> loss: 0.09123714979964434\n",
      "    At iteration 800 -> loss: 0.09165775472556886\n",
      "    At iteration 900 -> loss: 0.09178268069361915\n",
      "    At iteration 1000 -> loss: 0.09184794694204439\n",
      "    At iteration 1100 -> loss: 0.09181373344890532\n",
      "    At iteration 1200 -> loss: 0.09174553351921344\n",
      "    At iteration 1300 -> loss: 0.09187593424420301\n",
      "    At iteration 1400 -> loss: 0.09262499612834228\n",
      "    At iteration 1500 -> loss: 0.09267962981579508\n",
      "    At iteration 1600 -> loss: 0.09251387614410729\n",
      "    At iteration 1700 -> loss: 0.0924014157528827\n",
      "    At iteration 1800 -> loss: 0.09236671253962067\n",
      "    At iteration 1900 -> loss: 0.09270395570363711\n",
      "    At iteration 2000 -> loss: 0.0926528728443532\n",
      "    At iteration 2100 -> loss: 0.09266503819137918\n",
      "    At iteration 2200 -> loss: 0.09259454914973597\n",
      "    At iteration 2300 -> loss: 0.09262629260726661\n",
      "    At iteration 2400 -> loss: 0.09255715273585108\n",
      "    At iteration 2500 -> loss: 0.09245696269138035\n",
      "    At iteration 2600 -> loss: 0.09256004904806024\n",
      "    At iteration 2700 -> loss: 0.09249506147676693\n",
      "    At iteration 2800 -> loss: 0.09243866264157535\n",
      "    At iteration 2900 -> loss: 0.09235322278082721\n",
      "    At iteration 3000 -> loss: 0.09234037178591682\n",
      "    At iteration 3100 -> loss: 0.09227702455756662\n",
      "    At iteration 3200 -> loss: 0.09222599181082093\n",
      "    At iteration 3300 -> loss: 0.09236930776649512\n",
      "    At iteration 3400 -> loss: 0.09238946823738449\n",
      "    At iteration 3500 -> loss: 0.09228572774984055\n",
      "    At iteration 3600 -> loss: 0.09228092974824198\n",
      "    At iteration 3700 -> loss: 0.09235267988619451\n",
      "    At iteration 3800 -> loss: 0.09235783949770693\n",
      "    At iteration 3900 -> loss: 0.09224549685896279\n",
      "    At iteration 4000 -> loss: 0.09229721053439086\n",
      "    At iteration 4100 -> loss: 0.0923000369412623\n",
      "    At iteration 4200 -> loss: 0.09234015745049437\n",
      "    At iteration 4300 -> loss: 0.09226951928516855\n",
      "    At iteration 4400 -> loss: 0.09233831721249051\n",
      "    At iteration 4500 -> loss: 0.09231395771947735\n",
      "    At iteration 4600 -> loss: 0.09227092718708757\n",
      "    At iteration 4700 -> loss: 0.09226787161407896\n",
      "    At iteration 4800 -> loss: 0.09220602486535903\n",
      "    At iteration 4900 -> loss: 0.09220601674085194\n",
      "    At iteration 5000 -> loss: 0.09221758696365628\n",
      "    At iteration 5100 -> loss: 0.0922343973170098\n",
      "    At iteration 5200 -> loss: 0.09229258593315157\n",
      "    At iteration 5300 -> loss: 0.09255639414464673\n",
      "    At iteration 5400 -> loss: 0.09253116037392242\n",
      "    At iteration 5500 -> loss: 0.09249060327172645\n",
      "    At iteration 5600 -> loss: 0.0925692541290739\n",
      "    At iteration 5700 -> loss: 0.09252828396569547\n",
      "    At iteration 5800 -> loss: 0.09273450210746073\n",
      "    At iteration 5900 -> loss: 0.09288773780610471\n",
      "    At iteration 6000 -> loss: 0.09305641261687948\n",
      "    At iteration 6100 -> loss: 0.0930393938351234\n",
      "    At iteration 6200 -> loss: 0.09303476429049021\n",
      "    At iteration 6300 -> loss: 0.09299344144516782\n",
      "    At iteration 6400 -> loss: 0.09299657572489795\n",
      "    At iteration 6500 -> loss: 0.09295677918288234\n",
      "    At iteration 6600 -> loss: 0.09291906027638328\n",
      "    At iteration 6700 -> loss: 0.09297957191790356\n",
      "    At iteration 6800 -> loss: 0.09296358771056008\n",
      "    At iteration 6900 -> loss: 0.09292994282367721\n",
      "    At iteration 7000 -> loss: 0.0928871181068912\n",
      "    At iteration 7100 -> loss: 0.09288827081393718\n",
      "    At iteration 7200 -> loss: 0.09298869382693994\n",
      "    At iteration 7300 -> loss: 0.09297340816988\n",
      "    At iteration 7400 -> loss: 0.09299220189530594\n",
      "    At iteration 7500 -> loss: 0.09298145111497831\n",
      "    At iteration 7600 -> loss: 0.09297622555672151\n",
      "    At iteration 7700 -> loss: 0.09294632521617288\n",
      "    At iteration 7800 -> loss: 0.09291336853828\n",
      "    At iteration 7900 -> loss: 0.09296443859253424\n",
      "    At iteration 8000 -> loss: 0.09306739497946015\n",
      "    At iteration 8100 -> loss: 0.09307178887281954\n",
      "    At iteration 8200 -> loss: 0.09301430659495688\n",
      "    At iteration 8300 -> loss: 0.09310202451775139\n",
      "    At iteration 8400 -> loss: 0.09309033768194278\n",
      "    At iteration 8500 -> loss: 0.09305690464576558\n",
      "    At iteration 8600 -> loss: 0.09302803435357898\n",
      "    At iteration 8700 -> loss: 0.09311564598058074\n",
      "    At iteration 8800 -> loss: 0.09311537687434905\n",
      "    At iteration 8900 -> loss: 0.09309932424324602\n",
      "    At iteration 9000 -> loss: 0.09308424076766025\n",
      "    At iteration 9100 -> loss: 0.09315753680147057\n",
      "    At iteration 9200 -> loss: 0.09312928988912868\n",
      "    At iteration 9300 -> loss: 0.09314945269074908\n",
      "    At iteration 9400 -> loss: 0.0931018038139741\n",
      "    At iteration 9500 -> loss: 0.09311944229024222\n",
      "    At iteration 9600 -> loss: 0.09308519451089005\n",
      "    At iteration 9700 -> loss: 0.09312989839156728\n",
      "    At iteration 9800 -> loss: 0.09310046055368869\n",
      "    At iteration 9900 -> loss: 0.09305301786484634\n",
      "    At iteration 10000 -> loss: 0.09304343162171963\n",
      "    At iteration 10100 -> loss: 0.09302276477609431\n",
      "    At iteration 10200 -> loss: 0.09300000562293471\n",
      "    At iteration 10300 -> loss: 0.09295854186210267\n",
      "    At iteration 10400 -> loss: 0.09295705318328827\n",
      "    At iteration 10500 -> loss: 0.09295152396560392\n",
      "    At iteration 10600 -> loss: 0.09293336285803158\n",
      "    At iteration 10700 -> loss: 0.09294667936301255\n",
      "    At iteration 10800 -> loss: 0.09293796998556533\n",
      "    At iteration 10900 -> loss: 0.09296179229804317\n",
      "    At iteration 11000 -> loss: 0.09292120440843646\n",
      "    At iteration 11100 -> loss: 0.09289498474057153\n",
      "    At iteration 11200 -> loss: 0.09287902104754975\n",
      "    At iteration 11300 -> loss: 0.09287756930685598\n",
      "    At iteration 11400 -> loss: 0.09285822065115848\n",
      "    At iteration 11500 -> loss: 0.09287919287641107\n",
      "    At iteration 11600 -> loss: 0.09284453516634018\n",
      "    At iteration 11700 -> loss: 0.09282826528150409\n",
      "    At iteration 11800 -> loss: 0.0928057668724918\n",
      "    At iteration 11900 -> loss: 0.09281846491800573\n",
      "    At iteration 12000 -> loss: 0.09284837160225773\n",
      "    At iteration 12100 -> loss: 0.09288231298339827\n",
      "    At iteration 12200 -> loss: 0.09284830869963229\n",
      "    At iteration 12300 -> loss: 0.09284326326942649\n",
      "    At iteration 12400 -> loss: 0.09288844360992023\n",
      "    At iteration 12500 -> loss: 0.09286984478576822\n",
      "    At iteration 12600 -> loss: 0.09285123570199028\n",
      "    At iteration 12700 -> loss: 0.0928486044284029\n",
      "    At iteration 12800 -> loss: 0.09285184808272796\n",
      "    At iteration 12900 -> loss: 0.09283556397383808\n",
      "    At iteration 13000 -> loss: 0.09284269947366912\n",
      "    At iteration 13100 -> loss: 0.09285560202684572\n",
      "    At iteration 13200 -> loss: 0.0928476982178629\n",
      "    At iteration 13300 -> loss: 0.09282411559059561\n",
      "    At iteration 13400 -> loss: 0.09306430123575246\n",
      "    At iteration 13500 -> loss: 0.09303964948783475\n",
      "    At iteration 13600 -> loss: 0.09304371825716358\n",
      "Staring Epoch 89\n",
      "    At iteration 0 -> loss: 0.08504447352606803\n",
      "    At iteration 100 -> loss: 0.08879628048121888\n",
      "    At iteration 200 -> loss: 0.08953048766963176\n",
      "    At iteration 300 -> loss: 0.09236634238932656\n",
      "    At iteration 400 -> loss: 0.09285305718753546\n",
      "    At iteration 500 -> loss: 0.09270356001910748\n",
      "    At iteration 600 -> loss: 0.09487928679702255\n",
      "    At iteration 700 -> loss: 0.09406346900996652\n",
      "    At iteration 800 -> loss: 0.09376089052357786\n",
      "    At iteration 900 -> loss: 0.09397798716037878\n",
      "    At iteration 1000 -> loss: 0.09408901541767949\n",
      "    At iteration 1100 -> loss: 0.0940885103371918\n",
      "    At iteration 1200 -> loss: 0.09390032208943822\n",
      "    At iteration 1300 -> loss: 0.09370340794747584\n",
      "    At iteration 1400 -> loss: 0.0937446675191251\n",
      "    At iteration 1500 -> loss: 0.09373117849048838\n",
      "    At iteration 1600 -> loss: 0.09335544147505333\n",
      "    At iteration 1700 -> loss: 0.09313170120061297\n",
      "    At iteration 1800 -> loss: 0.09313038936429607\n",
      "    At iteration 1900 -> loss: 0.09303583353442216\n",
      "    At iteration 2000 -> loss: 0.09288957113245155\n",
      "    At iteration 2100 -> loss: 0.09293729856112125\n",
      "    At iteration 2200 -> loss: 0.09288952967397941\n",
      "    At iteration 2300 -> loss: 0.09326940008237822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2400 -> loss: 0.09308630452936024\n",
      "    At iteration 2500 -> loss: 0.09297183542947159\n",
      "    At iteration 2600 -> loss: 0.09289266805230093\n",
      "    At iteration 2700 -> loss: 0.09301635569146942\n",
      "    At iteration 2800 -> loss: 0.09293860714753309\n",
      "    At iteration 2900 -> loss: 0.09299633837697818\n",
      "    At iteration 3000 -> loss: 0.09288964033564955\n",
      "    At iteration 3100 -> loss: 0.09304292112642404\n",
      "    At iteration 3200 -> loss: 0.092922060201134\n",
      "    At iteration 3300 -> loss: 0.09283373455015183\n",
      "    At iteration 3400 -> loss: 0.09290513486724541\n",
      "    At iteration 3500 -> loss: 0.09301396851379712\n",
      "    At iteration 3600 -> loss: 0.09300518951138444\n",
      "    At iteration 3700 -> loss: 0.09293774492585738\n",
      "    At iteration 3800 -> loss: 0.09289674346218013\n",
      "    At iteration 3900 -> loss: 0.09288359224011662\n",
      "    At iteration 4000 -> loss: 0.09375663518349323\n",
      "    At iteration 4100 -> loss: 0.09367752785925769\n",
      "    At iteration 4200 -> loss: 0.09360686479316761\n",
      "    At iteration 4300 -> loss: 0.09375916691589409\n",
      "    At iteration 4400 -> loss: 0.09366360254640384\n",
      "    At iteration 4500 -> loss: 0.0936179522796703\n",
      "    At iteration 4600 -> loss: 0.09368149215532075\n",
      "    At iteration 4700 -> loss: 0.09362065839888778\n",
      "    At iteration 4800 -> loss: 0.0936200135021686\n",
      "    At iteration 4900 -> loss: 0.09362301570542665\n",
      "    At iteration 5000 -> loss: 0.09364482085534112\n",
      "    At iteration 5100 -> loss: 0.09367146851886501\n",
      "    At iteration 5200 -> loss: 0.09361450311435038\n",
      "    At iteration 5300 -> loss: 0.09356842776039127\n",
      "    At iteration 5400 -> loss: 0.09347055895227369\n",
      "    At iteration 5500 -> loss: 0.09346294953006508\n",
      "    At iteration 5600 -> loss: 0.09339183959840319\n",
      "    At iteration 5700 -> loss: 0.09332374860836315\n",
      "    At iteration 5800 -> loss: 0.093311940336019\n",
      "    At iteration 5900 -> loss: 0.09342725421119487\n",
      "    At iteration 6000 -> loss: 0.09338625668447906\n",
      "    At iteration 6100 -> loss: 0.09344461198423652\n",
      "    At iteration 6200 -> loss: 0.09337719741942144\n",
      "    At iteration 6300 -> loss: 0.09332997012452911\n",
      "    At iteration 6400 -> loss: 0.09331687764804072\n",
      "    At iteration 6500 -> loss: 0.09331515840499113\n",
      "    At iteration 6600 -> loss: 0.09333410020228618\n",
      "    At iteration 6700 -> loss: 0.09329721434456338\n",
      "    At iteration 6800 -> loss: 0.09326100387809334\n",
      "    At iteration 6900 -> loss: 0.0932793442457981\n",
      "    At iteration 7000 -> loss: 0.09346935597412334\n",
      "    At iteration 7100 -> loss: 0.09342045317040852\n",
      "    At iteration 7200 -> loss: 0.09339238661618147\n",
      "    At iteration 7300 -> loss: 0.09332317626367856\n",
      "    At iteration 7400 -> loss: 0.0933288450705064\n",
      "    At iteration 7500 -> loss: 0.09328236206940406\n",
      "    At iteration 7600 -> loss: 0.09328785269911333\n",
      "    At iteration 7700 -> loss: 0.09325759723994874\n",
      "    At iteration 7800 -> loss: 0.09320332900304493\n",
      "    At iteration 7900 -> loss: 0.09321720352841818\n",
      "    At iteration 8000 -> loss: 0.09323483185202043\n",
      "    At iteration 8100 -> loss: 0.09322937678081815\n",
      "    At iteration 8200 -> loss: 0.09319012825911807\n",
      "    At iteration 8300 -> loss: 0.09315414676962885\n",
      "    At iteration 8400 -> loss: 0.09312220482269365\n",
      "    At iteration 8500 -> loss: 0.09309097500209738\n",
      "    At iteration 8600 -> loss: 0.09308349175193645\n",
      "    At iteration 8700 -> loss: 0.09308408528012824\n",
      "    At iteration 8800 -> loss: 0.09305605037662174\n",
      "    At iteration 8900 -> loss: 0.09307860845951607\n",
      "    At iteration 9000 -> loss: 0.09307451230167121\n",
      "    At iteration 9100 -> loss: 0.09305573296277482\n",
      "    At iteration 9200 -> loss: 0.09311064406461031\n",
      "    At iteration 9300 -> loss: 0.09316255852883103\n",
      "    At iteration 9400 -> loss: 0.09319978502327403\n",
      "    At iteration 9500 -> loss: 0.09317674084669685\n",
      "    At iteration 9600 -> loss: 0.0931339214831345\n",
      "    At iteration 9700 -> loss: 0.0931284729957001\n",
      "    At iteration 9800 -> loss: 0.09311266048060443\n",
      "    At iteration 9900 -> loss: 0.09309051251180028\n",
      "    At iteration 10000 -> loss: 0.0930620610660852\n",
      "    At iteration 10100 -> loss: 0.09304334837853948\n",
      "    At iteration 10200 -> loss: 0.09304048395252844\n",
      "    At iteration 10300 -> loss: 0.09302037537844483\n",
      "    At iteration 10400 -> loss: 0.0930704482274574\n",
      "    At iteration 10500 -> loss: 0.09315164307770311\n",
      "    At iteration 10600 -> loss: 0.09315626551211505\n",
      "    At iteration 10700 -> loss: 0.09315586963285355\n",
      "    At iteration 10800 -> loss: 0.09311066711845759\n",
      "    At iteration 10900 -> loss: 0.09310670171757371\n",
      "    At iteration 11000 -> loss: 0.09313511821596361\n",
      "    At iteration 11100 -> loss: 0.09313178555098753\n",
      "    At iteration 11200 -> loss: 0.09310806283226847\n",
      "    At iteration 11300 -> loss: 0.09310541547187864\n",
      "    At iteration 11400 -> loss: 0.09313963418021995\n",
      "    At iteration 11500 -> loss: 0.09318579325426524\n",
      "    At iteration 11600 -> loss: 0.0931710524336577\n",
      "    At iteration 11700 -> loss: 0.09316269366368722\n",
      "    At iteration 11800 -> loss: 0.09313013931049371\n",
      "    At iteration 11900 -> loss: 0.0931357867039079\n",
      "    At iteration 12000 -> loss: 0.09310600794591184\n",
      "    At iteration 12100 -> loss: 0.09311068722319767\n",
      "    At iteration 12200 -> loss: 0.09313739785887454\n",
      "    At iteration 12300 -> loss: 0.09310515473197195\n",
      "    At iteration 12400 -> loss: 0.09308200141041864\n",
      "    At iteration 12500 -> loss: 0.0931095985713685\n",
      "    At iteration 12600 -> loss: 0.09308917280973508\n",
      "    At iteration 12700 -> loss: 0.09306923844357652\n",
      "    At iteration 12800 -> loss: 0.09305793724055338\n",
      "    At iteration 12900 -> loss: 0.09302747247379352\n",
      "    At iteration 13000 -> loss: 0.09302992065288977\n",
      "    At iteration 13100 -> loss: 0.09301618659790034\n",
      "    At iteration 13200 -> loss: 0.0929915184042199\n",
      "    At iteration 13300 -> loss: 0.09299921793517962\n",
      "    At iteration 13400 -> loss: 0.0929880317818431\n",
      "    At iteration 13500 -> loss: 0.09297689160008048\n",
      "    At iteration 13600 -> loss: 0.09300416980721322\n",
      "Staring Epoch 90\n",
      "    At iteration 0 -> loss: 0.09368247399106622\n",
      "    At iteration 100 -> loss: 0.09391837511997538\n",
      "    At iteration 200 -> loss: 0.09157875764925327\n",
      "    At iteration 300 -> loss: 0.0912001374879709\n",
      "    At iteration 400 -> loss: 0.09069604021080266\n",
      "    At iteration 500 -> loss: 0.09172403182633904\n",
      "    At iteration 600 -> loss: 0.09182958282049877\n",
      "    At iteration 700 -> loss: 0.09172834921871281\n",
      "    At iteration 800 -> loss: 0.09136012850858206\n",
      "    At iteration 900 -> loss: 0.09130434645216372\n",
      "    At iteration 1000 -> loss: 0.09208123960252854\n",
      "    At iteration 1100 -> loss: 0.09165514641141823\n",
      "    At iteration 1200 -> loss: 0.09236506362515415\n",
      "    At iteration 1300 -> loss: 0.09232174179228968\n",
      "    At iteration 1400 -> loss: 0.09226257542486377\n",
      "    At iteration 1500 -> loss: 0.09215044773540113\n",
      "    At iteration 1600 -> loss: 0.09201693180532042\n",
      "    At iteration 1700 -> loss: 0.09213662753329463\n",
      "    At iteration 1800 -> loss: 0.09220839822968277\n",
      "    At iteration 1900 -> loss: 0.09223443259628991\n",
      "    At iteration 2000 -> loss: 0.09209942972597931\n",
      "    At iteration 2100 -> loss: 0.09199460738799443\n",
      "    At iteration 2200 -> loss: 0.0918960187709059\n",
      "    At iteration 2300 -> loss: 0.09184902960172746\n",
      "    At iteration 2400 -> loss: 0.09182321029157064\n",
      "    At iteration 2500 -> loss: 0.09212598535892205\n",
      "    At iteration 2600 -> loss: 0.09202175771961342\n",
      "    At iteration 2700 -> loss: 0.09198327575168466\n",
      "    At iteration 2800 -> loss: 0.09209085686469155\n",
      "    At iteration 2900 -> loss: 0.09202213544925715\n",
      "    At iteration 3000 -> loss: 0.09195860585885224\n",
      "    At iteration 3100 -> loss: 0.09189591535001901\n",
      "    At iteration 3200 -> loss: 0.0918360488753328\n",
      "    At iteration 3300 -> loss: 0.09199732222217176\n",
      "    At iteration 3400 -> loss: 0.0919801556322415\n",
      "    At iteration 3500 -> loss: 0.0919627128409349\n",
      "    At iteration 3600 -> loss: 0.09197555100953365\n",
      "    At iteration 3700 -> loss: 0.0920114716059289\n",
      "    At iteration 3800 -> loss: 0.09215283866103101\n",
      "    At iteration 3900 -> loss: 0.09211457394527583\n",
      "    At iteration 4000 -> loss: 0.0922047998359982\n",
      "    At iteration 4100 -> loss: 0.09226258793902418\n",
      "    At iteration 4200 -> loss: 0.09228527423940291\n",
      "    At iteration 4300 -> loss: 0.0922867585572968\n",
      "    At iteration 4400 -> loss: 0.09237805056330704\n",
      "    At iteration 4500 -> loss: 0.09230076859414345\n",
      "    At iteration 4600 -> loss: 0.09233323041402995\n",
      "    At iteration 4700 -> loss: 0.09236615061039165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4800 -> loss: 0.09263033531491904\n",
      "    At iteration 4900 -> loss: 0.09253034696357626\n",
      "    At iteration 5000 -> loss: 0.09251902337145901\n",
      "    At iteration 5100 -> loss: 0.09255040884740694\n",
      "    At iteration 5200 -> loss: 0.09258416672439419\n",
      "    At iteration 5300 -> loss: 0.09257874620747661\n",
      "    At iteration 5400 -> loss: 0.09257943368848741\n",
      "    At iteration 5500 -> loss: 0.09255402312432114\n",
      "    At iteration 5600 -> loss: 0.09271909407684971\n",
      "    At iteration 5700 -> loss: 0.09266871656371571\n",
      "    At iteration 5800 -> loss: 0.09261675030845307\n",
      "    At iteration 5900 -> loss: 0.09264614025916346\n",
      "    At iteration 6000 -> loss: 0.09259428199677054\n",
      "    At iteration 6100 -> loss: 0.09261879318750131\n",
      "    At iteration 6200 -> loss: 0.09258472680191232\n",
      "    At iteration 6300 -> loss: 0.09256257402553114\n",
      "    At iteration 6400 -> loss: 0.09251122272688361\n",
      "    At iteration 6500 -> loss: 0.09252676738304465\n",
      "    At iteration 6600 -> loss: 0.09248765765201704\n",
      "    At iteration 6700 -> loss: 0.0925397247216151\n",
      "    At iteration 6800 -> loss: 0.09255885456549365\n",
      "    At iteration 6900 -> loss: 0.09257576738908588\n",
      "    At iteration 7000 -> loss: 0.09254806612808082\n",
      "    At iteration 7100 -> loss: 0.092491538327945\n",
      "    At iteration 7200 -> loss: 0.0924587877862654\n",
      "    At iteration 7300 -> loss: 0.09245742750276555\n",
      "    At iteration 7400 -> loss: 0.09241130028156148\n",
      "    At iteration 7500 -> loss: 0.0925742765584935\n",
      "    At iteration 7600 -> loss: 0.09253298698166475\n",
      "    At iteration 7700 -> loss: 0.0925939974301338\n",
      "    At iteration 7800 -> loss: 0.09259834962002912\n",
      "    At iteration 7900 -> loss: 0.09262575232828638\n",
      "    At iteration 8000 -> loss: 0.09261454836148618\n",
      "    At iteration 8100 -> loss: 0.09257593972078125\n",
      "    At iteration 8200 -> loss: 0.09259680122467813\n",
      "    At iteration 8300 -> loss: 0.09266198005498476\n",
      "    At iteration 8400 -> loss: 0.09309280214343721\n",
      "    At iteration 8500 -> loss: 0.09307776659240675\n",
      "    At iteration 8600 -> loss: 0.09302736788696771\n",
      "    At iteration 8700 -> loss: 0.09299605098805663\n",
      "    At iteration 8800 -> loss: 0.09296209398236695\n",
      "    At iteration 8900 -> loss: 0.09296498266642308\n",
      "    At iteration 9000 -> loss: 0.09300509885801182\n",
      "    At iteration 9100 -> loss: 0.09301052115799299\n",
      "    At iteration 9200 -> loss: 0.09300691706588117\n",
      "    At iteration 9300 -> loss: 0.0929868974982203\n",
      "    At iteration 9400 -> loss: 0.09297184789475793\n",
      "    At iteration 9500 -> loss: 0.09300138780436955\n",
      "    At iteration 9600 -> loss: 0.09296998260734163\n",
      "    At iteration 9700 -> loss: 0.09295554881481449\n",
      "    At iteration 9800 -> loss: 0.09293257486659595\n",
      "    At iteration 9900 -> loss: 0.09289908506855124\n",
      "    At iteration 10000 -> loss: 0.0928788137793763\n",
      "    At iteration 10100 -> loss: 0.09285072413748453\n",
      "    At iteration 10200 -> loss: 0.09283311092856168\n",
      "    At iteration 10300 -> loss: 0.09283650427852069\n",
      "    At iteration 10400 -> loss: 0.09282216978111052\n",
      "    At iteration 10500 -> loss: 0.0928689631912216\n",
      "    At iteration 10600 -> loss: 0.09283909584651118\n",
      "    At iteration 10700 -> loss: 0.09282577800836132\n",
      "    At iteration 10800 -> loss: 0.09287172701868698\n",
      "    At iteration 10900 -> loss: 0.09287484027667622\n",
      "    At iteration 11000 -> loss: 0.0928779523932466\n",
      "    At iteration 11100 -> loss: 0.0928901366288721\n",
      "    At iteration 11200 -> loss: 0.0929040706587734\n",
      "    At iteration 11300 -> loss: 0.09288482984468904\n",
      "    At iteration 11400 -> loss: 0.09292447772236721\n",
      "    At iteration 11500 -> loss: 0.09289486584725502\n",
      "    At iteration 11600 -> loss: 0.09287726934038304\n",
      "    At iteration 11700 -> loss: 0.09290765381001014\n",
      "    At iteration 11800 -> loss: 0.0929680022354332\n",
      "    At iteration 11900 -> loss: 0.09295056313916057\n",
      "    At iteration 12000 -> loss: 0.09299182999569394\n",
      "    At iteration 12100 -> loss: 0.09304903010772973\n",
      "    At iteration 12200 -> loss: 0.09316764213617773\n",
      "    At iteration 12300 -> loss: 0.09314053633665793\n",
      "    At iteration 12400 -> loss: 0.09312021380391489\n",
      "    At iteration 12500 -> loss: 0.09309792740822234\n",
      "    At iteration 12600 -> loss: 0.09306197429931265\n",
      "    At iteration 12700 -> loss: 0.0930524251594985\n",
      "    At iteration 12800 -> loss: 0.09304326458582446\n",
      "    At iteration 12900 -> loss: 0.09303531434893361\n",
      "    At iteration 13000 -> loss: 0.093043993582232\n",
      "    At iteration 13100 -> loss: 0.09301243517554453\n",
      "    At iteration 13200 -> loss: 0.09298467401797413\n",
      "    At iteration 13300 -> loss: 0.09298632409387225\n",
      "    At iteration 13400 -> loss: 0.09303446058922733\n",
      "    At iteration 13500 -> loss: 0.09304488485709354\n",
      "    At iteration 13600 -> loss: 0.09305036616093278\n",
      "Staring Epoch 91\n",
      "    At iteration 0 -> loss: 0.08061601052759215\n",
      "    At iteration 100 -> loss: 0.0942143568375658\n",
      "    At iteration 200 -> loss: 0.09384937111150686\n",
      "    At iteration 300 -> loss: 0.09307996914459157\n",
      "    At iteration 400 -> loss: 0.09249096303927593\n",
      "    At iteration 500 -> loss: 0.0926005073469212\n",
      "    At iteration 600 -> loss: 0.09226096181086131\n",
      "    At iteration 700 -> loss: 0.09348864964871588\n",
      "    At iteration 800 -> loss: 0.0935087796232214\n",
      "    At iteration 900 -> loss: 0.09371172999710407\n",
      "    At iteration 1000 -> loss: 0.09373015017703701\n",
      "    At iteration 1100 -> loss: 0.09349003086601249\n",
      "    At iteration 1200 -> loss: 0.0929300967387563\n",
      "    At iteration 1300 -> loss: 0.09280465984094873\n",
      "    At iteration 1400 -> loss: 0.09354954792420435\n",
      "    At iteration 1500 -> loss: 0.09332347809838991\n",
      "    At iteration 1600 -> loss: 0.09327693163301966\n",
      "    At iteration 1700 -> loss: 0.09306143983643654\n",
      "    At iteration 1800 -> loss: 0.0930200592797908\n",
      "    At iteration 1900 -> loss: 0.09307717227370536\n",
      "    At iteration 2000 -> loss: 0.09309590687379085\n",
      "    At iteration 2100 -> loss: 0.09301545888036647\n",
      "    At iteration 2200 -> loss: 0.09286179096098818\n",
      "    At iteration 2300 -> loss: 0.09306538049463049\n",
      "    At iteration 2400 -> loss: 0.09317624048521629\n",
      "    At iteration 2500 -> loss: 0.09304888808493553\n",
      "    At iteration 2600 -> loss: 0.0932609970443612\n",
      "    At iteration 2700 -> loss: 0.09316294614859703\n",
      "    At iteration 2800 -> loss: 0.09298599092076443\n",
      "    At iteration 2900 -> loss: 0.09306597716250559\n",
      "    At iteration 3000 -> loss: 0.09298735147495184\n",
      "    At iteration 3100 -> loss: 0.0929662122310646\n",
      "    At iteration 3200 -> loss: 0.09284991638048207\n",
      "    At iteration 3300 -> loss: 0.09292238282605898\n",
      "    At iteration 3400 -> loss: 0.09295963274175353\n",
      "    At iteration 3500 -> loss: 0.09298411645871947\n",
      "    At iteration 3600 -> loss: 0.09295218610687168\n",
      "    At iteration 3700 -> loss: 0.09290165320016078\n",
      "    At iteration 3800 -> loss: 0.09294772536149444\n",
      "    At iteration 3900 -> loss: 0.09286671844424592\n",
      "    At iteration 4000 -> loss: 0.0928568283585923\n",
      "    At iteration 4100 -> loss: 0.09283320135089447\n",
      "    At iteration 4200 -> loss: 0.0927565065340513\n",
      "    At iteration 4300 -> loss: 0.0926887400684863\n",
      "    At iteration 4400 -> loss: 0.09265929690504487\n",
      "    At iteration 4500 -> loss: 0.09266456436009118\n",
      "    At iteration 4600 -> loss: 0.09265258741370871\n",
      "    At iteration 4700 -> loss: 0.09276614160481945\n",
      "    At iteration 4800 -> loss: 0.09273967891525789\n",
      "    At iteration 4900 -> loss: 0.0927004564683207\n",
      "    At iteration 5000 -> loss: 0.09285500300548506\n",
      "    At iteration 5100 -> loss: 0.09282930986398227\n",
      "    At iteration 5200 -> loss: 0.09282750944618683\n",
      "    At iteration 5300 -> loss: 0.09284032190310428\n",
      "    At iteration 5400 -> loss: 0.09279880437110725\n",
      "    At iteration 5500 -> loss: 0.09272875034054086\n",
      "    At iteration 5600 -> loss: 0.09273078268291192\n",
      "    At iteration 5700 -> loss: 0.09267381529214339\n",
      "    At iteration 5800 -> loss: 0.09260286566514019\n",
      "    At iteration 5900 -> loss: 0.09269694392890773\n",
      "    At iteration 6000 -> loss: 0.09263419060007862\n",
      "    At iteration 6100 -> loss: 0.09258510871705879\n",
      "    At iteration 6200 -> loss: 0.09263879764777654\n",
      "    At iteration 6300 -> loss: 0.09262486213024979\n",
      "    At iteration 6400 -> loss: 0.09263454478083014\n",
      "    At iteration 6500 -> loss: 0.09263303840063584\n",
      "    At iteration 6600 -> loss: 0.09259149930292632\n",
      "    At iteration 6700 -> loss: 0.09259410722907707\n",
      "    At iteration 6800 -> loss: 0.0925484024014363\n",
      "    At iteration 6900 -> loss: 0.09249075308818136\n",
      "    At iteration 7000 -> loss: 0.09247130809245555\n",
      "    At iteration 7100 -> loss: 0.09248202286282085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7200 -> loss: 0.09243600314118021\n",
      "    At iteration 7300 -> loss: 0.09239690494492013\n",
      "    At iteration 7400 -> loss: 0.09238889036061063\n",
      "    At iteration 7500 -> loss: 0.0923681485011217\n",
      "    At iteration 7600 -> loss: 0.09233264831687468\n",
      "    At iteration 7700 -> loss: 0.09229461264481813\n",
      "    At iteration 7800 -> loss: 0.09226134847296508\n",
      "    At iteration 7900 -> loss: 0.0922654671937826\n",
      "    At iteration 8000 -> loss: 0.09237251581150596\n",
      "    At iteration 8100 -> loss: 0.09241996449094123\n",
      "    At iteration 8200 -> loss: 0.09240326279565256\n",
      "    At iteration 8300 -> loss: 0.09263203551610232\n",
      "    At iteration 8400 -> loss: 0.09259350943290387\n",
      "    At iteration 8500 -> loss: 0.09257921193968863\n",
      "    At iteration 8600 -> loss: 0.09260188857726082\n",
      "    At iteration 8700 -> loss: 0.09258713575527477\n",
      "    At iteration 8800 -> loss: 0.0925835762627706\n",
      "    At iteration 8900 -> loss: 0.09259731955139028\n",
      "    At iteration 9000 -> loss: 0.09255912065469636\n",
      "    At iteration 9100 -> loss: 0.09254067506182477\n",
      "    At iteration 9200 -> loss: 0.09251952024976749\n",
      "    At iteration 9300 -> loss: 0.09285666364307311\n",
      "    At iteration 9400 -> loss: 0.0929554577140956\n",
      "    At iteration 9500 -> loss: 0.09292484772663911\n",
      "    At iteration 9600 -> loss: 0.09294016725382187\n",
      "    At iteration 9700 -> loss: 0.09292527580869539\n",
      "    At iteration 9800 -> loss: 0.09292691628984194\n",
      "    At iteration 9900 -> loss: 0.09297833269586506\n",
      "    At iteration 10000 -> loss: 0.09303092666020518\n",
      "    At iteration 10100 -> loss: 0.09302304094672849\n",
      "    At iteration 10200 -> loss: 0.09298966378505133\n",
      "    At iteration 10300 -> loss: 0.09296110474887868\n",
      "    At iteration 10400 -> loss: 0.09295098603959859\n",
      "    At iteration 10500 -> loss: 0.0929282492590296\n",
      "    At iteration 10600 -> loss: 0.09290962645061843\n",
      "    At iteration 10700 -> loss: 0.09289256889904263\n",
      "    At iteration 10800 -> loss: 0.0929151784337519\n",
      "    At iteration 10900 -> loss: 0.0929432031316107\n",
      "    At iteration 11000 -> loss: 0.09297078382091381\n",
      "    At iteration 11100 -> loss: 0.09296691814828724\n",
      "    At iteration 11200 -> loss: 0.09301664046643318\n",
      "    At iteration 11300 -> loss: 0.09299995389583067\n",
      "    At iteration 11400 -> loss: 0.09299202389641295\n",
      "    At iteration 11500 -> loss: 0.09297179062978297\n",
      "    At iteration 11600 -> loss: 0.09293841802626047\n",
      "    At iteration 11700 -> loss: 0.09291792332077464\n",
      "    At iteration 11800 -> loss: 0.09296415015645686\n",
      "    At iteration 11900 -> loss: 0.09306467786002796\n",
      "    At iteration 12000 -> loss: 0.09306615456073769\n",
      "    At iteration 12100 -> loss: 0.09306720857048807\n",
      "    At iteration 12200 -> loss: 0.09302360306077956\n",
      "    At iteration 12300 -> loss: 0.093068427961831\n",
      "    At iteration 12400 -> loss: 0.09308669849512213\n",
      "    At iteration 12500 -> loss: 0.09312746858375853\n",
      "    At iteration 12600 -> loss: 0.09311752131595413\n",
      "    At iteration 12700 -> loss: 0.0930906985636923\n",
      "    At iteration 12800 -> loss: 0.09308398607096181\n",
      "    At iteration 12900 -> loss: 0.09311807854440123\n",
      "    At iteration 13000 -> loss: 0.09311118096749736\n",
      "    At iteration 13100 -> loss: 0.09309695086219474\n",
      "    At iteration 13200 -> loss: 0.09307324048563985\n",
      "    At iteration 13300 -> loss: 0.09305006731745903\n",
      "    At iteration 13400 -> loss: 0.09303583421737474\n",
      "    At iteration 13500 -> loss: 0.09303585073813848\n",
      "    At iteration 13600 -> loss: 0.0930071499064543\n",
      "Staring Epoch 92\n",
      "    At iteration 0 -> loss: 0.08053121140983421\n",
      "    At iteration 100 -> loss: 0.09157104209904972\n",
      "    At iteration 200 -> loss: 0.09108466785836075\n",
      "    At iteration 300 -> loss: 0.09108548750361371\n",
      "    At iteration 400 -> loss: 0.0915114409600383\n",
      "    At iteration 500 -> loss: 0.09283485862156099\n",
      "    At iteration 600 -> loss: 0.09273511988445884\n",
      "    At iteration 700 -> loss: 0.09263209272686455\n",
      "    At iteration 800 -> loss: 0.09243367539783809\n",
      "    At iteration 900 -> loss: 0.09256426195583044\n",
      "    At iteration 1000 -> loss: 0.09216609169623692\n",
      "    At iteration 1100 -> loss: 0.09226166986843637\n",
      "    At iteration 1200 -> loss: 0.09231200196145192\n",
      "    At iteration 1300 -> loss: 0.09224894915402944\n",
      "    At iteration 1400 -> loss: 0.09209712499530537\n",
      "    At iteration 1500 -> loss: 0.09294786709499349\n",
      "    At iteration 1600 -> loss: 0.09301961034130882\n",
      "    At iteration 1700 -> loss: 0.09306610204548146\n",
      "    At iteration 1800 -> loss: 0.09284614224614281\n",
      "    At iteration 1900 -> loss: 0.09290675521591334\n",
      "    At iteration 2000 -> loss: 0.09304483355993184\n",
      "    At iteration 2100 -> loss: 0.09303937141960501\n",
      "    At iteration 2200 -> loss: 0.09301708608703368\n",
      "    At iteration 2300 -> loss: 0.09295850130864178\n",
      "    At iteration 2400 -> loss: 0.09287084343883434\n",
      "    At iteration 2500 -> loss: 0.09279432094175676\n",
      "    At iteration 2600 -> loss: 0.09264922307860812\n",
      "    At iteration 2700 -> loss: 0.09260031838489934\n",
      "    At iteration 2800 -> loss: 0.09263801130211011\n",
      "    At iteration 2900 -> loss: 0.09259277425129396\n",
      "    At iteration 3000 -> loss: 0.09249583426565405\n",
      "    At iteration 3100 -> loss: 0.09265244464979079\n",
      "    At iteration 3200 -> loss: 0.09270011818566316\n",
      "    At iteration 3300 -> loss: 0.09263639744928559\n",
      "    At iteration 3400 -> loss: 0.09261601204935481\n",
      "    At iteration 3500 -> loss: 0.09252764182876616\n",
      "    At iteration 3600 -> loss: 0.09244371103553889\n",
      "    At iteration 3700 -> loss: 0.09252930756441292\n",
      "    At iteration 3800 -> loss: 0.09248424384883826\n",
      "    At iteration 3900 -> loss: 0.09249185600301\n",
      "    At iteration 4000 -> loss: 0.09264755353086652\n",
      "    At iteration 4100 -> loss: 0.0928230891958484\n",
      "    At iteration 4200 -> loss: 0.09288347395303405\n",
      "    At iteration 4300 -> loss: 0.09286838576175321\n",
      "    At iteration 4400 -> loss: 0.09283507247829936\n",
      "    At iteration 4500 -> loss: 0.09281675789806641\n",
      "    At iteration 4600 -> loss: 0.09294902580803953\n",
      "    At iteration 4700 -> loss: 0.09287074078888573\n",
      "    At iteration 4800 -> loss: 0.09291833806500596\n",
      "    At iteration 4900 -> loss: 0.09285267045354276\n",
      "    At iteration 5000 -> loss: 0.09281613156339474\n",
      "    At iteration 5100 -> loss: 0.09277967786943987\n",
      "    At iteration 5200 -> loss: 0.09280497907691485\n",
      "    At iteration 5300 -> loss: 0.09279400383874355\n",
      "    At iteration 5400 -> loss: 0.09275148484686484\n",
      "    At iteration 5500 -> loss: 0.09302506671661837\n",
      "    At iteration 5600 -> loss: 0.0929683107591751\n",
      "    At iteration 5700 -> loss: 0.09293987161350697\n",
      "    At iteration 5800 -> loss: 0.09293283831607008\n",
      "    At iteration 5900 -> loss: 0.09292409072516103\n",
      "    At iteration 6000 -> loss: 0.0929001930947177\n",
      "    At iteration 6100 -> loss: 0.09288520757176814\n",
      "    At iteration 6200 -> loss: 0.09291134604291598\n",
      "    At iteration 6300 -> loss: 0.09285839041152175\n",
      "    At iteration 6400 -> loss: 0.09284981748354351\n",
      "    At iteration 6500 -> loss: 0.09285149007291403\n",
      "    At iteration 6600 -> loss: 0.09282880646953712\n",
      "    At iteration 6700 -> loss: 0.092801785584717\n",
      "    At iteration 6800 -> loss: 0.092760516213882\n",
      "    At iteration 6900 -> loss: 0.09274623018209706\n",
      "    At iteration 7000 -> loss: 0.09278048523615222\n",
      "    At iteration 7100 -> loss: 0.09275403233986254\n",
      "    At iteration 7200 -> loss: 0.09271053331294739\n",
      "    At iteration 7300 -> loss: 0.0927399872886096\n",
      "    At iteration 7400 -> loss: 0.09268736649839086\n",
      "    At iteration 7500 -> loss: 0.09267677324586672\n",
      "    At iteration 7600 -> loss: 0.09266468478436563\n",
      "    At iteration 7700 -> loss: 0.09266244409940523\n",
      "    At iteration 7800 -> loss: 0.09263269915676682\n",
      "    At iteration 7900 -> loss: 0.09264779574756676\n",
      "    At iteration 8000 -> loss: 0.0926328772086918\n",
      "    At iteration 8100 -> loss: 0.09257612134679288\n",
      "    At iteration 8200 -> loss: 0.09256238454479163\n",
      "    At iteration 8300 -> loss: 0.09252097852612394\n",
      "    At iteration 8400 -> loss: 0.09254577881149917\n",
      "    At iteration 8500 -> loss: 0.09263310272285366\n",
      "    At iteration 8600 -> loss: 0.09261600426064667\n",
      "    At iteration 8700 -> loss: 0.09260079465394656\n",
      "    At iteration 8800 -> loss: 0.09276509298723738\n",
      "    At iteration 8900 -> loss: 0.09285581151449217\n",
      "    At iteration 9000 -> loss: 0.09283750993404698\n",
      "    At iteration 9100 -> loss: 0.09280894098016965\n",
      "    At iteration 9200 -> loss: 0.0927946572256555\n",
      "    At iteration 9300 -> loss: 0.09277271462273343\n",
      "    At iteration 9400 -> loss: 0.09272943279363335\n",
      "    At iteration 9500 -> loss: 0.09275997880555881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9600 -> loss: 0.09273607851988061\n",
      "    At iteration 9700 -> loss: 0.09275961769964045\n",
      "    At iteration 9800 -> loss: 0.09273772483640241\n",
      "    At iteration 9900 -> loss: 0.09272224023310932\n",
      "    At iteration 10000 -> loss: 0.09268357189068956\n",
      "    At iteration 10100 -> loss: 0.092750373461506\n",
      "    At iteration 10200 -> loss: 0.0927911070642623\n",
      "    At iteration 10300 -> loss: 0.09278796067288608\n",
      "    At iteration 10400 -> loss: 0.09281614163370669\n",
      "    At iteration 10500 -> loss: 0.09282520273936998\n",
      "    At iteration 10600 -> loss: 0.09280806314756598\n",
      "    At iteration 10700 -> loss: 0.09279975332821842\n",
      "    At iteration 10800 -> loss: 0.09278556877308307\n",
      "    At iteration 10900 -> loss: 0.09304032713103119\n",
      "    At iteration 11000 -> loss: 0.09306017658564893\n",
      "    At iteration 11100 -> loss: 0.09303527754490247\n",
      "    At iteration 11200 -> loss: 0.09308026607778103\n",
      "    At iteration 11300 -> loss: 0.09305148049482723\n",
      "    At iteration 11400 -> loss: 0.09303354818232723\n",
      "    At iteration 11500 -> loss: 0.09307629340681747\n",
      "    At iteration 11600 -> loss: 0.09304263676540468\n",
      "    At iteration 11700 -> loss: 0.09302633809700386\n",
      "    At iteration 11800 -> loss: 0.09304251084185368\n",
      "    At iteration 11900 -> loss: 0.09303352166704823\n",
      "    At iteration 12000 -> loss: 0.0930174533107381\n",
      "    At iteration 12100 -> loss: 0.09302577796681498\n",
      "    At iteration 12200 -> loss: 0.09302385252937485\n",
      "    At iteration 12300 -> loss: 0.0930288609630715\n",
      "    At iteration 12400 -> loss: 0.09302012440136713\n",
      "    At iteration 12500 -> loss: 0.09299394684275378\n",
      "    At iteration 12600 -> loss: 0.09298037280295227\n",
      "    At iteration 12700 -> loss: 0.09297810474269505\n",
      "    At iteration 12800 -> loss: 0.09299076844572636\n",
      "    At iteration 12900 -> loss: 0.09297584470824732\n",
      "    At iteration 13000 -> loss: 0.0929571355983725\n",
      "    At iteration 13100 -> loss: 0.09293431813217146\n",
      "    At iteration 13200 -> loss: 0.0929972664861251\n",
      "    At iteration 13300 -> loss: 0.09299810245206158\n",
      "    At iteration 13400 -> loss: 0.09304165473947593\n",
      "    At iteration 13500 -> loss: 0.09304520607447084\n",
      "    At iteration 13600 -> loss: 0.09301808688251395\n",
      "Staring Epoch 93\n",
      "    At iteration 0 -> loss: 0.10156423598527908\n",
      "    At iteration 100 -> loss: 0.09354974221315558\n",
      "    At iteration 200 -> loss: 0.09167195218821961\n",
      "    At iteration 300 -> loss: 0.09136439585998289\n",
      "    At iteration 400 -> loss: 0.0917246611793943\n",
      "    At iteration 500 -> loss: 0.09172676075712122\n",
      "    At iteration 600 -> loss: 0.09162137000507055\n",
      "    At iteration 700 -> loss: 0.09132422005811139\n",
      "    At iteration 800 -> loss: 0.09122118835730766\n",
      "    At iteration 900 -> loss: 0.0923798507231777\n",
      "    At iteration 1000 -> loss: 0.09220794346485302\n",
      "    At iteration 1100 -> loss: 0.09323802289432546\n",
      "    At iteration 1200 -> loss: 0.09347168475005657\n",
      "    At iteration 1300 -> loss: 0.09332979515029508\n",
      "    At iteration 1400 -> loss: 0.09368066287358703\n",
      "    At iteration 1500 -> loss: 0.09383817613812509\n",
      "    At iteration 1600 -> loss: 0.09377523211278825\n",
      "    At iteration 1700 -> loss: 0.09336724235248821\n",
      "    At iteration 1800 -> loss: 0.09331082931325994\n",
      "    At iteration 1900 -> loss: 0.09314731614603876\n",
      "    At iteration 2000 -> loss: 0.09319188772133666\n",
      "    At iteration 2100 -> loss: 0.09311260132522073\n",
      "    At iteration 2200 -> loss: 0.09314990578689766\n",
      "    At iteration 2300 -> loss: 0.09309732207188949\n",
      "    At iteration 2400 -> loss: 0.09318453665265357\n",
      "    At iteration 2500 -> loss: 0.09322127578826114\n",
      "    At iteration 2600 -> loss: 0.09304586548952593\n",
      "    At iteration 2700 -> loss: 0.09292134506320816\n",
      "    At iteration 2800 -> loss: 0.09295478471946218\n",
      "    At iteration 2900 -> loss: 0.09286869764833049\n",
      "    At iteration 3000 -> loss: 0.09294861222803977\n",
      "    At iteration 3100 -> loss: 0.0928752528858611\n",
      "    At iteration 3200 -> loss: 0.09284214159155126\n",
      "    At iteration 3300 -> loss: 0.09286026483801103\n",
      "    At iteration 3400 -> loss: 0.0929945550138065\n",
      "    At iteration 3500 -> loss: 0.09310293975227527\n",
      "    At iteration 3600 -> loss: 0.09310367483200879\n",
      "    At iteration 3700 -> loss: 0.09312823005470819\n",
      "    At iteration 3800 -> loss: 0.09317093904786643\n",
      "    At iteration 3900 -> loss: 0.09318182584903537\n",
      "    At iteration 4000 -> loss: 0.0930597035354339\n",
      "    At iteration 4100 -> loss: 0.09303904665633131\n",
      "    At iteration 4200 -> loss: 0.09302836453940809\n",
      "    At iteration 4300 -> loss: 0.09294888193063564\n",
      "    At iteration 4400 -> loss: 0.09294763777117385\n",
      "    At iteration 4500 -> loss: 0.09287746240892686\n",
      "    At iteration 4600 -> loss: 0.0930554919500531\n",
      "    At iteration 4700 -> loss: 0.09304463712226324\n",
      "    At iteration 4800 -> loss: 0.09301436046890998\n",
      "    At iteration 4900 -> loss: 0.09290713058558733\n",
      "    At iteration 5000 -> loss: 0.09293484311034074\n",
      "    At iteration 5100 -> loss: 0.09301759385011454\n",
      "    At iteration 5200 -> loss: 0.0929806889091504\n",
      "    At iteration 5300 -> loss: 0.09296199956151285\n",
      "    At iteration 5400 -> loss: 0.09287112314783867\n",
      "    At iteration 5500 -> loss: 0.09303855715510283\n",
      "    At iteration 5600 -> loss: 0.09297462380986428\n",
      "    At iteration 5700 -> loss: 0.09292174674832399\n",
      "    At iteration 5800 -> loss: 0.0929015800524469\n",
      "    At iteration 5900 -> loss: 0.09290178036718422\n",
      "    At iteration 6000 -> loss: 0.09288894209541752\n",
      "    At iteration 6100 -> loss: 0.09286878591436434\n",
      "    At iteration 6200 -> loss: 0.09281434318325148\n",
      "    At iteration 6300 -> loss: 0.09283111231361951\n",
      "    At iteration 6400 -> loss: 0.09282373174961064\n",
      "    At iteration 6500 -> loss: 0.09291729609346795\n",
      "    At iteration 6600 -> loss: 0.0929102136372702\n",
      "    At iteration 6700 -> loss: 0.09293812941836467\n",
      "    At iteration 6800 -> loss: 0.09292368713986422\n",
      "    At iteration 6900 -> loss: 0.09289770276917321\n",
      "    At iteration 7000 -> loss: 0.09284154019593863\n",
      "    At iteration 7100 -> loss: 0.09279288576892057\n",
      "    At iteration 7200 -> loss: 0.09279766991208434\n",
      "    At iteration 7300 -> loss: 0.09288474477508205\n",
      "    At iteration 7400 -> loss: 0.09324659935774034\n",
      "    At iteration 7500 -> loss: 0.09318758181555066\n",
      "    At iteration 7600 -> loss: 0.0931674825552219\n",
      "    At iteration 7700 -> loss: 0.09314548529318026\n",
      "    At iteration 7800 -> loss: 0.09311642301697716\n",
      "    At iteration 7900 -> loss: 0.09312563287180807\n",
      "    At iteration 8000 -> loss: 0.0931026545885202\n",
      "    At iteration 8100 -> loss: 0.0931089536012264\n",
      "    At iteration 8200 -> loss: 0.09310635334071805\n",
      "    At iteration 8300 -> loss: 0.09310634533795055\n",
      "    At iteration 8400 -> loss: 0.09307103663908717\n",
      "    At iteration 8500 -> loss: 0.09315584089238334\n",
      "    At iteration 8600 -> loss: 0.09312138127250376\n",
      "    At iteration 8700 -> loss: 0.09316368210775205\n",
      "    At iteration 8800 -> loss: 0.09314904605592597\n",
      "    At iteration 8900 -> loss: 0.09313558511968104\n",
      "    At iteration 9000 -> loss: 0.09316440179785333\n",
      "    At iteration 9100 -> loss: 0.09313378380362904\n",
      "    At iteration 9200 -> loss: 0.09308943807662225\n",
      "    At iteration 9300 -> loss: 0.09307325509030392\n",
      "    At iteration 9400 -> loss: 0.09304799202585855\n",
      "    At iteration 9500 -> loss: 0.09301470633544467\n",
      "    At iteration 9600 -> loss: 0.09298930152630755\n",
      "    At iteration 9700 -> loss: 0.09311405750027901\n",
      "    At iteration 9800 -> loss: 0.0931178521058417\n",
      "    At iteration 9900 -> loss: 0.09314156831441765\n",
      "    At iteration 10000 -> loss: 0.09311165001693637\n",
      "    At iteration 10100 -> loss: 0.0931703319405505\n",
      "    At iteration 10200 -> loss: 0.09316322889997952\n",
      "    At iteration 10300 -> loss: 0.0931738069525687\n",
      "    At iteration 10400 -> loss: 0.09316434420290255\n",
      "    At iteration 10500 -> loss: 0.09315424350617832\n",
      "    At iteration 10600 -> loss: 0.09319891924179877\n",
      "    At iteration 10700 -> loss: 0.09315946416035989\n",
      "    At iteration 10800 -> loss: 0.09313067362882\n",
      "    At iteration 10900 -> loss: 0.09314815066984389\n",
      "    At iteration 11000 -> loss: 0.09311615766692476\n",
      "    At iteration 11100 -> loss: 0.09309486290639758\n",
      "    At iteration 11200 -> loss: 0.09306942933336694\n",
      "    At iteration 11300 -> loss: 0.09304617303597205\n",
      "    At iteration 11400 -> loss: 0.09301241440881074\n",
      "    At iteration 11500 -> loss: 0.09300478654222515\n",
      "    At iteration 11600 -> loss: 0.09301938575577452\n",
      "    At iteration 11700 -> loss: 0.09299882077341928\n",
      "    At iteration 11800 -> loss: 0.09304298711458703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11900 -> loss: 0.09307387107832762\n",
      "    At iteration 12000 -> loss: 0.09312441109908152\n",
      "    At iteration 12100 -> loss: 0.09311218059942826\n",
      "    At iteration 12200 -> loss: 0.09311798127296435\n",
      "    At iteration 12300 -> loss: 0.09309219246051602\n",
      "    At iteration 12400 -> loss: 0.09310004699062853\n",
      "    At iteration 12500 -> loss: 0.09308325413119983\n",
      "    At iteration 12600 -> loss: 0.09308474603657402\n",
      "    At iteration 12700 -> loss: 0.09307531859066141\n",
      "    At iteration 12800 -> loss: 0.09303764747786422\n",
      "    At iteration 12900 -> loss: 0.093061223480007\n",
      "    At iteration 13000 -> loss: 0.09307348996656437\n",
      "    At iteration 13100 -> loss: 0.09307741597719323\n",
      "    At iteration 13200 -> loss: 0.09304625556083113\n",
      "    At iteration 13300 -> loss: 0.09304671748945258\n",
      "    At iteration 13400 -> loss: 0.0930234050057821\n",
      "    At iteration 13500 -> loss: 0.0930309854888269\n",
      "    At iteration 13600 -> loss: 0.0930329527840741\n",
      "Staring Epoch 94\n",
      "    At iteration 0 -> loss: 0.10989488009363413\n",
      "    At iteration 100 -> loss: 0.09310423715185656\n",
      "    At iteration 200 -> loss: 0.09160930370005474\n",
      "    At iteration 300 -> loss: 0.09235949443325499\n",
      "    At iteration 400 -> loss: 0.09211177071969623\n",
      "    At iteration 500 -> loss: 0.09190799625203774\n",
      "    At iteration 600 -> loss: 0.09188289960643173\n",
      "    At iteration 700 -> loss: 0.09143245051631341\n",
      "    At iteration 800 -> loss: 0.0916516089420046\n",
      "    At iteration 900 -> loss: 0.09126642322415975\n",
      "    At iteration 1000 -> loss: 0.09159874674032013\n",
      "    At iteration 1100 -> loss: 0.09207414857190069\n",
      "    At iteration 1200 -> loss: 0.09177691280046821\n",
      "    At iteration 1300 -> loss: 0.09158591747256542\n",
      "    At iteration 1400 -> loss: 0.09165143549308732\n",
      "    At iteration 1500 -> loss: 0.09180090047341907\n",
      "    At iteration 1600 -> loss: 0.09176889443907017\n",
      "    At iteration 1700 -> loss: 0.09159938535250145\n",
      "    At iteration 1800 -> loss: 0.09146140781738331\n",
      "    At iteration 1900 -> loss: 0.09164489950053395\n",
      "    At iteration 2000 -> loss: 0.09143261256517844\n",
      "    At iteration 2100 -> loss: 0.09138849422481017\n",
      "    At iteration 2200 -> loss: 0.09128455932431773\n",
      "    At iteration 2300 -> loss: 0.09119137915136977\n",
      "    At iteration 2400 -> loss: 0.09144581361696903\n",
      "    At iteration 2500 -> loss: 0.09168756859156452\n",
      "    At iteration 2600 -> loss: 0.09157359994935439\n",
      "    At iteration 2700 -> loss: 0.09184471036508035\n",
      "    At iteration 2800 -> loss: 0.09189637544381496\n",
      "    At iteration 2900 -> loss: 0.09185386797654235\n",
      "    At iteration 3000 -> loss: 0.0920070668163543\n",
      "    At iteration 3100 -> loss: 0.09193368023487804\n",
      "    At iteration 3200 -> loss: 0.09189468699464194\n",
      "    At iteration 3300 -> loss: 0.09295055557274327\n",
      "    At iteration 3400 -> loss: 0.09313117668717165\n",
      "    At iteration 3500 -> loss: 0.09321056103973953\n",
      "    At iteration 3600 -> loss: 0.09319241097023405\n",
      "    At iteration 3700 -> loss: 0.09312000865082512\n",
      "    At iteration 3800 -> loss: 0.09303102034063972\n",
      "    At iteration 3900 -> loss: 0.09292920008983664\n",
      "    At iteration 4000 -> loss: 0.09280988694596788\n",
      "    At iteration 4100 -> loss: 0.09300388633568017\n",
      "    At iteration 4200 -> loss: 0.09308900671344024\n",
      "    At iteration 4300 -> loss: 0.09311260163533465\n",
      "    At iteration 4400 -> loss: 0.09301972066393202\n",
      "    At iteration 4500 -> loss: 0.09321070591984577\n",
      "    At iteration 4600 -> loss: 0.09316602177638039\n",
      "    At iteration 4700 -> loss: 0.09311847786321477\n",
      "    At iteration 4800 -> loss: 0.09303751288196849\n",
      "    At iteration 4900 -> loss: 0.09315190680939718\n",
      "    At iteration 5000 -> loss: 0.09322089141699876\n",
      "    At iteration 5100 -> loss: 0.09321641136286447\n",
      "    At iteration 5200 -> loss: 0.09317055465967276\n",
      "    At iteration 5300 -> loss: 0.09311408596413139\n",
      "    At iteration 5400 -> loss: 0.09311693375299496\n",
      "    At iteration 5500 -> loss: 0.09313986218120826\n",
      "    At iteration 5600 -> loss: 0.0930994637966019\n",
      "    At iteration 5700 -> loss: 0.09314989319169037\n",
      "    At iteration 5800 -> loss: 0.0931277098482854\n",
      "    At iteration 5900 -> loss: 0.09308641377203637\n",
      "    At iteration 6000 -> loss: 0.093173330542786\n",
      "    At iteration 6100 -> loss: 0.09315013321027632\n",
      "    At iteration 6200 -> loss: 0.09311029846405004\n",
      "    At iteration 6300 -> loss: 0.09311203941280988\n",
      "    At iteration 6400 -> loss: 0.09313578351570173\n",
      "    At iteration 6500 -> loss: 0.09319708981658521\n",
      "    At iteration 6600 -> loss: 0.09318012851789537\n",
      "    At iteration 6700 -> loss: 0.09315209437472542\n",
      "    At iteration 6800 -> loss: 0.09319361726723283\n",
      "    At iteration 6900 -> loss: 0.09331267764993702\n",
      "    At iteration 7000 -> loss: 0.09326628285126894\n",
      "    At iteration 7100 -> loss: 0.09322912169388363\n",
      "    At iteration 7200 -> loss: 0.0932144272591086\n",
      "    At iteration 7300 -> loss: 0.09316739196744152\n",
      "    At iteration 7400 -> loss: 0.09318479629864236\n",
      "    At iteration 7500 -> loss: 0.09316905319733114\n",
      "    At iteration 7600 -> loss: 0.09313812312176817\n",
      "    At iteration 7700 -> loss: 0.09315457710723499\n",
      "    At iteration 7800 -> loss: 0.0931114963712868\n",
      "    At iteration 7900 -> loss: 0.09312267573047263\n",
      "    At iteration 8000 -> loss: 0.09308536208047857\n",
      "    At iteration 8100 -> loss: 0.09303737669668603\n",
      "    At iteration 8200 -> loss: 0.09300194462914452\n",
      "    At iteration 8300 -> loss: 0.09295752197172552\n",
      "    At iteration 8400 -> loss: 0.0929269601302826\n",
      "    At iteration 8500 -> loss: 0.09295134423363798\n",
      "    At iteration 8600 -> loss: 0.09300336371158366\n",
      "    At iteration 8700 -> loss: 0.09295798271310837\n",
      "    At iteration 8800 -> loss: 0.09292463377855625\n",
      "    At iteration 8900 -> loss: 0.09291096834246713\n",
      "    At iteration 9000 -> loss: 0.09289691932638154\n",
      "    At iteration 9100 -> loss: 0.0929247241878446\n",
      "    At iteration 9200 -> loss: 0.09287532032528525\n",
      "    At iteration 9300 -> loss: 0.0929748698348203\n",
      "    At iteration 9400 -> loss: 0.09294161431796584\n",
      "    At iteration 9500 -> loss: 0.09292875836579939\n",
      "    At iteration 9600 -> loss: 0.09289354446961387\n",
      "    At iteration 9700 -> loss: 0.09287442295173201\n",
      "    At iteration 9800 -> loss: 0.09285438469394433\n",
      "    At iteration 9900 -> loss: 0.09293364829081131\n",
      "    At iteration 10000 -> loss: 0.09291757507023085\n",
      "    At iteration 10100 -> loss: 0.09291446800552587\n",
      "    At iteration 10200 -> loss: 0.09296314778834247\n",
      "    At iteration 10300 -> loss: 0.09298629967828585\n",
      "    At iteration 10400 -> loss: 0.09296138087731202\n",
      "    At iteration 10500 -> loss: 0.0929859002775734\n",
      "    At iteration 10600 -> loss: 0.09297381874786705\n",
      "    At iteration 10700 -> loss: 0.09296069396210326\n",
      "    At iteration 10800 -> loss: 0.09300296287944772\n",
      "    At iteration 10900 -> loss: 0.09297453249305493\n",
      "    At iteration 11000 -> loss: 0.09294286925598039\n",
      "    At iteration 11100 -> loss: 0.0929452782444613\n",
      "    At iteration 11200 -> loss: 0.09296291276830258\n",
      "    At iteration 11300 -> loss: 0.0929584024801125\n",
      "    At iteration 11400 -> loss: 0.0929265808317713\n",
      "    At iteration 11500 -> loss: 0.09289962115006181\n",
      "    At iteration 11600 -> loss: 0.09294243933050797\n",
      "    At iteration 11700 -> loss: 0.09295720434022943\n",
      "    At iteration 11800 -> loss: 0.09293643806398828\n",
      "    At iteration 11900 -> loss: 0.09292087875271172\n",
      "    At iteration 12000 -> loss: 0.09290926637408102\n",
      "    At iteration 12100 -> loss: 0.09289254125503656\n",
      "    At iteration 12200 -> loss: 0.09291834520709849\n",
      "    At iteration 12300 -> loss: 0.09292670559030358\n",
      "    At iteration 12400 -> loss: 0.0929891604002763\n",
      "    At iteration 12500 -> loss: 0.09296514804971852\n",
      "    At iteration 12600 -> loss: 0.09299230616356226\n",
      "    At iteration 12700 -> loss: 0.09298225309197815\n",
      "    At iteration 12800 -> loss: 0.09296742568453119\n",
      "    At iteration 12900 -> loss: 0.09294775232992004\n",
      "    At iteration 13000 -> loss: 0.09292730981617901\n",
      "    At iteration 13100 -> loss: 0.0930110901470189\n",
      "    At iteration 13200 -> loss: 0.09300773238405861\n",
      "    At iteration 13300 -> loss: 0.09308899543734792\n",
      "    At iteration 13400 -> loss: 0.0930694700826857\n",
      "    At iteration 13500 -> loss: 0.09303681978757519\n",
      "    At iteration 13600 -> loss: 0.09302676000911418\n",
      "Staring Epoch 95\n",
      "    At iteration 0 -> loss: 0.08459651912562549\n",
      "    At iteration 100 -> loss: 0.0971586176825611\n",
      "    At iteration 200 -> loss: 0.09463275326682173\n",
      "    At iteration 300 -> loss: 0.09329721204557298\n",
      "    At iteration 400 -> loss: 0.09329092993193437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 500 -> loss: 0.09275519188133019\n",
      "    At iteration 600 -> loss: 0.09289895233970111\n",
      "    At iteration 700 -> loss: 0.09349211901779376\n",
      "    At iteration 800 -> loss: 0.0933465832949131\n",
      "    At iteration 900 -> loss: 0.09307984871770751\n",
      "    At iteration 1000 -> loss: 0.09313642676303334\n",
      "    At iteration 1100 -> loss: 0.093173186677441\n",
      "    At iteration 1200 -> loss: 0.0931721988657786\n",
      "    At iteration 1300 -> loss: 0.0930326213249231\n",
      "    At iteration 1400 -> loss: 0.0935909511927742\n",
      "    At iteration 1500 -> loss: 0.09337455362641466\n",
      "    At iteration 1600 -> loss: 0.09308577374852581\n",
      "    At iteration 1700 -> loss: 0.09324376396173041\n",
      "    At iteration 1800 -> loss: 0.09326760157208401\n",
      "    At iteration 1900 -> loss: 0.09307078919521007\n",
      "    At iteration 2000 -> loss: 0.09299115100880975\n",
      "    At iteration 2100 -> loss: 0.09308669734875917\n",
      "    At iteration 2200 -> loss: 0.09293248302202967\n",
      "    At iteration 2300 -> loss: 0.09276016478663247\n",
      "    At iteration 2400 -> loss: 0.09286457271066642\n",
      "    At iteration 2500 -> loss: 0.09283245573949034\n",
      "    At iteration 2600 -> loss: 0.0927542839048306\n",
      "    At iteration 2700 -> loss: 0.09282422622499861\n",
      "    At iteration 2800 -> loss: 0.09271401259771345\n",
      "    At iteration 2900 -> loss: 0.09262319047599298\n",
      "    At iteration 3000 -> loss: 0.0926209242467194\n",
      "    At iteration 3100 -> loss: 0.09278282405428881\n",
      "    At iteration 3200 -> loss: 0.09276850762771041\n",
      "    At iteration 3300 -> loss: 0.09281052793533309\n",
      "    At iteration 3400 -> loss: 0.09294424694717258\n",
      "    At iteration 3500 -> loss: 0.09279714934982576\n",
      "    At iteration 3600 -> loss: 0.09275035372330899\n",
      "    At iteration 3700 -> loss: 0.09296000046550311\n",
      "    At iteration 3800 -> loss: 0.09292126591424076\n",
      "    At iteration 3900 -> loss: 0.09280053835637829\n",
      "    At iteration 4000 -> loss: 0.09270558611570229\n",
      "    At iteration 4100 -> loss: 0.09268057003513042\n",
      "    At iteration 4200 -> loss: 0.0927425942483266\n",
      "    At iteration 4300 -> loss: 0.09280856613443664\n",
      "    At iteration 4400 -> loss: 0.09280434916822361\n",
      "    At iteration 4500 -> loss: 0.0928108111617725\n",
      "    At iteration 4600 -> loss: 0.09280633340524544\n",
      "    At iteration 4700 -> loss: 0.09277950189638633\n",
      "    At iteration 4800 -> loss: 0.09272440544591132\n",
      "    At iteration 4900 -> loss: 0.09266523064345046\n",
      "    At iteration 5000 -> loss: 0.0927258519108232\n",
      "    At iteration 5100 -> loss: 0.09263021017261562\n",
      "    At iteration 5200 -> loss: 0.09268967159616784\n",
      "    At iteration 5300 -> loss: 0.09275641768769927\n",
      "    At iteration 5400 -> loss: 0.09278310409873443\n",
      "    At iteration 5500 -> loss: 0.09274432279589861\n",
      "    At iteration 5600 -> loss: 0.09273281042641639\n",
      "    At iteration 5700 -> loss: 0.09267869007838479\n",
      "    At iteration 5800 -> loss: 0.09266029893694977\n",
      "    At iteration 5900 -> loss: 0.09264572944431917\n",
      "    At iteration 6000 -> loss: 0.09266971301051348\n",
      "    At iteration 6100 -> loss: 0.09268779260459194\n",
      "    At iteration 6200 -> loss: 0.09266543783800102\n",
      "    At iteration 6300 -> loss: 0.09267647705756384\n",
      "    At iteration 6400 -> loss: 0.0927784667287132\n",
      "    At iteration 6500 -> loss: 0.0927420235908259\n",
      "    At iteration 6600 -> loss: 0.0927399583403279\n",
      "    At iteration 6700 -> loss: 0.092793722656603\n",
      "    At iteration 6800 -> loss: 0.09273615097780652\n",
      "    At iteration 6900 -> loss: 0.09278011307948755\n",
      "    At iteration 7000 -> loss: 0.09289856471163317\n",
      "    At iteration 7100 -> loss: 0.09292810660399911\n",
      "    At iteration 7200 -> loss: 0.09292007048021284\n",
      "    At iteration 7300 -> loss: 0.09287613893862944\n",
      "    At iteration 7400 -> loss: 0.09288347127011073\n",
      "    At iteration 7500 -> loss: 0.09290686627523675\n",
      "    At iteration 7600 -> loss: 0.09299419772132582\n",
      "    At iteration 7700 -> loss: 0.09297442509585162\n",
      "    At iteration 7800 -> loss: 0.09297446302669837\n",
      "    At iteration 7900 -> loss: 0.09322251946987252\n",
      "    At iteration 8000 -> loss: 0.09319250669106982\n",
      "    At iteration 8100 -> loss: 0.0931471020694672\n",
      "    At iteration 8200 -> loss: 0.09312763105767295\n",
      "    At iteration 8300 -> loss: 0.09317652625101612\n",
      "    At iteration 8400 -> loss: 0.09314849590424024\n",
      "    At iteration 8500 -> loss: 0.09311992573556879\n",
      "    At iteration 8600 -> loss: 0.09306115694319685\n",
      "    At iteration 8700 -> loss: 0.09305197614921618\n",
      "    At iteration 8800 -> loss: 0.09304178725805534\n",
      "    At iteration 8900 -> loss: 0.09299982592233028\n",
      "    At iteration 9000 -> loss: 0.09295406796951451\n",
      "    At iteration 9100 -> loss: 0.09293893582295912\n",
      "    At iteration 9200 -> loss: 0.09293618279578952\n",
      "    At iteration 9300 -> loss: 0.09291951204492924\n",
      "    At iteration 9400 -> loss: 0.09291666954388897\n",
      "    At iteration 9500 -> loss: 0.09287241840259047\n",
      "    At iteration 9600 -> loss: 0.09288564764446273\n",
      "    At iteration 9700 -> loss: 0.09287950229158454\n",
      "    At iteration 9800 -> loss: 0.09287257440299877\n",
      "    At iteration 9900 -> loss: 0.09285303929556314\n",
      "    At iteration 10000 -> loss: 0.09288426312577854\n",
      "    At iteration 10100 -> loss: 0.09295817044115208\n",
      "    At iteration 10200 -> loss: 0.0929322969708857\n",
      "    At iteration 10300 -> loss: 0.09291712691539884\n",
      "    At iteration 10400 -> loss: 0.09300322558011356\n",
      "    At iteration 10500 -> loss: 0.09296266911778894\n",
      "    At iteration 10600 -> loss: 0.09294911551187458\n",
      "    At iteration 10700 -> loss: 0.09293260053747936\n",
      "    At iteration 10800 -> loss: 0.09294683250103931\n",
      "    At iteration 10900 -> loss: 0.09293053044506216\n",
      "    At iteration 11000 -> loss: 0.092933692057871\n",
      "    At iteration 11100 -> loss: 0.09294952719532093\n",
      "    At iteration 11200 -> loss: 0.09308341603837583\n",
      "    At iteration 11300 -> loss: 0.09304774206393125\n",
      "    At iteration 11400 -> loss: 0.09299994736339999\n",
      "    At iteration 11500 -> loss: 0.09295079348019744\n",
      "    At iteration 11600 -> loss: 0.0929338375302036\n",
      "    At iteration 11700 -> loss: 0.09292412813545403\n",
      "    At iteration 11800 -> loss: 0.09293058026513615\n",
      "    At iteration 11900 -> loss: 0.09292627162753703\n",
      "    At iteration 12000 -> loss: 0.09291311349966201\n",
      "    At iteration 12100 -> loss: 0.09289625293481688\n",
      "    At iteration 12200 -> loss: 0.09288637701020053\n",
      "    At iteration 12300 -> loss: 0.09289647264562106\n",
      "    At iteration 12400 -> loss: 0.09288636534136095\n",
      "    At iteration 12500 -> loss: 0.09288803627451139\n",
      "    At iteration 12600 -> loss: 0.09290264097546388\n",
      "    At iteration 12700 -> loss: 0.09291315227849778\n",
      "    At iteration 12800 -> loss: 0.09319070721898526\n",
      "    At iteration 12900 -> loss: 0.09320392482683172\n",
      "    At iteration 13000 -> loss: 0.09316715730836966\n",
      "    At iteration 13100 -> loss: 0.09314325587591792\n",
      "    At iteration 13200 -> loss: 0.09315281919457909\n",
      "    At iteration 13300 -> loss: 0.09315334645418978\n",
      "    At iteration 13400 -> loss: 0.0931150539474402\n",
      "    At iteration 13500 -> loss: 0.09308171391179602\n",
      "    At iteration 13600 -> loss: 0.09305290424454458\n",
      "Staring Epoch 96\n",
      "    At iteration 0 -> loss: 0.08128452397068031\n",
      "    At iteration 100 -> loss: 0.09796921859696461\n",
      "    At iteration 200 -> loss: 0.09430781419858843\n",
      "    At iteration 300 -> loss: 0.09458520618506999\n",
      "    At iteration 400 -> loss: 0.09357562959128732\n",
      "    At iteration 500 -> loss: 0.09325353911191488\n",
      "    At iteration 600 -> loss: 0.09332312263581355\n",
      "    At iteration 700 -> loss: 0.09538109562257584\n",
      "    At iteration 800 -> loss: 0.09468364332243903\n",
      "    At iteration 900 -> loss: 0.09439908031488092\n",
      "    At iteration 1000 -> loss: 0.09405540916160593\n",
      "    At iteration 1100 -> loss: 0.09401544112860931\n",
      "    At iteration 1200 -> loss: 0.0938817388271549\n",
      "    At iteration 1300 -> loss: 0.0934561638053167\n",
      "    At iteration 1400 -> loss: 0.0932552821703139\n",
      "    At iteration 1500 -> loss: 0.09352241071632082\n",
      "    At iteration 1600 -> loss: 0.09350067149088244\n",
      "    At iteration 1700 -> loss: 0.09339023264339379\n",
      "    At iteration 1800 -> loss: 0.0932875249341365\n",
      "    At iteration 1900 -> loss: 0.09320145775626237\n",
      "    At iteration 2000 -> loss: 0.09307978979580657\n",
      "    At iteration 2100 -> loss: 0.09330171587755719\n",
      "    At iteration 2200 -> loss: 0.09309963485797583\n",
      "    At iteration 2300 -> loss: 0.09301890007971325\n",
      "    At iteration 2400 -> loss: 0.09291407426222219\n",
      "    At iteration 2500 -> loss: 0.09292928818951683\n",
      "    At iteration 2600 -> loss: 0.09339165287440829\n",
      "    At iteration 2700 -> loss: 0.09329608110456586\n",
      "    At iteration 2800 -> loss: 0.09322931720437717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2900 -> loss: 0.09319207062990686\n",
      "    At iteration 3000 -> loss: 0.09318265593540327\n",
      "    At iteration 3100 -> loss: 0.09318199597028863\n",
      "    At iteration 3200 -> loss: 0.09406910278535831\n",
      "    At iteration 3300 -> loss: 0.09399100518804614\n",
      "    At iteration 3400 -> loss: 0.0938422539470535\n",
      "    At iteration 3500 -> loss: 0.09386691469074339\n",
      "    At iteration 3600 -> loss: 0.0937107728415091\n",
      "    At iteration 3700 -> loss: 0.09370889749124188\n",
      "    At iteration 3800 -> loss: 0.09364515339364327\n",
      "    At iteration 3900 -> loss: 0.09357956098240823\n",
      "    At iteration 4000 -> loss: 0.0935986545954692\n",
      "    At iteration 4100 -> loss: 0.09348590893955694\n",
      "    At iteration 4200 -> loss: 0.0934333376795179\n",
      "    At iteration 4300 -> loss: 0.09347071323113486\n",
      "    At iteration 4400 -> loss: 0.09341527751231786\n",
      "    At iteration 4500 -> loss: 0.09336123921155168\n",
      "    At iteration 4600 -> loss: 0.09324630720265847\n",
      "    At iteration 4700 -> loss: 0.09319904976376191\n",
      "    At iteration 4800 -> loss: 0.09315398080345062\n",
      "    At iteration 4900 -> loss: 0.09307594290026151\n",
      "    At iteration 5000 -> loss: 0.09303431986030043\n",
      "    At iteration 5100 -> loss: 0.09310428530085528\n",
      "    At iteration 5200 -> loss: 0.0931154609193786\n",
      "    At iteration 5300 -> loss: 0.09311405266130571\n",
      "    At iteration 5400 -> loss: 0.09310175013338626\n",
      "    At iteration 5500 -> loss: 0.09302417886773079\n",
      "    At iteration 5600 -> loss: 0.09297590539520245\n",
      "    At iteration 5700 -> loss: 0.09295008771811628\n",
      "    At iteration 5800 -> loss: 0.09291850602764389\n",
      "    At iteration 5900 -> loss: 0.09291365630849131\n",
      "    At iteration 6000 -> loss: 0.09291493082273695\n",
      "    At iteration 6100 -> loss: 0.09288936465047454\n",
      "    At iteration 6200 -> loss: 0.09286104980365899\n",
      "    At iteration 6300 -> loss: 0.09280674665318724\n",
      "    At iteration 6400 -> loss: 0.09280312257000896\n",
      "    At iteration 6500 -> loss: 0.09280386434940172\n",
      "    At iteration 6600 -> loss: 0.0928916639378812\n",
      "    At iteration 6700 -> loss: 0.09284916954729779\n",
      "    At iteration 6800 -> loss: 0.09279749710495065\n",
      "    At iteration 6900 -> loss: 0.09277581783783624\n",
      "    At iteration 7000 -> loss: 0.09282805511344\n",
      "    At iteration 7100 -> loss: 0.09278203274765334\n",
      "    At iteration 7200 -> loss: 0.09280615855867429\n",
      "    At iteration 7300 -> loss: 0.09290278101409005\n",
      "    At iteration 7400 -> loss: 0.09285462236448229\n",
      "    At iteration 7500 -> loss: 0.0928110987101712\n",
      "    At iteration 7600 -> loss: 0.09284759220093398\n",
      "    At iteration 7700 -> loss: 0.09286847285430529\n",
      "    At iteration 7800 -> loss: 0.09287778806883558\n",
      "    At iteration 7900 -> loss: 0.09284960798623587\n",
      "    At iteration 8000 -> loss: 0.09281249718632929\n",
      "    At iteration 8100 -> loss: 0.09282774148300113\n",
      "    At iteration 8200 -> loss: 0.09285344977610482\n",
      "    At iteration 8300 -> loss: 0.09282254285959705\n",
      "    At iteration 8400 -> loss: 0.09282024049566341\n",
      "    At iteration 8500 -> loss: 0.09283895128949439\n",
      "    At iteration 8600 -> loss: 0.09284369179691793\n",
      "    At iteration 8700 -> loss: 0.0929118555464093\n",
      "    At iteration 8800 -> loss: 0.09289180593134062\n",
      "    At iteration 8900 -> loss: 0.09288612793307706\n",
      "    At iteration 9000 -> loss: 0.09290747520821648\n",
      "    At iteration 9100 -> loss: 0.09289738020785895\n",
      "    At iteration 9200 -> loss: 0.09290412821598365\n",
      "    At iteration 9300 -> loss: 0.09285712574848264\n",
      "    At iteration 9400 -> loss: 0.09290084542539802\n",
      "    At iteration 9500 -> loss: 0.09303005755202745\n",
      "    At iteration 9600 -> loss: 0.09303510371957233\n",
      "    At iteration 9700 -> loss: 0.09299686858779521\n",
      "    At iteration 9800 -> loss: 0.0929678359258611\n",
      "    At iteration 9900 -> loss: 0.09308620522423752\n",
      "    At iteration 10000 -> loss: 0.09307403724381429\n",
      "    At iteration 10100 -> loss: 0.09303964609659665\n",
      "    At iteration 10200 -> loss: 0.09303968360151221\n",
      "    At iteration 10300 -> loss: 0.09299170183470265\n",
      "    At iteration 10400 -> loss: 0.09296037491003309\n",
      "    At iteration 10500 -> loss: 0.0929548021983102\n",
      "    At iteration 10600 -> loss: 0.09306555990995423\n",
      "    At iteration 10700 -> loss: 0.09305615140625914\n",
      "    At iteration 10800 -> loss: 0.09308147796698683\n",
      "    At iteration 10900 -> loss: 0.09306780101212925\n",
      "    At iteration 11000 -> loss: 0.09303327422212831\n",
      "    At iteration 11100 -> loss: 0.09300083326390313\n",
      "    At iteration 11200 -> loss: 0.09299353908512063\n",
      "    At iteration 11300 -> loss: 0.09300234659496433\n",
      "    At iteration 11400 -> loss: 0.09299494423937521\n",
      "    At iteration 11500 -> loss: 0.09298242881066962\n",
      "    At iteration 11600 -> loss: 0.09296652761172397\n",
      "    At iteration 11700 -> loss: 0.09297667984389038\n",
      "    At iteration 11800 -> loss: 0.09296121856090692\n",
      "    At iteration 11900 -> loss: 0.09298672831380095\n",
      "    At iteration 12000 -> loss: 0.09297426242485297\n",
      "    At iteration 12100 -> loss: 0.09294037372820557\n",
      "    At iteration 12200 -> loss: 0.09291974012776494\n",
      "    At iteration 12300 -> loss: 0.09290040995213024\n",
      "    At iteration 12400 -> loss: 0.09290957254012398\n",
      "    At iteration 12500 -> loss: 0.09289269947122472\n",
      "    At iteration 12600 -> loss: 0.09294041120594393\n",
      "    At iteration 12700 -> loss: 0.09296135580110416\n",
      "    At iteration 12800 -> loss: 0.09294745158493274\n",
      "    At iteration 12900 -> loss: 0.09299738291230672\n",
      "    At iteration 13000 -> loss: 0.0929957344051165\n",
      "    At iteration 13100 -> loss: 0.09299579974138876\n",
      "    At iteration 13200 -> loss: 0.09298523356641862\n",
      "    At iteration 13300 -> loss: 0.09299050835285191\n",
      "    At iteration 13400 -> loss: 0.09304572059054421\n",
      "    At iteration 13500 -> loss: 0.09303104145674812\n",
      "    At iteration 13600 -> loss: 0.09300931399924274\n",
      "Staring Epoch 97\n",
      "    At iteration 0 -> loss: 0.08138827365473844\n",
      "    At iteration 100 -> loss: 0.09557474141815771\n",
      "    At iteration 200 -> loss: 0.09703233002012047\n",
      "    At iteration 300 -> loss: 0.09782119726626504\n",
      "    At iteration 400 -> loss: 0.09578188796833885\n",
      "    At iteration 500 -> loss: 0.0945537606294414\n",
      "    At iteration 600 -> loss: 0.09390037651708354\n",
      "    At iteration 700 -> loss: 0.09336076147378697\n",
      "    At iteration 800 -> loss: 0.09294590321174054\n",
      "    At iteration 900 -> loss: 0.09318440252658662\n",
      "    At iteration 1000 -> loss: 0.09296411861851037\n",
      "    At iteration 1100 -> loss: 0.09268913819705484\n",
      "    At iteration 1200 -> loss: 0.09284800647375253\n",
      "    At iteration 1300 -> loss: 0.09299992706974632\n",
      "    At iteration 1400 -> loss: 0.0929240372259092\n",
      "    At iteration 1500 -> loss: 0.09267809959403203\n",
      "    At iteration 1600 -> loss: 0.0926378851424059\n",
      "    At iteration 1700 -> loss: 0.09249451196433964\n",
      "    At iteration 1800 -> loss: 0.09293030031719766\n",
      "    At iteration 1900 -> loss: 0.09304015734421527\n",
      "    At iteration 2000 -> loss: 0.09285177873879835\n",
      "    At iteration 2100 -> loss: 0.09289241687482717\n",
      "    At iteration 2200 -> loss: 0.09282496284459471\n",
      "    At iteration 2300 -> loss: 0.09281618542350475\n",
      "    At iteration 2400 -> loss: 0.09278184354963932\n",
      "    At iteration 2500 -> loss: 0.09276287147728032\n",
      "    At iteration 2600 -> loss: 0.09264905859635159\n",
      "    At iteration 2700 -> loss: 0.09252779721820101\n",
      "    At iteration 2800 -> loss: 0.09267787921912939\n",
      "    At iteration 2900 -> loss: 0.09260942555876826\n",
      "    At iteration 3000 -> loss: 0.09250609950605501\n",
      "    At iteration 3100 -> loss: 0.09268517090893998\n",
      "    At iteration 3200 -> loss: 0.09256000298383373\n",
      "    At iteration 3300 -> loss: 0.09257087137516606\n",
      "    At iteration 3400 -> loss: 0.09261079608130121\n",
      "    At iteration 3500 -> loss: 0.09254200332493166\n",
      "    At iteration 3600 -> loss: 0.09260887560691801\n",
      "    At iteration 3700 -> loss: 0.09252545921168089\n",
      "    At iteration 3800 -> loss: 0.09248907923828507\n",
      "    At iteration 3900 -> loss: 0.09243190077993324\n",
      "    At iteration 4000 -> loss: 0.0925489185613805\n",
      "    At iteration 4100 -> loss: 0.09249142090003049\n",
      "    At iteration 4200 -> loss: 0.09255257221837608\n",
      "    At iteration 4300 -> loss: 0.09247997359960543\n",
      "    At iteration 4400 -> loss: 0.09250373295969698\n",
      "    At iteration 4500 -> loss: 0.0924894351475294\n",
      "    At iteration 4600 -> loss: 0.092596948405502\n",
      "    At iteration 4700 -> loss: 0.09259332303628291\n",
      "    At iteration 4800 -> loss: 0.0932283574183235\n",
      "    At iteration 4900 -> loss: 0.09320061023595241\n",
      "    At iteration 5000 -> loss: 0.09313102279122494\n",
      "    At iteration 5100 -> loss: 0.09303676094691178\n",
      "    At iteration 5200 -> loss: 0.09300097850042925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5300 -> loss: 0.09303814395858247\n",
      "    At iteration 5400 -> loss: 0.09301423081263561\n",
      "    At iteration 5500 -> loss: 0.09299001336801604\n",
      "    At iteration 5600 -> loss: 0.09302285809038024\n",
      "    At iteration 5700 -> loss: 0.09296224124983736\n",
      "    At iteration 5800 -> loss: 0.09297005793485205\n",
      "    At iteration 5900 -> loss: 0.09296084849406333\n",
      "    At iteration 6000 -> loss: 0.09291119828938707\n",
      "    At iteration 6100 -> loss: 0.09298139061705067\n",
      "    At iteration 6200 -> loss: 0.09306240953429394\n",
      "    At iteration 6300 -> loss: 0.09307346450846767\n",
      "    At iteration 6400 -> loss: 0.09318679305925646\n",
      "    At iteration 6500 -> loss: 0.09314599538957048\n",
      "    At iteration 6600 -> loss: 0.09307546666407629\n",
      "    At iteration 6700 -> loss: 0.0930951320178835\n",
      "    At iteration 6800 -> loss: 0.09305687979179282\n",
      "    At iteration 6900 -> loss: 0.09308702627008134\n",
      "    At iteration 7000 -> loss: 0.09301102010685235\n",
      "    At iteration 7100 -> loss: 0.0930101924168421\n",
      "    At iteration 7200 -> loss: 0.0930340606795577\n",
      "    At iteration 7300 -> loss: 0.09310600960253199\n",
      "    At iteration 7400 -> loss: 0.0930567541497279\n",
      "    At iteration 7500 -> loss: 0.09316098363888542\n",
      "    At iteration 7600 -> loss: 0.0931259521273615\n",
      "    At iteration 7700 -> loss: 0.0931537285184511\n",
      "    At iteration 7800 -> loss: 0.09322614400282228\n",
      "    At iteration 7900 -> loss: 0.09320094891090393\n",
      "    At iteration 8000 -> loss: 0.09316459321097605\n",
      "    At iteration 8100 -> loss: 0.0931248795426845\n",
      "    At iteration 8200 -> loss: 0.09309308312877354\n",
      "    At iteration 8300 -> loss: 0.09306509453165569\n",
      "    At iteration 8400 -> loss: 0.09316563089709592\n",
      "    At iteration 8500 -> loss: 0.0931214338092377\n",
      "    At iteration 8600 -> loss: 0.09316628570844025\n",
      "    At iteration 8700 -> loss: 0.09316367036278533\n",
      "    At iteration 8800 -> loss: 0.09317074306543568\n",
      "    At iteration 8900 -> loss: 0.09318101662913345\n",
      "    At iteration 9000 -> loss: 0.09325592726122374\n",
      "    At iteration 9100 -> loss: 0.0932149890732952\n",
      "    At iteration 9200 -> loss: 0.09318908508731051\n",
      "    At iteration 9300 -> loss: 0.09321763599966951\n",
      "    At iteration 9400 -> loss: 0.09331815414300762\n",
      "    At iteration 9500 -> loss: 0.09328868419578028\n",
      "    At iteration 9600 -> loss: 0.09327917766807202\n",
      "    At iteration 9700 -> loss: 0.09328763201876888\n",
      "    At iteration 9800 -> loss: 0.09327224939122909\n",
      "    At iteration 9900 -> loss: 0.09322538940069057\n",
      "    At iteration 10000 -> loss: 0.0932234576804379\n",
      "    At iteration 10100 -> loss: 0.09317808436862429\n",
      "    At iteration 10200 -> loss: 0.09319221457884841\n",
      "    At iteration 10300 -> loss: 0.09325644641447174\n",
      "    At iteration 10400 -> loss: 0.09327964301455964\n",
      "    At iteration 10500 -> loss: 0.09329791830771583\n",
      "    At iteration 10600 -> loss: 0.09329715637844425\n",
      "    At iteration 10700 -> loss: 0.09327336336278295\n",
      "    At iteration 10800 -> loss: 0.09325034772432902\n",
      "    At iteration 10900 -> loss: 0.09321754310455371\n",
      "    At iteration 11000 -> loss: 0.09321537133545951\n",
      "    At iteration 11100 -> loss: 0.09321297559559547\n",
      "    At iteration 11200 -> loss: 0.09322703639842275\n",
      "    At iteration 11300 -> loss: 0.09321100198883228\n",
      "    At iteration 11400 -> loss: 0.09319240183192702\n",
      "    At iteration 11500 -> loss: 0.09317668787557444\n",
      "    At iteration 11600 -> loss: 0.09315148200122875\n",
      "    At iteration 11700 -> loss: 0.09313926961988551\n",
      "    At iteration 11800 -> loss: 0.09314457541620688\n",
      "    At iteration 11900 -> loss: 0.09314109645676023\n",
      "    At iteration 12000 -> loss: 0.09314443109226717\n",
      "    At iteration 12100 -> loss: 0.09311837547646011\n",
      "    At iteration 12200 -> loss: 0.09320341774431463\n",
      "    At iteration 12300 -> loss: 0.09319770376880199\n",
      "    At iteration 12400 -> loss: 0.09319343703976843\n",
      "    At iteration 12500 -> loss: 0.09317446860377569\n",
      "    At iteration 12600 -> loss: 0.09316505839144337\n",
      "    At iteration 12700 -> loss: 0.09312747616588585\n",
      "    At iteration 12800 -> loss: 0.09315159469611042\n",
      "    At iteration 12900 -> loss: 0.09314240952263\n",
      "    At iteration 13000 -> loss: 0.093126340762733\n",
      "    At iteration 13100 -> loss: 0.09310801285072769\n",
      "    At iteration 13200 -> loss: 0.09308053188433187\n",
      "    At iteration 13300 -> loss: 0.0930613738901647\n",
      "    At iteration 13400 -> loss: 0.09303688544559348\n",
      "    At iteration 13500 -> loss: 0.09305556986379855\n",
      "    At iteration 13600 -> loss: 0.09303107871020871\n",
      "Staring Epoch 98\n",
      "    At iteration 0 -> loss: 0.08016608202160569\n",
      "    At iteration 100 -> loss: 0.09254133339687196\n",
      "    At iteration 200 -> loss: 0.09170447751123723\n",
      "    At iteration 300 -> loss: 0.09284823108408233\n",
      "    At iteration 400 -> loss: 0.09210294186986755\n",
      "    At iteration 500 -> loss: 0.0918667405008378\n",
      "    At iteration 600 -> loss: 0.09165890035076737\n",
      "    At iteration 700 -> loss: 0.09178451050108312\n",
      "    At iteration 800 -> loss: 0.09139697601093055\n",
      "    At iteration 900 -> loss: 0.09159733797001475\n",
      "    At iteration 1000 -> loss: 0.09183882875348533\n",
      "    At iteration 1100 -> loss: 0.09178076968039142\n",
      "    At iteration 1200 -> loss: 0.09164947582289613\n",
      "    At iteration 1300 -> loss: 0.09175205794001505\n",
      "    At iteration 1400 -> loss: 0.09168971266795768\n",
      "    At iteration 1500 -> loss: 0.0921876721040033\n",
      "    At iteration 1600 -> loss: 0.09234550601082002\n",
      "    At iteration 1700 -> loss: 0.09261130475073033\n",
      "    At iteration 1800 -> loss: 0.09274921292095921\n",
      "    At iteration 1900 -> loss: 0.09271775481887154\n",
      "    At iteration 2000 -> loss: 0.09268013911843645\n",
      "    At iteration 2100 -> loss: 0.09259657782392416\n",
      "    At iteration 2200 -> loss: 0.09248399173443396\n",
      "    At iteration 2300 -> loss: 0.09249799312288777\n",
      "    At iteration 2400 -> loss: 0.09260278666610308\n",
      "    At iteration 2500 -> loss: 0.09263767215426845\n",
      "    At iteration 2600 -> loss: 0.0929063653122767\n",
      "    At iteration 2700 -> loss: 0.09284934804526072\n",
      "    At iteration 2800 -> loss: 0.0928259524858042\n",
      "    At iteration 2900 -> loss: 0.09289538996505448\n",
      "    At iteration 3000 -> loss: 0.09301397170251807\n",
      "    At iteration 3100 -> loss: 0.09291246113414851\n",
      "    At iteration 3200 -> loss: 0.092871356906436\n",
      "    At iteration 3300 -> loss: 0.09278594199036301\n",
      "    At iteration 3400 -> loss: 0.09281896003887767\n",
      "    At iteration 3500 -> loss: 0.09282505601463026\n",
      "    At iteration 3600 -> loss: 0.09271166692817887\n",
      "    At iteration 3700 -> loss: 0.09264276904611357\n",
      "    At iteration 3800 -> loss: 0.09265953883500923\n",
      "    At iteration 3900 -> loss: 0.09263165738129336\n",
      "    At iteration 4000 -> loss: 0.09274623151250026\n",
      "    At iteration 4100 -> loss: 0.0928791498435036\n",
      "    At iteration 4200 -> loss: 0.09279717423552389\n",
      "    At iteration 4300 -> loss: 0.09279798922597783\n",
      "    At iteration 4400 -> loss: 0.09285535400704459\n",
      "    At iteration 4500 -> loss: 0.09290617746683845\n",
      "    At iteration 4600 -> loss: 0.09285357403021871\n",
      "    At iteration 4700 -> loss: 0.09304622126819552\n",
      "    At iteration 4800 -> loss: 0.09298372749465378\n",
      "    At iteration 4900 -> loss: 0.09288307977880897\n",
      "    At iteration 5000 -> loss: 0.09285121265115374\n",
      "    At iteration 5100 -> loss: 0.09285100649184942\n",
      "    At iteration 5200 -> loss: 0.09313042739925421\n",
      "    At iteration 5300 -> loss: 0.09306260112953789\n",
      "    At iteration 5400 -> loss: 0.09302861486793597\n",
      "    At iteration 5500 -> loss: 0.09300436897730177\n",
      "    At iteration 5600 -> loss: 0.09295949707657543\n",
      "    At iteration 5700 -> loss: 0.09293105955800647\n",
      "    At iteration 5800 -> loss: 0.09286970180701781\n",
      "    At iteration 5900 -> loss: 0.09284098721570674\n",
      "    At iteration 6000 -> loss: 0.09282312685559005\n",
      "    At iteration 6100 -> loss: 0.09277095364811674\n",
      "    At iteration 6200 -> loss: 0.09274275048854919\n",
      "    At iteration 6300 -> loss: 0.09275502505584521\n",
      "    At iteration 6400 -> loss: 0.09275155348236863\n",
      "    At iteration 6500 -> loss: 0.09277182427721607\n",
      "    At iteration 6600 -> loss: 0.0927341786281632\n",
      "    At iteration 6700 -> loss: 0.09272808813763508\n",
      "    At iteration 6800 -> loss: 0.09271416226432017\n",
      "    At iteration 6900 -> loss: 0.0926852691013993\n",
      "    At iteration 7000 -> loss: 0.09265768372367034\n",
      "    At iteration 7100 -> loss: 0.09263490033140137\n",
      "    At iteration 7200 -> loss: 0.09261669549440546\n",
      "    At iteration 7300 -> loss: 0.09268412896228526\n",
      "    At iteration 7400 -> loss: 0.09265627044689105\n",
      "    At iteration 7500 -> loss: 0.09262039563724937\n",
      "    At iteration 7600 -> loss: 0.09262044007435837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7700 -> loss: 0.09261021078818253\n",
      "    At iteration 7800 -> loss: 0.09256939793023244\n",
      "    At iteration 7900 -> loss: 0.09257444006052758\n",
      "    At iteration 8000 -> loss: 0.09292441772413473\n",
      "    At iteration 8100 -> loss: 0.09288457136984245\n",
      "    At iteration 8200 -> loss: 0.0928590131362435\n",
      "    At iteration 8300 -> loss: 0.0928780260096531\n",
      "    At iteration 8400 -> loss: 0.0928855328216886\n",
      "    At iteration 8500 -> loss: 0.09285246939298898\n",
      "    At iteration 8600 -> loss: 0.09283383487080642\n",
      "    At iteration 8700 -> loss: 0.09290477590581815\n",
      "    At iteration 8800 -> loss: 0.09305066779132058\n",
      "    At iteration 8900 -> loss: 0.0930235753025604\n",
      "    At iteration 9000 -> loss: 0.09302742746510559\n",
      "    At iteration 9100 -> loss: 0.0930253351431899\n",
      "    At iteration 9200 -> loss: 0.09303965801704091\n",
      "    At iteration 9300 -> loss: 0.09304426929040924\n",
      "    At iteration 9400 -> loss: 0.09306259811942523\n",
      "    At iteration 9500 -> loss: 0.09302494581879502\n",
      "    At iteration 9600 -> loss: 0.0930768205341483\n",
      "    At iteration 9700 -> loss: 0.09307012262585293\n",
      "    At iteration 9800 -> loss: 0.09305931176698393\n",
      "    At iteration 9900 -> loss: 0.09302749359805704\n",
      "    At iteration 10000 -> loss: 0.092990207047908\n",
      "    At iteration 10100 -> loss: 0.09301721089510015\n",
      "    At iteration 10200 -> loss: 0.09309082860447156\n",
      "    At iteration 10300 -> loss: 0.09305090648055346\n",
      "    At iteration 10400 -> loss: 0.09303728103192802\n",
      "    At iteration 10500 -> loss: 0.0929921005245444\n",
      "    At iteration 10600 -> loss: 0.09297141930251783\n",
      "    At iteration 10700 -> loss: 0.092967377046376\n",
      "    At iteration 10800 -> loss: 0.0929423953756712\n",
      "    At iteration 10900 -> loss: 0.09299602004130891\n",
      "    At iteration 11000 -> loss: 0.09298786700701418\n",
      "    At iteration 11100 -> loss: 0.09296907016311518\n",
      "    At iteration 11200 -> loss: 0.092932695130219\n",
      "    At iteration 11300 -> loss: 0.0929257964557424\n",
      "    At iteration 11400 -> loss: 0.09292405168999411\n",
      "    At iteration 11500 -> loss: 0.09293203381371994\n",
      "    At iteration 11600 -> loss: 0.0929154849287264\n",
      "    At iteration 11700 -> loss: 0.09291639890543862\n",
      "    At iteration 11800 -> loss: 0.09296952668933939\n",
      "    At iteration 11900 -> loss: 0.09297322433747757\n",
      "    At iteration 12000 -> loss: 0.09299470861034269\n",
      "    At iteration 12100 -> loss: 0.09303225273481432\n",
      "    At iteration 12200 -> loss: 0.09301217051907688\n",
      "    At iteration 12300 -> loss: 0.0929898664626662\n",
      "    At iteration 12400 -> loss: 0.09295139889119823\n",
      "    At iteration 12500 -> loss: 0.09297715387418624\n",
      "    At iteration 12600 -> loss: 0.09298531049743741\n",
      "    At iteration 12700 -> loss: 0.0929663141053457\n",
      "    At iteration 12800 -> loss: 0.09300778985242882\n",
      "    At iteration 12900 -> loss: 0.09304425474452221\n",
      "    At iteration 13000 -> loss: 0.09301362249835174\n",
      "    At iteration 13100 -> loss: 0.09301620263059539\n",
      "    At iteration 13200 -> loss: 0.09302090448520567\n",
      "    At iteration 13300 -> loss: 0.09300237859423854\n",
      "    At iteration 13400 -> loss: 0.0929846308100474\n",
      "    At iteration 13500 -> loss: 0.09298376827074482\n",
      "    At iteration 13600 -> loss: 0.09300088756955391\n",
      "Staring Epoch 99\n",
      "    At iteration 0 -> loss: 0.08020103581657168\n",
      "    At iteration 100 -> loss: 0.09076829981325116\n",
      "    At iteration 200 -> loss: 0.09500975541025711\n",
      "    At iteration 300 -> loss: 0.09278326306971854\n",
      "    At iteration 400 -> loss: 0.09206155884202259\n",
      "    At iteration 500 -> loss: 0.09182235440492344\n",
      "    At iteration 600 -> loss: 0.09194291175802781\n",
      "    At iteration 700 -> loss: 0.09215707921097616\n",
      "    At iteration 800 -> loss: 0.09264218248375695\n",
      "    At iteration 900 -> loss: 0.09225211048060507\n",
      "    At iteration 1000 -> loss: 0.09211076753889355\n",
      "    At iteration 1100 -> loss: 0.09210219608080226\n",
      "    At iteration 1200 -> loss: 0.09268178810690515\n",
      "    At iteration 1300 -> loss: 0.09296315711945018\n",
      "    At iteration 1400 -> loss: 0.09380754271329052\n",
      "    At iteration 1500 -> loss: 0.09364492162880239\n",
      "    At iteration 1600 -> loss: 0.09357555955712442\n",
      "    At iteration 1700 -> loss: 0.0934432475069177\n",
      "    At iteration 1800 -> loss: 0.09322767299734669\n",
      "    At iteration 1900 -> loss: 0.09302573280284278\n",
      "    At iteration 2000 -> loss: 0.09308440678946414\n",
      "    At iteration 2100 -> loss: 0.09302817616065895\n",
      "    At iteration 2200 -> loss: 0.0928243712370034\n",
      "    At iteration 2300 -> loss: 0.09290642722536858\n",
      "    At iteration 2400 -> loss: 0.09276782111407027\n",
      "    At iteration 2500 -> loss: 0.09269372080443172\n",
      "    At iteration 2600 -> loss: 0.09255897971855177\n",
      "    At iteration 2700 -> loss: 0.09254160485230221\n",
      "    At iteration 2800 -> loss: 0.0925561635228872\n",
      "    At iteration 2900 -> loss: 0.09289198633311949\n",
      "    At iteration 3000 -> loss: 0.09293690300742292\n",
      "    At iteration 3100 -> loss: 0.0929295118453367\n",
      "    At iteration 3200 -> loss: 0.09281793376417584\n",
      "    At iteration 3300 -> loss: 0.09285732675142998\n",
      "    At iteration 3400 -> loss: 0.09285425841138488\n",
      "    At iteration 3500 -> loss: 0.09273770711600407\n",
      "    At iteration 3600 -> loss: 0.0929656866231739\n",
      "    At iteration 3700 -> loss: 0.09292389459337988\n",
      "    At iteration 3800 -> loss: 0.09288529543186058\n",
      "    At iteration 3900 -> loss: 0.09280281279845466\n",
      "    At iteration 4000 -> loss: 0.09276488426888266\n",
      "    At iteration 4100 -> loss: 0.09276252538755485\n",
      "    At iteration 4200 -> loss: 0.09267171810612623\n",
      "    At iteration 4300 -> loss: 0.09271026620464709\n",
      "    At iteration 4400 -> loss: 0.09267146536113088\n",
      "    At iteration 4500 -> loss: 0.09277577355246162\n",
      "    At iteration 4600 -> loss: 0.09282091307146735\n",
      "    At iteration 4700 -> loss: 0.09278036240757019\n",
      "    At iteration 4800 -> loss: 0.09285593954528741\n",
      "    At iteration 4900 -> loss: 0.09281621933182871\n",
      "    At iteration 5000 -> loss: 0.09278712277840796\n",
      "    At iteration 5100 -> loss: 0.0928786379125414\n",
      "    At iteration 5200 -> loss: 0.09292981673152034\n",
      "    At iteration 5300 -> loss: 0.09291037846779909\n",
      "    At iteration 5400 -> loss: 0.09291616113306486\n",
      "    At iteration 5500 -> loss: 0.09286977630026463\n",
      "    At iteration 5600 -> loss: 0.0928258737388936\n",
      "    At iteration 5700 -> loss: 0.09283127924345345\n",
      "    At iteration 5800 -> loss: 0.092755763202347\n",
      "    At iteration 5900 -> loss: 0.09273252914640062\n",
      "    At iteration 6000 -> loss: 0.0927006264594596\n",
      "    At iteration 6100 -> loss: 0.09268314848584164\n",
      "    At iteration 6200 -> loss: 0.0927017986428848\n",
      "    At iteration 6300 -> loss: 0.09272500215304898\n",
      "    At iteration 6400 -> loss: 0.09285874335361093\n",
      "    At iteration 6500 -> loss: 0.09285447731791802\n",
      "    At iteration 6600 -> loss: 0.09284017523356337\n",
      "    At iteration 6700 -> loss: 0.09288131810655803\n",
      "    At iteration 6800 -> loss: 0.09283842191433639\n",
      "    At iteration 6900 -> loss: 0.09280028756473359\n",
      "    At iteration 7000 -> loss: 0.09280242677330283\n",
      "    At iteration 7100 -> loss: 0.09283313069904936\n",
      "    At iteration 7200 -> loss: 0.09283621825670786\n",
      "    At iteration 7300 -> loss: 0.0928288134025917\n",
      "    At iteration 7400 -> loss: 0.09285101720604215\n",
      "    At iteration 7500 -> loss: 0.09281200980542365\n",
      "    At iteration 7600 -> loss: 0.09278906646138384\n",
      "    At iteration 7700 -> loss: 0.0928863411802864\n",
      "    At iteration 7800 -> loss: 0.09286496321137673\n",
      "    At iteration 7900 -> loss: 0.09284085554976737\n",
      "    At iteration 8000 -> loss: 0.092803474576995\n",
      "    At iteration 8100 -> loss: 0.09281251514902565\n",
      "    At iteration 8200 -> loss: 0.09279975227089587\n",
      "    At iteration 8300 -> loss: 0.09281137905159892\n",
      "    At iteration 8400 -> loss: 0.0927761375309496\n",
      "    At iteration 8500 -> loss: 0.09277726773031784\n",
      "    At iteration 8600 -> loss: 0.09277721866698334\n",
      "    At iteration 8700 -> loss: 0.09274697708388074\n",
      "    At iteration 8800 -> loss: 0.09283645601466434\n",
      "    At iteration 8900 -> loss: 0.09287035938063766\n",
      "    At iteration 9000 -> loss: 0.09287027556377808\n",
      "    At iteration 9100 -> loss: 0.09284081838035346\n",
      "    At iteration 9200 -> loss: 0.09280666722789939\n",
      "    At iteration 9300 -> loss: 0.09281194450550449\n",
      "    At iteration 9400 -> loss: 0.09282917509394792\n",
      "    At iteration 9500 -> loss: 0.09282870389440324\n",
      "    At iteration 9600 -> loss: 0.09288873407798466\n",
      "    At iteration 9700 -> loss: 0.09289041195342429\n",
      "    At iteration 9800 -> loss: 0.09286674379156933\n",
      "    At iteration 9900 -> loss: 0.09288042801748933\n",
      "    At iteration 10000 -> loss: 0.09297024352123479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10100 -> loss: 0.09296269576099166\n",
      "    At iteration 10200 -> loss: 0.09297506799871601\n",
      "    At iteration 10300 -> loss: 0.09293694981114013\n",
      "    At iteration 10400 -> loss: 0.09294943391243435\n",
      "    At iteration 10500 -> loss: 0.09291075230953132\n",
      "    At iteration 10600 -> loss: 0.09291457411786737\n",
      "    At iteration 10700 -> loss: 0.09286967073359426\n",
      "    At iteration 10800 -> loss: 0.09289648607743148\n",
      "    At iteration 10900 -> loss: 0.09287003992965705\n",
      "    At iteration 11000 -> loss: 0.09286293495417083\n",
      "    At iteration 11100 -> loss: 0.0928408702868399\n",
      "    At iteration 11200 -> loss: 0.09281138661962936\n",
      "    At iteration 11300 -> loss: 0.0927947260330853\n",
      "    At iteration 11400 -> loss: 0.09279128456811782\n",
      "    At iteration 11500 -> loss: 0.09278048567747699\n",
      "    At iteration 11600 -> loss: 0.09287318662780468\n",
      "    At iteration 11700 -> loss: 0.09283894046364316\n",
      "    At iteration 11800 -> loss: 0.09284609200316257\n",
      "    At iteration 11900 -> loss: 0.09283939627445065\n",
      "    At iteration 12000 -> loss: 0.0928219872018802\n",
      "    At iteration 12100 -> loss: 0.0928514579172269\n",
      "    At iteration 12200 -> loss: 0.09283272089181745\n",
      "    At iteration 12300 -> loss: 0.09281579151868548\n",
      "    At iteration 12400 -> loss: 0.09280601955282318\n",
      "    At iteration 12500 -> loss: 0.09280017573476687\n",
      "    At iteration 12600 -> loss: 0.09285738030355271\n",
      "    At iteration 12700 -> loss: 0.09285124542580388\n",
      "    At iteration 12800 -> loss: 0.0928269054498215\n",
      "    At iteration 12900 -> loss: 0.09281507671274562\n",
      "    At iteration 13000 -> loss: 0.0927998638460194\n",
      "    At iteration 13100 -> loss: 0.092788780980151\n",
      "    At iteration 13200 -> loss: 0.0930314133365858\n",
      "    At iteration 13300 -> loss: 0.09303748812794227\n",
      "    At iteration 13400 -> loss: 0.09302351724875717\n",
      "    At iteration 13500 -> loss: 0.0930626859741884\n",
      "    At iteration 13600 -> loss: 0.09303541663652916\n",
      "Staring Epoch 100\n",
      "    At iteration 0 -> loss: 0.08208444068441167\n",
      "    At iteration 100 -> loss: 0.09232686145675846\n",
      "    At iteration 200 -> loss: 0.09071607530025108\n",
      "    At iteration 300 -> loss: 0.09107755370502404\n",
      "    At iteration 400 -> loss: 0.09084363267029792\n",
      "    At iteration 500 -> loss: 0.09153095884675802\n",
      "    At iteration 600 -> loss: 0.09330286628857198\n",
      "    At iteration 700 -> loss: 0.0927679790326251\n",
      "    At iteration 800 -> loss: 0.0927708494493017\n",
      "    At iteration 900 -> loss: 0.09255324807032166\n",
      "    At iteration 1000 -> loss: 0.09261590202827037\n",
      "    At iteration 1100 -> loss: 0.0924855444205742\n",
      "    At iteration 1200 -> loss: 0.09244849791996974\n",
      "    At iteration 1300 -> loss: 0.09365669001017048\n",
      "    At iteration 1400 -> loss: 0.09419240039636788\n",
      "    At iteration 1500 -> loss: 0.09398874017814307\n",
      "    At iteration 1600 -> loss: 0.09370638453936099\n",
      "    At iteration 1700 -> loss: 0.09357833016981992\n",
      "    At iteration 1800 -> loss: 0.09366837724520706\n",
      "    At iteration 1900 -> loss: 0.09354653832860266\n",
      "    At iteration 2000 -> loss: 0.0934750091400238\n",
      "    At iteration 2100 -> loss: 0.0932986158073793\n",
      "    At iteration 2200 -> loss: 0.09330862235648205\n",
      "    At iteration 2300 -> loss: 0.09331658219450531\n",
      "    At iteration 2400 -> loss: 0.0932151845917045\n",
      "    At iteration 2500 -> loss: 0.09315920277663169\n",
      "    At iteration 2600 -> loss: 0.09324220388642247\n",
      "    At iteration 2700 -> loss: 0.09316576040415758\n",
      "    At iteration 2800 -> loss: 0.09315024525484225\n",
      "    At iteration 2900 -> loss: 0.09334394289548081\n",
      "    At iteration 3000 -> loss: 0.09319527654952738\n",
      "    At iteration 3100 -> loss: 0.0931377717004419\n",
      "    At iteration 3200 -> loss: 0.09322527712529445\n",
      "    At iteration 3300 -> loss: 0.09317848794920791\n",
      "    At iteration 3400 -> loss: 0.09314981252954266\n",
      "    At iteration 3500 -> loss: 0.09303898072184744\n",
      "    At iteration 3600 -> loss: 0.0930880248176733\n",
      "    At iteration 3700 -> loss: 0.0930719211089408\n",
      "    At iteration 3800 -> loss: 0.09306562558855117\n",
      "    At iteration 3900 -> loss: 0.09300990382624716\n",
      "    At iteration 4000 -> loss: 0.09303560027703782\n",
      "    At iteration 4100 -> loss: 0.09305985367896037\n",
      "    At iteration 4200 -> loss: 0.09315811212199004\n",
      "    At iteration 4300 -> loss: 0.09309069926922324\n",
      "    At iteration 4400 -> loss: 0.09330749685731553\n",
      "    At iteration 4500 -> loss: 0.09328747347324724\n",
      "    At iteration 4600 -> loss: 0.09324102141017906\n",
      "    At iteration 4700 -> loss: 0.09314157698007133\n",
      "    At iteration 4800 -> loss: 0.09309188191210267\n",
      "    At iteration 4900 -> loss: 0.09310577418329795\n",
      "    At iteration 5000 -> loss: 0.09311658732284954\n",
      "    At iteration 5100 -> loss: 0.09317976896658949\n",
      "    At iteration 5200 -> loss: 0.09316633605072328\n",
      "    At iteration 5300 -> loss: 0.0933826690897954\n",
      "    At iteration 5400 -> loss: 0.0933283553750772\n",
      "    At iteration 5500 -> loss: 0.09326418611547271\n",
      "    At iteration 5600 -> loss: 0.09320649036665879\n",
      "    At iteration 5700 -> loss: 0.0931406640712136\n",
      "    At iteration 5800 -> loss: 0.09314475057831441\n",
      "    At iteration 5900 -> loss: 0.09310085459434102\n",
      "    At iteration 6000 -> loss: 0.0932175405140919\n",
      "    At iteration 6100 -> loss: 0.093142040603841\n",
      "    At iteration 6200 -> loss: 0.09313326697350588\n",
      "    At iteration 6300 -> loss: 0.0931153589652721\n",
      "    At iteration 6400 -> loss: 0.09306532974738047\n",
      "    At iteration 6500 -> loss: 0.09312315571958323\n",
      "    At iteration 6600 -> loss: 0.09309056033875948\n",
      "    At iteration 6700 -> loss: 0.09305068424979347\n",
      "    At iteration 6800 -> loss: 0.09302571061547557\n",
      "    At iteration 6900 -> loss: 0.09312659772780951\n",
      "    At iteration 7000 -> loss: 0.09307160253579064\n",
      "    At iteration 7100 -> loss: 0.09304573835433444\n",
      "    At iteration 7200 -> loss: 0.09297252591111524\n",
      "    At iteration 7300 -> loss: 0.09300070628912095\n",
      "    At iteration 7400 -> loss: 0.09296234297173586\n",
      "    At iteration 7500 -> loss: 0.0929613761520786\n",
      "    At iteration 7600 -> loss: 0.092910977264188\n",
      "    At iteration 7700 -> loss: 0.0929417464389447\n",
      "    At iteration 7800 -> loss: 0.09297644228288242\n",
      "    At iteration 7900 -> loss: 0.09295233672386624\n",
      "    At iteration 8000 -> loss: 0.09293645329967586\n",
      "    At iteration 8100 -> loss: 0.09291304689046118\n",
      "    At iteration 8200 -> loss: 0.09298407761732352\n",
      "    At iteration 8300 -> loss: 0.09303604541914398\n",
      "    At iteration 8400 -> loss: 0.09306059151901246\n",
      "    At iteration 8500 -> loss: 0.09304366715787946\n",
      "    At iteration 8600 -> loss: 0.0930841324861244\n",
      "    At iteration 8700 -> loss: 0.09306933663333243\n",
      "    At iteration 8800 -> loss: 0.0930561600807111\n",
      "    At iteration 8900 -> loss: 0.09303828386284153\n",
      "    At iteration 9000 -> loss: 0.09301845497648482\n",
      "    At iteration 9100 -> loss: 0.0929887018528018\n",
      "    At iteration 9200 -> loss: 0.09298390887470272\n",
      "    At iteration 9300 -> loss: 0.09297472113201204\n",
      "    At iteration 9400 -> loss: 0.09299105757654834\n",
      "    At iteration 9500 -> loss: 0.09298495462574716\n",
      "    At iteration 9600 -> loss: 0.09296319560407114\n",
      "    At iteration 9700 -> loss: 0.092974855115419\n",
      "    At iteration 9800 -> loss: 0.09293101221170148\n",
      "    At iteration 9900 -> loss: 0.09289769893299471\n",
      "    At iteration 10000 -> loss: 0.09284645968525693\n",
      "    At iteration 10100 -> loss: 0.09283037889667523\n",
      "    At iteration 10200 -> loss: 0.0928063194220315\n",
      "    At iteration 10300 -> loss: 0.09279243389476138\n",
      "    At iteration 10400 -> loss: 0.09275778466771838\n",
      "    At iteration 10500 -> loss: 0.09282487411088061\n",
      "    At iteration 10600 -> loss: 0.09309700849665387\n",
      "    At iteration 10700 -> loss: 0.0930922415791306\n",
      "    At iteration 10800 -> loss: 0.09310191229523454\n",
      "    At iteration 10900 -> loss: 0.09307654255042332\n",
      "    At iteration 11000 -> loss: 0.09308156129530314\n",
      "    At iteration 11100 -> loss: 0.09305781680535616\n",
      "    At iteration 11200 -> loss: 0.09311075369966001\n",
      "    At iteration 11300 -> loss: 0.09307743725297901\n",
      "    At iteration 11400 -> loss: 0.09308329959143083\n",
      "    At iteration 11500 -> loss: 0.09307751546726667\n",
      "    At iteration 11600 -> loss: 0.09306494085333078\n",
      "    At iteration 11700 -> loss: 0.09303644897899828\n",
      "    At iteration 11800 -> loss: 0.09305569944787458\n",
      "    At iteration 11900 -> loss: 0.09307415437987558\n",
      "    At iteration 12000 -> loss: 0.09310870535251367\n",
      "    At iteration 12100 -> loss: 0.09316352138694894\n",
      "    At iteration 12200 -> loss: 0.09313318996244103\n",
      "    At iteration 12300 -> loss: 0.09310105096188624\n",
      "    At iteration 12400 -> loss: 0.0930698172827324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12500 -> loss: 0.09308860314420338\n",
      "    At iteration 12600 -> loss: 0.09306302307172629\n",
      "    At iteration 12700 -> loss: 0.09303461892505162\n",
      "    At iteration 12800 -> loss: 0.09303654612741\n",
      "    At iteration 12900 -> loss: 0.09302999366909812\n",
      "    At iteration 13000 -> loss: 0.09301568585382826\n",
      "    At iteration 13100 -> loss: 0.09299951203193874\n",
      "    At iteration 13200 -> loss: 0.09298854560877831\n",
      "    At iteration 13300 -> loss: 0.09298379894516713\n",
      "    At iteration 13400 -> loss: 0.09296790890952766\n",
      "    At iteration 13500 -> loss: 0.093032581838597\n",
      "    At iteration 13600 -> loss: 0.09302574138385471\n",
      "Staring Epoch 101\n",
      "    At iteration 0 -> loss: 0.15486166859045625\n",
      "    At iteration 100 -> loss: 0.09864722664413883\n",
      "    At iteration 200 -> loss: 0.09536371764287081\n",
      "    At iteration 300 -> loss: 0.09501964242767504\n",
      "    At iteration 400 -> loss: 0.10284588443121608\n",
      "    At iteration 500 -> loss: 0.1032827932587068\n",
      "    At iteration 600 -> loss: 0.10094912512959275\n",
      "    At iteration 700 -> loss: 0.09962119898334627\n",
      "    At iteration 800 -> loss: 0.09947894512576737\n",
      "    At iteration 900 -> loss: 0.09842848423433771\n",
      "    At iteration 1000 -> loss: 0.09766168033143288\n",
      "    At iteration 1100 -> loss: 0.09702460959949864\n",
      "    At iteration 1200 -> loss: 0.09797101374897219\n",
      "    At iteration 1300 -> loss: 0.09750938318799486\n",
      "    At iteration 1400 -> loss: 0.09730244262883461\n",
      "    At iteration 1500 -> loss: 0.09684441988101156\n",
      "    At iteration 1600 -> loss: 0.09633506335242627\n",
      "    At iteration 1700 -> loss: 0.09595763993644148\n",
      "    At iteration 1800 -> loss: 0.09566196410653127\n",
      "    At iteration 1900 -> loss: 0.09605468284426794\n",
      "    At iteration 2000 -> loss: 0.09581766765601213\n",
      "    At iteration 2100 -> loss: 0.09574434899786813\n",
      "    At iteration 2200 -> loss: 0.09546828980519384\n",
      "    At iteration 2300 -> loss: 0.09526829385283546\n",
      "    At iteration 2400 -> loss: 0.0949934678491879\n",
      "    At iteration 2500 -> loss: 0.09480180179269648\n",
      "    At iteration 2600 -> loss: 0.09467789778422583\n",
      "    At iteration 2700 -> loss: 0.09466248274934135\n",
      "    At iteration 2800 -> loss: 0.0945701105271282\n",
      "    At iteration 2900 -> loss: 0.09439258836355575\n",
      "    At iteration 3000 -> loss: 0.09469633678581843\n",
      "    At iteration 3100 -> loss: 0.09455850019449107\n",
      "    At iteration 3200 -> loss: 0.09466749694154336\n",
      "    At iteration 3300 -> loss: 0.0945116250421386\n",
      "    At iteration 3400 -> loss: 0.09450294685416467\n",
      "    At iteration 3500 -> loss: 0.09443779398745267\n",
      "    At iteration 3600 -> loss: 0.09433113750715293\n",
      "    At iteration 3700 -> loss: 0.09425010096896858\n",
      "    At iteration 3800 -> loss: 0.09432120231301952\n",
      "    At iteration 3900 -> loss: 0.09426406976977768\n",
      "    At iteration 4000 -> loss: 0.09446398955760547\n",
      "    At iteration 4100 -> loss: 0.09442923378127892\n",
      "    At iteration 4200 -> loss: 0.09434687510034541\n",
      "    At iteration 4300 -> loss: 0.09432739008804447\n",
      "    At iteration 4400 -> loss: 0.09420417271974375\n",
      "    At iteration 4500 -> loss: 0.09416588421083523\n",
      "    At iteration 4600 -> loss: 0.09418930535246593\n",
      "    At iteration 4700 -> loss: 0.09407382341076477\n",
      "    At iteration 4800 -> loss: 0.09401923439007308\n",
      "    At iteration 4900 -> loss: 0.09393588236618257\n",
      "    At iteration 5000 -> loss: 0.09388794993083449\n",
      "    At iteration 5100 -> loss: 0.09380687786058085\n",
      "    At iteration 5200 -> loss: 0.09377529286866199\n",
      "    At iteration 5300 -> loss: 0.09370399813541111\n",
      "    At iteration 5400 -> loss: 0.09375209725562873\n",
      "    At iteration 5500 -> loss: 0.09381775577393933\n",
      "    At iteration 5600 -> loss: 0.09384220946692351\n",
      "    At iteration 5700 -> loss: 0.09379572495301126\n",
      "    At iteration 5800 -> loss: 0.09377143275112666\n",
      "    At iteration 5900 -> loss: 0.0937526550047611\n",
      "    At iteration 6000 -> loss: 0.09369252309174915\n",
      "    At iteration 6100 -> loss: 0.09360657037439957\n",
      "    At iteration 6200 -> loss: 0.09357009887892674\n",
      "    At iteration 6300 -> loss: 0.09356862348839538\n",
      "    At iteration 6400 -> loss: 0.09363709048117982\n",
      "    At iteration 6500 -> loss: 0.09357829519046038\n",
      "    At iteration 6600 -> loss: 0.09362167650457884\n",
      "    At iteration 6700 -> loss: 0.09359687284556788\n",
      "    At iteration 6800 -> loss: 0.09358678015042454\n",
      "    At iteration 6900 -> loss: 0.09353974680445733\n",
      "    At iteration 7000 -> loss: 0.09347356584844171\n",
      "    At iteration 7100 -> loss: 0.09341818231250455\n",
      "    At iteration 7200 -> loss: 0.09343939474057138\n",
      "    At iteration 7300 -> loss: 0.09339078181599852\n",
      "    At iteration 7400 -> loss: 0.09333387257892016\n",
      "    At iteration 7500 -> loss: 0.09331189222299507\n",
      "    At iteration 7600 -> loss: 0.09328269846991485\n",
      "    At iteration 7700 -> loss: 0.09338200214429719\n",
      "    At iteration 7800 -> loss: 0.09335396671815883\n",
      "    At iteration 7900 -> loss: 0.09342989244113825\n",
      "    At iteration 8000 -> loss: 0.09349127735108567\n",
      "    At iteration 8100 -> loss: 0.09354510059254481\n",
      "    At iteration 8200 -> loss: 0.09350831982120554\n",
      "    At iteration 8300 -> loss: 0.09359898771419616\n",
      "    At iteration 8400 -> loss: 0.09353965351574886\n",
      "    At iteration 8500 -> loss: 0.09350595255070412\n",
      "    At iteration 8600 -> loss: 0.09350263522187165\n",
      "    At iteration 8700 -> loss: 0.09354359723798696\n",
      "    At iteration 8800 -> loss: 0.09350898932319028\n",
      "    At iteration 8900 -> loss: 0.0934820791899398\n",
      "    At iteration 9000 -> loss: 0.09349211443678354\n",
      "    At iteration 9100 -> loss: 0.09346938373924825\n",
      "    At iteration 9200 -> loss: 0.09344390519420193\n",
      "    At iteration 9300 -> loss: 0.09339686272776877\n",
      "    At iteration 9400 -> loss: 0.09335371882152692\n",
      "    At iteration 9500 -> loss: 0.09335036870344038\n",
      "    At iteration 9600 -> loss: 0.09335856432896239\n",
      "    At iteration 9700 -> loss: 0.09332293619265059\n",
      "    At iteration 9800 -> loss: 0.09329996125453249\n",
      "    At iteration 9900 -> loss: 0.0933376725240045\n",
      "    At iteration 10000 -> loss: 0.09331985159037817\n",
      "    At iteration 10100 -> loss: 0.09335074049823328\n",
      "    At iteration 10200 -> loss: 0.09331975900027153\n",
      "    At iteration 10300 -> loss: 0.09338166079246053\n",
      "    At iteration 10400 -> loss: 0.09335955756899156\n",
      "    At iteration 10500 -> loss: 0.09333250361987473\n",
      "    At iteration 10600 -> loss: 0.0932988566977368\n",
      "    At iteration 10700 -> loss: 0.09335796315043342\n",
      "    At iteration 10800 -> loss: 0.09332187829271907\n",
      "    At iteration 10900 -> loss: 0.09329576203261158\n",
      "    At iteration 11000 -> loss: 0.09332004306658967\n",
      "    At iteration 11100 -> loss: 0.0933028943408401\n",
      "    At iteration 11200 -> loss: 0.09329569331051948\n",
      "    At iteration 11300 -> loss: 0.09327256908650977\n",
      "    At iteration 11400 -> loss: 0.09323936939855691\n",
      "    At iteration 11500 -> loss: 0.093217744355236\n",
      "    At iteration 11600 -> loss: 0.093190375852333\n",
      "    At iteration 11700 -> loss: 0.09314565135189636\n",
      "    At iteration 11800 -> loss: 0.09314743983464563\n",
      "    At iteration 11900 -> loss: 0.09313749119292974\n",
      "    At iteration 12000 -> loss: 0.09310282208952966\n",
      "    At iteration 12100 -> loss: 0.09309383989874825\n",
      "    At iteration 12200 -> loss: 0.0930945758051111\n",
      "    At iteration 12300 -> loss: 0.09310679554157042\n",
      "    At iteration 12400 -> loss: 0.093078408968465\n",
      "    At iteration 12500 -> loss: 0.09304110721327719\n",
      "    At iteration 12600 -> loss: 0.09303150449962871\n",
      "    At iteration 12700 -> loss: 0.0930165751951221\n",
      "    At iteration 12800 -> loss: 0.0930144039368619\n",
      "    At iteration 12900 -> loss: 0.09306315325155093\n",
      "    At iteration 13000 -> loss: 0.0930375347516616\n",
      "    At iteration 13100 -> loss: 0.09302410245006103\n",
      "    At iteration 13200 -> loss: 0.09303071044728099\n",
      "    At iteration 13300 -> loss: 0.09302848937940406\n",
      "    At iteration 13400 -> loss: 0.09301072936625673\n",
      "    At iteration 13500 -> loss: 0.09304438923482283\n",
      "    At iteration 13600 -> loss: 0.09303954563792997\n",
      "Staring Epoch 102\n",
      "    At iteration 0 -> loss: 0.09262390481308103\n",
      "    At iteration 100 -> loss: 0.0939689389897568\n",
      "    At iteration 200 -> loss: 0.09810156554497025\n",
      "    At iteration 300 -> loss: 0.09656775479008999\n",
      "    At iteration 400 -> loss: 0.09544992170731872\n",
      "    At iteration 500 -> loss: 0.09420444591896777\n",
      "    At iteration 600 -> loss: 0.09482162157464402\n",
      "    At iteration 700 -> loss: 0.09433107017272732\n",
      "    At iteration 800 -> loss: 0.09450203348500005\n",
      "    At iteration 900 -> loss: 0.09476409945009956\n",
      "    At iteration 1000 -> loss: 0.09456753789900516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1100 -> loss: 0.09444009184342968\n",
      "    At iteration 1200 -> loss: 0.09439962263529023\n",
      "    At iteration 1300 -> loss: 0.0943541865932913\n",
      "    At iteration 1400 -> loss: 0.09406652312596953\n",
      "    At iteration 1500 -> loss: 0.09407278275114174\n",
      "    At iteration 1600 -> loss: 0.09418069433457564\n",
      "    At iteration 1700 -> loss: 0.09401420087791011\n",
      "    At iteration 1800 -> loss: 0.09378300955984606\n",
      "    At iteration 1900 -> loss: 0.09362322309639519\n",
      "    At iteration 2000 -> loss: 0.09348168491821568\n",
      "    At iteration 2100 -> loss: 0.09335687417789351\n",
      "    At iteration 2200 -> loss: 0.09318846460651366\n",
      "    At iteration 2300 -> loss: 0.09315698741687403\n",
      "    At iteration 2400 -> loss: 0.09308172652534205\n",
      "    At iteration 2500 -> loss: 0.09316216507439708\n",
      "    At iteration 2600 -> loss: 0.09375064396489591\n",
      "    At iteration 2700 -> loss: 0.09374151232965572\n",
      "    At iteration 2800 -> loss: 0.09359561676619742\n",
      "    At iteration 2900 -> loss: 0.09350772987790296\n",
      "    At iteration 3000 -> loss: 0.0934111162989511\n",
      "    At iteration 3100 -> loss: 0.09332266374815687\n",
      "    At iteration 3200 -> loss: 0.09331307870884133\n",
      "    At iteration 3300 -> loss: 0.09335400158743433\n",
      "    At iteration 3400 -> loss: 0.09331694696772506\n",
      "    At iteration 3500 -> loss: 0.09330573021040894\n",
      "    At iteration 3600 -> loss: 0.09324315999313111\n",
      "    At iteration 3700 -> loss: 0.09342840124146407\n",
      "    At iteration 3800 -> loss: 0.09328864945235833\n",
      "    At iteration 3900 -> loss: 0.09325096201724391\n",
      "    At iteration 4000 -> loss: 0.09346471500272467\n",
      "    At iteration 4100 -> loss: 0.09348923849036715\n",
      "    At iteration 4200 -> loss: 0.09339903622084886\n",
      "    At iteration 4300 -> loss: 0.09339181757816771\n",
      "    At iteration 4400 -> loss: 0.09333342989523209\n",
      "    At iteration 4500 -> loss: 0.09328712229596627\n",
      "    At iteration 4600 -> loss: 0.09323796957263916\n",
      "    At iteration 4700 -> loss: 0.09318839715161646\n",
      "    At iteration 4800 -> loss: 0.09312808445872423\n",
      "    At iteration 4900 -> loss: 0.09312564350788863\n",
      "    At iteration 5000 -> loss: 0.09310390132508187\n",
      "    At iteration 5100 -> loss: 0.09302860597143539\n",
      "    At iteration 5200 -> loss: 0.09315358294296272\n",
      "    At iteration 5300 -> loss: 0.0931186770320444\n",
      "    At iteration 5400 -> loss: 0.0931303181314483\n",
      "    At iteration 5500 -> loss: 0.09306063200231375\n",
      "    At iteration 5600 -> loss: 0.09302261840980963\n",
      "    At iteration 5700 -> loss: 0.09299442763266694\n",
      "    At iteration 5800 -> loss: 0.09294711475802762\n",
      "    At iteration 5900 -> loss: 0.09305759431092343\n",
      "    At iteration 6000 -> loss: 0.09322211801543127\n",
      "    At iteration 6100 -> loss: 0.09314363378326114\n",
      "    At iteration 6200 -> loss: 0.09308830246261278\n",
      "    At iteration 6300 -> loss: 0.09308617337593604\n",
      "    At iteration 6400 -> loss: 0.09309421998187856\n",
      "    At iteration 6500 -> loss: 0.09309750464365946\n",
      "    At iteration 6600 -> loss: 0.09319589860588227\n",
      "    At iteration 6700 -> loss: 0.09324959687476739\n",
      "    At iteration 6800 -> loss: 0.09322536859821595\n",
      "    At iteration 6900 -> loss: 0.09320889348300511\n",
      "    At iteration 7000 -> loss: 0.09319880580155317\n",
      "    At iteration 7100 -> loss: 0.09315633631397002\n",
      "    At iteration 7200 -> loss: 0.0931093328102312\n",
      "    At iteration 7300 -> loss: 0.0930789050166248\n",
      "    At iteration 7400 -> loss: 0.09304569075037537\n",
      "    At iteration 7500 -> loss: 0.09310012885461968\n",
      "    At iteration 7600 -> loss: 0.0930442201793041\n",
      "    At iteration 7700 -> loss: 0.09302559982470002\n",
      "    At iteration 7800 -> loss: 0.093050144452133\n",
      "    At iteration 7900 -> loss: 0.09302187061748386\n",
      "    At iteration 8000 -> loss: 0.0929669378059988\n",
      "    At iteration 8100 -> loss: 0.0929314063746826\n",
      "    At iteration 8200 -> loss: 0.09293407176468925\n",
      "    At iteration 8300 -> loss: 0.09288354013581508\n",
      "    At iteration 8400 -> loss: 0.09286572537193066\n",
      "    At iteration 8500 -> loss: 0.09291146407309783\n",
      "    At iteration 8600 -> loss: 0.09292731777577878\n",
      "    At iteration 8700 -> loss: 0.09289515456014982\n",
      "    At iteration 8800 -> loss: 0.09285891431360993\n",
      "    At iteration 8900 -> loss: 0.09285627109461563\n",
      "    At iteration 9000 -> loss: 0.09296162324754749\n",
      "    At iteration 9100 -> loss: 0.09290608635929071\n",
      "    At iteration 9200 -> loss: 0.09290374291002679\n",
      "    At iteration 9300 -> loss: 0.09290869581911529\n",
      "    At iteration 9400 -> loss: 0.0928801150036102\n",
      "    At iteration 9500 -> loss: 0.09286173138306308\n",
      "    At iteration 9600 -> loss: 0.09286861832717741\n",
      "    At iteration 9700 -> loss: 0.09286775354647632\n",
      "    At iteration 9800 -> loss: 0.09286703235546595\n",
      "    At iteration 9900 -> loss: 0.09284600091711408\n",
      "    At iteration 10000 -> loss: 0.09288604964121168\n",
      "    At iteration 10100 -> loss: 0.0928399778320554\n",
      "    At iteration 10200 -> loss: 0.09282565812484417\n",
      "    At iteration 10300 -> loss: 0.09285010914720583\n",
      "    At iteration 10400 -> loss: 0.09290642796021462\n",
      "    At iteration 10500 -> loss: 0.09289015945746633\n",
      "    At iteration 10600 -> loss: 0.09292675025490596\n",
      "    At iteration 10700 -> loss: 0.09291175846132294\n",
      "    At iteration 10800 -> loss: 0.09288579923127756\n",
      "    At iteration 10900 -> loss: 0.0928688545549463\n",
      "    At iteration 11000 -> loss: 0.09285690440593052\n",
      "    At iteration 11100 -> loss: 0.09283495170341538\n",
      "    At iteration 11200 -> loss: 0.09283328205030893\n",
      "    At iteration 11300 -> loss: 0.0929095082297274\n",
      "    At iteration 11400 -> loss: 0.09287513327561946\n",
      "    At iteration 11500 -> loss: 0.09285064128693594\n",
      "    At iteration 11600 -> loss: 0.09284946130717475\n",
      "    At iteration 11700 -> loss: 0.09288172363789857\n",
      "    At iteration 11800 -> loss: 0.09284661096369858\n",
      "    At iteration 11900 -> loss: 0.0928334067249694\n",
      "    At iteration 12000 -> loss: 0.09283567268126339\n",
      "    At iteration 12100 -> loss: 0.0928071603383995\n",
      "    At iteration 12200 -> loss: 0.09286777133660955\n",
      "    At iteration 12300 -> loss: 0.09284403138183876\n",
      "    At iteration 12400 -> loss: 0.09311213148247119\n",
      "    At iteration 12500 -> loss: 0.09313775578186007\n",
      "    At iteration 12600 -> loss: 0.09313025303644765\n",
      "    At iteration 12700 -> loss: 0.09310365760605263\n",
      "    At iteration 12800 -> loss: 0.09307211133389133\n",
      "    At iteration 12900 -> loss: 0.09305905429351137\n",
      "    At iteration 13000 -> loss: 0.09306148772726155\n",
      "    At iteration 13100 -> loss: 0.09304544645732454\n",
      "    At iteration 13200 -> loss: 0.09303228865161804\n",
      "    At iteration 13300 -> loss: 0.09302239934546883\n",
      "    At iteration 13400 -> loss: 0.09303929785757423\n",
      "    At iteration 13500 -> loss: 0.09300067936920463\n",
      "    At iteration 13600 -> loss: 0.09302713113169268\n",
      "Staring Epoch 103\n",
      "    At iteration 0 -> loss: 0.09656820073723793\n",
      "    At iteration 100 -> loss: 0.09450563501302733\n",
      "    At iteration 200 -> loss: 0.09357889241453184\n",
      "    At iteration 300 -> loss: 0.0913580080167948\n",
      "    At iteration 400 -> loss: 0.09141006208133703\n",
      "    At iteration 500 -> loss: 0.09215040695148403\n",
      "    At iteration 600 -> loss: 0.09185754635732317\n",
      "    At iteration 700 -> loss: 0.09173554164063637\n",
      "    At iteration 800 -> loss: 0.09185586531338719\n",
      "    At iteration 900 -> loss: 0.09174236114516984\n",
      "    At iteration 1000 -> loss: 0.09155148746204368\n",
      "    At iteration 1100 -> loss: 0.0916314112683533\n",
      "    At iteration 1200 -> loss: 0.09151935072988708\n",
      "    At iteration 1300 -> loss: 0.09141965044279006\n",
      "    At iteration 1400 -> loss: 0.09149884921537789\n",
      "    At iteration 1500 -> loss: 0.09196571181939277\n",
      "    At iteration 1600 -> loss: 0.09204464672776812\n",
      "    At iteration 1700 -> loss: 0.09222262331076674\n",
      "    At iteration 1800 -> loss: 0.09229467071000107\n",
      "    At iteration 1900 -> loss: 0.09228959758235954\n",
      "    At iteration 2000 -> loss: 0.09234534516199529\n",
      "    At iteration 2100 -> loss: 0.0925470980005715\n",
      "    At iteration 2200 -> loss: 0.09270881593301135\n",
      "    At iteration 2300 -> loss: 0.09268574224170671\n",
      "    At iteration 2400 -> loss: 0.09256511447635314\n",
      "    At iteration 2500 -> loss: 0.09251483952042455\n",
      "    At iteration 2600 -> loss: 0.09239955372614497\n",
      "    At iteration 2700 -> loss: 0.0923002098663741\n",
      "    At iteration 2800 -> loss: 0.09229434620978608\n",
      "    At iteration 2900 -> loss: 0.09221255329688315\n",
      "    At iteration 3000 -> loss: 0.0921664454751482\n",
      "    At iteration 3100 -> loss: 0.09225415243049198\n",
      "    At iteration 3200 -> loss: 0.09250748103213334\n",
      "    At iteration 3300 -> loss: 0.09244170946174919\n",
      "    At iteration 3400 -> loss: 0.0923810910311082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3500 -> loss: 0.09276165096445896\n",
      "    At iteration 3600 -> loss: 0.09282581849833628\n",
      "    At iteration 3700 -> loss: 0.09279096137756683\n",
      "    At iteration 3800 -> loss: 0.09285001978221315\n",
      "    At iteration 3900 -> loss: 0.09281371643099112\n",
      "    At iteration 4000 -> loss: 0.09282587403627705\n",
      "    At iteration 4100 -> loss: 0.09293957114328673\n",
      "    At iteration 4200 -> loss: 0.09288439399603729\n",
      "    At iteration 4300 -> loss: 0.09280357945049655\n",
      "    At iteration 4400 -> loss: 0.09276129855745066\n",
      "    At iteration 4500 -> loss: 0.09281281975174178\n",
      "    At iteration 4600 -> loss: 0.09280531909337619\n",
      "    At iteration 4700 -> loss: 0.09280249828690738\n",
      "    At iteration 4800 -> loss: 0.09276419845952806\n",
      "    At iteration 4900 -> loss: 0.09276141086517478\n",
      "    At iteration 5000 -> loss: 0.09272695893476157\n",
      "    At iteration 5100 -> loss: 0.09272155970184\n",
      "    At iteration 5200 -> loss: 0.09268202351156073\n",
      "    At iteration 5300 -> loss: 0.09263804430960368\n",
      "    At iteration 5400 -> loss: 0.0925557125319665\n",
      "    At iteration 5500 -> loss: 0.09251480006795236\n",
      "    At iteration 5600 -> loss: 0.0925021798211066\n",
      "    At iteration 5700 -> loss: 0.09249790137825907\n",
      "    At iteration 5800 -> loss: 0.0926551026241862\n",
      "    At iteration 5900 -> loss: 0.09263120003705899\n",
      "    At iteration 6000 -> loss: 0.09277491582013556\n",
      "    At iteration 6100 -> loss: 0.09276343368049864\n",
      "    At iteration 6200 -> loss: 0.09272729124297573\n",
      "    At iteration 6300 -> loss: 0.09269344480461225\n",
      "    At iteration 6400 -> loss: 0.09267882075828933\n",
      "    At iteration 6500 -> loss: 0.09264318556162358\n",
      "    At iteration 6600 -> loss: 0.09270131453273148\n",
      "    At iteration 6700 -> loss: 0.0926570157421992\n",
      "    At iteration 6800 -> loss: 0.09264935186623839\n",
      "    At iteration 6900 -> loss: 0.09280553985610664\n",
      "    At iteration 7000 -> loss: 0.09277252707998655\n",
      "    At iteration 7100 -> loss: 0.09279843537392722\n",
      "    At iteration 7200 -> loss: 0.09286575101787745\n",
      "    At iteration 7300 -> loss: 0.09286970229243506\n",
      "    At iteration 7400 -> loss: 0.09284180880025564\n",
      "    At iteration 7500 -> loss: 0.09287354417353752\n",
      "    At iteration 7600 -> loss: 0.09284985689860266\n",
      "    At iteration 7700 -> loss: 0.09286542597004795\n",
      "    At iteration 7800 -> loss: 0.09299327668027832\n",
      "    At iteration 7900 -> loss: 0.09295043518997748\n",
      "    At iteration 8000 -> loss: 0.09292277729573822\n",
      "    At iteration 8100 -> loss: 0.09289250430121128\n",
      "    At iteration 8200 -> loss: 0.09285197753440022\n",
      "    At iteration 8300 -> loss: 0.09283086102270822\n",
      "    At iteration 8400 -> loss: 0.09287719890947009\n",
      "    At iteration 8500 -> loss: 0.0928522956950874\n",
      "    At iteration 8600 -> loss: 0.0928594652925877\n",
      "    At iteration 8700 -> loss: 0.09283330341437641\n",
      "    At iteration 8800 -> loss: 0.09280975684902365\n",
      "    At iteration 8900 -> loss: 0.09284409808130241\n",
      "    At iteration 9000 -> loss: 0.09280868878137906\n",
      "    At iteration 9100 -> loss: 0.09280622070301384\n",
      "    At iteration 9200 -> loss: 0.0928017753146134\n",
      "    At iteration 9300 -> loss: 0.09278051945331721\n",
      "    At iteration 9400 -> loss: 0.09278816185514988\n",
      "    At iteration 9500 -> loss: 0.09277566965935428\n",
      "    At iteration 9600 -> loss: 0.09273441447952689\n",
      "    At iteration 9700 -> loss: 0.09279403718609006\n",
      "    At iteration 9800 -> loss: 0.09278601953405201\n",
      "    At iteration 9900 -> loss: 0.09286654386308506\n",
      "    At iteration 10000 -> loss: 0.0928334056677802\n",
      "    At iteration 10100 -> loss: 0.09282551320539166\n",
      "    At iteration 10200 -> loss: 0.09283738050490847\n",
      "    At iteration 10300 -> loss: 0.09286497134766047\n",
      "    At iteration 10400 -> loss: 0.09283422967843706\n",
      "    At iteration 10500 -> loss: 0.09285770644725674\n",
      "    At iteration 10600 -> loss: 0.09282486453223601\n",
      "    At iteration 10700 -> loss: 0.09280365754179111\n",
      "    At iteration 10800 -> loss: 0.09282047252436465\n",
      "    At iteration 10900 -> loss: 0.09279876851859256\n",
      "    At iteration 11000 -> loss: 0.09277446735131308\n",
      "    At iteration 11100 -> loss: 0.09277085099673604\n",
      "    At iteration 11200 -> loss: 0.09273520229466709\n",
      "    At iteration 11300 -> loss: 0.0927714809400986\n",
      "    At iteration 11400 -> loss: 0.09279886890457152\n",
      "    At iteration 11500 -> loss: 0.09276817027880517\n",
      "    At iteration 11600 -> loss: 0.09280278217024251\n",
      "    At iteration 11700 -> loss: 0.09280832777347428\n",
      "    At iteration 11800 -> loss: 0.09281634691907792\n",
      "    At iteration 11900 -> loss: 0.09279003758901992\n",
      "    At iteration 12000 -> loss: 0.09277363464937495\n",
      "    At iteration 12100 -> loss: 0.09277395465582598\n",
      "    At iteration 12200 -> loss: 0.09282460005881572\n",
      "    At iteration 12300 -> loss: 0.09280111332703264\n",
      "    At iteration 12400 -> loss: 0.092779591123716\n",
      "    At iteration 12500 -> loss: 0.09278027464229686\n",
      "    At iteration 12600 -> loss: 0.09299781589480552\n",
      "    At iteration 12700 -> loss: 0.09306483524306536\n",
      "    At iteration 12800 -> loss: 0.09304955483813848\n",
      "    At iteration 12900 -> loss: 0.09308465655150716\n",
      "    At iteration 13000 -> loss: 0.09305532005163142\n",
      "    At iteration 13100 -> loss: 0.09303756254552085\n",
      "    At iteration 13200 -> loss: 0.09302827760182392\n",
      "    At iteration 13300 -> loss: 0.09299697861511662\n",
      "    At iteration 13400 -> loss: 0.09299556007175469\n",
      "    At iteration 13500 -> loss: 0.0929982746371024\n",
      "    At iteration 13600 -> loss: 0.09298668769296095\n",
      "Staring Epoch 104\n",
      "    At iteration 0 -> loss: 0.08199178660288453\n",
      "    At iteration 100 -> loss: 0.08998934544289955\n",
      "    At iteration 200 -> loss: 0.09046189159141238\n",
      "    At iteration 300 -> loss: 0.09079980321069846\n",
      "    At iteration 400 -> loss: 0.09165208414321685\n",
      "    At iteration 500 -> loss: 0.09360124030813283\n",
      "    At iteration 600 -> loss: 0.09321662186999496\n",
      "    At iteration 700 -> loss: 0.09279517833756928\n",
      "    At iteration 800 -> loss: 0.09253068781921386\n",
      "    At iteration 900 -> loss: 0.09217255527669152\n",
      "    At iteration 1000 -> loss: 0.09226267018321452\n",
      "    At iteration 1100 -> loss: 0.09204030308871267\n",
      "    At iteration 1200 -> loss: 0.09216929939412688\n",
      "    At iteration 1300 -> loss: 0.09197560504800306\n",
      "    At iteration 1400 -> loss: 0.09172889810197371\n",
      "    At iteration 1500 -> loss: 0.09155776271505864\n",
      "    At iteration 1600 -> loss: 0.09163965747168414\n",
      "    At iteration 1700 -> loss: 0.09173477100081279\n",
      "    At iteration 1800 -> loss: 0.09164319637191505\n",
      "    At iteration 1900 -> loss: 0.09165740948398911\n",
      "    At iteration 2000 -> loss: 0.09196424438651617\n",
      "    At iteration 2100 -> loss: 0.09186136745765051\n",
      "    At iteration 2200 -> loss: 0.09188085928015516\n",
      "    At iteration 2300 -> loss: 0.0920190617153944\n",
      "    At iteration 2400 -> loss: 0.09192327345656072\n",
      "    At iteration 2500 -> loss: 0.09196987303865396\n",
      "    At iteration 2600 -> loss: 0.09195485140315972\n",
      "    At iteration 2700 -> loss: 0.09185562117989808\n",
      "    At iteration 2800 -> loss: 0.09178974030736904\n",
      "    At iteration 2900 -> loss: 0.09173937172648924\n",
      "    At iteration 3000 -> loss: 0.09192095971590335\n",
      "    At iteration 3100 -> loss: 0.0919271229534597\n",
      "    At iteration 3200 -> loss: 0.09203358337037494\n",
      "    At iteration 3300 -> loss: 0.09205509985792505\n",
      "    At iteration 3400 -> loss: 0.09222780605985144\n",
      "    At iteration 3500 -> loss: 0.09217228960815874\n",
      "    At iteration 3600 -> loss: 0.09209490942069036\n",
      "    At iteration 3700 -> loss: 0.09211025787193265\n",
      "    At iteration 3800 -> loss: 0.09207017082618604\n",
      "    At iteration 3900 -> loss: 0.09208403045205746\n",
      "    At iteration 4000 -> loss: 0.09242635425184417\n",
      "    At iteration 4100 -> loss: 0.09242585537754958\n",
      "    At iteration 4200 -> loss: 0.09236318139421078\n",
      "    At iteration 4300 -> loss: 0.09266704547481178\n",
      "    At iteration 4400 -> loss: 0.09257835445384312\n",
      "    At iteration 4500 -> loss: 0.09255872672510428\n",
      "    At iteration 4600 -> loss: 0.0924932910594913\n",
      "    At iteration 4700 -> loss: 0.09251927984407547\n",
      "    At iteration 4800 -> loss: 0.0925305306603519\n",
      "    At iteration 4900 -> loss: 0.0924765902421295\n",
      "    At iteration 5000 -> loss: 0.09239819290071614\n",
      "    At iteration 5100 -> loss: 0.09236971103615427\n",
      "    At iteration 5200 -> loss: 0.09235650925138608\n",
      "    At iteration 5300 -> loss: 0.09235648177020987\n",
      "    At iteration 5400 -> loss: 0.09229230913806308\n",
      "    At iteration 5500 -> loss: 0.09246450509220473\n",
      "    At iteration 5600 -> loss: 0.09254463043231288\n",
      "    At iteration 5700 -> loss: 0.09254638054208982\n",
      "    At iteration 5800 -> loss: 0.09249603461875132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5900 -> loss: 0.09243821529114536\n",
      "    At iteration 6000 -> loss: 0.09239596743117665\n",
      "    At iteration 6100 -> loss: 0.09241499958982653\n",
      "    At iteration 6200 -> loss: 0.09244972017220404\n",
      "    At iteration 6300 -> loss: 0.09240163838718438\n",
      "    At iteration 6400 -> loss: 0.09237925797034646\n",
      "    At iteration 6500 -> loss: 0.09234031188032345\n",
      "    At iteration 6600 -> loss: 0.09233327694231996\n",
      "    At iteration 6700 -> loss: 0.09228954955363919\n",
      "    At iteration 6800 -> loss: 0.09234826665139724\n",
      "    At iteration 6900 -> loss: 0.09239226768286107\n",
      "    At iteration 7000 -> loss: 0.0924176652223197\n",
      "    At iteration 7100 -> loss: 0.09241378726434502\n",
      "    At iteration 7200 -> loss: 0.09236078443195031\n",
      "    At iteration 7300 -> loss: 0.09245396997073693\n",
      "    At iteration 7400 -> loss: 0.0924543177581147\n",
      "    At iteration 7500 -> loss: 0.09244650146657599\n",
      "    At iteration 7600 -> loss: 0.09253115610402961\n",
      "    At iteration 7700 -> loss: 0.09247975317688245\n",
      "    At iteration 7800 -> loss: 0.09247029174862192\n",
      "    At iteration 7900 -> loss: 0.09250400732687178\n",
      "    At iteration 8000 -> loss: 0.09251015723801208\n",
      "    At iteration 8100 -> loss: 0.09246433352281468\n",
      "    At iteration 8200 -> loss: 0.09249713527755081\n",
      "    At iteration 8300 -> loss: 0.09247131417914757\n",
      "    At iteration 8400 -> loss: 0.09245084838400312\n",
      "    At iteration 8500 -> loss: 0.09244006956948655\n",
      "    At iteration 8600 -> loss: 0.09242701131105871\n",
      "    At iteration 8700 -> loss: 0.09243096385715915\n",
      "    At iteration 8800 -> loss: 0.09256680858982096\n",
      "    At iteration 8900 -> loss: 0.09255881897828233\n",
      "    At iteration 9000 -> loss: 0.0925735615301123\n",
      "    At iteration 9100 -> loss: 0.09260528436334081\n",
      "    At iteration 9200 -> loss: 0.0926039009946472\n",
      "    At iteration 9300 -> loss: 0.09264502447376374\n",
      "    At iteration 9400 -> loss: 0.09261041742276217\n",
      "    At iteration 9500 -> loss: 0.09257922216464388\n",
      "    At iteration 9600 -> loss: 0.09259135129402099\n",
      "    At iteration 9700 -> loss: 0.09275139400734705\n",
      "    At iteration 9800 -> loss: 0.09274270696620124\n",
      "    At iteration 9900 -> loss: 0.0927126598609863\n",
      "    At iteration 10000 -> loss: 0.09270364262148824\n",
      "    At iteration 10100 -> loss: 0.09267599421217099\n",
      "    At iteration 10200 -> loss: 0.09265961004894391\n",
      "    At iteration 10300 -> loss: 0.0927121410711181\n",
      "    At iteration 10400 -> loss: 0.09267658257720406\n",
      "    At iteration 10500 -> loss: 0.09270494523389375\n",
      "    At iteration 10600 -> loss: 0.09269309860747559\n",
      "    At iteration 10700 -> loss: 0.09268832360839366\n",
      "    At iteration 10800 -> loss: 0.09265582630490678\n",
      "    At iteration 10900 -> loss: 0.09262979171607892\n",
      "    At iteration 11000 -> loss: 0.09264951562074757\n",
      "    At iteration 11100 -> loss: 0.09263790262622296\n",
      "    At iteration 11200 -> loss: 0.09261872808786153\n",
      "    At iteration 11300 -> loss: 0.09260417654202197\n",
      "    At iteration 11400 -> loss: 0.09258220278546327\n",
      "    At iteration 11500 -> loss: 0.09256112064841676\n",
      "    At iteration 11600 -> loss: 0.0926082227771795\n",
      "    At iteration 11700 -> loss: 0.09261706636642626\n",
      "    At iteration 11800 -> loss: 0.0926033678517262\n",
      "    At iteration 11900 -> loss: 0.09261858945489679\n",
      "    At iteration 12000 -> loss: 0.09264799799410448\n",
      "    At iteration 12100 -> loss: 0.09268107779363534\n",
      "    At iteration 12200 -> loss: 0.09273525829997703\n",
      "    At iteration 12300 -> loss: 0.09272564188108919\n",
      "    At iteration 12400 -> loss: 0.09273513107000647\n",
      "    At iteration 12500 -> loss: 0.09272953781032807\n",
      "    At iteration 12600 -> loss: 0.09274997941559124\n",
      "    At iteration 12700 -> loss: 0.09274156560876877\n",
      "    At iteration 12800 -> loss: 0.09272393240274067\n",
      "    At iteration 12900 -> loss: 0.092726203684483\n",
      "    At iteration 13000 -> loss: 0.09297803580267597\n",
      "    At iteration 13100 -> loss: 0.09298171719038638\n",
      "    At iteration 13200 -> loss: 0.09297127651908162\n",
      "    At iteration 13300 -> loss: 0.09295133050187451\n",
      "    At iteration 13400 -> loss: 0.09297214036987142\n",
      "    At iteration 13500 -> loss: 0.09298583050436313\n",
      "    At iteration 13600 -> loss: 0.09302416789839774\n",
      "Staring Epoch 105\n",
      "    At iteration 0 -> loss: 0.09079881431534886\n",
      "    At iteration 100 -> loss: 0.09351696675187979\n",
      "    At iteration 200 -> loss: 0.09504712575898187\n",
      "    At iteration 300 -> loss: 0.09298947616590185\n",
      "    At iteration 400 -> loss: 0.09351045364324415\n",
      "    At iteration 500 -> loss: 0.09601950512843287\n",
      "    At iteration 600 -> loss: 0.09469953354350073\n",
      "    At iteration 700 -> loss: 0.09473305717961869\n",
      "    At iteration 800 -> loss: 0.09507952717314563\n",
      "    At iteration 900 -> loss: 0.09466731476012885\n",
      "    At iteration 1000 -> loss: 0.09449936905068836\n",
      "    At iteration 1100 -> loss: 0.09418920221881256\n",
      "    At iteration 1200 -> loss: 0.09392669027328503\n",
      "    At iteration 1300 -> loss: 0.09397497633757633\n",
      "    At iteration 1400 -> loss: 0.0936803004579532\n",
      "    At iteration 1500 -> loss: 0.09355984907751357\n",
      "    At iteration 1600 -> loss: 0.09355843038478305\n",
      "    At iteration 1700 -> loss: 0.0934251518830131\n",
      "    At iteration 1800 -> loss: 0.0935703926716399\n",
      "    At iteration 1900 -> loss: 0.0932445160078328\n",
      "    At iteration 2000 -> loss: 0.09333165985380491\n",
      "    At iteration 2100 -> loss: 0.09353771107841304\n",
      "    At iteration 2200 -> loss: 0.0933346375808307\n",
      "    At iteration 2300 -> loss: 0.0931623270733193\n",
      "    At iteration 2400 -> loss: 0.09303330849287744\n",
      "    At iteration 2500 -> loss: 0.0931944571756497\n",
      "    At iteration 2600 -> loss: 0.09306771389777571\n",
      "    At iteration 2700 -> loss: 0.09323192951714218\n",
      "    At iteration 2800 -> loss: 0.09355855424786347\n",
      "    At iteration 2900 -> loss: 0.09359688509426907\n",
      "    At iteration 3000 -> loss: 0.09353978426749214\n",
      "    At iteration 3100 -> loss: 0.09345382416715389\n",
      "    At iteration 3200 -> loss: 0.09346644515342452\n",
      "    At iteration 3300 -> loss: 0.09350753553321846\n",
      "    At iteration 3400 -> loss: 0.09344849300154173\n",
      "    At iteration 3500 -> loss: 0.09335844218376053\n",
      "    At iteration 3600 -> loss: 0.09334839326415768\n",
      "    At iteration 3700 -> loss: 0.09339204517732547\n",
      "    At iteration 3800 -> loss: 0.09329383814255514\n",
      "    At iteration 3900 -> loss: 0.0933537481404039\n",
      "    At iteration 4000 -> loss: 0.09328734194425785\n",
      "    At iteration 4100 -> loss: 0.09323408750646978\n",
      "    At iteration 4200 -> loss: 0.09317684355947779\n",
      "    At iteration 4300 -> loss: 0.09320619356389225\n",
      "    At iteration 4400 -> loss: 0.09309000312800313\n",
      "    At iteration 4500 -> loss: 0.09305241136647424\n",
      "    At iteration 4600 -> loss: 0.09302287946454031\n",
      "    At iteration 4700 -> loss: 0.09298425048904647\n",
      "    At iteration 4800 -> loss: 0.09288067247369335\n",
      "    At iteration 4900 -> loss: 0.09286676604363585\n",
      "    At iteration 5000 -> loss: 0.09281552666017387\n",
      "    At iteration 5100 -> loss: 0.09280058788750475\n",
      "    At iteration 5200 -> loss: 0.09292234430432664\n",
      "    At iteration 5300 -> loss: 0.09289287783394255\n",
      "    At iteration 5400 -> loss: 0.09284971252543621\n",
      "    At iteration 5500 -> loss: 0.09289613216296394\n",
      "    At iteration 5600 -> loss: 0.0928233944608004\n",
      "    At iteration 5700 -> loss: 0.0927977140241327\n",
      "    At iteration 5800 -> loss: 0.0927384651901192\n",
      "    At iteration 5900 -> loss: 0.09272333018341299\n",
      "    At iteration 6000 -> loss: 0.0926810816538017\n",
      "    At iteration 6100 -> loss: 0.09266431861692385\n",
      "    At iteration 6200 -> loss: 0.09267244720662589\n",
      "    At iteration 6300 -> loss: 0.09264193085586102\n",
      "    At iteration 6400 -> loss: 0.09262234137965805\n",
      "    At iteration 6500 -> loss: 0.09266734026462577\n",
      "    At iteration 6600 -> loss: 0.0927182997141767\n",
      "    At iteration 6700 -> loss: 0.09268944475878425\n",
      "    At iteration 6800 -> loss: 0.0926359052090325\n",
      "    At iteration 6900 -> loss: 0.09265148657206644\n",
      "    At iteration 7000 -> loss: 0.09261792391520728\n",
      "    At iteration 7100 -> loss: 0.09260566969588709\n",
      "    At iteration 7200 -> loss: 0.09262663308471661\n",
      "    At iteration 7300 -> loss: 0.09263566080797409\n",
      "    At iteration 7400 -> loss: 0.09271048454996256\n",
      "    At iteration 7500 -> loss: 0.09265888701928808\n",
      "    At iteration 7600 -> loss: 0.09266178183167487\n",
      "    At iteration 7700 -> loss: 0.09267292359047359\n",
      "    At iteration 7800 -> loss: 0.09264346006886506\n",
      "    At iteration 7900 -> loss: 0.0927732598493375\n",
      "    At iteration 8000 -> loss: 0.09273609781465698\n",
      "    At iteration 8100 -> loss: 0.09275289265628903\n",
      "    At iteration 8200 -> loss: 0.0927774208620121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8300 -> loss: 0.09291339505464974\n",
      "    At iteration 8400 -> loss: 0.09287351862741246\n",
      "    At iteration 8500 -> loss: 0.09284651949943294\n",
      "    At iteration 8600 -> loss: 0.0928036702938688\n",
      "    At iteration 8700 -> loss: 0.09275358707970671\n",
      "    At iteration 8800 -> loss: 0.09278971033668676\n",
      "    At iteration 8900 -> loss: 0.09276519842926544\n",
      "    At iteration 9000 -> loss: 0.09279979937642129\n",
      "    At iteration 9100 -> loss: 0.09276305387089884\n",
      "    At iteration 9200 -> loss: 0.09274037136069804\n",
      "    At iteration 9300 -> loss: 0.09272889085112095\n",
      "    At iteration 9400 -> loss: 0.09268769391262004\n",
      "    At iteration 9500 -> loss: 0.0927274892023441\n",
      "    At iteration 9600 -> loss: 0.09274265195044744\n",
      "    At iteration 9700 -> loss: 0.09304891951187962\n",
      "    At iteration 9800 -> loss: 0.09300672409749042\n",
      "    At iteration 9900 -> loss: 0.09301803698106476\n",
      "    At iteration 10000 -> loss: 0.09300835895439793\n",
      "    At iteration 10100 -> loss: 0.09298249486768848\n",
      "    At iteration 10200 -> loss: 0.09297287451805748\n",
      "    At iteration 10300 -> loss: 0.09296827233270423\n",
      "    At iteration 10400 -> loss: 0.09295067909438948\n",
      "    At iteration 10500 -> loss: 0.09293549534531706\n",
      "    At iteration 10600 -> loss: 0.09292816857195389\n",
      "    At iteration 10700 -> loss: 0.09294682814467632\n",
      "    At iteration 10800 -> loss: 0.09295437351443227\n",
      "    At iteration 10900 -> loss: 0.092932945621383\n",
      "    At iteration 11000 -> loss: 0.09294227718732521\n",
      "    At iteration 11100 -> loss: 0.09298881616131065\n",
      "    At iteration 11200 -> loss: 0.09296614172549615\n",
      "    At iteration 11300 -> loss: 0.09294124237373724\n",
      "    At iteration 11400 -> loss: 0.09295512284499088\n",
      "    At iteration 11500 -> loss: 0.0929468394691679\n",
      "    At iteration 11600 -> loss: 0.09297184519702677\n",
      "    At iteration 11700 -> loss: 0.09293547060967162\n",
      "    At iteration 11800 -> loss: 0.09291570250346054\n",
      "    At iteration 11900 -> loss: 0.09287846994259812\n",
      "    At iteration 12000 -> loss: 0.09285713163308214\n",
      "    At iteration 12100 -> loss: 0.09294130800935368\n",
      "    At iteration 12200 -> loss: 0.09292738524890337\n",
      "    At iteration 12300 -> loss: 0.09292238265001468\n",
      "    At iteration 12400 -> loss: 0.09297208259763609\n",
      "    At iteration 12500 -> loss: 0.09300719840944295\n",
      "    At iteration 12600 -> loss: 0.09299098963944712\n",
      "    At iteration 12700 -> loss: 0.09296750206649776\n",
      "    At iteration 12800 -> loss: 0.09296178809276591\n",
      "    At iteration 12900 -> loss: 0.09297220052625163\n",
      "    At iteration 13000 -> loss: 0.09296710940540145\n",
      "    At iteration 13100 -> loss: 0.09301077291761055\n",
      "    At iteration 13200 -> loss: 0.09303647089268195\n",
      "    At iteration 13300 -> loss: 0.09301814996799859\n",
      "    At iteration 13400 -> loss: 0.09303825259240084\n",
      "    At iteration 13500 -> loss: 0.0930202648223576\n",
      "    At iteration 13600 -> loss: 0.09301711395863581\n",
      "Staring Epoch 106\n",
      "    At iteration 0 -> loss: 0.0832822872325778\n",
      "    At iteration 100 -> loss: 0.08868334658025263\n",
      "    At iteration 200 -> loss: 0.08899502664493597\n",
      "    At iteration 300 -> loss: 0.0893512415510514\n",
      "    At iteration 400 -> loss: 0.09057101173220083\n",
      "    At iteration 500 -> loss: 0.09081330735555075\n",
      "    At iteration 600 -> loss: 0.09048313150003462\n",
      "    At iteration 700 -> loss: 0.09116504145178991\n",
      "    At iteration 800 -> loss: 0.09167370092238948\n",
      "    At iteration 900 -> loss: 0.09153087188320824\n",
      "    At iteration 1000 -> loss: 0.09151033168903835\n",
      "    At iteration 1100 -> loss: 0.09155406092472892\n",
      "    At iteration 1200 -> loss: 0.09151519578983155\n",
      "    At iteration 1300 -> loss: 0.09251885348040632\n",
      "    At iteration 1400 -> loss: 0.09245257394093691\n",
      "    At iteration 1500 -> loss: 0.09266630337473786\n",
      "    At iteration 1600 -> loss: 0.09258836625156933\n",
      "    At iteration 1700 -> loss: 0.09243773456738903\n",
      "    At iteration 1800 -> loss: 0.09256406408935455\n",
      "    At iteration 1900 -> loss: 0.09290795024065157\n",
      "    At iteration 2000 -> loss: 0.09280295305939874\n",
      "    At iteration 2100 -> loss: 0.09299251772431943\n",
      "    At iteration 2200 -> loss: 0.09290143119721492\n",
      "    At iteration 2300 -> loss: 0.09278552628414481\n",
      "    At iteration 2400 -> loss: 0.09267031568146551\n",
      "    At iteration 2500 -> loss: 0.09278505818893043\n",
      "    At iteration 2600 -> loss: 0.09282055922451002\n",
      "    At iteration 2700 -> loss: 0.09271490878263382\n",
      "    At iteration 2800 -> loss: 0.09267205569470484\n",
      "    At iteration 2900 -> loss: 0.09274595611593987\n",
      "    At iteration 3000 -> loss: 0.092747848605128\n",
      "    At iteration 3100 -> loss: 0.09275992146702097\n",
      "    At iteration 3200 -> loss: 0.0927195187788198\n",
      "    At iteration 3300 -> loss: 0.09264988773092835\n",
      "    At iteration 3400 -> loss: 0.09260588946652719\n",
      "    At iteration 3500 -> loss: 0.09253008265242448\n",
      "    At iteration 3600 -> loss: 0.09275674975112536\n",
      "    At iteration 3700 -> loss: 0.09272561384673829\n",
      "    At iteration 3800 -> loss: 0.09267484692036285\n",
      "    At iteration 3900 -> loss: 0.09273002024698936\n",
      "    At iteration 4000 -> loss: 0.09266056455157747\n",
      "    At iteration 4100 -> loss: 0.09271400764816998\n",
      "    At iteration 4200 -> loss: 0.09262623061316332\n",
      "    At iteration 4300 -> loss: 0.09282254755696076\n",
      "    At iteration 4400 -> loss: 0.09275736120077069\n",
      "    At iteration 4500 -> loss: 0.0927450645711556\n",
      "    At iteration 4600 -> loss: 0.09265233325794926\n",
      "    At iteration 4700 -> loss: 0.09265100192335667\n",
      "    At iteration 4800 -> loss: 0.09262847720754891\n",
      "    At iteration 4900 -> loss: 0.09264325083762344\n",
      "    At iteration 5000 -> loss: 0.09268132248110976\n",
      "    At iteration 5100 -> loss: 0.09263248992648243\n",
      "    At iteration 5200 -> loss: 0.09270695724333247\n",
      "    At iteration 5300 -> loss: 0.0926803814149565\n",
      "    At iteration 5400 -> loss: 0.0926937847800694\n",
      "    At iteration 5500 -> loss: 0.09266506647252078\n",
      "    At iteration 5600 -> loss: 0.09263516099210943\n",
      "    At iteration 5700 -> loss: 0.09268970321380494\n",
      "    At iteration 5800 -> loss: 0.09289541959431277\n",
      "    At iteration 5900 -> loss: 0.09281222960536235\n",
      "    At iteration 6000 -> loss: 0.09274437359844026\n",
      "    At iteration 6100 -> loss: 0.09271259383198113\n",
      "    At iteration 6200 -> loss: 0.09266424044771757\n",
      "    At iteration 6300 -> loss: 0.09261800243734257\n",
      "    At iteration 6400 -> loss: 0.0926689173324968\n",
      "    At iteration 6500 -> loss: 0.09266480883444755\n",
      "    At iteration 6600 -> loss: 0.09259580475970354\n",
      "    At iteration 6700 -> loss: 0.09256151596054177\n",
      "    At iteration 6800 -> loss: 0.09249379187478256\n",
      "    At iteration 6900 -> loss: 0.0925059680776082\n",
      "    At iteration 7000 -> loss: 0.09261977275841636\n",
      "    At iteration 7100 -> loss: 0.0926475669560319\n",
      "    At iteration 7200 -> loss: 0.09260744731989556\n",
      "    At iteration 7300 -> loss: 0.09259360043614909\n",
      "    At iteration 7400 -> loss: 0.09260064615040779\n",
      "    At iteration 7500 -> loss: 0.09264113538388806\n",
      "    At iteration 7600 -> loss: 0.09270080561674232\n",
      "    At iteration 7700 -> loss: 0.09281850641520473\n",
      "    At iteration 7800 -> loss: 0.09293218725035494\n",
      "    At iteration 7900 -> loss: 0.09292201320412591\n",
      "    At iteration 8000 -> loss: 0.09290732448588628\n",
      "    At iteration 8100 -> loss: 0.09292237328883242\n",
      "    At iteration 8200 -> loss: 0.09298897570738414\n",
      "    At iteration 8300 -> loss: 0.09297699840639402\n",
      "    At iteration 8400 -> loss: 0.09301430745437514\n",
      "    At iteration 8500 -> loss: 0.09297307012936866\n",
      "    At iteration 8600 -> loss: 0.0929825832222653\n",
      "    At iteration 8700 -> loss: 0.0929710627971912\n",
      "    At iteration 8800 -> loss: 0.09297591653872214\n",
      "    At iteration 8900 -> loss: 0.09301980561196414\n",
      "    At iteration 9000 -> loss: 0.0930135620635416\n",
      "    At iteration 9100 -> loss: 0.09301898096769816\n",
      "    At iteration 9200 -> loss: 0.09299945241756304\n",
      "    At iteration 9300 -> loss: 0.09301417724644237\n",
      "    At iteration 9400 -> loss: 0.09298163129348637\n",
      "    At iteration 9500 -> loss: 0.09293093910008732\n",
      "    At iteration 9600 -> loss: 0.09290605948398611\n",
      "    At iteration 9700 -> loss: 0.09288287479591352\n",
      "    At iteration 9800 -> loss: 0.09287264518487902\n",
      "    At iteration 9900 -> loss: 0.09285342240214525\n",
      "    At iteration 10000 -> loss: 0.09288237792177831\n",
      "    At iteration 10100 -> loss: 0.09289155450447373\n",
      "    At iteration 10200 -> loss: 0.09287296474543728\n",
      "    At iteration 10300 -> loss: 0.09286786604225149\n",
      "    At iteration 10400 -> loss: 0.0928181598647451\n",
      "    At iteration 10500 -> loss: 0.0928633025193565\n",
      "    At iteration 10600 -> loss: 0.09284030294197682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10700 -> loss: 0.092813854906241\n",
      "    At iteration 10800 -> loss: 0.09279924658296686\n",
      "    At iteration 10900 -> loss: 0.09278875610513132\n",
      "    At iteration 11000 -> loss: 0.09276240475923553\n",
      "    At iteration 11100 -> loss: 0.09275053498946864\n",
      "    At iteration 11200 -> loss: 0.09279022801896947\n",
      "    At iteration 11300 -> loss: 0.09278764478325216\n",
      "    At iteration 11400 -> loss: 0.09305532150918744\n",
      "    At iteration 11500 -> loss: 0.09305271422192536\n",
      "    At iteration 11600 -> loss: 0.09304357711996795\n",
      "    At iteration 11700 -> loss: 0.09304239641029571\n",
      "    At iteration 11800 -> loss: 0.09308881336029418\n",
      "    At iteration 11900 -> loss: 0.09305432785407959\n",
      "    At iteration 12000 -> loss: 0.09302501134402828\n",
      "    At iteration 12100 -> loss: 0.09301217858638418\n",
      "    At iteration 12200 -> loss: 0.09301427914853563\n",
      "    At iteration 12300 -> loss: 0.09308437326164025\n",
      "    At iteration 12400 -> loss: 0.09305060743506953\n",
      "    At iteration 12500 -> loss: 0.09312853729502658\n",
      "    At iteration 12600 -> loss: 0.09309860834886437\n",
      "    At iteration 12700 -> loss: 0.09308045623299224\n",
      "    At iteration 12800 -> loss: 0.09308821647370699\n",
      "    At iteration 12900 -> loss: 0.0930870854671302\n",
      "    At iteration 13000 -> loss: 0.09314609325872585\n",
      "    At iteration 13100 -> loss: 0.09311943001388719\n",
      "    At iteration 13200 -> loss: 0.09309768267805746\n",
      "    At iteration 13300 -> loss: 0.0930977222414696\n",
      "    At iteration 13400 -> loss: 0.09306186532393822\n",
      "    At iteration 13500 -> loss: 0.09306016998383186\n",
      "    At iteration 13600 -> loss: 0.09303021198054048\n",
      "Staring Epoch 107\n",
      "    At iteration 0 -> loss: 0.09802580578252673\n",
      "    At iteration 100 -> loss: 0.09197234046043809\n",
      "    At iteration 200 -> loss: 0.09075319848006745\n",
      "    At iteration 300 -> loss: 0.09138178851918852\n",
      "    At iteration 400 -> loss: 0.09189095134081239\n",
      "    At iteration 500 -> loss: 0.09205075206846129\n",
      "    At iteration 600 -> loss: 0.09183657608180383\n",
      "    At iteration 700 -> loss: 0.09200651954699011\n",
      "    At iteration 800 -> loss: 0.09187790411918309\n",
      "    At iteration 900 -> loss: 0.09184481448049184\n",
      "    At iteration 1000 -> loss: 0.09158105443442044\n",
      "    At iteration 1100 -> loss: 0.09235870179079603\n",
      "    At iteration 1200 -> loss: 0.09315264437736495\n",
      "    At iteration 1300 -> loss: 0.09275766811808185\n",
      "    At iteration 1400 -> loss: 0.0926860130968243\n",
      "    At iteration 1500 -> loss: 0.09255034646357863\n",
      "    At iteration 1600 -> loss: 0.09271217239512472\n",
      "    At iteration 1700 -> loss: 0.09353154776692867\n",
      "    At iteration 1800 -> loss: 0.09322337804319464\n",
      "    At iteration 1900 -> loss: 0.09310787855740364\n",
      "    At iteration 2000 -> loss: 0.09322075341011271\n",
      "    At iteration 2100 -> loss: 0.09304218258821112\n",
      "    At iteration 2200 -> loss: 0.0929887930015022\n",
      "    At iteration 2300 -> loss: 0.09293129938936186\n",
      "    At iteration 2400 -> loss: 0.09285226249847259\n",
      "    At iteration 2500 -> loss: 0.09271650566796708\n",
      "    At iteration 2600 -> loss: 0.0926512378459438\n",
      "    At iteration 2700 -> loss: 0.0930296512216293\n",
      "    At iteration 2800 -> loss: 0.09290366044079501\n",
      "    At iteration 2900 -> loss: 0.0928795950861289\n",
      "    At iteration 3000 -> loss: 0.09299270736156125\n",
      "    At iteration 3100 -> loss: 0.09287700741038953\n",
      "    At iteration 3200 -> loss: 0.0929315436024902\n",
      "    At iteration 3300 -> loss: 0.09292728048691443\n",
      "    At iteration 3400 -> loss: 0.0928404375353753\n",
      "    At iteration 3500 -> loss: 0.09271014544450223\n",
      "    At iteration 3600 -> loss: 0.09272455325479403\n",
      "    At iteration 3700 -> loss: 0.09274646559351607\n",
      "    At iteration 3800 -> loss: 0.09265754055703972\n",
      "    At iteration 3900 -> loss: 0.09295530623230322\n",
      "    At iteration 4000 -> loss: 0.09292727646837814\n",
      "    At iteration 4100 -> loss: 0.09297893403082186\n",
      "    At iteration 4200 -> loss: 0.09297525099391356\n",
      "    At iteration 4300 -> loss: 0.09290836495057475\n",
      "    At iteration 4400 -> loss: 0.0928736445457798\n",
      "    At iteration 4500 -> loss: 0.0929341609091267\n",
      "    At iteration 4600 -> loss: 0.09294547639561006\n",
      "    At iteration 4700 -> loss: 0.09291121506922527\n",
      "    At iteration 4800 -> loss: 0.09286315405166719\n",
      "    At iteration 4900 -> loss: 0.0929779284820784\n",
      "    At iteration 5000 -> loss: 0.09308714626155182\n",
      "    At iteration 5100 -> loss: 0.09297394752144036\n",
      "    At iteration 5200 -> loss: 0.09292589402060636\n",
      "    At iteration 5300 -> loss: 0.09297613526770397\n",
      "    At iteration 5400 -> loss: 0.09294633215320347\n",
      "    At iteration 5500 -> loss: 0.09296893502354556\n",
      "    At iteration 5600 -> loss: 0.09299399737215829\n",
      "    At iteration 5700 -> loss: 0.09294334854145005\n",
      "    At iteration 5800 -> loss: 0.09298937399279526\n",
      "    At iteration 5900 -> loss: 0.09294451194567967\n",
      "    At iteration 6000 -> loss: 0.09303638364112102\n",
      "    At iteration 6100 -> loss: 0.09299118660490187\n",
      "    At iteration 6200 -> loss: 0.09292770044322042\n",
      "    At iteration 6300 -> loss: 0.09294940315617796\n",
      "    At iteration 6400 -> loss: 0.0929232075001516\n",
      "    At iteration 6500 -> loss: 0.09287946155275643\n",
      "    At iteration 6600 -> loss: 0.09294926709445189\n",
      "    At iteration 6700 -> loss: 0.09289631215888249\n",
      "    At iteration 6800 -> loss: 0.09292494423672011\n",
      "    At iteration 6900 -> loss: 0.0929109735744638\n",
      "    At iteration 7000 -> loss: 0.09298393753009858\n",
      "    At iteration 7100 -> loss: 0.09294637141125783\n",
      "    At iteration 7200 -> loss: 0.09299983403701043\n",
      "    At iteration 7300 -> loss: 0.09296254750293009\n",
      "    At iteration 7400 -> loss: 0.09291924007769929\n",
      "    At iteration 7500 -> loss: 0.09301337665148415\n",
      "    At iteration 7600 -> loss: 0.09298650438500664\n",
      "    At iteration 7700 -> loss: 0.0931714385232575\n",
      "    At iteration 7800 -> loss: 0.09313888488614498\n",
      "    At iteration 7900 -> loss: 0.09311681338901445\n",
      "    At iteration 8000 -> loss: 0.09309275400470891\n",
      "    At iteration 8100 -> loss: 0.09311073803220728\n",
      "    At iteration 8200 -> loss: 0.09309490172826185\n",
      "    At iteration 8300 -> loss: 0.09311180394695215\n",
      "    At iteration 8400 -> loss: 0.09314294143994764\n",
      "    At iteration 8500 -> loss: 0.09311583496297368\n",
      "    At iteration 8600 -> loss: 0.09310608907889836\n",
      "    At iteration 8700 -> loss: 0.09307726552048728\n",
      "    At iteration 8800 -> loss: 0.09305388453759797\n",
      "    At iteration 8900 -> loss: 0.0930310566387271\n",
      "    At iteration 9000 -> loss: 0.09303808802189172\n",
      "    At iteration 9100 -> loss: 0.09300312589658628\n",
      "    At iteration 9200 -> loss: 0.09300455349585729\n",
      "    At iteration 9300 -> loss: 0.09295006005817329\n",
      "    At iteration 9400 -> loss: 0.09292226729741172\n",
      "    At iteration 9500 -> loss: 0.09291419007171414\n",
      "    At iteration 9600 -> loss: 0.09296521645308184\n",
      "    At iteration 9700 -> loss: 0.09297761609990134\n",
      "    At iteration 9800 -> loss: 0.09293803305383525\n",
      "    At iteration 9900 -> loss: 0.09291007644339998\n",
      "    At iteration 10000 -> loss: 0.0928718806850084\n",
      "    At iteration 10100 -> loss: 0.09289424406891056\n",
      "    At iteration 10200 -> loss: 0.09286230139293869\n",
      "    At iteration 10300 -> loss: 0.09286110488480995\n",
      "    At iteration 10400 -> loss: 0.0928507202808803\n",
      "    At iteration 10500 -> loss: 0.09283724356211973\n",
      "    At iteration 10600 -> loss: 0.09284491922735538\n",
      "    At iteration 10700 -> loss: 0.09281335347499035\n",
      "    At iteration 10800 -> loss: 0.09281884086375998\n",
      "    At iteration 10900 -> loss: 0.0928161416049285\n",
      "    At iteration 11000 -> loss: 0.09286859573329245\n",
      "    At iteration 11100 -> loss: 0.09282519383580659\n",
      "    At iteration 11200 -> loss: 0.09280506149738753\n",
      "    At iteration 11300 -> loss: 0.09278174783230277\n",
      "    At iteration 11400 -> loss: 0.09275325701343327\n",
      "    At iteration 11500 -> loss: 0.09277308488396592\n",
      "    At iteration 11600 -> loss: 0.09279073668565321\n",
      "    At iteration 11700 -> loss: 0.09292927885970871\n",
      "    At iteration 11800 -> loss: 0.09288533623063742\n",
      "    At iteration 11900 -> loss: 0.0931637723244738\n",
      "    At iteration 12000 -> loss: 0.09316191756456725\n",
      "    At iteration 12100 -> loss: 0.0931595084902282\n",
      "    At iteration 12200 -> loss: 0.09313413334757362\n",
      "    At iteration 12300 -> loss: 0.09309442496306164\n",
      "    At iteration 12400 -> loss: 0.09309745719923238\n",
      "    At iteration 12500 -> loss: 0.09307184663472465\n",
      "    At iteration 12600 -> loss: 0.09308647255766145\n",
      "    At iteration 12700 -> loss: 0.09306703193711892\n",
      "    At iteration 12800 -> loss: 0.0930827335468057\n",
      "    At iteration 12900 -> loss: 0.0931155468270402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 13000 -> loss: 0.09308133966816648\n",
      "    At iteration 13100 -> loss: 0.093075008882684\n",
      "    At iteration 13200 -> loss: 0.09304549348649106\n",
      "    At iteration 13300 -> loss: 0.09307339571567122\n",
      "    At iteration 13400 -> loss: 0.09304933223816203\n",
      "    At iteration 13500 -> loss: 0.09302436278339134\n",
      "    At iteration 13600 -> loss: 0.09303602747143348\n",
      "Staring Epoch 108\n",
      "    At iteration 0 -> loss: 0.10694124922156334\n",
      "    At iteration 100 -> loss: 0.09323555630302989\n",
      "    At iteration 200 -> loss: 0.09437501905485772\n",
      "    At iteration 300 -> loss: 0.09505660051190776\n",
      "    At iteration 400 -> loss: 0.09542883387953953\n",
      "    At iteration 500 -> loss: 0.09491910061885281\n",
      "    At iteration 600 -> loss: 0.09387103747235355\n",
      "    At iteration 700 -> loss: 0.09363349360912102\n",
      "    At iteration 800 -> loss: 0.09368450773939194\n",
      "    At iteration 900 -> loss: 0.09356455259226551\n",
      "    At iteration 1000 -> loss: 0.09342959248079408\n",
      "    At iteration 1100 -> loss: 0.09304045709265502\n",
      "    At iteration 1200 -> loss: 0.09286770254602582\n",
      "    At iteration 1300 -> loss: 0.09299306561422481\n",
      "    At iteration 1400 -> loss: 0.09309646388820764\n",
      "    At iteration 1500 -> loss: 0.09318676211219283\n",
      "    At iteration 1600 -> loss: 0.09313419655709543\n",
      "    At iteration 1700 -> loss: 0.09294430754385204\n",
      "    At iteration 1800 -> loss: 0.09308866273192977\n",
      "    At iteration 1900 -> loss: 0.094603429119763\n",
      "    At iteration 2000 -> loss: 0.09430502415595328\n",
      "    At iteration 2100 -> loss: 0.09420779327033861\n",
      "    At iteration 2200 -> loss: 0.09410149556052545\n",
      "    At iteration 2300 -> loss: 0.09419789722040761\n",
      "    At iteration 2400 -> loss: 0.0940464101926227\n",
      "    At iteration 2500 -> loss: 0.0938829758288966\n",
      "    At iteration 2600 -> loss: 0.0938016425784262\n",
      "    At iteration 2700 -> loss: 0.09408058799960668\n",
      "    At iteration 2800 -> loss: 0.09406070313162199\n",
      "    At iteration 2900 -> loss: 0.09390991921908155\n",
      "    At iteration 3000 -> loss: 0.09403387737728357\n",
      "    At iteration 3100 -> loss: 0.09386444398509516\n",
      "    At iteration 3200 -> loss: 0.09390313310481414\n",
      "    At iteration 3300 -> loss: 0.09417904267121986\n",
      "    At iteration 3400 -> loss: 0.09406116674287321\n",
      "    At iteration 3500 -> loss: 0.09405787124876644\n",
      "    At iteration 3600 -> loss: 0.09397014352024947\n",
      "    At iteration 3700 -> loss: 0.09391952437390783\n",
      "    At iteration 3800 -> loss: 0.09387818437764413\n",
      "    At iteration 3900 -> loss: 0.09386475391893563\n",
      "    At iteration 4000 -> loss: 0.09381258885411244\n",
      "    At iteration 4100 -> loss: 0.09373054140808632\n",
      "    At iteration 4200 -> loss: 0.09390616427372825\n",
      "    At iteration 4300 -> loss: 0.09382892608750323\n",
      "    At iteration 4400 -> loss: 0.09380205800844839\n",
      "    At iteration 4500 -> loss: 0.0937174428918035\n",
      "    At iteration 4600 -> loss: 0.0936767037066675\n",
      "    At iteration 4700 -> loss: 0.09360864134519992\n",
      "    At iteration 4800 -> loss: 0.09357067589620825\n",
      "    At iteration 4900 -> loss: 0.09349936658891052\n",
      "    At iteration 5000 -> loss: 0.09342884053460938\n",
      "    At iteration 5100 -> loss: 0.093579236106113\n",
      "    At iteration 5200 -> loss: 0.09367909756115694\n",
      "    At iteration 5300 -> loss: 0.09364066391001565\n",
      "    At iteration 5400 -> loss: 0.09356353609129095\n",
      "    At iteration 5500 -> loss: 0.09356083765079659\n",
      "    At iteration 5600 -> loss: 0.09349896256743301\n",
      "    At iteration 5700 -> loss: 0.09351090669961196\n",
      "    At iteration 5800 -> loss: 0.09348578883673757\n",
      "    At iteration 5900 -> loss: 0.09346451680736098\n",
      "    At iteration 6000 -> loss: 0.09344081075539988\n",
      "    At iteration 6100 -> loss: 0.09343659961802592\n",
      "    At iteration 6200 -> loss: 0.09341321942450266\n",
      "    At iteration 6300 -> loss: 0.09351356095854683\n",
      "    At iteration 6400 -> loss: 0.09359489268584041\n",
      "    At iteration 6500 -> loss: 0.09357477258790656\n",
      "    At iteration 6600 -> loss: 0.09350449768305852\n",
      "    At iteration 6700 -> loss: 0.09349351385739578\n",
      "    At iteration 6800 -> loss: 0.09352366805971361\n",
      "    At iteration 6900 -> loss: 0.09351882571026061\n",
      "    At iteration 7000 -> loss: 0.09345194314491172\n",
      "    At iteration 7100 -> loss: 0.09338877413460484\n",
      "    At iteration 7200 -> loss: 0.0934385637572602\n",
      "    At iteration 7300 -> loss: 0.0934820640202337\n",
      "    At iteration 7400 -> loss: 0.09343922773902864\n",
      "    At iteration 7500 -> loss: 0.09339688873378041\n",
      "    At iteration 7600 -> loss: 0.09350999871864728\n",
      "    At iteration 7700 -> loss: 0.09349998374532645\n",
      "    At iteration 7800 -> loss: 0.09348393170187785\n",
      "    At iteration 7900 -> loss: 0.09348720657933919\n",
      "    At iteration 8000 -> loss: 0.0934465710495582\n",
      "    At iteration 8100 -> loss: 0.09342223304572946\n",
      "    At iteration 8200 -> loss: 0.09340219751082506\n",
      "    At iteration 8300 -> loss: 0.09340352106177786\n",
      "    At iteration 8400 -> loss: 0.09337494442882552\n",
      "    At iteration 8500 -> loss: 0.09335504375838442\n",
      "    At iteration 8600 -> loss: 0.09342848546030424\n",
      "    At iteration 8700 -> loss: 0.09338077955086005\n",
      "    At iteration 8800 -> loss: 0.09335150304633623\n",
      "    At iteration 8900 -> loss: 0.09331719710950939\n",
      "    At iteration 9000 -> loss: 0.09331208651811233\n",
      "    At iteration 9100 -> loss: 0.09329621881256915\n",
      "    At iteration 9200 -> loss: 0.09329866388190024\n",
      "    At iteration 9300 -> loss: 0.09325637071003141\n",
      "    At iteration 9400 -> loss: 0.09322781640811659\n",
      "    At iteration 9500 -> loss: 0.09320919532289189\n",
      "    At iteration 9600 -> loss: 0.09333373276000617\n",
      "    At iteration 9700 -> loss: 0.09328381806943226\n",
      "    At iteration 9800 -> loss: 0.0932623865373756\n",
      "    At iteration 9900 -> loss: 0.0933344990149844\n",
      "    At iteration 10000 -> loss: 0.09330511185751872\n",
      "    At iteration 10100 -> loss: 0.09326188617432676\n",
      "    At iteration 10200 -> loss: 0.09325087780340985\n",
      "    At iteration 10300 -> loss: 0.09324087305117605\n",
      "    At iteration 10400 -> loss: 0.0932470809305232\n",
      "    At iteration 10500 -> loss: 0.09324379107228833\n",
      "    At iteration 10600 -> loss: 0.09319813402583521\n",
      "    At iteration 10700 -> loss: 0.09317848299714168\n",
      "    At iteration 10800 -> loss: 0.09318084917289027\n",
      "    At iteration 10900 -> loss: 0.09319960814732786\n",
      "    At iteration 11000 -> loss: 0.09318335258800081\n",
      "    At iteration 11100 -> loss: 0.09315423176585239\n",
      "    At iteration 11200 -> loss: 0.09323348986437777\n",
      "    At iteration 11300 -> loss: 0.09320420863580675\n",
      "    At iteration 11400 -> loss: 0.0931822986825149\n",
      "    At iteration 11500 -> loss: 0.09314408148627051\n",
      "    At iteration 11600 -> loss: 0.0931317601222585\n",
      "    At iteration 11700 -> loss: 0.09310954219013748\n",
      "    At iteration 11800 -> loss: 0.09313293363223292\n",
      "    At iteration 11900 -> loss: 0.09308922754253018\n",
      "    At iteration 12000 -> loss: 0.09311171432271392\n",
      "    At iteration 12100 -> loss: 0.0930761340919596\n",
      "    At iteration 12200 -> loss: 0.09305498592570628\n",
      "    At iteration 12300 -> loss: 0.09305936879443104\n",
      "    At iteration 12400 -> loss: 0.09306134133535013\n",
      "    At iteration 12500 -> loss: 0.09305220365668904\n",
      "    At iteration 12600 -> loss: 0.09302431792960991\n",
      "    At iteration 12700 -> loss: 0.09300008243664293\n",
      "    At iteration 12800 -> loss: 0.09297820554760308\n",
      "    At iteration 12900 -> loss: 0.09296447896722981\n",
      "    At iteration 13000 -> loss: 0.09299291851459277\n",
      "    At iteration 13100 -> loss: 0.09296600705639473\n",
      "    At iteration 13200 -> loss: 0.0929388971728194\n",
      "    At iteration 13300 -> loss: 0.09293227298611603\n",
      "    At iteration 13400 -> loss: 0.0929410572552345\n",
      "    At iteration 13500 -> loss: 0.09297640542034727\n",
      "    At iteration 13600 -> loss: 0.09303792198658224\n",
      "Staring Epoch 109\n",
      "    At iteration 0 -> loss: 0.09248101059347391\n",
      "    At iteration 100 -> loss: 0.10763380602612856\n",
      "    At iteration 200 -> loss: 0.09836089587299592\n",
      "    At iteration 300 -> loss: 0.09557833164159087\n",
      "    At iteration 400 -> loss: 0.09434049301516627\n",
      "    At iteration 500 -> loss: 0.09402416454496318\n",
      "    At iteration 600 -> loss: 0.09373445037685736\n",
      "    At iteration 700 -> loss: 0.09350546219198437\n",
      "    At iteration 800 -> loss: 0.09345237300067781\n",
      "    At iteration 900 -> loss: 0.09376836148498054\n",
      "    At iteration 1000 -> loss: 0.09381525061681602\n",
      "    At iteration 1100 -> loss: 0.0935977680056893\n",
      "    At iteration 1200 -> loss: 0.09316978642169924\n",
      "    At iteration 1300 -> loss: 0.09308827883483199\n",
      "    At iteration 1400 -> loss: 0.09331609747827023\n",
      "    At iteration 1500 -> loss: 0.09320017847134641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1600 -> loss: 0.09299811025237492\n",
      "    At iteration 1700 -> loss: 0.09286763608915195\n",
      "    At iteration 1800 -> loss: 0.09291307474329538\n",
      "    At iteration 1900 -> loss: 0.09289559042040957\n",
      "    At iteration 2000 -> loss: 0.0929416755816739\n",
      "    At iteration 2100 -> loss: 0.09303319443244565\n",
      "    At iteration 2200 -> loss: 0.0929669045533779\n",
      "    At iteration 2300 -> loss: 0.09287690835455728\n",
      "    At iteration 2400 -> loss: 0.09281708396825247\n",
      "    At iteration 2500 -> loss: 0.0934607792695687\n",
      "    At iteration 2600 -> loss: 0.09340677702057039\n",
      "    At iteration 2700 -> loss: 0.09340886146300457\n",
      "    At iteration 2800 -> loss: 0.09337651131436202\n",
      "    At iteration 2900 -> loss: 0.09333513282198755\n",
      "    At iteration 3000 -> loss: 0.0932522141949716\n",
      "    At iteration 3100 -> loss: 0.09315970554013485\n",
      "    At iteration 3200 -> loss: 0.09316671069366476\n",
      "    At iteration 3300 -> loss: 0.09319542888690949\n",
      "    At iteration 3400 -> loss: 0.09315854741964377\n",
      "    At iteration 3500 -> loss: 0.09307072917912158\n",
      "    At iteration 3600 -> loss: 0.09299482269567566\n",
      "    At iteration 3700 -> loss: 0.09294442234299904\n",
      "    At iteration 3800 -> loss: 0.0929615237129677\n",
      "    At iteration 3900 -> loss: 0.09293685071719053\n",
      "    At iteration 4000 -> loss: 0.0928948905026093\n",
      "    At iteration 4100 -> loss: 0.09282522667130283\n",
      "    At iteration 4200 -> loss: 0.09288059061952025\n",
      "    At iteration 4300 -> loss: 0.09290004215026343\n",
      "    At iteration 4400 -> loss: 0.09286293955019063\n",
      "    At iteration 4500 -> loss: 0.09276226656125776\n",
      "    At iteration 4600 -> loss: 0.09277136088550243\n",
      "    At iteration 4700 -> loss: 0.09271134789326829\n",
      "    At iteration 4800 -> loss: 0.09272614816823842\n",
      "    At iteration 4900 -> loss: 0.09295808865968126\n",
      "    At iteration 5000 -> loss: 0.09295176043618089\n",
      "    At iteration 5100 -> loss: 0.09296234268751455\n",
      "    At iteration 5200 -> loss: 0.09304549776507158\n",
      "    At iteration 5300 -> loss: 0.09314148240646336\n",
      "    At iteration 5400 -> loss: 0.09304438182454379\n",
      "    At iteration 5500 -> loss: 0.0929989605339723\n",
      "    At iteration 5600 -> loss: 0.09305919990332744\n",
      "    At iteration 5700 -> loss: 0.09303680280988887\n",
      "    At iteration 5800 -> loss: 0.09313217711499217\n",
      "    At iteration 5900 -> loss: 0.093052846171707\n",
      "    At iteration 6000 -> loss: 0.09307829080920597\n",
      "    At iteration 6100 -> loss: 0.093035516120195\n",
      "    At iteration 6200 -> loss: 0.0930193061065298\n",
      "    At iteration 6300 -> loss: 0.09298266682029219\n",
      "    At iteration 6400 -> loss: 0.09294346622332647\n",
      "    At iteration 6500 -> loss: 0.09304161863920825\n",
      "    At iteration 6600 -> loss: 0.09299227667114096\n",
      "    At iteration 6700 -> loss: 0.09301546349357331\n",
      "    At iteration 6800 -> loss: 0.09298879085621055\n",
      "    At iteration 6900 -> loss: 0.09292738833002043\n",
      "    At iteration 7000 -> loss: 0.09294897374211294\n",
      "    At iteration 7100 -> loss: 0.09292964915988955\n",
      "    At iteration 7200 -> loss: 0.09290485353228317\n",
      "    At iteration 7300 -> loss: 0.09286435764342725\n",
      "    At iteration 7400 -> loss: 0.09284792627281961\n",
      "    At iteration 7500 -> loss: 0.09283270741839368\n",
      "    At iteration 7600 -> loss: 0.09281503269301726\n",
      "    At iteration 7700 -> loss: 0.09277567478652117\n",
      "    At iteration 7800 -> loss: 0.0927725538509537\n",
      "    At iteration 7900 -> loss: 0.09275421127925645\n",
      "    At iteration 8000 -> loss: 0.09272796540684486\n",
      "    At iteration 8100 -> loss: 0.09273663792470663\n",
      "    At iteration 8200 -> loss: 0.09271059462444962\n",
      "    At iteration 8300 -> loss: 0.09271136670627708\n",
      "    At iteration 8400 -> loss: 0.09268777861796197\n",
      "    At iteration 8500 -> loss: 0.09266604309602665\n",
      "    At iteration 8600 -> loss: 0.09264168966682046\n",
      "    At iteration 8700 -> loss: 0.09297351279405777\n",
      "    At iteration 8800 -> loss: 0.09295307134859726\n",
      "    At iteration 8900 -> loss: 0.09294510694457887\n",
      "    At iteration 9000 -> loss: 0.09289215540295223\n",
      "    At iteration 9100 -> loss: 0.09288139973898268\n",
      "    At iteration 9200 -> loss: 0.09288801239030851\n",
      "    At iteration 9300 -> loss: 0.0928955230815128\n",
      "    At iteration 9400 -> loss: 0.0928780153494774\n",
      "    At iteration 9500 -> loss: 0.09287571006341043\n",
      "    At iteration 9600 -> loss: 0.09294059226075885\n",
      "    At iteration 9700 -> loss: 0.09295192187258186\n",
      "    At iteration 9800 -> loss: 0.09297235746920034\n",
      "    At iteration 9900 -> loss: 0.09294652750468821\n",
      "    At iteration 10000 -> loss: 0.09292361153758479\n",
      "    At iteration 10100 -> loss: 0.09300329365641094\n",
      "    At iteration 10200 -> loss: 0.09301734927676629\n",
      "    At iteration 10300 -> loss: 0.09299246781300038\n",
      "    At iteration 10400 -> loss: 0.09296908690113008\n",
      "    At iteration 10500 -> loss: 0.09295699458310139\n",
      "    At iteration 10600 -> loss: 0.09295984701738556\n",
      "    At iteration 10700 -> loss: 0.09296690348950931\n",
      "    At iteration 10800 -> loss: 0.09301288116819664\n",
      "    At iteration 10900 -> loss: 0.09298550317506918\n",
      "    At iteration 11000 -> loss: 0.09301058831518311\n",
      "    At iteration 11100 -> loss: 0.09298518052977069\n",
      "    At iteration 11200 -> loss: 0.09301373571470935\n",
      "    At iteration 11300 -> loss: 0.09298243533996885\n",
      "    At iteration 11400 -> loss: 0.09294216685609911\n",
      "    At iteration 11500 -> loss: 0.09293267336601421\n",
      "    At iteration 11600 -> loss: 0.09290524883780553\n",
      "    At iteration 11700 -> loss: 0.09291959270819276\n",
      "    At iteration 11800 -> loss: 0.09294696039440842\n",
      "    At iteration 11900 -> loss: 0.09292464267615042\n",
      "    At iteration 12000 -> loss: 0.09294006656208152\n",
      "    At iteration 12100 -> loss: 0.0929091371071464\n",
      "    At iteration 12200 -> loss: 0.09289209400309645\n",
      "    At iteration 12300 -> loss: 0.09287531260467925\n",
      "    At iteration 12400 -> loss: 0.0928575494571968\n",
      "    At iteration 12500 -> loss: 0.09286726458638175\n",
      "    At iteration 12600 -> loss: 0.09286839222101358\n",
      "    At iteration 12700 -> loss: 0.0928619159779117\n",
      "    At iteration 12800 -> loss: 0.09286528994819476\n",
      "    At iteration 12900 -> loss: 0.09286357967090286\n",
      "    At iteration 13000 -> loss: 0.09285516304388342\n",
      "    At iteration 13100 -> loss: 0.09289055856193897\n",
      "    At iteration 13200 -> loss: 0.092936295332881\n",
      "    At iteration 13300 -> loss: 0.09298090471538935\n",
      "    At iteration 13400 -> loss: 0.09297222723858943\n",
      "    At iteration 13500 -> loss: 0.09298068196082458\n",
      "    At iteration 13600 -> loss: 0.09297909922048966\n",
      "Staring Epoch 110\n",
      "    At iteration 0 -> loss: 0.08346591470763087\n",
      "    At iteration 100 -> loss: 0.08884140075094582\n",
      "    At iteration 200 -> loss: 0.09010039003306397\n",
      "    At iteration 300 -> loss: 0.09131871921388769\n",
      "    At iteration 400 -> loss: 0.09237250126426602\n",
      "    At iteration 500 -> loss: 0.09218732072757295\n",
      "    At iteration 600 -> loss: 0.09204570544926187\n",
      "    At iteration 700 -> loss: 0.0920982990450359\n",
      "    At iteration 800 -> loss: 0.09181830866891257\n",
      "    At iteration 900 -> loss: 0.0921884502037621\n",
      "    At iteration 1000 -> loss: 0.09191843799274574\n",
      "    At iteration 1100 -> loss: 0.09165114528343493\n",
      "    At iteration 1200 -> loss: 0.0918047003035708\n",
      "    At iteration 1300 -> loss: 0.09152920670197368\n",
      "    At iteration 1400 -> loss: 0.09124277616004038\n",
      "    At iteration 1500 -> loss: 0.09121725782067851\n",
      "    At iteration 1600 -> loss: 0.09117810210638674\n",
      "    At iteration 1700 -> loss: 0.09237229582082272\n",
      "    At iteration 1800 -> loss: 0.0924976945969201\n",
      "    At iteration 1900 -> loss: 0.09246057060883817\n",
      "    At iteration 2000 -> loss: 0.09255952268925766\n",
      "    At iteration 2100 -> loss: 0.09249867581085272\n",
      "    At iteration 2200 -> loss: 0.0923996818413133\n",
      "    At iteration 2300 -> loss: 0.09233593187970189\n",
      "    At iteration 2400 -> loss: 0.09237879917761127\n",
      "    At iteration 2500 -> loss: 0.0922777086801628\n",
      "    At iteration 2600 -> loss: 0.0926990288907148\n",
      "    At iteration 2700 -> loss: 0.09287802703768473\n",
      "    At iteration 2800 -> loss: 0.09279415265901415\n",
      "    At iteration 2900 -> loss: 0.09268360938708921\n",
      "    At iteration 3000 -> loss: 0.0929208759422506\n",
      "    At iteration 3100 -> loss: 0.09287888666950772\n",
      "    At iteration 3200 -> loss: 0.09279668649215249\n",
      "    At iteration 3300 -> loss: 0.09271419245907882\n",
      "    At iteration 3400 -> loss: 0.09288851838805008\n",
      "    At iteration 3500 -> loss: 0.09273857901973223\n",
      "    At iteration 3600 -> loss: 0.09311697165065051\n",
      "    At iteration 3700 -> loss: 0.09318443438550421\n",
      "    At iteration 3800 -> loss: 0.09328321805893428\n",
      "    At iteration 3900 -> loss: 0.0933076793084716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4000 -> loss: 0.09329009556774787\n",
      "    At iteration 4100 -> loss: 0.09328629306129854\n",
      "    At iteration 4200 -> loss: 0.09342960697745288\n",
      "    At iteration 4300 -> loss: 0.09338441291704185\n",
      "    At iteration 4400 -> loss: 0.09333815506287542\n",
      "    At iteration 4500 -> loss: 0.09371036187938873\n",
      "    At iteration 4600 -> loss: 0.0936462155793877\n",
      "    At iteration 4700 -> loss: 0.093582995719767\n",
      "    At iteration 4800 -> loss: 0.09354614994893333\n",
      "    At iteration 4900 -> loss: 0.09351773270525854\n",
      "    At iteration 5000 -> loss: 0.09345358309522654\n",
      "    At iteration 5100 -> loss: 0.09349112091144096\n",
      "    At iteration 5200 -> loss: 0.09351664484900642\n",
      "    At iteration 5300 -> loss: 0.09346945627091928\n",
      "    At iteration 5400 -> loss: 0.09353609801887272\n",
      "    At iteration 5500 -> loss: 0.09368017540573653\n",
      "    At iteration 5600 -> loss: 0.09367444370313044\n",
      "    At iteration 5700 -> loss: 0.09359955534996518\n",
      "    At iteration 5800 -> loss: 0.09355557253967202\n",
      "    At iteration 5900 -> loss: 0.09355209486437588\n",
      "    At iteration 6000 -> loss: 0.09355270394612031\n",
      "    At iteration 6100 -> loss: 0.09350958986462526\n",
      "    At iteration 6200 -> loss: 0.09344603169817743\n",
      "    At iteration 6300 -> loss: 0.09352339786045157\n",
      "    At iteration 6400 -> loss: 0.09345569341894355\n",
      "    At iteration 6500 -> loss: 0.09338287676152854\n",
      "    At iteration 6600 -> loss: 0.09333405748938549\n",
      "    At iteration 6700 -> loss: 0.093305146203915\n",
      "    At iteration 6800 -> loss: 0.09325996365345007\n",
      "    At iteration 6900 -> loss: 0.09321744779213643\n",
      "    At iteration 7000 -> loss: 0.09328893263363856\n",
      "    At iteration 7100 -> loss: 0.09327584672346695\n",
      "    At iteration 7200 -> loss: 0.09323783466356599\n",
      "    At iteration 7300 -> loss: 0.09321947416416541\n",
      "    At iteration 7400 -> loss: 0.09325242156112012\n",
      "    At iteration 7500 -> loss: 0.09327613619419539\n",
      "    At iteration 7600 -> loss: 0.09329925077574572\n",
      "    At iteration 7700 -> loss: 0.09323516771845251\n",
      "    At iteration 7800 -> loss: 0.09328711368709713\n",
      "    At iteration 7900 -> loss: 0.09326735154102588\n",
      "    At iteration 8000 -> loss: 0.09326470637703901\n",
      "    At iteration 8100 -> loss: 0.09326207846838946\n",
      "    At iteration 8200 -> loss: 0.09324797806358577\n",
      "    At iteration 8300 -> loss: 0.09321635893987909\n",
      "    At iteration 8400 -> loss: 0.09321526016342872\n",
      "    At iteration 8500 -> loss: 0.09318368396978176\n",
      "    At iteration 8600 -> loss: 0.09316412490614105\n",
      "    At iteration 8700 -> loss: 0.0931642136728361\n",
      "    At iteration 8800 -> loss: 0.09311980231034463\n",
      "    At iteration 8900 -> loss: 0.09313070088008503\n",
      "    At iteration 9000 -> loss: 0.09310608802404104\n",
      "    At iteration 9100 -> loss: 0.09311234411097176\n",
      "    At iteration 9200 -> loss: 0.09310596370103741\n",
      "    At iteration 9300 -> loss: 0.09314209825944376\n",
      "    At iteration 9400 -> loss: 0.09345167144883933\n",
      "    At iteration 9500 -> loss: 0.09340746953449026\n",
      "    At iteration 9600 -> loss: 0.09335664205043108\n",
      "    At iteration 9700 -> loss: 0.09336367338391663\n",
      "    At iteration 9800 -> loss: 0.09334844959756965\n",
      "    At iteration 9900 -> loss: 0.09333759915425217\n",
      "    At iteration 10000 -> loss: 0.09331475745084719\n",
      "    At iteration 10100 -> loss: 0.09332249819366611\n",
      "    At iteration 10200 -> loss: 0.0933099871872944\n",
      "    At iteration 10300 -> loss: 0.09334625127876355\n",
      "    At iteration 10400 -> loss: 0.09336260266211238\n",
      "    At iteration 10500 -> loss: 0.0933558602125946\n",
      "    At iteration 10600 -> loss: 0.0933209308466574\n",
      "    At iteration 10700 -> loss: 0.09329721524046035\n",
      "    At iteration 10800 -> loss: 0.09329418979394942\n",
      "    At iteration 10900 -> loss: 0.09325354434894918\n",
      "    At iteration 11000 -> loss: 0.09322339817429816\n",
      "    At iteration 11100 -> loss: 0.09321388602142684\n",
      "    At iteration 11200 -> loss: 0.0931994895057646\n",
      "    At iteration 11300 -> loss: 0.09322985441773127\n",
      "    At iteration 11400 -> loss: 0.09319135659954902\n",
      "    At iteration 11500 -> loss: 0.09317980023253344\n",
      "    At iteration 11600 -> loss: 0.09315899802932588\n",
      "    At iteration 11700 -> loss: 0.09313020873225493\n",
      "    At iteration 11800 -> loss: 0.09310367848368996\n",
      "    At iteration 11900 -> loss: 0.09308986219723489\n",
      "    At iteration 12000 -> loss: 0.09309036953532557\n",
      "    At iteration 12100 -> loss: 0.09307733345092793\n",
      "    At iteration 12200 -> loss: 0.09307005712514503\n",
      "    At iteration 12300 -> loss: 0.09305503337015064\n",
      "    At iteration 12400 -> loss: 0.09310357094572584\n",
      "    At iteration 12500 -> loss: 0.0931010892147935\n",
      "    At iteration 12600 -> loss: 0.0931112523794777\n",
      "    At iteration 12700 -> loss: 0.09309345687605874\n",
      "    At iteration 12800 -> loss: 0.09307152992506855\n",
      "    At iteration 12900 -> loss: 0.09312684917393797\n",
      "    At iteration 13000 -> loss: 0.09313381428282276\n",
      "    At iteration 13100 -> loss: 0.09314686158093274\n",
      "    At iteration 13200 -> loss: 0.09310550556771169\n",
      "    At iteration 13300 -> loss: 0.0930722931749464\n",
      "    At iteration 13400 -> loss: 0.0930723594766888\n",
      "    At iteration 13500 -> loss: 0.09305522632077402\n",
      "    At iteration 13600 -> loss: 0.09303198612278128\n",
      "Staring Epoch 111\n",
      "    At iteration 0 -> loss: 0.09443537890911102\n",
      "    At iteration 100 -> loss: 0.09112858106491424\n",
      "    At iteration 200 -> loss: 0.09242517856354095\n",
      "    At iteration 300 -> loss: 0.09216065823523177\n",
      "    At iteration 400 -> loss: 0.09148473282561796\n",
      "    At iteration 500 -> loss: 0.09183640302309468\n",
      "    At iteration 600 -> loss: 0.09218713266852563\n",
      "    At iteration 700 -> loss: 0.09241410723004216\n",
      "    At iteration 800 -> loss: 0.09205452450849749\n",
      "    At iteration 900 -> loss: 0.0921482306133211\n",
      "    At iteration 1000 -> loss: 0.0920000941927919\n",
      "    At iteration 1100 -> loss: 0.09207145111761846\n",
      "    At iteration 1200 -> loss: 0.09195611344509277\n",
      "    At iteration 1300 -> loss: 0.09184475426444343\n",
      "    At iteration 1400 -> loss: 0.0918284311557146\n",
      "    At iteration 1500 -> loss: 0.09220892423009974\n",
      "    At iteration 1600 -> loss: 0.0922164500834332\n",
      "    At iteration 1700 -> loss: 0.0925031760050734\n",
      "    At iteration 1800 -> loss: 0.09242040204546606\n",
      "    At iteration 1900 -> loss: 0.09244446578335754\n",
      "    At iteration 2000 -> loss: 0.09237182122804886\n",
      "    At iteration 2100 -> loss: 0.09248988188380912\n",
      "    At iteration 2200 -> loss: 0.09240432415849466\n",
      "    At iteration 2300 -> loss: 0.09248038581841365\n",
      "    At iteration 2400 -> loss: 0.09238651981597877\n",
      "    At iteration 2500 -> loss: 0.09249071007083196\n",
      "    At iteration 2600 -> loss: 0.0924417164213456\n",
      "    At iteration 2700 -> loss: 0.09242104797755495\n",
      "    At iteration 2800 -> loss: 0.0924416908167018\n",
      "    At iteration 2900 -> loss: 0.09243471057112418\n",
      "    At iteration 3000 -> loss: 0.09275964230196078\n",
      "    At iteration 3100 -> loss: 0.09269767426154168\n",
      "    At iteration 3200 -> loss: 0.09276418685792193\n",
      "    At iteration 3300 -> loss: 0.09273579113287825\n",
      "    At iteration 3400 -> loss: 0.09271383037708156\n",
      "    At iteration 3500 -> loss: 0.09267618055529048\n",
      "    At iteration 3600 -> loss: 0.09258761594111416\n",
      "    At iteration 3700 -> loss: 0.09263007319556538\n",
      "    At iteration 3800 -> loss: 0.09253211457775368\n",
      "    At iteration 3900 -> loss: 0.09251328448290291\n",
      "    At iteration 4000 -> loss: 0.09253055438942202\n",
      "    At iteration 4100 -> loss: 0.09245489641014823\n",
      "    At iteration 4200 -> loss: 0.09253071616934645\n",
      "    At iteration 4300 -> loss: 0.09253055011273574\n",
      "    At iteration 4400 -> loss: 0.09258156065794285\n",
      "    At iteration 4500 -> loss: 0.09257537485941392\n",
      "    At iteration 4600 -> loss: 0.09250749306359739\n",
      "    At iteration 4700 -> loss: 0.09243886415279866\n",
      "    At iteration 4800 -> loss: 0.09239260627586669\n",
      "    At iteration 4900 -> loss: 0.0923595427165808\n",
      "    At iteration 5000 -> loss: 0.09243108389917673\n",
      "    At iteration 5100 -> loss: 0.09247176162848386\n",
      "    At iteration 5200 -> loss: 0.09245209144879267\n",
      "    At iteration 5300 -> loss: 0.0924507617729701\n",
      "    At iteration 5400 -> loss: 0.09250424943475485\n",
      "    At iteration 5500 -> loss: 0.09243313344105093\n",
      "    At iteration 5600 -> loss: 0.09245822482201389\n",
      "    At iteration 5700 -> loss: 0.09243905506442532\n",
      "    At iteration 5800 -> loss: 0.09245925242663003\n",
      "    At iteration 5900 -> loss: 0.09246125432068608\n",
      "    At iteration 6000 -> loss: 0.09254665961505733\n",
      "    At iteration 6100 -> loss: 0.09251006339796525\n",
      "    At iteration 6200 -> loss: 0.09249358190596618\n",
      "    At iteration 6300 -> loss: 0.09247210067607718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 6400 -> loss: 0.0924781839603049\n",
      "    At iteration 6500 -> loss: 0.09249744172112274\n",
      "    At iteration 6600 -> loss: 0.09249227847916444\n",
      "    At iteration 6700 -> loss: 0.09251373679083633\n",
      "    At iteration 6800 -> loss: 0.09248543802672599\n",
      "    At iteration 6900 -> loss: 0.09248701620159364\n",
      "    At iteration 7000 -> loss: 0.0924782264467375\n",
      "    At iteration 7100 -> loss: 0.09256132417747699\n",
      "    At iteration 7200 -> loss: 0.09261615021675018\n",
      "    At iteration 7300 -> loss: 0.09260418153731866\n",
      "    At iteration 7400 -> loss: 0.09297524001942997\n",
      "    At iteration 7500 -> loss: 0.09299756686439747\n",
      "    At iteration 7600 -> loss: 0.09307238165972928\n",
      "    At iteration 7700 -> loss: 0.09301438643996425\n",
      "    At iteration 7800 -> loss: 0.0929926366226761\n",
      "    At iteration 7900 -> loss: 0.09294987776088402\n",
      "    At iteration 8000 -> loss: 0.09294473335011888\n",
      "    At iteration 8100 -> loss: 0.0929568908104877\n",
      "    At iteration 8200 -> loss: 0.0929088776176818\n",
      "    At iteration 8300 -> loss: 0.09298332821936724\n",
      "    At iteration 8400 -> loss: 0.0929371595435103\n",
      "    At iteration 8500 -> loss: 0.09302761331547212\n",
      "    At iteration 8600 -> loss: 0.09303556449979489\n",
      "    At iteration 8700 -> loss: 0.09300311447967963\n",
      "    At iteration 8800 -> loss: 0.09295446172838258\n",
      "    At iteration 8900 -> loss: 0.09293310026336009\n",
      "    At iteration 9000 -> loss: 0.09307018269125532\n",
      "    At iteration 9100 -> loss: 0.09307006113443593\n",
      "    At iteration 9200 -> loss: 0.09303694033657925\n",
      "    At iteration 9300 -> loss: 0.09299513762200066\n",
      "    At iteration 9400 -> loss: 0.09294774707981088\n",
      "    At iteration 9500 -> loss: 0.09292112548120453\n",
      "    At iteration 9600 -> loss: 0.09289899219138244\n",
      "    At iteration 9700 -> loss: 0.09289429249216878\n",
      "    At iteration 9800 -> loss: 0.09287898442640748\n",
      "    At iteration 9900 -> loss: 0.0928879013185716\n",
      "    At iteration 10000 -> loss: 0.09287568157415482\n",
      "    At iteration 10100 -> loss: 0.09293013532205661\n",
      "    At iteration 10200 -> loss: 0.09289592103781652\n",
      "    At iteration 10300 -> loss: 0.09287287669743274\n",
      "    At iteration 10400 -> loss: 0.09285574225807827\n",
      "    At iteration 10500 -> loss: 0.09283652747684625\n",
      "    At iteration 10600 -> loss: 0.09281806342385593\n",
      "    At iteration 10700 -> loss: 0.09282670361592499\n",
      "    At iteration 10800 -> loss: 0.09279850561682892\n",
      "    At iteration 10900 -> loss: 0.09279001080307885\n",
      "    At iteration 11000 -> loss: 0.09278340858945774\n",
      "    At iteration 11100 -> loss: 0.09276681235127514\n",
      "    At iteration 11200 -> loss: 0.09277616827651813\n",
      "    At iteration 11300 -> loss: 0.0927476078691213\n",
      "    At iteration 11400 -> loss: 0.09274917235894484\n",
      "    At iteration 11500 -> loss: 0.09273060008667053\n",
      "    At iteration 11600 -> loss: 0.0927068698956734\n",
      "    At iteration 11700 -> loss: 0.09267672852675463\n",
      "    At iteration 11800 -> loss: 0.09270230742073854\n",
      "    At iteration 11900 -> loss: 0.09268176911994644\n",
      "    At iteration 12000 -> loss: 0.09269240375678398\n",
      "    At iteration 12100 -> loss: 0.09269273112346145\n",
      "    At iteration 12200 -> loss: 0.09266269385093008\n",
      "    At iteration 12300 -> loss: 0.09264523486466278\n",
      "    At iteration 12400 -> loss: 0.09268252943561556\n",
      "    At iteration 12500 -> loss: 0.09271797976582334\n",
      "    At iteration 12600 -> loss: 0.09271359842996103\n",
      "    At iteration 12700 -> loss: 0.09270557324563694\n",
      "    At iteration 12800 -> loss: 0.09271641970462503\n",
      "    At iteration 12900 -> loss: 0.09273107333325482\n",
      "    At iteration 13000 -> loss: 0.09273862208769282\n",
      "    At iteration 13100 -> loss: 0.0928100870439853\n",
      "    At iteration 13200 -> loss: 0.0928066512222818\n",
      "    At iteration 13300 -> loss: 0.09286195997366553\n",
      "    At iteration 13400 -> loss: 0.09286056439902475\n",
      "    At iteration 13500 -> loss: 0.09302898671942716\n",
      "    At iteration 13600 -> loss: 0.09301324061407104\n",
      "Staring Epoch 112\n",
      "    At iteration 0 -> loss: 0.09580372460186481\n",
      "    At iteration 100 -> loss: 0.09304718713023269\n",
      "    At iteration 200 -> loss: 0.09119135665830803\n",
      "    At iteration 300 -> loss: 0.09159773283302156\n",
      "    At iteration 400 -> loss: 0.09246873070959333\n",
      "    At iteration 500 -> loss: 0.09278753645582445\n",
      "    At iteration 600 -> loss: 0.0924812019300385\n",
      "    At iteration 700 -> loss: 0.09230351794462063\n",
      "    At iteration 800 -> loss: 0.09196310914882447\n",
      "    At iteration 900 -> loss: 0.09278980980310333\n",
      "    At iteration 1000 -> loss: 0.09278920571245976\n",
      "    At iteration 1100 -> loss: 0.0926412597949897\n",
      "    At iteration 1200 -> loss: 0.09251500784709908\n",
      "    At iteration 1300 -> loss: 0.09228560445829348\n",
      "    At iteration 1400 -> loss: 0.09218858804355984\n",
      "    At iteration 1500 -> loss: 0.09271691773285033\n",
      "    At iteration 1600 -> loss: 0.09284677081331537\n",
      "    At iteration 1700 -> loss: 0.09266003630440954\n",
      "    At iteration 1800 -> loss: 0.09256942237606967\n",
      "    At iteration 1900 -> loss: 0.0925655816858189\n",
      "    At iteration 2000 -> loss: 0.09242509113060153\n",
      "    At iteration 2100 -> loss: 0.09245241294748383\n",
      "    At iteration 2200 -> loss: 0.09288444469681494\n",
      "    At iteration 2300 -> loss: 0.09284656624241969\n",
      "    At iteration 2400 -> loss: 0.09295527573108926\n",
      "    At iteration 2500 -> loss: 0.09289277402613248\n",
      "    At iteration 2600 -> loss: 0.09283358085744818\n",
      "    At iteration 2700 -> loss: 0.09297989349741163\n",
      "    At iteration 2800 -> loss: 0.09291582744028508\n",
      "    At iteration 2900 -> loss: 0.09284866814201657\n",
      "    At iteration 3000 -> loss: 0.09290380062737513\n",
      "    At iteration 3100 -> loss: 0.09280240684769507\n",
      "    At iteration 3200 -> loss: 0.09268705868901425\n",
      "    At iteration 3300 -> loss: 0.09272299219014321\n",
      "    At iteration 3400 -> loss: 0.09268470922358642\n",
      "    At iteration 3500 -> loss: 0.09275288538526937\n",
      "    At iteration 3600 -> loss: 0.09275412571211043\n",
      "    At iteration 3700 -> loss: 0.09264644359745952\n",
      "    At iteration 3800 -> loss: 0.0926584462417445\n",
      "    At iteration 3900 -> loss: 0.09266201255245121\n",
      "    At iteration 4000 -> loss: 0.09262327213374177\n",
      "    At iteration 4100 -> loss: 0.0926057327519627\n",
      "    At iteration 4200 -> loss: 0.09258485057602218\n",
      "    At iteration 4300 -> loss: 0.0926269951232142\n",
      "    At iteration 4400 -> loss: 0.09259966826470875\n",
      "    At iteration 4500 -> loss: 0.09267036203759231\n",
      "    At iteration 4600 -> loss: 0.09262493413433491\n",
      "    At iteration 4700 -> loss: 0.09257641104916327\n",
      "    At iteration 4800 -> loss: 0.09250933288447181\n",
      "    At iteration 4900 -> loss: 0.09246760992220178\n",
      "    At iteration 5000 -> loss: 0.09240133158845093\n",
      "    At iteration 5100 -> loss: 0.09244530415860026\n",
      "    At iteration 5200 -> loss: 0.09245011061610289\n",
      "    At iteration 5300 -> loss: 0.09250889785841514\n",
      "    At iteration 5400 -> loss: 0.09277846415271465\n",
      "    At iteration 5500 -> loss: 0.09287091522266576\n",
      "    At iteration 5600 -> loss: 0.0928763187903762\n",
      "    At iteration 5700 -> loss: 0.09285044529021051\n",
      "    At iteration 5800 -> loss: 0.092825592924058\n",
      "    At iteration 5900 -> loss: 0.09278368191969709\n",
      "    At iteration 6000 -> loss: 0.09277628194463684\n",
      "    At iteration 6100 -> loss: 0.09330536808461455\n",
      "    At iteration 6200 -> loss: 0.09329667300144615\n",
      "    At iteration 6300 -> loss: 0.09325275175023916\n",
      "    At iteration 6400 -> loss: 0.09323587606899437\n",
      "    At iteration 6500 -> loss: 0.09327757088979853\n",
      "    At iteration 6600 -> loss: 0.09325584770241623\n",
      "    At iteration 6700 -> loss: 0.09333846367661076\n",
      "    At iteration 6800 -> loss: 0.0933083819770143\n",
      "    At iteration 6900 -> loss: 0.0932344226705853\n",
      "    At iteration 7000 -> loss: 0.09324962320755262\n",
      "    At iteration 7100 -> loss: 0.09327606924306987\n",
      "    At iteration 7200 -> loss: 0.09323202734218437\n",
      "    At iteration 7300 -> loss: 0.09325880843470674\n",
      "    At iteration 7400 -> loss: 0.09321455912914213\n",
      "    At iteration 7500 -> loss: 0.09322738831469218\n",
      "    At iteration 7600 -> loss: 0.09321024701497882\n",
      "    At iteration 7700 -> loss: 0.09324717163554574\n",
      "    At iteration 7800 -> loss: 0.09321762087384768\n",
      "    At iteration 7900 -> loss: 0.09320985030387575\n",
      "    At iteration 8000 -> loss: 0.0931812479931622\n",
      "    At iteration 8100 -> loss: 0.09315996481506668\n",
      "    At iteration 8200 -> loss: 0.09310457634342385\n",
      "    At iteration 8300 -> loss: 0.09321018306637481\n",
      "    At iteration 8400 -> loss: 0.09333640732493931\n",
      "    At iteration 8500 -> loss: 0.09331196592452193\n",
      "    At iteration 8600 -> loss: 0.09328697920119625\n",
      "    At iteration 8700 -> loss: 0.09326621008284401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8800 -> loss: 0.09338278928618125\n",
      "    At iteration 8900 -> loss: 0.0933371272954905\n",
      "    At iteration 9000 -> loss: 0.09331964316416451\n",
      "    At iteration 9100 -> loss: 0.09333999904394\n",
      "    At iteration 9200 -> loss: 0.09334941699221833\n",
      "    At iteration 9300 -> loss: 0.09329553846598027\n",
      "    At iteration 9400 -> loss: 0.09327314880857968\n",
      "    At iteration 9500 -> loss: 0.0932517526050386\n",
      "    At iteration 9600 -> loss: 0.09330038245917284\n",
      "    At iteration 9700 -> loss: 0.09330740259673347\n",
      "    At iteration 9800 -> loss: 0.0933649269885287\n",
      "    At iteration 9900 -> loss: 0.09339427046395095\n",
      "    At iteration 10000 -> loss: 0.09339545551321428\n",
      "    At iteration 10100 -> loss: 0.09338483887257427\n",
      "    At iteration 10200 -> loss: 0.0933687993037422\n",
      "    At iteration 10300 -> loss: 0.09335206449635738\n",
      "    At iteration 10400 -> loss: 0.09333775231476123\n",
      "    At iteration 10500 -> loss: 0.09332413063934532\n",
      "    At iteration 10600 -> loss: 0.09330834697159279\n",
      "    At iteration 10700 -> loss: 0.09328860829547779\n",
      "    At iteration 10800 -> loss: 0.09334968347477057\n",
      "    At iteration 10900 -> loss: 0.09333934921901996\n",
      "    At iteration 11000 -> loss: 0.09332848355867562\n",
      "    At iteration 11100 -> loss: 0.09332153898614567\n",
      "    At iteration 11200 -> loss: 0.09329163502863456\n",
      "    At iteration 11300 -> loss: 0.09329157259845933\n",
      "    At iteration 11400 -> loss: 0.09326292861911584\n",
      "    At iteration 11500 -> loss: 0.093226467764095\n",
      "    At iteration 11600 -> loss: 0.09320566632880185\n",
      "    At iteration 11700 -> loss: 0.09321270362403886\n",
      "    At iteration 11800 -> loss: 0.09317437335144893\n",
      "    At iteration 11900 -> loss: 0.09314329621595006\n",
      "    At iteration 12000 -> loss: 0.09314643011822492\n",
      "    At iteration 12100 -> loss: 0.09318229844620624\n",
      "    At iteration 12200 -> loss: 0.09318505647844619\n",
      "    At iteration 12300 -> loss: 0.09318775490898518\n",
      "    At iteration 12400 -> loss: 0.0931685967538273\n",
      "    At iteration 12500 -> loss: 0.09314789685164025\n",
      "    At iteration 12600 -> loss: 0.09311893750813793\n",
      "    At iteration 12700 -> loss: 0.09308906845104685\n",
      "    At iteration 12800 -> loss: 0.09310600196842943\n",
      "    At iteration 12900 -> loss: 0.09309420764114112\n",
      "    At iteration 13000 -> loss: 0.09306810194920978\n",
      "    At iteration 13100 -> loss: 0.09308555396198545\n",
      "    At iteration 13200 -> loss: 0.09306476768754156\n",
      "    At iteration 13300 -> loss: 0.09302924163262358\n",
      "    At iteration 13400 -> loss: 0.09302880545613282\n",
      "    At iteration 13500 -> loss: 0.093014802607402\n",
      "    At iteration 13600 -> loss: 0.09302692520855428\n",
      "Staring Epoch 113\n",
      "    At iteration 0 -> loss: 0.08720766799524426\n",
      "    At iteration 100 -> loss: 0.0915291646724035\n",
      "    At iteration 200 -> loss: 0.09403830817738405\n",
      "    At iteration 300 -> loss: 0.09407532006374615\n",
      "    At iteration 400 -> loss: 0.09463334228339648\n",
      "    At iteration 500 -> loss: 0.0936477608679827\n",
      "    At iteration 600 -> loss: 0.09301069721579991\n",
      "    At iteration 700 -> loss: 0.09274861480604595\n",
      "    At iteration 800 -> loss: 0.09243156673535965\n",
      "    At iteration 900 -> loss: 0.09224742954934687\n",
      "    At iteration 1000 -> loss: 0.092984714004127\n",
      "    At iteration 1100 -> loss: 0.09289514837444704\n",
      "    At iteration 1200 -> loss: 0.09300759169469934\n",
      "    At iteration 1300 -> loss: 0.09290947164053842\n",
      "    At iteration 1400 -> loss: 0.09306865662284367\n",
      "    At iteration 1500 -> loss: 0.09293303884926293\n",
      "    At iteration 1600 -> loss: 0.09380741423088379\n",
      "    At iteration 1700 -> loss: 0.09357930528844975\n",
      "    At iteration 1800 -> loss: 0.09375161451025094\n",
      "    At iteration 1900 -> loss: 0.09351436227257451\n",
      "    At iteration 2000 -> loss: 0.09331318491757719\n",
      "    At iteration 2100 -> loss: 0.09310919298354442\n",
      "    At iteration 2200 -> loss: 0.09313911405419296\n",
      "    At iteration 2300 -> loss: 0.0934257272609336\n",
      "    At iteration 2400 -> loss: 0.0932735384004672\n",
      "    At iteration 2500 -> loss: 0.09323698168892894\n",
      "    At iteration 2600 -> loss: 0.09315643682023339\n",
      "    At iteration 2700 -> loss: 0.09302994811296747\n",
      "    At iteration 2800 -> loss: 0.09302289283264184\n",
      "    At iteration 2900 -> loss: 0.09402696890182959\n",
      "    At iteration 3000 -> loss: 0.09394681254971961\n",
      "    At iteration 3100 -> loss: 0.09384158852289222\n",
      "    At iteration 3200 -> loss: 0.09380689457134835\n",
      "    At iteration 3300 -> loss: 0.09378310144861556\n",
      "    At iteration 3400 -> loss: 0.09378031358050785\n",
      "    At iteration 3500 -> loss: 0.09367196778915576\n",
      "    At iteration 3600 -> loss: 0.0936236494838907\n",
      "    At iteration 3700 -> loss: 0.09354919587441152\n",
      "    At iteration 3800 -> loss: 0.09346705829362502\n",
      "    At iteration 3900 -> loss: 0.09341495257627551\n",
      "    At iteration 4000 -> loss: 0.09327285355571258\n",
      "    At iteration 4100 -> loss: 0.09321649790294832\n",
      "    At iteration 4200 -> loss: 0.09344484658079337\n",
      "    At iteration 4300 -> loss: 0.09344615588728121\n",
      "    At iteration 4400 -> loss: 0.09334978595347386\n",
      "    At iteration 4500 -> loss: 0.09323522502113668\n",
      "    At iteration 4600 -> loss: 0.09327340174373128\n",
      "    At iteration 4700 -> loss: 0.09320859407090762\n",
      "    At iteration 4800 -> loss: 0.09325420059277435\n",
      "    At iteration 4900 -> loss: 0.0933122150268939\n",
      "    At iteration 5000 -> loss: 0.0932693820907647\n",
      "    At iteration 5100 -> loss: 0.09326034156927325\n",
      "    At iteration 5200 -> loss: 0.09319692552512272\n",
      "    At iteration 5300 -> loss: 0.09323270605235905\n",
      "    At iteration 5400 -> loss: 0.09315871076185368\n",
      "    At iteration 5500 -> loss: 0.09307691035187401\n",
      "    At iteration 5600 -> loss: 0.0931075718847638\n",
      "    At iteration 5700 -> loss: 0.09312385632941629\n",
      "    At iteration 5800 -> loss: 0.09312939337110913\n",
      "    At iteration 5900 -> loss: 0.09309198333705479\n",
      "    At iteration 6000 -> loss: 0.09305124756447716\n",
      "    At iteration 6100 -> loss: 0.0930262828143424\n",
      "    At iteration 6200 -> loss: 0.0930353091434664\n",
      "    At iteration 6300 -> loss: 0.0929998723925772\n",
      "    At iteration 6400 -> loss: 0.09296457258902958\n",
      "    At iteration 6500 -> loss: 0.09294062365188127\n",
      "    At iteration 6600 -> loss: 0.09292321127203676\n",
      "    At iteration 6700 -> loss: 0.09300102285655118\n",
      "    At iteration 6800 -> loss: 0.09300505482405577\n",
      "    At iteration 6900 -> loss: 0.09294047261848869\n",
      "    At iteration 7000 -> loss: 0.09291881235872544\n",
      "    At iteration 7100 -> loss: 0.09293298921449795\n",
      "    At iteration 7200 -> loss: 0.09291180963655511\n",
      "    At iteration 7300 -> loss: 0.0929921160923125\n",
      "    At iteration 7400 -> loss: 0.09297987844137283\n",
      "    At iteration 7500 -> loss: 0.0930253472437696\n",
      "    At iteration 7600 -> loss: 0.09300377373969879\n",
      "    At iteration 7700 -> loss: 0.09297016036923765\n",
      "    At iteration 7800 -> loss: 0.0930067039844723\n",
      "    At iteration 7900 -> loss: 0.09295526300655406\n",
      "    At iteration 8000 -> loss: 0.09292671111826292\n",
      "    At iteration 8100 -> loss: 0.09291695540115179\n",
      "    At iteration 8200 -> loss: 0.09290349830289617\n",
      "    At iteration 8300 -> loss: 0.09294280840294211\n",
      "    At iteration 8400 -> loss: 0.09297034498806506\n",
      "    At iteration 8500 -> loss: 0.09295829769408034\n",
      "    At iteration 8600 -> loss: 0.09294977066011094\n",
      "    At iteration 8700 -> loss: 0.09293366818028727\n",
      "    At iteration 8800 -> loss: 0.09301300037041328\n",
      "    At iteration 8900 -> loss: 0.09300762682212078\n",
      "    At iteration 9000 -> loss: 0.09307730879917721\n",
      "    At iteration 9100 -> loss: 0.09302879911260865\n",
      "    At iteration 9200 -> loss: 0.09300794196299166\n",
      "    At iteration 9300 -> loss: 0.09299491249306743\n",
      "    At iteration 9400 -> loss: 0.09295368022016219\n",
      "    At iteration 9500 -> loss: 0.09291002853668963\n",
      "    At iteration 9600 -> loss: 0.09287524918626637\n",
      "    At iteration 9700 -> loss: 0.09284391611556106\n",
      "    At iteration 9800 -> loss: 0.09284635943851399\n",
      "    At iteration 9900 -> loss: 0.09285061340194704\n",
      "    At iteration 10000 -> loss: 0.0928231501783151\n",
      "    At iteration 10100 -> loss: 0.09281499738925353\n",
      "    At iteration 10200 -> loss: 0.09281994093322872\n",
      "    At iteration 10300 -> loss: 0.0928487213189052\n",
      "    At iteration 10400 -> loss: 0.09293073606200451\n",
      "    At iteration 10500 -> loss: 0.09296400630021709\n",
      "    At iteration 10600 -> loss: 0.09294201224635582\n",
      "    At iteration 10700 -> loss: 0.09290183961172373\n",
      "    At iteration 10800 -> loss: 0.09291829885056231\n",
      "    At iteration 10900 -> loss: 0.09291037474646814\n",
      "    At iteration 11000 -> loss: 0.09297275806594402\n",
      "    At iteration 11100 -> loss: 0.0929325057238567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11200 -> loss: 0.09297654346254386\n",
      "    At iteration 11300 -> loss: 0.09296439943032958\n",
      "    At iteration 11400 -> loss: 0.0929646572991726\n",
      "    At iteration 11500 -> loss: 0.09294137109766684\n",
      "    At iteration 11600 -> loss: 0.09294878627776466\n",
      "    At iteration 11700 -> loss: 0.09295235410377642\n",
      "    At iteration 11800 -> loss: 0.09294950250383027\n",
      "    At iteration 11900 -> loss: 0.09298507998575474\n",
      "    At iteration 12000 -> loss: 0.09301114328596889\n",
      "    At iteration 12100 -> loss: 0.09297470319896624\n",
      "    At iteration 12200 -> loss: 0.09298164599062292\n",
      "    At iteration 12300 -> loss: 0.09297956937370391\n",
      "    At iteration 12400 -> loss: 0.09305440239229525\n",
      "    At iteration 12500 -> loss: 0.09303211807709387\n",
      "    At iteration 12600 -> loss: 0.09299560568124311\n",
      "    At iteration 12700 -> loss: 0.09306647517966134\n",
      "    At iteration 12800 -> loss: 0.09311031781595266\n",
      "    At iteration 12900 -> loss: 0.09309941687384124\n",
      "    At iteration 13000 -> loss: 0.09307675296182423\n",
      "    At iteration 13100 -> loss: 0.09305474389909413\n",
      "    At iteration 13200 -> loss: 0.0931269573123201\n",
      "    At iteration 13300 -> loss: 0.09308786420552324\n",
      "    At iteration 13400 -> loss: 0.09309925945920443\n",
      "    At iteration 13500 -> loss: 0.09308142471927501\n",
      "    At iteration 13600 -> loss: 0.09306858928899996\n",
      "Staring Epoch 114\n",
      "    At iteration 0 -> loss: 0.1915282805057359\n",
      "    At iteration 100 -> loss: 0.09055556540665342\n",
      "    At iteration 200 -> loss: 0.09099351573013563\n",
      "    At iteration 300 -> loss: 0.09168968668093315\n",
      "    At iteration 400 -> loss: 0.09185874139903713\n",
      "    At iteration 500 -> loss: 0.09102683035823264\n",
      "    At iteration 600 -> loss: 0.090793040953161\n",
      "    At iteration 700 -> loss: 0.09104387738299542\n",
      "    At iteration 800 -> loss: 0.09169681935125493\n",
      "    At iteration 900 -> loss: 0.09155928032345748\n",
      "    At iteration 1000 -> loss: 0.09315178526444222\n",
      "    At iteration 1100 -> loss: 0.0928664833218207\n",
      "    At iteration 1200 -> loss: 0.0925203783530535\n",
      "    At iteration 1300 -> loss: 0.09269281156801154\n",
      "    At iteration 1400 -> loss: 0.092926482091788\n",
      "    At iteration 1500 -> loss: 0.09328312258429323\n",
      "    At iteration 1600 -> loss: 0.09324033752686836\n",
      "    At iteration 1700 -> loss: 0.09301883276289291\n",
      "    At iteration 1800 -> loss: 0.0929925517402503\n",
      "    At iteration 1900 -> loss: 0.09347017667992409\n",
      "    At iteration 2000 -> loss: 0.09321167457611242\n",
      "    At iteration 2100 -> loss: 0.09332015918764838\n",
      "    At iteration 2200 -> loss: 0.09322787612906995\n",
      "    At iteration 2300 -> loss: 0.09315676350251653\n",
      "    At iteration 2400 -> loss: 0.09339054898917742\n",
      "    At iteration 2500 -> loss: 0.09339441168731502\n",
      "    At iteration 2600 -> loss: 0.09340884920735341\n",
      "    At iteration 2700 -> loss: 0.0933042212313177\n",
      "    At iteration 2800 -> loss: 0.0931588270738731\n",
      "    At iteration 2900 -> loss: 0.09309626768009836\n",
      "    At iteration 3000 -> loss: 0.09339360143492627\n",
      "    At iteration 3100 -> loss: 0.09327631045336207\n",
      "    At iteration 3200 -> loss: 0.0931500128918084\n",
      "    At iteration 3300 -> loss: 0.0930873676166792\n",
      "    At iteration 3400 -> loss: 0.09320083760312167\n",
      "    At iteration 3500 -> loss: 0.09321723997382471\n",
      "    At iteration 3600 -> loss: 0.09310117756073692\n",
      "    At iteration 3700 -> loss: 0.09307562506342443\n",
      "    At iteration 3800 -> loss: 0.0930329699562094\n",
      "    At iteration 3900 -> loss: 0.09312587007165476\n",
      "    At iteration 4000 -> loss: 0.09307621643007456\n",
      "    At iteration 4100 -> loss: 0.09302360996243281\n",
      "    At iteration 4200 -> loss: 0.09307564992868685\n",
      "    At iteration 4300 -> loss: 0.09319518114620005\n",
      "    At iteration 4400 -> loss: 0.09314387704616577\n",
      "    At iteration 4500 -> loss: 0.09305936126939944\n",
      "    At iteration 4600 -> loss: 0.09318232461791542\n",
      "    At iteration 4700 -> loss: 0.0931348522573641\n",
      "    At iteration 4800 -> loss: 0.09311197577006107\n",
      "    At iteration 4900 -> loss: 0.09309514180643062\n",
      "    At iteration 5000 -> loss: 0.09301785460860242\n",
      "    At iteration 5100 -> loss: 0.09298435555921752\n",
      "    At iteration 5200 -> loss: 0.0930349478387274\n",
      "    At iteration 5300 -> loss: 0.09305799476481937\n",
      "    At iteration 5400 -> loss: 0.09300437167264752\n",
      "    At iteration 5500 -> loss: 0.09299717063609383\n",
      "    At iteration 5600 -> loss: 0.09296491407618371\n",
      "    At iteration 5700 -> loss: 0.09297027609883853\n",
      "    At iteration 5800 -> loss: 0.09294232326595472\n",
      "    At iteration 5900 -> loss: 0.09293600423532146\n",
      "    At iteration 6000 -> loss: 0.09290088869342124\n",
      "    At iteration 6100 -> loss: 0.09288509787861442\n",
      "    At iteration 6200 -> loss: 0.09285127371140149\n",
      "    At iteration 6300 -> loss: 0.09289088262447243\n",
      "    At iteration 6400 -> loss: 0.09288468735640766\n",
      "    At iteration 6500 -> loss: 0.09288044711803708\n",
      "    At iteration 6600 -> loss: 0.0928772880368091\n",
      "    At iteration 6700 -> loss: 0.09292619998058121\n",
      "    At iteration 6800 -> loss: 0.09302781896566206\n",
      "    At iteration 6900 -> loss: 0.0930646344745522\n",
      "    At iteration 7000 -> loss: 0.09306012709187357\n",
      "    At iteration 7100 -> loss: 0.09303449911926032\n",
      "    At iteration 7200 -> loss: 0.09304767553807092\n",
      "    At iteration 7300 -> loss: 0.09304663465479851\n",
      "    At iteration 7400 -> loss: 0.0930457071471197\n",
      "    At iteration 7500 -> loss: 0.0930054738905852\n",
      "    At iteration 7600 -> loss: 0.09299891298589667\n",
      "    At iteration 7700 -> loss: 0.09299190935627276\n",
      "    At iteration 7800 -> loss: 0.0930665229513106\n",
      "    At iteration 7900 -> loss: 0.09307241815749602\n",
      "    At iteration 8000 -> loss: 0.09308518962301934\n",
      "    At iteration 8100 -> loss: 0.09303583941930081\n",
      "    At iteration 8200 -> loss: 0.09299323641674219\n",
      "    At iteration 8300 -> loss: 0.09304513919659743\n",
      "    At iteration 8400 -> loss: 0.09305284833210557\n",
      "    At iteration 8500 -> loss: 0.09306506953350785\n",
      "    At iteration 8600 -> loss: 0.09311634332007779\n",
      "    At iteration 8700 -> loss: 0.09310372187047353\n",
      "    At iteration 8800 -> loss: 0.09304738832670251\n",
      "    At iteration 8900 -> loss: 0.09300145332922119\n",
      "    At iteration 9000 -> loss: 0.0930229106930894\n",
      "    At iteration 9100 -> loss: 0.0930543555283066\n",
      "    At iteration 9200 -> loss: 0.09302177687739607\n",
      "    At iteration 9300 -> loss: 0.09325375038085106\n",
      "    At iteration 9400 -> loss: 0.09325036975221583\n",
      "    At iteration 9500 -> loss: 0.09323774053157405\n",
      "    At iteration 9600 -> loss: 0.09321566068925594\n",
      "    At iteration 9700 -> loss: 0.09319690342752682\n",
      "    At iteration 9800 -> loss: 0.09318082533625197\n",
      "    At iteration 9900 -> loss: 0.09314229282774796\n",
      "    At iteration 10000 -> loss: 0.09312229851502797\n",
      "    At iteration 10100 -> loss: 0.0931291805786705\n",
      "    At iteration 10200 -> loss: 0.09309280509273368\n",
      "    At iteration 10300 -> loss: 0.0931025347054143\n",
      "    At iteration 10400 -> loss: 0.09310859032645206\n",
      "    At iteration 10500 -> loss: 0.09312662576775167\n",
      "    At iteration 10600 -> loss: 0.0930899980833727\n",
      "    At iteration 10700 -> loss: 0.09307079612205772\n",
      "    At iteration 10800 -> loss: 0.09308189023841007\n",
      "    At iteration 10900 -> loss: 0.09312275502000598\n",
      "    At iteration 11000 -> loss: 0.09309782888678494\n",
      "    At iteration 11100 -> loss: 0.09306828101153572\n",
      "    At iteration 11200 -> loss: 0.09305437259525197\n",
      "    At iteration 11300 -> loss: 0.09304739371974168\n",
      "    At iteration 11400 -> loss: 0.09301834195115753\n",
      "    At iteration 11500 -> loss: 0.09299567175621014\n",
      "    At iteration 11600 -> loss: 0.09298510466916894\n",
      "    At iteration 11700 -> loss: 0.09296964500312646\n",
      "    At iteration 11800 -> loss: 0.09293794273009338\n",
      "    At iteration 11900 -> loss: 0.09292100411775409\n",
      "    At iteration 12000 -> loss: 0.09290595098242369\n",
      "    At iteration 12100 -> loss: 0.0928743620717361\n",
      "    At iteration 12200 -> loss: 0.09285348982442908\n",
      "    At iteration 12300 -> loss: 0.09308621260616087\n",
      "    At iteration 12400 -> loss: 0.09311453542161387\n",
      "    At iteration 12500 -> loss: 0.0930884385512144\n",
      "    At iteration 12600 -> loss: 0.09306387208264358\n",
      "    At iteration 12700 -> loss: 0.09313296300381829\n",
      "    At iteration 12800 -> loss: 0.0931330579878037\n",
      "    At iteration 12900 -> loss: 0.09310404421275284\n",
      "    At iteration 13000 -> loss: 0.09308779163642687\n",
      "    At iteration 13100 -> loss: 0.09307851206781198\n",
      "    At iteration 13200 -> loss: 0.09307933952291432\n",
      "    At iteration 13300 -> loss: 0.0930373602785397\n",
      "    At iteration 13400 -> loss: 0.09302236617913487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 13500 -> loss: 0.09299718722839309\n",
      "    At iteration 13600 -> loss: 0.09300311994992243\n",
      "Staring Epoch 115\n",
      "    At iteration 0 -> loss: 0.1433780798688531\n",
      "    At iteration 100 -> loss: 0.09214936216975807\n",
      "    At iteration 200 -> loss: 0.09067733634477941\n",
      "    At iteration 300 -> loss: 0.09520039376608737\n",
      "    At iteration 400 -> loss: 0.09426872817181939\n",
      "    At iteration 500 -> loss: 0.0938263731962169\n",
      "    At iteration 600 -> loss: 0.09350907766585807\n",
      "    At iteration 700 -> loss: 0.09319509017357211\n",
      "    At iteration 800 -> loss: 0.0930055926367856\n",
      "    At iteration 900 -> loss: 0.09269633072082771\n",
      "    At iteration 1000 -> loss: 0.09243297167355558\n",
      "    At iteration 1100 -> loss: 0.09238935009658684\n",
      "    At iteration 1200 -> loss: 0.0924262291862278\n",
      "    At iteration 1300 -> loss: 0.09232704025059726\n",
      "    At iteration 1400 -> loss: 0.09230557688031396\n",
      "    At iteration 1500 -> loss: 0.09242020434792897\n",
      "    At iteration 1600 -> loss: 0.09217222535414817\n",
      "    At iteration 1700 -> loss: 0.09200331772770959\n",
      "    At iteration 1800 -> loss: 0.09208733320890583\n",
      "    At iteration 1900 -> loss: 0.09229519407577891\n",
      "    At iteration 2000 -> loss: 0.0923029095301316\n",
      "    At iteration 2100 -> loss: 0.09222166386808264\n",
      "    At iteration 2200 -> loss: 0.09228839941107285\n",
      "    At iteration 2300 -> loss: 0.09217819880862843\n",
      "    At iteration 2400 -> loss: 0.09236077263944083\n",
      "    At iteration 2500 -> loss: 0.09281837869788559\n",
      "    At iteration 2600 -> loss: 0.092721010233683\n",
      "    At iteration 2700 -> loss: 0.09263513052869174\n",
      "    At iteration 2800 -> loss: 0.09258020222966724\n",
      "    At iteration 2900 -> loss: 0.09255382304506048\n",
      "    At iteration 3000 -> loss: 0.09256391329254599\n",
      "    At iteration 3100 -> loss: 0.09263664948157706\n",
      "    At iteration 3200 -> loss: 0.09315645608611366\n",
      "    At iteration 3300 -> loss: 0.09319518283534661\n",
      "    At iteration 3400 -> loss: 0.09310631833501364\n",
      "    At iteration 3500 -> loss: 0.09324888968542451\n",
      "    At iteration 3600 -> loss: 0.09319766322870113\n",
      "    At iteration 3700 -> loss: 0.09322272854226162\n",
      "    At iteration 3800 -> loss: 0.09327193131550195\n",
      "    At iteration 3900 -> loss: 0.09341018881836015\n",
      "    At iteration 4000 -> loss: 0.09337952762898474\n",
      "    At iteration 4100 -> loss: 0.09335694056218453\n",
      "    At iteration 4200 -> loss: 0.09344393358962495\n",
      "    At iteration 4300 -> loss: 0.0934830888955382\n",
      "    At iteration 4400 -> loss: 0.09342161087940082\n",
      "    At iteration 4500 -> loss: 0.09340635899824665\n",
      "    At iteration 4600 -> loss: 0.09332285227761858\n",
      "    At iteration 4700 -> loss: 0.09334414321046582\n",
      "    At iteration 4800 -> loss: 0.09328278830461492\n",
      "    At iteration 4900 -> loss: 0.09324064300189816\n",
      "    At iteration 5000 -> loss: 0.09320016850539979\n",
      "    At iteration 5100 -> loss: 0.09334298322939273\n",
      "    At iteration 5200 -> loss: 0.093318433288452\n",
      "    At iteration 5300 -> loss: 0.09324544164471774\n",
      "    At iteration 5400 -> loss: 0.09321626146725952\n",
      "    At iteration 5500 -> loss: 0.09320493363783358\n",
      "    At iteration 5600 -> loss: 0.09319749438944253\n",
      "    At iteration 5700 -> loss: 0.09370067728962188\n",
      "    At iteration 5800 -> loss: 0.09364068841865268\n",
      "    At iteration 5900 -> loss: 0.0936861338526798\n",
      "    At iteration 6000 -> loss: 0.09368745604157333\n",
      "    At iteration 6100 -> loss: 0.09367285531286715\n",
      "    At iteration 6200 -> loss: 0.09362927772758114\n",
      "    At iteration 6300 -> loss: 0.09365810215559658\n",
      "    At iteration 6400 -> loss: 0.09363434118201847\n",
      "    At iteration 6500 -> loss: 0.09358446599577844\n",
      "    At iteration 6600 -> loss: 0.09351452371099099\n",
      "    At iteration 6700 -> loss: 0.09356350188039354\n",
      "    At iteration 6800 -> loss: 0.09352028872588845\n",
      "    At iteration 6900 -> loss: 0.09346193794432242\n",
      "    At iteration 7000 -> loss: 0.09359211502538739\n",
      "    At iteration 7100 -> loss: 0.09352902061789006\n",
      "    At iteration 7200 -> loss: 0.09352523892631712\n",
      "    At iteration 7300 -> loss: 0.09350663642468936\n",
      "    At iteration 7400 -> loss: 0.09346785184666223\n",
      "    At iteration 7500 -> loss: 0.09348942423127969\n",
      "    At iteration 7600 -> loss: 0.09347867220309374\n",
      "    At iteration 7700 -> loss: 0.09355644603353923\n",
      "    At iteration 7800 -> loss: 0.09350445132878638\n",
      "    At iteration 7900 -> loss: 0.09345205190839592\n",
      "    At iteration 8000 -> loss: 0.09342593024346517\n",
      "    At iteration 8100 -> loss: 0.09347895280362366\n",
      "    At iteration 8200 -> loss: 0.09346190930625027\n",
      "    At iteration 8300 -> loss: 0.09345681337003406\n",
      "    At iteration 8400 -> loss: 0.0934004221132368\n",
      "    At iteration 8500 -> loss: 0.09337621590553818\n",
      "    At iteration 8600 -> loss: 0.09335794960219933\n",
      "    At iteration 8700 -> loss: 0.09330409825773572\n",
      "    At iteration 8800 -> loss: 0.09326917718159983\n",
      "    At iteration 8900 -> loss: 0.09336260111497768\n",
      "    At iteration 9000 -> loss: 0.09335725457418394\n",
      "    At iteration 9100 -> loss: 0.09334534947462234\n",
      "    At iteration 9200 -> loss: 0.09332140049184595\n",
      "    At iteration 9300 -> loss: 0.09329328736394076\n",
      "    At iteration 9400 -> loss: 0.09328579959600458\n",
      "    At iteration 9500 -> loss: 0.09327472281331697\n",
      "    At iteration 9600 -> loss: 0.09324973965239985\n",
      "    At iteration 9700 -> loss: 0.09319550658326135\n",
      "    At iteration 9800 -> loss: 0.09315569895399048\n",
      "    At iteration 9900 -> loss: 0.0931451623047735\n",
      "    At iteration 10000 -> loss: 0.09314453512618912\n",
      "    At iteration 10100 -> loss: 0.09314549277483883\n",
      "    At iteration 10200 -> loss: 0.09310823653769539\n",
      "    At iteration 10300 -> loss: 0.09307436503638143\n",
      "    At iteration 10400 -> loss: 0.09309058924107344\n",
      "    At iteration 10500 -> loss: 0.09308748310369609\n",
      "    At iteration 10600 -> loss: 0.09312420593993696\n",
      "    At iteration 10700 -> loss: 0.09310765308602297\n",
      "    At iteration 10800 -> loss: 0.09312909268224066\n",
      "    At iteration 10900 -> loss: 0.0931673870683411\n",
      "    At iteration 11000 -> loss: 0.09314145086333431\n",
      "    At iteration 11100 -> loss: 0.09312793100422308\n",
      "    At iteration 11200 -> loss: 0.09311034860215038\n",
      "    At iteration 11300 -> loss: 0.09310901624413732\n",
      "    At iteration 11400 -> loss: 0.09309404789571021\n",
      "    At iteration 11500 -> loss: 0.0931127935257676\n",
      "    At iteration 11600 -> loss: 0.09308728392324185\n",
      "    At iteration 11700 -> loss: 0.09310435344046326\n",
      "    At iteration 11800 -> loss: 0.09310062963126385\n",
      "    At iteration 11900 -> loss: 0.09308750689109707\n",
      "    At iteration 12000 -> loss: 0.0930869449705753\n",
      "    At iteration 12100 -> loss: 0.09308400485533634\n",
      "    At iteration 12200 -> loss: 0.0930493967918528\n",
      "    At iteration 12300 -> loss: 0.09305919291187006\n",
      "    At iteration 12400 -> loss: 0.09303900258101352\n",
      "    At iteration 12500 -> loss: 0.09306723862590742\n",
      "    At iteration 12600 -> loss: 0.09307822428677039\n",
      "    At iteration 12700 -> loss: 0.09312117484871031\n",
      "    At iteration 12800 -> loss: 0.09313541725003548\n",
      "    At iteration 12900 -> loss: 0.09313267869486974\n",
      "    At iteration 13000 -> loss: 0.09311633471758624\n",
      "    At iteration 13100 -> loss: 0.0930973908231391\n",
      "    At iteration 13200 -> loss: 0.09308642455333632\n",
      "    At iteration 13300 -> loss: 0.09306200441063124\n",
      "    At iteration 13400 -> loss: 0.09305764559504569\n",
      "    At iteration 13500 -> loss: 0.09303766109042477\n",
      "    At iteration 13600 -> loss: 0.0930220309511297\n",
      "Staring Epoch 116\n",
      "    At iteration 0 -> loss: 0.08859404339455068\n",
      "    At iteration 100 -> loss: 0.09364741590346001\n",
      "    At iteration 200 -> loss: 0.09497254568715484\n",
      "    At iteration 300 -> loss: 0.0934309737380074\n",
      "    At iteration 400 -> loss: 0.09360130966927514\n",
      "    At iteration 500 -> loss: 0.0940271914981307\n",
      "    At iteration 600 -> loss: 0.09393242802536474\n",
      "    At iteration 700 -> loss: 0.09466154723354632\n",
      "    At iteration 800 -> loss: 0.09469109427274326\n",
      "    At iteration 900 -> loss: 0.09516210877718399\n",
      "    At iteration 1000 -> loss: 0.09469930221997143\n",
      "    At iteration 1100 -> loss: 0.09810030351735179\n",
      "    At iteration 1200 -> loss: 0.09739878980238224\n",
      "    At iteration 1300 -> loss: 0.09745488844661926\n",
      "    At iteration 1400 -> loss: 0.09681884718095202\n",
      "    At iteration 1500 -> loss: 0.09654428504607733\n",
      "    At iteration 1600 -> loss: 0.09622506993576166\n",
      "    At iteration 1700 -> loss: 0.09592630011193667\n",
      "    At iteration 1800 -> loss: 0.09555145592643625\n",
      "    At iteration 1900 -> loss: 0.09539007443537323\n",
      "    At iteration 2000 -> loss: 0.09520398562080726\n",
      "    At iteration 2100 -> loss: 0.0951743184616865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2200 -> loss: 0.09496359854969978\n",
      "    At iteration 2300 -> loss: 0.09476108551384654\n",
      "    At iteration 2400 -> loss: 0.09480911673399842\n",
      "    At iteration 2500 -> loss: 0.0948912780844541\n",
      "    At iteration 2600 -> loss: 0.0948101975301786\n",
      "    At iteration 2700 -> loss: 0.09487994682801533\n",
      "    At iteration 2800 -> loss: 0.09474243317891164\n",
      "    At iteration 2900 -> loss: 0.0946845665847931\n",
      "    At iteration 3000 -> loss: 0.09460788862966396\n",
      "    At iteration 3100 -> loss: 0.09443337827312441\n",
      "    At iteration 3200 -> loss: 0.09432836498569672\n",
      "    At iteration 3300 -> loss: 0.09426212069869049\n",
      "    At iteration 3400 -> loss: 0.09430387678698368\n",
      "    At iteration 3500 -> loss: 0.09427053466198276\n",
      "    At iteration 3600 -> loss: 0.09420471877768889\n",
      "    At iteration 3700 -> loss: 0.09412925184135676\n",
      "    At iteration 3800 -> loss: 0.09397528551660679\n",
      "    At iteration 3900 -> loss: 0.09399313083071559\n",
      "    At iteration 4000 -> loss: 0.09386275498170242\n",
      "    At iteration 4100 -> loss: 0.09376071740440071\n",
      "    At iteration 4200 -> loss: 0.0938766169941776\n",
      "    At iteration 4300 -> loss: 0.09385753871684281\n",
      "    At iteration 4400 -> loss: 0.09373516304451704\n",
      "    At iteration 4500 -> loss: 0.09376999503887178\n",
      "    At iteration 4600 -> loss: 0.09376186682378912\n",
      "    At iteration 4700 -> loss: 0.09365868903647384\n",
      "    At iteration 4800 -> loss: 0.09370482076478562\n",
      "    At iteration 4900 -> loss: 0.09364822660403267\n",
      "    At iteration 5000 -> loss: 0.09359812195259619\n",
      "    At iteration 5100 -> loss: 0.09365460967706367\n",
      "    At iteration 5200 -> loss: 0.09360179245038265\n",
      "    At iteration 5300 -> loss: 0.09366298279787494\n",
      "    At iteration 5400 -> loss: 0.09380317239296562\n",
      "    At iteration 5500 -> loss: 0.09381918824309511\n",
      "    At iteration 5600 -> loss: 0.09372452522483088\n",
      "    At iteration 5700 -> loss: 0.09364285561895702\n",
      "    At iteration 5800 -> loss: 0.09360310032189705\n",
      "    At iteration 5900 -> loss: 0.09363227769265824\n",
      "    At iteration 6000 -> loss: 0.09352630474692006\n",
      "    At iteration 6100 -> loss: 0.09350199418317652\n",
      "    At iteration 6200 -> loss: 0.09344061654695812\n",
      "    At iteration 6300 -> loss: 0.0934787314436661\n",
      "    At iteration 6400 -> loss: 0.09341084550731858\n",
      "    At iteration 6500 -> loss: 0.09339542562749968\n",
      "    At iteration 6600 -> loss: 0.09335554446080778\n",
      "    At iteration 6700 -> loss: 0.0933476708974773\n",
      "    At iteration 6800 -> loss: 0.09330438460327199\n",
      "    At iteration 6900 -> loss: 0.09333379049406804\n",
      "    At iteration 7000 -> loss: 0.09343907878419383\n",
      "    At iteration 7100 -> loss: 0.09342720889343745\n",
      "    At iteration 7200 -> loss: 0.09340543741730908\n",
      "    At iteration 7300 -> loss: 0.09336808716076789\n",
      "    At iteration 7400 -> loss: 0.09334821433057347\n",
      "    At iteration 7500 -> loss: 0.09331293584399218\n",
      "    At iteration 7600 -> loss: 0.0932979422834379\n",
      "    At iteration 7700 -> loss: 0.09324027861027348\n",
      "    At iteration 7800 -> loss: 0.0931984278587547\n",
      "    At iteration 7900 -> loss: 0.0933752351821766\n",
      "    At iteration 8000 -> loss: 0.09332605093864503\n",
      "    At iteration 8100 -> loss: 0.09329924540983146\n",
      "    At iteration 8200 -> loss: 0.09327507338394093\n",
      "    At iteration 8300 -> loss: 0.09323233006601674\n",
      "    At iteration 8400 -> loss: 0.09319419640515325\n",
      "    At iteration 8500 -> loss: 0.09315746325230327\n",
      "    At iteration 8600 -> loss: 0.09320336805673624\n",
      "    At iteration 8700 -> loss: 0.09318856598753981\n",
      "    At iteration 8800 -> loss: 0.09315170446368821\n",
      "    At iteration 8900 -> loss: 0.09312129083842283\n",
      "    At iteration 9000 -> loss: 0.0931007798617084\n",
      "    At iteration 9100 -> loss: 0.0930548871330921\n",
      "    At iteration 9200 -> loss: 0.09310394552124776\n",
      "    At iteration 9300 -> loss: 0.09308568447371433\n",
      "    At iteration 9400 -> loss: 0.09305492495866056\n",
      "    At iteration 9500 -> loss: 0.09306475814054861\n",
      "    At iteration 9600 -> loss: 0.09305380605558268\n",
      "    At iteration 9700 -> loss: 0.09301651778567913\n",
      "    At iteration 9800 -> loss: 0.09300866928124359\n",
      "    At iteration 9900 -> loss: 0.0929972726538657\n",
      "    At iteration 10000 -> loss: 0.09296131012159908\n",
      "    At iteration 10100 -> loss: 0.09293203558134568\n",
      "    At iteration 10200 -> loss: 0.09300731903829335\n",
      "    At iteration 10300 -> loss: 0.09298871351892073\n",
      "    At iteration 10400 -> loss: 0.09300246774347598\n",
      "    At iteration 10500 -> loss: 0.09299385233959219\n",
      "    At iteration 10600 -> loss: 0.09298967999873145\n",
      "    At iteration 10700 -> loss: 0.09302986787785038\n",
      "    At iteration 10800 -> loss: 0.09299002568268833\n",
      "    At iteration 10900 -> loss: 0.09297476990442188\n",
      "    At iteration 11000 -> loss: 0.09302202838606319\n",
      "    At iteration 11100 -> loss: 0.09299794398556625\n",
      "    At iteration 11200 -> loss: 0.09297528299330851\n",
      "    At iteration 11300 -> loss: 0.09294992513548994\n",
      "    At iteration 11400 -> loss: 0.09296263265467225\n",
      "    At iteration 11500 -> loss: 0.09293417203035143\n",
      "    At iteration 11600 -> loss: 0.09295101301495624\n",
      "    At iteration 11700 -> loss: 0.09296813340748612\n",
      "    At iteration 11800 -> loss: 0.09307890063837575\n",
      "    At iteration 11900 -> loss: 0.09304734440638289\n",
      "    At iteration 12000 -> loss: 0.09307904223227344\n",
      "    At iteration 12100 -> loss: 0.09308078991974308\n",
      "    At iteration 12200 -> loss: 0.0930749081734514\n",
      "    At iteration 12300 -> loss: 0.0930965707060726\n",
      "    At iteration 12400 -> loss: 0.09308720814433259\n",
      "    At iteration 12500 -> loss: 0.09304877059127932\n",
      "    At iteration 12600 -> loss: 0.09304889443960755\n",
      "    At iteration 12700 -> loss: 0.09304726690931668\n",
      "    At iteration 12800 -> loss: 0.09301308291708346\n",
      "    At iteration 12900 -> loss: 0.0930170510951117\n",
      "    At iteration 13000 -> loss: 0.09301446637614688\n",
      "    At iteration 13100 -> loss: 0.09302174939448853\n",
      "    At iteration 13200 -> loss: 0.09302055927769083\n",
      "    At iteration 13300 -> loss: 0.09303782078235741\n",
      "    At iteration 13400 -> loss: 0.0930249313254039\n",
      "    At iteration 13500 -> loss: 0.09306162438890385\n",
      "    At iteration 13600 -> loss: 0.09302754706009617\n",
      "Staring Epoch 117\n",
      "    At iteration 0 -> loss: 0.09177916077896953\n",
      "    At iteration 100 -> loss: 0.08832710942860529\n",
      "    At iteration 200 -> loss: 0.0899879132297398\n",
      "    At iteration 300 -> loss: 0.08980070758481563\n",
      "    At iteration 400 -> loss: 0.09066127049319385\n",
      "    At iteration 500 -> loss: 0.09099824376582591\n",
      "    At iteration 600 -> loss: 0.09178504102395123\n",
      "    At iteration 700 -> loss: 0.09173965376401413\n",
      "    At iteration 800 -> loss: 0.0935867227377503\n",
      "    At iteration 900 -> loss: 0.0939256167720695\n",
      "    At iteration 1000 -> loss: 0.09383405729271162\n",
      "    At iteration 1100 -> loss: 0.09341293546438113\n",
      "    At iteration 1200 -> loss: 0.09303560441173159\n",
      "    At iteration 1300 -> loss: 0.09351589446843692\n",
      "    At iteration 1400 -> loss: 0.09342795281752897\n",
      "    At iteration 1500 -> loss: 0.09321394502456085\n",
      "    At iteration 1600 -> loss: 0.09305613680200586\n",
      "    At iteration 1700 -> loss: 0.09284030369139923\n",
      "    At iteration 1800 -> loss: 0.09294237846849854\n",
      "    At iteration 1900 -> loss: 0.09281078169511278\n",
      "    At iteration 2000 -> loss: 0.0928860583154971\n",
      "    At iteration 2100 -> loss: 0.09274139826562523\n",
      "    At iteration 2200 -> loss: 0.09319989667297979\n",
      "    At iteration 2300 -> loss: 0.09338959116909738\n",
      "    At iteration 2400 -> loss: 0.09319655336393626\n",
      "    At iteration 2500 -> loss: 0.0931790332737633\n",
      "    At iteration 2600 -> loss: 0.09315044710304864\n",
      "    At iteration 2700 -> loss: 0.09305575837354498\n",
      "    At iteration 2800 -> loss: 0.09296197953345114\n",
      "    At iteration 2900 -> loss: 0.09285883604811702\n",
      "    At iteration 3000 -> loss: 0.09285020767152781\n",
      "    At iteration 3100 -> loss: 0.09276743223513233\n",
      "    At iteration 3200 -> loss: 0.09276569649669564\n",
      "    At iteration 3300 -> loss: 0.0927952394822869\n",
      "    At iteration 3400 -> loss: 0.09281161478663835\n",
      "    At iteration 3500 -> loss: 0.09266598278420922\n",
      "    At iteration 3600 -> loss: 0.09260187992754113\n",
      "    At iteration 3700 -> loss: 0.09260359705314772\n",
      "    At iteration 3800 -> loss: 0.09269678494178389\n",
      "    At iteration 3900 -> loss: 0.09267154487912953\n",
      "    At iteration 4000 -> loss: 0.09262757212690947\n",
      "    At iteration 4100 -> loss: 0.0925542737631037\n",
      "    At iteration 4200 -> loss: 0.09251307986184713\n",
      "    At iteration 4300 -> loss: 0.09321673628569817\n",
      "    At iteration 4400 -> loss: 0.09315091684827681\n",
      "    At iteration 4500 -> loss: 0.09310179981967584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 4600 -> loss: 0.09322994520311127\n",
      "    At iteration 4700 -> loss: 0.09339293198623284\n",
      "    At iteration 4800 -> loss: 0.09334617346639894\n",
      "    At iteration 4900 -> loss: 0.09325254192068766\n",
      "    At iteration 5000 -> loss: 0.09322830255905912\n",
      "    At iteration 5100 -> loss: 0.09327573677125668\n",
      "    At iteration 5200 -> loss: 0.09329868434437968\n",
      "    At iteration 5300 -> loss: 0.093255508273303\n",
      "    At iteration 5400 -> loss: 0.09315235270232756\n",
      "    At iteration 5500 -> loss: 0.09322989978941476\n",
      "    At iteration 5600 -> loss: 0.09314117940146012\n",
      "    At iteration 5700 -> loss: 0.09329329855011927\n",
      "    At iteration 5800 -> loss: 0.093261601243231\n",
      "    At iteration 5900 -> loss: 0.09319804630780876\n",
      "    At iteration 6000 -> loss: 0.09314930156408487\n",
      "    At iteration 6100 -> loss: 0.09319878956316056\n",
      "    At iteration 6200 -> loss: 0.09314273327999525\n",
      "    At iteration 6300 -> loss: 0.09312500306237823\n",
      "    At iteration 6400 -> loss: 0.09313449508271525\n",
      "    At iteration 6500 -> loss: 0.09311275675630665\n",
      "    At iteration 6600 -> loss: 0.09306160949217016\n",
      "    At iteration 6700 -> loss: 0.0930445072362594\n",
      "    At iteration 6800 -> loss: 0.0930259518294354\n",
      "    At iteration 6900 -> loss: 0.09317006536981075\n",
      "    At iteration 7000 -> loss: 0.09314802701650111\n",
      "    At iteration 7100 -> loss: 0.09310926137826069\n",
      "    At iteration 7200 -> loss: 0.09308159243783866\n",
      "    At iteration 7300 -> loss: 0.09311291315669416\n",
      "    At iteration 7400 -> loss: 0.09318484069144646\n",
      "    At iteration 7500 -> loss: 0.09319927346892758\n",
      "    At iteration 7600 -> loss: 0.09318608515458517\n",
      "    At iteration 7700 -> loss: 0.09320888360173334\n",
      "    At iteration 7800 -> loss: 0.09319019448344906\n",
      "    At iteration 7900 -> loss: 0.09318167241364622\n",
      "    At iteration 8000 -> loss: 0.09318194288187107\n",
      "    At iteration 8100 -> loss: 0.09314698512168387\n",
      "    At iteration 8200 -> loss: 0.09312781715969892\n",
      "    At iteration 8300 -> loss: 0.09313367913050814\n",
      "    At iteration 8400 -> loss: 0.09310729562243843\n",
      "    At iteration 8500 -> loss: 0.09315283276154539\n",
      "    At iteration 8600 -> loss: 0.0931353397742218\n",
      "    At iteration 8700 -> loss: 0.09310813420024559\n",
      "    At iteration 8800 -> loss: 0.09312530371915081\n",
      "    At iteration 8900 -> loss: 0.09311780967803616\n",
      "    At iteration 9000 -> loss: 0.09311680012562289\n",
      "    At iteration 9100 -> loss: 0.09306188747475874\n",
      "    At iteration 9200 -> loss: 0.09308502176854189\n",
      "    At iteration 9300 -> loss: 0.093111329965175\n",
      "    At iteration 9400 -> loss: 0.09306967905772215\n",
      "    At iteration 9500 -> loss: 0.09308838968051944\n",
      "    At iteration 9600 -> loss: 0.09306781855089029\n",
      "    At iteration 9700 -> loss: 0.09307513068027429\n",
      "    At iteration 9800 -> loss: 0.09304245409713023\n",
      "    At iteration 9900 -> loss: 0.0930956911684762\n",
      "    At iteration 10000 -> loss: 0.09311673957782379\n",
      "    At iteration 10100 -> loss: 0.09306437709280713\n",
      "    At iteration 10200 -> loss: 0.09303392447244736\n",
      "    At iteration 10300 -> loss: 0.09302236468865067\n",
      "    At iteration 10400 -> loss: 0.09312183858987315\n",
      "    At iteration 10500 -> loss: 0.09306831589389795\n",
      "    At iteration 10600 -> loss: 0.09308608070287384\n",
      "    At iteration 10700 -> loss: 0.09305785544882718\n",
      "    At iteration 10800 -> loss: 0.0931267419993495\n",
      "    At iteration 10900 -> loss: 0.09310604157781832\n",
      "    At iteration 11000 -> loss: 0.09310593990609543\n",
      "    At iteration 11100 -> loss: 0.0930796449116538\n",
      "    At iteration 11200 -> loss: 0.09313528510414619\n",
      "    At iteration 11300 -> loss: 0.09311508747730982\n",
      "    At iteration 11400 -> loss: 0.09309732217226553\n",
      "    At iteration 11500 -> loss: 0.09309209853630775\n",
      "    At iteration 11600 -> loss: 0.09308833936713834\n",
      "    At iteration 11700 -> loss: 0.09308383636494023\n",
      "    At iteration 11800 -> loss: 0.09308576661506754\n",
      "    At iteration 11900 -> loss: 0.09307808589802109\n",
      "    At iteration 12000 -> loss: 0.09307735790100911\n",
      "    At iteration 12100 -> loss: 0.0930666239070111\n",
      "    At iteration 12200 -> loss: 0.09304059642269609\n",
      "    At iteration 12300 -> loss: 0.09301228910617493\n",
      "    At iteration 12400 -> loss: 0.09299562672799294\n",
      "    At iteration 12500 -> loss: 0.09296469398034975\n",
      "    At iteration 12600 -> loss: 0.09293516940387027\n",
      "    At iteration 12700 -> loss: 0.09294234513156457\n",
      "    At iteration 12800 -> loss: 0.09291821135887196\n",
      "    At iteration 12900 -> loss: 0.0929126697187212\n",
      "    At iteration 13000 -> loss: 0.09290137979443853\n",
      "    At iteration 13100 -> loss: 0.09287496207721987\n",
      "    At iteration 13200 -> loss: 0.09287528380666853\n",
      "    At iteration 13300 -> loss: 0.09287813614263535\n",
      "    At iteration 13400 -> loss: 0.09296548434302727\n",
      "    At iteration 13500 -> loss: 0.09299185780071889\n",
      "    At iteration 13600 -> loss: 0.09299346789529987\n",
      "Staring Epoch 118\n",
      "    At iteration 0 -> loss: 0.08890517754480243\n",
      "    At iteration 100 -> loss: 0.09562716479690034\n",
      "    At iteration 200 -> loss: 0.09273576665310695\n",
      "    At iteration 300 -> loss: 0.09449616223478098\n",
      "    At iteration 400 -> loss: 0.09389037002309297\n",
      "    At iteration 500 -> loss: 0.09321976636846761\n",
      "    At iteration 600 -> loss: 0.09265548562990791\n",
      "    At iteration 700 -> loss: 0.0992703360931745\n",
      "    At iteration 800 -> loss: 0.09946535754806178\n",
      "    At iteration 900 -> loss: 0.09850767409998767\n",
      "    At iteration 1000 -> loss: 0.09769349075822632\n",
      "    At iteration 1100 -> loss: 0.0969758504445016\n",
      "    At iteration 1200 -> loss: 0.0966652499674674\n",
      "    At iteration 1300 -> loss: 0.09729164787737755\n",
      "    At iteration 1400 -> loss: 0.09669775569554931\n",
      "    At iteration 1500 -> loss: 0.09638795242441292\n",
      "    At iteration 1600 -> loss: 0.09608365956160028\n",
      "    At iteration 1700 -> loss: 0.0958051045364223\n",
      "    At iteration 1800 -> loss: 0.09625692438806867\n",
      "    At iteration 1900 -> loss: 0.0959822297706482\n",
      "    At iteration 2000 -> loss: 0.09589562533238462\n",
      "    At iteration 2100 -> loss: 0.09582086602627003\n",
      "    At iteration 2200 -> loss: 0.09551165618385245\n",
      "    At iteration 2300 -> loss: 0.095443215848936\n",
      "    At iteration 2400 -> loss: 0.09525708516563672\n",
      "    At iteration 2500 -> loss: 0.09501453651531673\n",
      "    At iteration 2600 -> loss: 0.09491808467747485\n",
      "    At iteration 2700 -> loss: 0.0947696216877279\n",
      "    At iteration 2800 -> loss: 0.09471712519783627\n",
      "    At iteration 2900 -> loss: 0.09450432594276241\n",
      "    At iteration 3000 -> loss: 0.09434817842214223\n",
      "    At iteration 3100 -> loss: 0.09438711537450699\n",
      "    At iteration 3200 -> loss: 0.09446062846484785\n",
      "    At iteration 3300 -> loss: 0.09451201770457865\n",
      "    At iteration 3400 -> loss: 0.09464073373830614\n",
      "    At iteration 3500 -> loss: 0.0945144997034367\n",
      "    At iteration 3600 -> loss: 0.0944502510871093\n",
      "    At iteration 3700 -> loss: 0.09437836408443123\n",
      "    At iteration 3800 -> loss: 0.09427278828205367\n",
      "    At iteration 3900 -> loss: 0.09412987459577912\n",
      "    At iteration 4000 -> loss: 0.09404078192912313\n",
      "    At iteration 4100 -> loss: 0.09404733400568042\n",
      "    At iteration 4200 -> loss: 0.09394110409846429\n",
      "    At iteration 4300 -> loss: 0.09391599091375771\n",
      "    At iteration 4400 -> loss: 0.09388621686310267\n",
      "    At iteration 4500 -> loss: 0.09379743989695671\n",
      "    At iteration 4600 -> loss: 0.09389502328472801\n",
      "    At iteration 4700 -> loss: 0.09388841698576585\n",
      "    At iteration 4800 -> loss: 0.09383098199356017\n",
      "    At iteration 4900 -> loss: 0.09384910545713254\n",
      "    At iteration 5000 -> loss: 0.0938571793620485\n",
      "    At iteration 5100 -> loss: 0.09378690284888014\n",
      "    At iteration 5200 -> loss: 0.0937296472529744\n",
      "    At iteration 5300 -> loss: 0.09366817866191934\n",
      "    At iteration 5400 -> loss: 0.09362826792374296\n",
      "    At iteration 5500 -> loss: 0.09360188091075215\n",
      "    At iteration 5600 -> loss: 0.09354795088719166\n",
      "    At iteration 5700 -> loss: 0.09354673874350042\n",
      "    At iteration 5800 -> loss: 0.0936178794857221\n",
      "    At iteration 5900 -> loss: 0.09363178384214906\n",
      "    At iteration 6000 -> loss: 0.09360832017540013\n",
      "    At iteration 6100 -> loss: 0.09353335524235197\n",
      "    At iteration 6200 -> loss: 0.09349339670215899\n",
      "    At iteration 6300 -> loss: 0.09351017994479155\n",
      "    At iteration 6400 -> loss: 0.09349759912646853\n",
      "    At iteration 6500 -> loss: 0.09355425794084102\n",
      "    At iteration 6600 -> loss: 0.09350094430168161\n",
      "    At iteration 6700 -> loss: 0.09352672983866048\n",
      "    At iteration 6800 -> loss: 0.0935694918081257\n",
      "    At iteration 6900 -> loss: 0.09353202470393783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7000 -> loss: 0.09347332769256071\n",
      "    At iteration 7100 -> loss: 0.0934620008372768\n",
      "    At iteration 7200 -> loss: 0.09351344873238097\n",
      "    At iteration 7300 -> loss: 0.09350693174704673\n",
      "    At iteration 7400 -> loss: 0.09349577527073782\n",
      "    At iteration 7500 -> loss: 0.09345195026359239\n",
      "    At iteration 7600 -> loss: 0.09345791191228697\n",
      "    At iteration 7700 -> loss: 0.09355954412241194\n",
      "    At iteration 7800 -> loss: 0.09354182877830751\n",
      "    At iteration 7900 -> loss: 0.09349087676100533\n",
      "    At iteration 8000 -> loss: 0.09356540913440863\n",
      "    At iteration 8100 -> loss: 0.09351874180598851\n",
      "    At iteration 8200 -> loss: 0.09356204037251796\n",
      "    At iteration 8300 -> loss: 0.09353068401032562\n",
      "    At iteration 8400 -> loss: 0.09346472829472077\n",
      "    At iteration 8500 -> loss: 0.09344290348388337\n",
      "    At iteration 8600 -> loss: 0.09339251009685196\n",
      "    At iteration 8700 -> loss: 0.0934044418183892\n",
      "    At iteration 8800 -> loss: 0.09335453152961136\n",
      "    At iteration 8900 -> loss: 0.0933674028855285\n",
      "    At iteration 9000 -> loss: 0.0933997474391105\n",
      "    At iteration 9100 -> loss: 0.09340372938102265\n",
      "    At iteration 9200 -> loss: 0.09337481643148747\n",
      "    At iteration 9300 -> loss: 0.09335346606959262\n",
      "    At iteration 9400 -> loss: 0.09338057582453077\n",
      "    At iteration 9500 -> loss: 0.09341456797151958\n",
      "    At iteration 9600 -> loss: 0.09338165517420363\n",
      "    At iteration 9700 -> loss: 0.09333414278565645\n",
      "    At iteration 9800 -> loss: 0.0933058926892345\n",
      "    At iteration 9900 -> loss: 0.09326901191160611\n",
      "    At iteration 10000 -> loss: 0.09327448253066882\n",
      "    At iteration 10100 -> loss: 0.09323415050668861\n",
      "    At iteration 10200 -> loss: 0.09322085606405013\n",
      "    At iteration 10300 -> loss: 0.09324029641698169\n",
      "    At iteration 10400 -> loss: 0.09322477028879701\n",
      "    At iteration 10500 -> loss: 0.09325390669667515\n",
      "    At iteration 10600 -> loss: 0.09330666468476248\n",
      "    At iteration 10700 -> loss: 0.09329884249467581\n",
      "    At iteration 10800 -> loss: 0.09327140460456845\n",
      "    At iteration 10900 -> loss: 0.09327781056038005\n",
      "    At iteration 11000 -> loss: 0.09325109433528242\n",
      "    At iteration 11100 -> loss: 0.0932250548619994\n",
      "    At iteration 11200 -> loss: 0.09323584642557536\n",
      "    At iteration 11300 -> loss: 0.09320262745571772\n",
      "    At iteration 11400 -> loss: 0.09318906170155583\n",
      "    At iteration 11500 -> loss: 0.09322029302923901\n",
      "    At iteration 11600 -> loss: 0.09324506517669404\n",
      "    At iteration 11700 -> loss: 0.09320449355518605\n",
      "    At iteration 11800 -> loss: 0.09318324749823896\n",
      "    At iteration 11900 -> loss: 0.09316040233988981\n",
      "    At iteration 12000 -> loss: 0.09314214191682829\n",
      "    At iteration 12100 -> loss: 0.09313533821675327\n",
      "    At iteration 12200 -> loss: 0.09310756406616282\n",
      "    At iteration 12300 -> loss: 0.09308023583335542\n",
      "    At iteration 12400 -> loss: 0.09308916394435938\n",
      "    At iteration 12500 -> loss: 0.09307631958742899\n",
      "    At iteration 12600 -> loss: 0.09304687263634935\n",
      "    At iteration 12700 -> loss: 0.09306147432235375\n",
      "    At iteration 12800 -> loss: 0.09307916065978612\n",
      "    At iteration 12900 -> loss: 0.09306761383152837\n",
      "    At iteration 13000 -> loss: 0.09303793804179082\n",
      "    At iteration 13100 -> loss: 0.09305144838379112\n",
      "    At iteration 13200 -> loss: 0.09303482703092884\n",
      "    At iteration 13300 -> loss: 0.0930195329834697\n",
      "    At iteration 13400 -> loss: 0.09300229943443915\n",
      "    At iteration 13500 -> loss: 0.09299581735020741\n",
      "    At iteration 13600 -> loss: 0.0930026772531838\n",
      "Staring Epoch 119\n",
      "    At iteration 0 -> loss: 0.08596359915100038\n",
      "    At iteration 100 -> loss: 0.08957415194416166\n",
      "    At iteration 200 -> loss: 0.09028832898041111\n",
      "    At iteration 300 -> loss: 0.0909908187529793\n",
      "    At iteration 400 -> loss: 0.09093170256765758\n",
      "    At iteration 500 -> loss: 0.09155180853864568\n",
      "    At iteration 600 -> loss: 0.091389150058739\n",
      "    At iteration 700 -> loss: 0.09127279857895017\n",
      "    At iteration 800 -> loss: 0.0909514755740017\n",
      "    At iteration 900 -> loss: 0.0909500322250553\n",
      "    At iteration 1000 -> loss: 0.09140019532885882\n",
      "    At iteration 1100 -> loss: 0.09218644535950878\n",
      "    At iteration 1200 -> loss: 0.09185524449753506\n",
      "    At iteration 1300 -> loss: 0.09162590757688559\n",
      "    At iteration 1400 -> loss: 0.09251964234085769\n",
      "    At iteration 1500 -> loss: 0.09224635066539251\n",
      "    At iteration 1600 -> loss: 0.09236239188440792\n",
      "    At iteration 1700 -> loss: 0.09219190747755818\n",
      "    At iteration 1800 -> loss: 0.09205010844232336\n",
      "    At iteration 1900 -> loss: 0.09231038546619558\n",
      "    At iteration 2000 -> loss: 0.09238723639974268\n",
      "    At iteration 2100 -> loss: 0.09220793841031028\n",
      "    At iteration 2200 -> loss: 0.09207631469338676\n",
      "    At iteration 2300 -> loss: 0.09241877823087337\n",
      "    At iteration 2400 -> loss: 0.09242474009159828\n",
      "    At iteration 2500 -> loss: 0.09241981529896073\n",
      "    At iteration 2600 -> loss: 0.09268114712912916\n",
      "    At iteration 2700 -> loss: 0.09263302067322973\n",
      "    At iteration 2800 -> loss: 0.09252058297510656\n",
      "    At iteration 2900 -> loss: 0.09250779573863908\n",
      "    At iteration 3000 -> loss: 0.09247339804251739\n",
      "    At iteration 3100 -> loss: 0.0923940299623873\n",
      "    At iteration 3200 -> loss: 0.09232989405570344\n",
      "    At iteration 3300 -> loss: 0.09234982956106293\n",
      "    At iteration 3400 -> loss: 0.09227799585957333\n",
      "    At iteration 3500 -> loss: 0.09230070445942871\n",
      "    At iteration 3600 -> loss: 0.09237626376166691\n",
      "    At iteration 3700 -> loss: 0.09244067545552735\n",
      "    At iteration 3800 -> loss: 0.09241242256352714\n",
      "    At iteration 3900 -> loss: 0.09237901109744366\n",
      "    At iteration 4000 -> loss: 0.09248209769436923\n",
      "    At iteration 4100 -> loss: 0.09264410303083297\n",
      "    At iteration 4200 -> loss: 0.09292841652229403\n",
      "    At iteration 4300 -> loss: 0.09286302032125329\n",
      "    At iteration 4400 -> loss: 0.09282277693133048\n",
      "    At iteration 4500 -> loss: 0.09273421897016995\n",
      "    At iteration 4600 -> loss: 0.09266191742147734\n",
      "    At iteration 4700 -> loss: 0.09278414750349864\n",
      "    At iteration 4800 -> loss: 0.09271065769415748\n",
      "    At iteration 4900 -> loss: 0.09271028405124239\n",
      "    At iteration 5000 -> loss: 0.09278294937135152\n",
      "    At iteration 5100 -> loss: 0.09280675347507113\n",
      "    At iteration 5200 -> loss: 0.09277323246585989\n",
      "    At iteration 5300 -> loss: 0.09278251124940644\n",
      "    At iteration 5400 -> loss: 0.09291850703795061\n",
      "    At iteration 5500 -> loss: 0.09285409897887775\n",
      "    At iteration 5600 -> loss: 0.09287410653543068\n",
      "    At iteration 5700 -> loss: 0.0928253471636707\n",
      "    At iteration 5800 -> loss: 0.09299140303561966\n",
      "    At iteration 5900 -> loss: 0.09298431487798933\n",
      "    At iteration 6000 -> loss: 0.09298886132066982\n",
      "    At iteration 6100 -> loss: 0.09297590824389669\n",
      "    At iteration 6200 -> loss: 0.09301192886264911\n",
      "    At iteration 6300 -> loss: 0.09299960602715576\n",
      "    At iteration 6400 -> loss: 0.09296022246011178\n",
      "    At iteration 6500 -> loss: 0.09295610018863494\n",
      "    At iteration 6600 -> loss: 0.09289493245400247\n",
      "    At iteration 6700 -> loss: 0.09288694751810078\n",
      "    At iteration 6800 -> loss: 0.09286927179448362\n",
      "    At iteration 6900 -> loss: 0.09287187137839245\n",
      "    At iteration 7000 -> loss: 0.09283229007835465\n",
      "    At iteration 7100 -> loss: 0.09285445797578651\n",
      "    At iteration 7200 -> loss: 0.09281855617800833\n",
      "    At iteration 7300 -> loss: 0.0927997640870141\n",
      "    At iteration 7400 -> loss: 0.09277091331595193\n",
      "    At iteration 7500 -> loss: 0.09275015316105188\n",
      "    At iteration 7600 -> loss: 0.09272273182639139\n",
      "    At iteration 7700 -> loss: 0.0927195745911255\n",
      "    At iteration 7800 -> loss: 0.09270278619727818\n",
      "    At iteration 7900 -> loss: 0.0926853078668681\n",
      "    At iteration 8000 -> loss: 0.09266541943338535\n",
      "    At iteration 8100 -> loss: 0.09268986582440333\n",
      "    At iteration 8200 -> loss: 0.09266659748573926\n",
      "    At iteration 8300 -> loss: 0.09274127741627235\n",
      "    At iteration 8400 -> loss: 0.09285095331795735\n",
      "    At iteration 8500 -> loss: 0.09288069118440921\n",
      "    At iteration 8600 -> loss: 0.09285006133496979\n",
      "    At iteration 8700 -> loss: 0.09288906945203966\n",
      "    At iteration 8800 -> loss: 0.09292244049308247\n",
      "    At iteration 8900 -> loss: 0.09287016921079679\n",
      "    At iteration 9000 -> loss: 0.09284025699461881\n",
      "    At iteration 9100 -> loss: 0.0928052092615979\n",
      "    At iteration 9200 -> loss: 0.09278642554997871\n",
      "    At iteration 9300 -> loss: 0.09278025900838424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9400 -> loss: 0.09275242689389501\n",
      "    At iteration 9500 -> loss: 0.09274993256288688\n",
      "    At iteration 9600 -> loss: 0.09277682739121476\n",
      "    At iteration 9700 -> loss: 0.09275675859801535\n",
      "    At iteration 9800 -> loss: 0.0927330394491318\n",
      "    At iteration 9900 -> loss: 0.09281471255528777\n",
      "    At iteration 10000 -> loss: 0.09279008635840091\n",
      "    At iteration 10100 -> loss: 0.09280694846173324\n",
      "    At iteration 10200 -> loss: 0.09276954852772076\n",
      "    At iteration 10300 -> loss: 0.09274916510178711\n",
      "    At iteration 10400 -> loss: 0.0928327941118607\n",
      "    At iteration 10500 -> loss: 0.09283470603779446\n",
      "    At iteration 10600 -> loss: 0.09281073630159721\n",
      "    At iteration 10700 -> loss: 0.09278667510461566\n",
      "    At iteration 10800 -> loss: 0.09279981267772468\n",
      "    At iteration 10900 -> loss: 0.09286780731313526\n",
      "    At iteration 11000 -> loss: 0.09283589381983161\n",
      "    At iteration 11100 -> loss: 0.09285590751462829\n",
      "    At iteration 11200 -> loss: 0.09283209429746551\n",
      "    At iteration 11300 -> loss: 0.09286358505031739\n",
      "    At iteration 11400 -> loss: 0.09285554278706115\n",
      "    At iteration 11500 -> loss: 0.092835438323889\n",
      "    At iteration 11600 -> loss: 0.09318305439633162\n",
      "    At iteration 11700 -> loss: 0.0931571436665618\n",
      "    At iteration 11800 -> loss: 0.09316016025649196\n",
      "    At iteration 11900 -> loss: 0.09315331132800415\n",
      "    At iteration 12000 -> loss: 0.09314828034202335\n",
      "    At iteration 12100 -> loss: 0.09313038345510191\n",
      "    At iteration 12200 -> loss: 0.09314037173968577\n",
      "    At iteration 12300 -> loss: 0.09320896794334202\n",
      "    At iteration 12400 -> loss: 0.09324926251780745\n",
      "    At iteration 12500 -> loss: 0.09322210372961132\n",
      "    At iteration 12600 -> loss: 0.09320099001238846\n",
      "    At iteration 12700 -> loss: 0.09317436646698994\n",
      "    At iteration 12800 -> loss: 0.09316610725638683\n",
      "    At iteration 12900 -> loss: 0.093121359141664\n",
      "    At iteration 13000 -> loss: 0.09313269622242595\n",
      "    At iteration 13100 -> loss: 0.09309983761865126\n",
      "    At iteration 13200 -> loss: 0.09310142084658204\n",
      "    At iteration 13300 -> loss: 0.09306976487977393\n",
      "    At iteration 13400 -> loss: 0.09307044467380539\n",
      "    At iteration 13500 -> loss: 0.09304300250609722\n",
      "    At iteration 13600 -> loss: 0.09302432458997492\n",
      "Staring Epoch 120\n",
      "    At iteration 0 -> loss: 0.08003228276611196\n",
      "    At iteration 100 -> loss: 0.10105182814141844\n",
      "    At iteration 200 -> loss: 0.09550144605119866\n",
      "    At iteration 300 -> loss: 0.09331437176644193\n",
      "    At iteration 400 -> loss: 0.0928566488713422\n",
      "    At iteration 500 -> loss: 0.0924019559008853\n",
      "    At iteration 600 -> loss: 0.09232816980911579\n",
      "    At iteration 700 -> loss: 0.09249047388402128\n",
      "    At iteration 800 -> loss: 0.09253104374192961\n",
      "    At iteration 900 -> loss: 0.09263958103220327\n",
      "    At iteration 1000 -> loss: 0.09277718900520665\n",
      "    At iteration 1100 -> loss: 0.09340234708156209\n",
      "    At iteration 1200 -> loss: 0.09323571998971826\n",
      "    At iteration 1300 -> loss: 0.09313546850550197\n",
      "    At iteration 1400 -> loss: 0.09304037995711145\n",
      "    At iteration 1500 -> loss: 0.09330568619114514\n",
      "    At iteration 1600 -> loss: 0.09344676422684546\n",
      "    At iteration 1700 -> loss: 0.09326746807860349\n",
      "    At iteration 1800 -> loss: 0.09325598188642478\n",
      "    At iteration 1900 -> loss: 0.09333652683736307\n",
      "    At iteration 2000 -> loss: 0.09362663474919994\n",
      "    At iteration 2100 -> loss: 0.09361539215956038\n",
      "    At iteration 2200 -> loss: 0.09344701674590228\n",
      "    At iteration 2300 -> loss: 0.09342218308980946\n",
      "    At iteration 2400 -> loss: 0.09328312734947904\n",
      "    At iteration 2500 -> loss: 0.0931778808387359\n",
      "    At iteration 2600 -> loss: 0.09301364005860467\n",
      "    At iteration 2700 -> loss: 0.0931066654861323\n",
      "    At iteration 2800 -> loss: 0.0930712050167823\n",
      "    At iteration 2900 -> loss: 0.09301541979624912\n",
      "    At iteration 3000 -> loss: 0.09300576523328415\n",
      "    At iteration 3100 -> loss: 0.09307860430174199\n",
      "    At iteration 3200 -> loss: 0.09294237696991539\n",
      "    At iteration 3300 -> loss: 0.0930274964906216\n",
      "    At iteration 3400 -> loss: 0.09307085109959147\n",
      "    At iteration 3500 -> loss: 0.09304375072800797\n",
      "    At iteration 3600 -> loss: 0.09304667306394637\n",
      "    At iteration 3700 -> loss: 0.09303123818498227\n",
      "    At iteration 3800 -> loss: 0.09291791369218587\n",
      "    At iteration 3900 -> loss: 0.09288377255981127\n",
      "    At iteration 4000 -> loss: 0.09294184502042545\n",
      "    At iteration 4100 -> loss: 0.09295129035432605\n",
      "    At iteration 4200 -> loss: 0.09302908707685356\n",
      "    At iteration 4300 -> loss: 0.09295013596399156\n",
      "    At iteration 4400 -> loss: 0.09295817848870569\n",
      "    At iteration 4500 -> loss: 0.09290085989140354\n",
      "    At iteration 4600 -> loss: 0.09287564067900367\n",
      "    At iteration 4700 -> loss: 0.09279030498074468\n",
      "    At iteration 4800 -> loss: 0.09275497847908745\n",
      "    At iteration 4900 -> loss: 0.09277866852120606\n",
      "    At iteration 5000 -> loss: 0.09270228711119141\n",
      "    At iteration 5100 -> loss: 0.09270268269288437\n",
      "    At iteration 5200 -> loss: 0.09264635447464782\n",
      "    At iteration 5300 -> loss: 0.0926694239573764\n",
      "    At iteration 5400 -> loss: 0.09260491226908339\n",
      "    At iteration 5500 -> loss: 0.0926177043464832\n",
      "    At iteration 5600 -> loss: 0.09259395547825795\n",
      "    At iteration 5700 -> loss: 0.09257778649371222\n",
      "    At iteration 5800 -> loss: 0.0926399948478429\n",
      "    At iteration 5900 -> loss: 0.09256391744835916\n",
      "    At iteration 6000 -> loss: 0.09264577198705512\n",
      "    At iteration 6100 -> loss: 0.09270704480609389\n",
      "    At iteration 6200 -> loss: 0.09268002086903178\n",
      "    At iteration 6300 -> loss: 0.09266039904838572\n",
      "    At iteration 6400 -> loss: 0.0926887014904757\n",
      "    At iteration 6500 -> loss: 0.09269443403527469\n",
      "    At iteration 6600 -> loss: 0.09269416809486371\n",
      "    At iteration 6700 -> loss: 0.09272913077600899\n",
      "    At iteration 6800 -> loss: 0.09270182876567874\n",
      "    At iteration 6900 -> loss: 0.09264951033402029\n",
      "    At iteration 7000 -> loss: 0.09268295975036615\n",
      "    At iteration 7100 -> loss: 0.09264884200970933\n",
      "    At iteration 7200 -> loss: 0.09263596235772747\n",
      "    At iteration 7300 -> loss: 0.0926133298938681\n",
      "    At iteration 7400 -> loss: 0.09298906906824747\n",
      "    At iteration 7500 -> loss: 0.09297790720528955\n",
      "    At iteration 7600 -> loss: 0.09301402949314388\n",
      "    At iteration 7700 -> loss: 0.09299525986763256\n",
      "    At iteration 7800 -> loss: 0.09296214475036155\n",
      "    At iteration 7900 -> loss: 0.09299010275984375\n",
      "    At iteration 8000 -> loss: 0.092957607790014\n",
      "    At iteration 8100 -> loss: 0.09292871064215216\n",
      "    At iteration 8200 -> loss: 0.09291121171338979\n",
      "    At iteration 8300 -> loss: 0.09292002759384729\n",
      "    At iteration 8400 -> loss: 0.0929342334957125\n",
      "    At iteration 8500 -> loss: 0.09289927301564982\n",
      "    At iteration 8600 -> loss: 0.09289804259650437\n",
      "    At iteration 8700 -> loss: 0.09288828989177846\n",
      "    At iteration 8800 -> loss: 0.09289385011077445\n",
      "    At iteration 8900 -> loss: 0.09287548611903954\n",
      "    At iteration 9000 -> loss: 0.09286484666308167\n",
      "    At iteration 9100 -> loss: 0.09293018847411161\n",
      "    At iteration 9200 -> loss: 0.0929453405068148\n",
      "    At iteration 9300 -> loss: 0.09291669579847117\n",
      "    At iteration 9400 -> loss: 0.09288862912908873\n",
      "    At iteration 9500 -> loss: 0.09288084911482321\n",
      "    At iteration 9600 -> loss: 0.09289961131743278\n",
      "    At iteration 9700 -> loss: 0.09287870470934384\n",
      "    At iteration 9800 -> loss: 0.09295921636832667\n",
      "    At iteration 9900 -> loss: 0.09299716999502743\n",
      "    At iteration 10000 -> loss: 0.09300338130715954\n",
      "    At iteration 10100 -> loss: 0.09306179557253186\n",
      "    At iteration 10200 -> loss: 0.09320473831458623\n",
      "    At iteration 10300 -> loss: 0.0931810305944699\n",
      "    At iteration 10400 -> loss: 0.09319204999964031\n",
      "    At iteration 10500 -> loss: 0.09320651467128536\n",
      "    At iteration 10600 -> loss: 0.09319553926978574\n",
      "    At iteration 10700 -> loss: 0.09317754603639296\n",
      "    At iteration 10800 -> loss: 0.09314112984491262\n",
      "    At iteration 10900 -> loss: 0.09314116995569836\n",
      "    At iteration 11000 -> loss: 0.09312714069504281\n",
      "    At iteration 11100 -> loss: 0.09311889917537895\n",
      "    At iteration 11200 -> loss: 0.0930917332967594\n",
      "    At iteration 11300 -> loss: 0.09305258290075616\n",
      "    At iteration 11400 -> loss: 0.0930349975089033\n",
      "    At iteration 11500 -> loss: 0.09311633034632953\n",
      "    At iteration 11600 -> loss: 0.09309217354671294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 11700 -> loss: 0.09312105868079207\n",
      "    At iteration 11800 -> loss: 0.0930997401537187\n",
      "    At iteration 11900 -> loss: 0.09308884189522466\n",
      "    At iteration 12000 -> loss: 0.09305233978024448\n",
      "    At iteration 12100 -> loss: 0.09303353842800795\n",
      "    At iteration 12200 -> loss: 0.09299921881121209\n",
      "    At iteration 12300 -> loss: 0.09301589103702786\n",
      "    At iteration 12400 -> loss: 0.09305018432062663\n",
      "    At iteration 12500 -> loss: 0.09301758692840512\n",
      "    At iteration 12600 -> loss: 0.09304798054791344\n",
      "    At iteration 12700 -> loss: 0.09302601741594087\n",
      "    At iteration 12800 -> loss: 0.09304104134892739\n",
      "    At iteration 12900 -> loss: 0.0930116462552854\n",
      "    At iteration 13000 -> loss: 0.09299301793085507\n",
      "    At iteration 13100 -> loss: 0.09305942893987866\n",
      "    At iteration 13200 -> loss: 0.0930364080760504\n",
      "    At iteration 13300 -> loss: 0.09302446977673187\n",
      "    At iteration 13400 -> loss: 0.09303431648816353\n",
      "    At iteration 13500 -> loss: 0.09303470129814154\n",
      "    At iteration 13600 -> loss: 0.09301012978106403\n",
      "Staring Epoch 121\n",
      "    At iteration 0 -> loss: 0.08003101227333786\n",
      "    At iteration 100 -> loss: 0.0912359857774231\n",
      "    At iteration 200 -> loss: 0.09221950070274636\n",
      "    At iteration 300 -> loss: 0.09121287688409838\n",
      "    At iteration 400 -> loss: 0.09118249272329927\n",
      "    At iteration 500 -> loss: 0.09060185297466326\n",
      "    At iteration 600 -> loss: 0.09107630071030078\n",
      "    At iteration 700 -> loss: 0.09100078802284133\n",
      "    At iteration 800 -> loss: 0.09090863414628873\n",
      "    At iteration 900 -> loss: 0.09086311706365603\n",
      "    At iteration 1000 -> loss: 0.09084440454024363\n",
      "    At iteration 1100 -> loss: 0.09078062175241212\n",
      "    At iteration 1200 -> loss: 0.09075645736634634\n",
      "    At iteration 1300 -> loss: 0.09132305161534683\n",
      "    At iteration 1400 -> loss: 0.09234240728093612\n",
      "    At iteration 1500 -> loss: 0.09217325863073535\n",
      "    At iteration 1600 -> loss: 0.09218674746515025\n",
      "    At iteration 1700 -> loss: 0.09208411743298187\n",
      "    At iteration 1800 -> loss: 0.09246528098326486\n",
      "    At iteration 1900 -> loss: 0.09252491358942815\n",
      "    At iteration 2000 -> loss: 0.09268004255122878\n",
      "    At iteration 2100 -> loss: 0.0926118510472166\n",
      "    At iteration 2200 -> loss: 0.09245307357864618\n",
      "    At iteration 2300 -> loss: 0.09249299286939565\n",
      "    At iteration 2400 -> loss: 0.09242265742912532\n",
      "    At iteration 2500 -> loss: 0.0923813374101692\n",
      "    At iteration 2600 -> loss: 0.09238194913711409\n",
      "    At iteration 2700 -> loss: 0.09235753338729503\n",
      "    At iteration 2800 -> loss: 0.09224814047532845\n",
      "    At iteration 2900 -> loss: 0.09226230861744238\n",
      "    At iteration 3000 -> loss: 0.0924670510662583\n",
      "    At iteration 3100 -> loss: 0.09263229224519813\n",
      "    At iteration 3200 -> loss: 0.09263191325785586\n",
      "    At iteration 3300 -> loss: 0.0925634989963094\n",
      "    At iteration 3400 -> loss: 0.09262715085717453\n",
      "    At iteration 3500 -> loss: 0.09292364072668294\n",
      "    At iteration 3600 -> loss: 0.09281894012280516\n",
      "    At iteration 3700 -> loss: 0.09278424232348212\n",
      "    At iteration 3800 -> loss: 0.09276918617301161\n",
      "    At iteration 3900 -> loss: 0.09276002822478127\n",
      "    At iteration 4000 -> loss: 0.09280422371775106\n",
      "    At iteration 4100 -> loss: 0.09284061163364783\n",
      "    At iteration 4200 -> loss: 0.09274743605578457\n",
      "    At iteration 4300 -> loss: 0.09271005174578578\n",
      "    At iteration 4400 -> loss: 0.09268381538374637\n",
      "    At iteration 4500 -> loss: 0.09262219049062013\n",
      "    At iteration 4600 -> loss: 0.09257758407010658\n",
      "    At iteration 4700 -> loss: 0.09252855531081057\n",
      "    At iteration 4800 -> loss: 0.09248092075944636\n",
      "    At iteration 4900 -> loss: 0.09267952176617812\n",
      "    At iteration 5000 -> loss: 0.09264074145664389\n",
      "    At iteration 5100 -> loss: 0.09274420557276229\n",
      "    At iteration 5200 -> loss: 0.09276776352626831\n",
      "    At iteration 5300 -> loss: 0.09271650639916813\n",
      "    At iteration 5400 -> loss: 0.0927324598974067\n",
      "    At iteration 5500 -> loss: 0.09297121036272739\n",
      "    At iteration 5600 -> loss: 0.09290800472232291\n",
      "    At iteration 5700 -> loss: 0.09285810829825168\n",
      "    At iteration 5800 -> loss: 0.09296066895262345\n",
      "    At iteration 5900 -> loss: 0.09291472205543017\n",
      "    At iteration 6000 -> loss: 0.09292086252824668\n",
      "    At iteration 6100 -> loss: 0.09293697787572981\n",
      "    At iteration 6200 -> loss: 0.09290814323152956\n",
      "    At iteration 6300 -> loss: 0.09285127074597596\n",
      "    At iteration 6400 -> loss: 0.09283602362511437\n",
      "    At iteration 6500 -> loss: 0.09276223059172492\n",
      "    At iteration 6600 -> loss: 0.09278866907086823\n",
      "    At iteration 6700 -> loss: 0.09277382685408404\n",
      "    At iteration 6800 -> loss: 0.09274991256872205\n",
      "    At iteration 6900 -> loss: 0.09272375015781072\n",
      "    At iteration 7000 -> loss: 0.09268810967543421\n",
      "    At iteration 7100 -> loss: 0.09267108152578138\n",
      "    At iteration 7200 -> loss: 0.0927008221047943\n",
      "    At iteration 7300 -> loss: 0.09269938401814516\n",
      "    At iteration 7400 -> loss: 0.09268589740833919\n",
      "    At iteration 7500 -> loss: 0.09266072868350544\n",
      "    At iteration 7600 -> loss: 0.09263266061695641\n",
      "    At iteration 7700 -> loss: 0.09266387563699169\n",
      "    At iteration 7800 -> loss: 0.09262279198321345\n",
      "    At iteration 7900 -> loss: 0.0926611079297282\n",
      "    At iteration 8000 -> loss: 0.09263984179160838\n",
      "    At iteration 8100 -> loss: 0.09259583168989448\n",
      "    At iteration 8200 -> loss: 0.09264928734324751\n",
      "    At iteration 8300 -> loss: 0.09298239286750148\n",
      "    At iteration 8400 -> loss: 0.09297291218569852\n",
      "    At iteration 8500 -> loss: 0.0929426358715505\n",
      "    At iteration 8600 -> loss: 0.09292780691480433\n",
      "    At iteration 8700 -> loss: 0.09292966909605281\n",
      "    At iteration 8800 -> loss: 0.09292910774389714\n",
      "    At iteration 8900 -> loss: 0.09294255592599539\n",
      "    At iteration 9000 -> loss: 0.09293750889630915\n",
      "    At iteration 9100 -> loss: 0.09294031377827852\n",
      "    At iteration 9200 -> loss: 0.09289647100658063\n",
      "    At iteration 9300 -> loss: 0.09287036486930876\n",
      "    At iteration 9400 -> loss: 0.09290771285180509\n",
      "    At iteration 9500 -> loss: 0.09289394883367168\n",
      "    At iteration 9600 -> loss: 0.09290991913941783\n",
      "    At iteration 9700 -> loss: 0.09303272577764198\n",
      "    At iteration 9800 -> loss: 0.09310220273516792\n",
      "    At iteration 9900 -> loss: 0.09304670148998298\n",
      "    At iteration 10000 -> loss: 0.09311139213125813\n",
      "    At iteration 10100 -> loss: 0.0930910581431882\n",
      "    At iteration 10200 -> loss: 0.09307119410435441\n",
      "    At iteration 10300 -> loss: 0.09302602650756668\n",
      "    At iteration 10400 -> loss: 0.0930126696176594\n",
      "    At iteration 10500 -> loss: 0.09300848335507368\n",
      "    At iteration 10600 -> loss: 0.09300131037403485\n",
      "    At iteration 10700 -> loss: 0.0929630797008147\n",
      "    At iteration 10800 -> loss: 0.09302276073536198\n",
      "    At iteration 10900 -> loss: 0.09301379482922414\n",
      "    At iteration 11000 -> loss: 0.09308101087377839\n",
      "    At iteration 11100 -> loss: 0.09305980578478047\n",
      "    At iteration 11200 -> loss: 0.09304381324779688\n",
      "    At iteration 11300 -> loss: 0.09307917783333351\n",
      "    At iteration 11400 -> loss: 0.09306646924279582\n",
      "    At iteration 11500 -> loss: 0.09308543810265027\n",
      "    At iteration 11600 -> loss: 0.09309080173766557\n",
      "    At iteration 11700 -> loss: 0.09306474921202629\n",
      "    At iteration 11800 -> loss: 0.09303637442889465\n",
      "    At iteration 11900 -> loss: 0.09302905723478232\n",
      "    At iteration 12000 -> loss: 0.0930219582530665\n",
      "    At iteration 12100 -> loss: 0.09299215939950081\n",
      "    At iteration 12200 -> loss: 0.09302468708530666\n",
      "    At iteration 12300 -> loss: 0.09303223140015399\n",
      "    At iteration 12400 -> loss: 0.09301233372386733\n",
      "    At iteration 12500 -> loss: 0.09300578337840794\n",
      "    At iteration 12600 -> loss: 0.09302583256951671\n",
      "    At iteration 12700 -> loss: 0.09302597123403518\n",
      "    At iteration 12800 -> loss: 0.09299167415410595\n",
      "    At iteration 12900 -> loss: 0.09297332586437124\n",
      "    At iteration 13000 -> loss: 0.09299827383763876\n",
      "    At iteration 13100 -> loss: 0.09303420903531275\n",
      "    At iteration 13200 -> loss: 0.09304800704150525\n",
      "    At iteration 13300 -> loss: 0.09302836746921116\n",
      "    At iteration 13400 -> loss: 0.09302366148696163\n",
      "    At iteration 13500 -> loss: 0.09301845591711559\n",
      "    At iteration 13600 -> loss: 0.09301131187830597\n",
      "Staring Epoch 122\n",
      "    At iteration 0 -> loss: 0.08202393015380949\n",
      "    At iteration 100 -> loss: 0.09004828860386146\n",
      "    At iteration 200 -> loss: 0.09037081884033642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 300 -> loss: 0.09273469653832334\n",
      "    At iteration 400 -> loss: 0.09407179523226068\n",
      "    At iteration 500 -> loss: 0.09317362288210475\n",
      "    At iteration 600 -> loss: 0.09280889739573511\n",
      "    At iteration 700 -> loss: 0.09256294687564047\n",
      "    At iteration 800 -> loss: 0.09257216208523415\n",
      "    At iteration 900 -> loss: 0.09257591020424391\n",
      "    At iteration 1000 -> loss: 0.09254271083082172\n",
      "    At iteration 1100 -> loss: 0.09226512499542497\n",
      "    At iteration 1200 -> loss: 0.09291690797257537\n",
      "    At iteration 1300 -> loss: 0.09292325496071542\n",
      "    At iteration 1400 -> loss: 0.09301526124620944\n",
      "    At iteration 1500 -> loss: 0.0930297739723309\n",
      "    At iteration 1600 -> loss: 0.09283493421569794\n",
      "    At iteration 1700 -> loss: 0.09269938269179649\n",
      "    At iteration 1800 -> loss: 0.09275992057692765\n",
      "    At iteration 1900 -> loss: 0.09272378410803571\n",
      "    At iteration 2000 -> loss: 0.09277488321111554\n",
      "    At iteration 2100 -> loss: 0.09267826955846703\n",
      "    At iteration 2200 -> loss: 0.09276273134946555\n",
      "    At iteration 2300 -> loss: 0.09262647661401821\n",
      "    At iteration 2400 -> loss: 0.09254809725829055\n",
      "    At iteration 2500 -> loss: 0.0924939616694767\n",
      "    At iteration 2600 -> loss: 0.09257276912405929\n",
      "    At iteration 2700 -> loss: 0.09268890750084381\n",
      "    At iteration 2800 -> loss: 0.09285343685186762\n",
      "    At iteration 2900 -> loss: 0.09276133749150207\n",
      "    At iteration 3000 -> loss: 0.09274926656011025\n",
      "    At iteration 3100 -> loss: 0.09267283569115883\n",
      "    At iteration 3200 -> loss: 0.09261859200675375\n",
      "    At iteration 3300 -> loss: 0.09261844293541957\n",
      "    At iteration 3400 -> loss: 0.0925336193877942\n",
      "    At iteration 3500 -> loss: 0.09247625512423412\n",
      "    At iteration 3600 -> loss: 0.09242611475120024\n",
      "    At iteration 3700 -> loss: 0.09237917404002345\n",
      "    At iteration 3800 -> loss: 0.09250704124088456\n",
      "    At iteration 3900 -> loss: 0.09250083616811358\n",
      "    At iteration 4000 -> loss: 0.09242793361386414\n",
      "    At iteration 4100 -> loss: 0.0923797063411682\n",
      "    At iteration 4200 -> loss: 0.09241458241638394\n",
      "    At iteration 4300 -> loss: 0.09244008531790318\n",
      "    At iteration 4400 -> loss: 0.09240557879571157\n",
      "    At iteration 4500 -> loss: 0.09245936206799973\n",
      "    At iteration 4600 -> loss: 0.09260568474796714\n",
      "    At iteration 4700 -> loss: 0.09258354487547597\n",
      "    At iteration 4800 -> loss: 0.0925486622588253\n",
      "    At iteration 4900 -> loss: 0.09252111880115306\n",
      "    At iteration 5000 -> loss: 0.09259723239778207\n",
      "    At iteration 5100 -> loss: 0.09254694227240273\n",
      "    At iteration 5200 -> loss: 0.09256046833030082\n",
      "    At iteration 5300 -> loss: 0.09250591573954563\n",
      "    At iteration 5400 -> loss: 0.09248797830859064\n",
      "    At iteration 5500 -> loss: 0.0924627868508921\n",
      "    At iteration 5600 -> loss: 0.09244285744162958\n",
      "    At iteration 5700 -> loss: 0.09242535263424018\n",
      "    At iteration 5800 -> loss: 0.09238396866962252\n",
      "    At iteration 5900 -> loss: 0.09234365312830602\n",
      "    At iteration 6000 -> loss: 0.09235033643522483\n",
      "    At iteration 6100 -> loss: 0.09232598373853915\n",
      "    At iteration 6200 -> loss: 0.09247829237677174\n",
      "    At iteration 6300 -> loss: 0.09242648306730475\n",
      "    At iteration 6400 -> loss: 0.09245145025427187\n",
      "    At iteration 6500 -> loss: 0.09243932651149202\n",
      "    At iteration 6600 -> loss: 0.0924142865707013\n",
      "    At iteration 6700 -> loss: 0.09239588376679936\n",
      "    At iteration 6800 -> loss: 0.09243016662239134\n",
      "    At iteration 6900 -> loss: 0.09241237224132097\n",
      "    At iteration 7000 -> loss: 0.09236406998709179\n",
      "    At iteration 7100 -> loss: 0.09236448513767938\n",
      "    At iteration 7200 -> loss: 0.09234017054634358\n",
      "    At iteration 7300 -> loss: 0.0924657890131383\n",
      "    At iteration 7400 -> loss: 0.09247114074758826\n",
      "    At iteration 7500 -> loss: 0.09252367937443774\n",
      "    At iteration 7600 -> loss: 0.09293902630390637\n",
      "    At iteration 7700 -> loss: 0.09293892413882004\n",
      "    At iteration 7800 -> loss: 0.09293336915005956\n",
      "    At iteration 7900 -> loss: 0.0929114804704106\n",
      "    At iteration 8000 -> loss: 0.09287517381847796\n",
      "    At iteration 8100 -> loss: 0.09286799218745893\n",
      "    At iteration 8200 -> loss: 0.09292690248373527\n",
      "    At iteration 8300 -> loss: 0.0929188480450359\n",
      "    At iteration 8400 -> loss: 0.09290408791189664\n",
      "    At iteration 8500 -> loss: 0.09287793637039371\n",
      "    At iteration 8600 -> loss: 0.09289750412267915\n",
      "    At iteration 8700 -> loss: 0.09287754067272225\n",
      "    At iteration 8800 -> loss: 0.09293155600502968\n",
      "    At iteration 8900 -> loss: 0.09289180021521118\n",
      "    At iteration 9000 -> loss: 0.09288214719677053\n",
      "    At iteration 9100 -> loss: 0.09283917859446361\n",
      "    At iteration 9200 -> loss: 0.09281687055906174\n",
      "    At iteration 9300 -> loss: 0.09284739174639704\n",
      "    At iteration 9400 -> loss: 0.09282299147063673\n",
      "    At iteration 9500 -> loss: 0.09281542718584844\n",
      "    At iteration 9600 -> loss: 0.09280897148792933\n",
      "    At iteration 9700 -> loss: 0.09277355006595653\n",
      "    At iteration 9800 -> loss: 0.09274563038677745\n",
      "    At iteration 9900 -> loss: 0.09272336676187164\n",
      "    At iteration 10000 -> loss: 0.09272287745420178\n",
      "    At iteration 10100 -> loss: 0.09271186549278376\n",
      "    At iteration 10200 -> loss: 0.0927575657979369\n",
      "    At iteration 10300 -> loss: 0.09275372842401401\n",
      "    At iteration 10400 -> loss: 0.09276338501683797\n",
      "    At iteration 10500 -> loss: 0.09277370844388166\n",
      "    At iteration 10600 -> loss: 0.0928640242105893\n",
      "    At iteration 10700 -> loss: 0.09287823600550742\n",
      "    At iteration 10800 -> loss: 0.0928795717994566\n",
      "    At iteration 10900 -> loss: 0.09294432537103811\n",
      "    At iteration 11000 -> loss: 0.09294878499088148\n",
      "    At iteration 11100 -> loss: 0.09297652107930118\n",
      "    At iteration 11200 -> loss: 0.0929500529941986\n",
      "    At iteration 11300 -> loss: 0.09292624483702443\n",
      "    At iteration 11400 -> loss: 0.09290077147440402\n",
      "    At iteration 11500 -> loss: 0.09288334979438623\n",
      "    At iteration 11600 -> loss: 0.09287102442069718\n",
      "    At iteration 11700 -> loss: 0.0928668871132258\n",
      "    At iteration 11800 -> loss: 0.0928460164109442\n",
      "    At iteration 11900 -> loss: 0.09289650821524831\n",
      "    At iteration 12000 -> loss: 0.09288754088067194\n",
      "    At iteration 12100 -> loss: 0.0928698923179127\n",
      "    At iteration 12200 -> loss: 0.09285662399448082\n",
      "    At iteration 12300 -> loss: 0.09282860013818829\n",
      "    At iteration 12400 -> loss: 0.09286638851345973\n",
      "    At iteration 12500 -> loss: 0.0929307393960659\n",
      "    At iteration 12600 -> loss: 0.09290936305681916\n",
      "    At iteration 12700 -> loss: 0.09289483962575305\n",
      "    At iteration 12800 -> loss: 0.09286214626375612\n",
      "    At iteration 12900 -> loss: 0.09287053180609439\n",
      "    At iteration 13000 -> loss: 0.09290154151350573\n",
      "    At iteration 13100 -> loss: 0.09294948869416858\n",
      "    At iteration 13200 -> loss: 0.09296393762779119\n",
      "    At iteration 13300 -> loss: 0.09300363378040839\n",
      "    At iteration 13400 -> loss: 0.09299740850151637\n",
      "    At iteration 13500 -> loss: 0.0929680262960393\n",
      "    At iteration 13600 -> loss: 0.09303734078391677\n",
      "Staring Epoch 123\n",
      "    At iteration 0 -> loss: 0.08061829919461161\n",
      "    At iteration 100 -> loss: 0.0918010733687034\n",
      "    At iteration 200 -> loss: 0.09057753906149867\n",
      "    At iteration 300 -> loss: 0.09101983059617784\n",
      "    At iteration 400 -> loss: 0.09069291123728754\n",
      "    At iteration 500 -> loss: 0.09086207750984525\n",
      "    At iteration 600 -> loss: 0.09087711414877463\n",
      "    At iteration 700 -> loss: 0.09099247635828127\n",
      "    At iteration 800 -> loss: 0.0912371265909057\n",
      "    At iteration 900 -> loss: 0.0919631434902165\n",
      "    At iteration 1000 -> loss: 0.09171331428372921\n",
      "    At iteration 1100 -> loss: 0.09167089995252117\n",
      "    At iteration 1200 -> loss: 0.09191670062495731\n",
      "    At iteration 1300 -> loss: 0.09162301091442993\n",
      "    At iteration 1400 -> loss: 0.09165503197345407\n",
      "    At iteration 1500 -> loss: 0.09176185845378998\n",
      "    At iteration 1600 -> loss: 0.09187056946048294\n",
      "    At iteration 1700 -> loss: 0.09196088415801704\n",
      "    At iteration 1800 -> loss: 0.09224260048440554\n",
      "    At iteration 1900 -> loss: 0.09224658115526167\n",
      "    At iteration 2000 -> loss: 0.09247062238187088\n",
      "    At iteration 2100 -> loss: 0.09233666013413418\n",
      "    At iteration 2200 -> loss: 0.09217228798389784\n",
      "    At iteration 2300 -> loss: 0.09206437661100961\n",
      "    At iteration 2400 -> loss: 0.09203314667464459\n",
      "    At iteration 2500 -> loss: 0.09194398681896356\n",
      "    At iteration 2600 -> loss: 0.09183073396587502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 2700 -> loss: 0.09201287146131651\n",
      "    At iteration 2800 -> loss: 0.09208104992568983\n",
      "    At iteration 2900 -> loss: 0.09208639601471513\n",
      "    At iteration 3000 -> loss: 0.09223290580386527\n",
      "    At iteration 3100 -> loss: 0.09214951744718422\n",
      "    At iteration 3200 -> loss: 0.09218275790340824\n",
      "    At iteration 3300 -> loss: 0.09215491001530444\n",
      "    At iteration 3400 -> loss: 0.09220964510501221\n",
      "    At iteration 3500 -> loss: 0.09249559704124075\n",
      "    At iteration 3600 -> loss: 0.0925217676926124\n",
      "    At iteration 3700 -> loss: 0.09252403904694374\n",
      "    At iteration 3800 -> loss: 0.09251556005387276\n",
      "    At iteration 3900 -> loss: 0.09258208017757351\n",
      "    At iteration 4000 -> loss: 0.0925796492786393\n",
      "    At iteration 4100 -> loss: 0.09257096749290482\n",
      "    At iteration 4200 -> loss: 0.09271380191912612\n",
      "    At iteration 4300 -> loss: 0.09263330108546722\n",
      "    At iteration 4400 -> loss: 0.09257269969772482\n",
      "    At iteration 4500 -> loss: 0.09269550269810353\n",
      "    At iteration 4600 -> loss: 0.09268145912349635\n",
      "    At iteration 4700 -> loss: 0.0927605814751607\n",
      "    At iteration 4800 -> loss: 0.09286532460477356\n",
      "    At iteration 4900 -> loss: 0.0930755698486623\n",
      "    At iteration 5000 -> loss: 0.09302507920320059\n",
      "    At iteration 5100 -> loss: 0.09304320315830872\n",
      "    At iteration 5200 -> loss: 0.09306591468339996\n",
      "    At iteration 5300 -> loss: 0.09300818402238334\n",
      "    At iteration 5400 -> loss: 0.09298694244329866\n",
      "    At iteration 5500 -> loss: 0.09291798009166714\n",
      "    At iteration 5600 -> loss: 0.09287800793417975\n",
      "    At iteration 5700 -> loss: 0.09280047443863908\n",
      "    At iteration 5800 -> loss: 0.09283479443555935\n",
      "    At iteration 5900 -> loss: 0.09290203920126082\n",
      "    At iteration 6000 -> loss: 0.09288648365365548\n",
      "    At iteration 6100 -> loss: 0.09287942267944328\n",
      "    At iteration 6200 -> loss: 0.09292485143460263\n",
      "    At iteration 6300 -> loss: 0.0929041258950343\n",
      "    At iteration 6400 -> loss: 0.09293218225076887\n",
      "    At iteration 6500 -> loss: 0.09289206021421935\n",
      "    At iteration 6600 -> loss: 0.09309024226475181\n",
      "    At iteration 6700 -> loss: 0.09304481272171276\n",
      "    At iteration 6800 -> loss: 0.09299270923848003\n",
      "    At iteration 6900 -> loss: 0.09297488440781802\n",
      "    At iteration 7000 -> loss: 0.09293164387609669\n",
      "    At iteration 7100 -> loss: 0.09290398509690655\n",
      "    At iteration 7200 -> loss: 0.09286935540144042\n",
      "    At iteration 7300 -> loss: 0.09285194343582144\n",
      "    At iteration 7400 -> loss: 0.09291369135241606\n",
      "    At iteration 7500 -> loss: 0.0929096174598105\n",
      "    At iteration 7600 -> loss: 0.09286465093355172\n",
      "    At iteration 7700 -> loss: 0.09285459768044463\n",
      "    At iteration 7800 -> loss: 0.09288447934542728\n",
      "    At iteration 7900 -> loss: 0.09295778236429478\n",
      "    At iteration 8000 -> loss: 0.0929726895627398\n",
      "    At iteration 8100 -> loss: 0.09294371738814447\n",
      "    At iteration 8200 -> loss: 0.09304142358938502\n",
      "    At iteration 8300 -> loss: 0.09311955720236044\n",
      "    At iteration 8400 -> loss: 0.0931311871967482\n",
      "    At iteration 8500 -> loss: 0.09306213638748903\n",
      "    At iteration 8600 -> loss: 0.09302245967775835\n",
      "    At iteration 8700 -> loss: 0.09301514912077564\n",
      "    At iteration 8800 -> loss: 0.09301665523922072\n",
      "    At iteration 8900 -> loss: 0.09303419302838475\n",
      "    At iteration 9000 -> loss: 0.09302785173981919\n",
      "    At iteration 9100 -> loss: 0.0930009165012951\n",
      "    At iteration 9200 -> loss: 0.0929757972098293\n",
      "    At iteration 9300 -> loss: 0.09296102948948283\n",
      "    At iteration 9400 -> loss: 0.09294113645142628\n",
      "    At iteration 9500 -> loss: 0.09289038491733465\n",
      "    At iteration 9600 -> loss: 0.09284372856969443\n",
      "    At iteration 9700 -> loss: 0.09287574355456905\n",
      "    At iteration 9800 -> loss: 0.09285255004213329\n",
      "    At iteration 9900 -> loss: 0.09284933882284564\n",
      "    At iteration 10000 -> loss: 0.0928363129299111\n",
      "    At iteration 10100 -> loss: 0.09281859185587911\n",
      "    At iteration 10200 -> loss: 0.0928098299463036\n",
      "    At iteration 10300 -> loss: 0.09280612233333126\n",
      "    At iteration 10400 -> loss: 0.09280845437453951\n",
      "    At iteration 10500 -> loss: 0.09309914913613765\n",
      "    At iteration 10600 -> loss: 0.09308378700277407\n",
      "    At iteration 10700 -> loss: 0.09307588563880814\n",
      "    At iteration 10800 -> loss: 0.09320994664633123\n",
      "    At iteration 10900 -> loss: 0.09327274141322185\n",
      "    At iteration 11000 -> loss: 0.09324304490413422\n",
      "    At iteration 11100 -> loss: 0.09323041627729323\n",
      "    At iteration 11200 -> loss: 0.09319472826214822\n",
      "    At iteration 11300 -> loss: 0.09317793580690412\n",
      "    At iteration 11400 -> loss: 0.09316541330454499\n",
      "    At iteration 11500 -> loss: 0.0931280789063249\n",
      "    At iteration 11600 -> loss: 0.09316546558797477\n",
      "    At iteration 11700 -> loss: 0.09318863731458145\n",
      "    At iteration 11800 -> loss: 0.09316702520657333\n",
      "    At iteration 11900 -> loss: 0.09318222385360644\n",
      "    At iteration 12000 -> loss: 0.0931515497313649\n",
      "    At iteration 12100 -> loss: 0.09315653376364808\n",
      "    At iteration 12200 -> loss: 0.0931674043213617\n",
      "    At iteration 12300 -> loss: 0.09314502168628346\n",
      "    At iteration 12400 -> loss: 0.09312998039022173\n",
      "    At iteration 12500 -> loss: 0.0931272331016017\n",
      "    At iteration 12600 -> loss: 0.09311225552376583\n",
      "    At iteration 12700 -> loss: 0.09311290646184203\n",
      "    At iteration 12800 -> loss: 0.0931028776430458\n",
      "    At iteration 12900 -> loss: 0.09308727328948375\n",
      "    At iteration 13000 -> loss: 0.09307504879447616\n",
      "    At iteration 13100 -> loss: 0.09306539378954658\n",
      "    At iteration 13200 -> loss: 0.09304152292317189\n",
      "    At iteration 13300 -> loss: 0.09299448343682393\n",
      "    At iteration 13400 -> loss: 0.09299282053293673\n",
      "    At iteration 13500 -> loss: 0.09300255995802405\n",
      "    At iteration 13600 -> loss: 0.09302394953698018\n",
      "Staring Epoch 124\n",
      "    At iteration 0 -> loss: 0.09018495422787964\n",
      "    At iteration 100 -> loss: 0.09017235498653788\n",
      "    At iteration 200 -> loss: 0.08908602420353647\n",
      "    At iteration 300 -> loss: 0.08994536099550099\n",
      "    At iteration 400 -> loss: 0.0905920560656747\n",
      "    At iteration 500 -> loss: 0.09031411803257723\n",
      "    At iteration 600 -> loss: 0.09172451286613298\n",
      "    At iteration 700 -> loss: 0.09165296533084498\n",
      "    At iteration 800 -> loss: 0.09172511955013295\n",
      "    At iteration 900 -> loss: 0.09208423494219801\n",
      "    At iteration 1000 -> loss: 0.09260541235356906\n",
      "    At iteration 1100 -> loss: 0.09258624878851629\n",
      "    At iteration 1200 -> loss: 0.09241151474669207\n",
      "    At iteration 1300 -> loss: 0.09210808382818236\n",
      "    At iteration 1400 -> loss: 0.092076161104507\n",
      "    At iteration 1500 -> loss: 0.0919313067173343\n",
      "    At iteration 1600 -> loss: 0.09218475872925656\n",
      "    At iteration 1700 -> loss: 0.09254258673093799\n",
      "    At iteration 1800 -> loss: 0.0925326443555736\n",
      "    At iteration 1900 -> loss: 0.09246201692427207\n",
      "    At iteration 2000 -> loss: 0.09294601848732516\n",
      "    At iteration 2100 -> loss: 0.09304080186712428\n",
      "    At iteration 2200 -> loss: 0.09311223903562905\n",
      "    At iteration 2300 -> loss: 0.09314578181952697\n",
      "    At iteration 2400 -> loss: 0.09299498220700221\n",
      "    At iteration 2500 -> loss: 0.09341342703934206\n",
      "    At iteration 2600 -> loss: 0.09337970400313954\n",
      "    At iteration 2700 -> loss: 0.09343138500576777\n",
      "    At iteration 2800 -> loss: 0.09335040088416652\n",
      "    At iteration 2900 -> loss: 0.09346263590334297\n",
      "    At iteration 3000 -> loss: 0.09345694008471277\n",
      "    At iteration 3100 -> loss: 0.09333303529708709\n",
      "    At iteration 3200 -> loss: 0.09325444659390754\n",
      "    At iteration 3300 -> loss: 0.09327463966162246\n",
      "    At iteration 3400 -> loss: 0.09326716426463064\n",
      "    At iteration 3500 -> loss: 0.0933774642295884\n",
      "    At iteration 3600 -> loss: 0.09326668521161133\n",
      "    At iteration 3700 -> loss: 0.09319992047683329\n",
      "    At iteration 3800 -> loss: 0.0932515178184273\n",
      "    At iteration 3900 -> loss: 0.09317560107927209\n",
      "    At iteration 4000 -> loss: 0.0930849793913213\n",
      "    At iteration 4100 -> loss: 0.09317100107668744\n",
      "    At iteration 4200 -> loss: 0.09311729655946831\n",
      "    At iteration 4300 -> loss: 0.0930542712820366\n",
      "    At iteration 4400 -> loss: 0.0929972999625642\n",
      "    At iteration 4500 -> loss: 0.09293121540172523\n",
      "    At iteration 4600 -> loss: 0.09285473406194336\n",
      "    At iteration 4700 -> loss: 0.09280421675226605\n",
      "    At iteration 4800 -> loss: 0.0927584173996252\n",
      "    At iteration 4900 -> loss: 0.09276648896869708\n",
      "    At iteration 5000 -> loss: 0.09270867503464628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5100 -> loss: 0.09270332141393232\n",
      "    At iteration 5200 -> loss: 0.09262531882078964\n",
      "    At iteration 5300 -> loss: 0.09269422135521328\n",
      "    At iteration 5400 -> loss: 0.0926725763771913\n",
      "    At iteration 5500 -> loss: 0.09264394107200231\n",
      "    At iteration 5600 -> loss: 0.09266888871866767\n",
      "    At iteration 5700 -> loss: 0.09262173879634873\n",
      "    At iteration 5800 -> loss: 0.09268869190059037\n",
      "    At iteration 5900 -> loss: 0.09267052434621864\n",
      "    At iteration 6000 -> loss: 0.09263719823106326\n",
      "    At iteration 6100 -> loss: 0.09264057145984418\n",
      "    At iteration 6200 -> loss: 0.09272992756685407\n",
      "    At iteration 6300 -> loss: 0.09266506459283344\n",
      "    At iteration 6400 -> loss: 0.09261089434251177\n",
      "    At iteration 6500 -> loss: 0.09311779126023072\n",
      "    At iteration 6600 -> loss: 0.09308242871936251\n",
      "    At iteration 6700 -> loss: 0.093071878040177\n",
      "    At iteration 6800 -> loss: 0.09311237404399167\n",
      "    At iteration 6900 -> loss: 0.09313845712138852\n",
      "    At iteration 7000 -> loss: 0.0931221527596526\n",
      "    At iteration 7100 -> loss: 0.09310946678403588\n",
      "    At iteration 7200 -> loss: 0.093089375513248\n",
      "    At iteration 7300 -> loss: 0.09310815859237248\n",
      "    At iteration 7400 -> loss: 0.09314469927226732\n",
      "    At iteration 7500 -> loss: 0.09307880934816323\n",
      "    At iteration 7600 -> loss: 0.09305625824717073\n",
      "    At iteration 7700 -> loss: 0.09303310755744063\n",
      "    At iteration 7800 -> loss: 0.09302443599759556\n",
      "    At iteration 7900 -> loss: 0.09301719353814107\n",
      "    At iteration 8000 -> loss: 0.0929757160189524\n",
      "    At iteration 8100 -> loss: 0.09296462176900222\n",
      "    At iteration 8200 -> loss: 0.09295109077952624\n",
      "    At iteration 8300 -> loss: 0.09290093811554825\n",
      "    At iteration 8400 -> loss: 0.09300216527368448\n",
      "    At iteration 8500 -> loss: 0.0929959747685714\n",
      "    At iteration 8600 -> loss: 0.09296075613674361\n",
      "    At iteration 8700 -> loss: 0.09293621109443945\n",
      "    At iteration 8800 -> loss: 0.09297140410132475\n",
      "    At iteration 8900 -> loss: 0.0929503925791135\n",
      "    At iteration 9000 -> loss: 0.09293345584943591\n",
      "    At iteration 9100 -> loss: 0.09298098233225127\n",
      "    At iteration 9200 -> loss: 0.09299059412519221\n",
      "    At iteration 9300 -> loss: 0.09302858508541606\n",
      "    At iteration 9400 -> loss: 0.09297641466592402\n",
      "    At iteration 9500 -> loss: 0.09297654332171924\n",
      "    At iteration 9600 -> loss: 0.09294987738075891\n",
      "    At iteration 9700 -> loss: 0.09295313832983974\n",
      "    At iteration 9800 -> loss: 0.09295655230401725\n",
      "    At iteration 9900 -> loss: 0.09298009617332018\n",
      "    At iteration 10000 -> loss: 0.09294755237656163\n",
      "    At iteration 10100 -> loss: 0.09290789487009554\n",
      "    At iteration 10200 -> loss: 0.09287358613692602\n",
      "    At iteration 10300 -> loss: 0.09287373026338691\n",
      "    At iteration 10400 -> loss: 0.09287876219398501\n",
      "    At iteration 10500 -> loss: 0.0929236818740878\n",
      "    At iteration 10600 -> loss: 0.09289970323643863\n",
      "    At iteration 10700 -> loss: 0.0928600246534838\n",
      "    At iteration 10800 -> loss: 0.09292258449213048\n",
      "    At iteration 10900 -> loss: 0.09289118130411769\n",
      "    At iteration 11000 -> loss: 0.09288704024953344\n",
      "    At iteration 11100 -> loss: 0.09284731502494326\n",
      "    At iteration 11200 -> loss: 0.0928602920513853\n",
      "    At iteration 11300 -> loss: 0.09293225762642429\n",
      "    At iteration 11400 -> loss: 0.09289474125006791\n",
      "    At iteration 11500 -> loss: 0.09298087655955738\n",
      "    At iteration 11600 -> loss: 0.09296927022508694\n",
      "    At iteration 11700 -> loss: 0.09296929351497539\n",
      "    At iteration 11800 -> loss: 0.09293556006822798\n",
      "    At iteration 11900 -> loss: 0.0929281108112766\n",
      "    At iteration 12000 -> loss: 0.09292504779969148\n",
      "    At iteration 12100 -> loss: 0.0929537399465274\n",
      "    At iteration 12200 -> loss: 0.09295043734328205\n",
      "    At iteration 12300 -> loss: 0.09296854499249342\n",
      "    At iteration 12400 -> loss: 0.09296466994348732\n",
      "    At iteration 12500 -> loss: 0.09294362608189345\n",
      "    At iteration 12600 -> loss: 0.09291856670383576\n",
      "    At iteration 12700 -> loss: 0.09290277279800195\n",
      "    At iteration 12800 -> loss: 0.09290729593276527\n",
      "    At iteration 12900 -> loss: 0.09291897577171465\n",
      "    At iteration 13000 -> loss: 0.09303652699508805\n",
      "    At iteration 13100 -> loss: 0.09305580889828331\n",
      "    At iteration 13200 -> loss: 0.09306637620256061\n",
      "    At iteration 13300 -> loss: 0.09304886624665286\n",
      "    At iteration 13400 -> loss: 0.09302919046130347\n",
      "    At iteration 13500 -> loss: 0.09302163481275584\n",
      "    At iteration 13600 -> loss: 0.09302958144904173\n",
      "Staring Epoch 125\n",
      "    At iteration 0 -> loss: 0.08029202111720224\n",
      "    At iteration 100 -> loss: 0.09359648287388002\n",
      "    At iteration 200 -> loss: 0.09202020937868081\n",
      "    At iteration 300 -> loss: 0.09141685537118063\n",
      "    At iteration 400 -> loss: 0.0919214854957228\n",
      "    At iteration 500 -> loss: 0.09165985840396823\n",
      "    At iteration 600 -> loss: 0.09159362993220767\n",
      "    At iteration 700 -> loss: 0.09167326909747549\n",
      "    At iteration 800 -> loss: 0.09172025641929937\n",
      "    At iteration 900 -> loss: 0.09153075240104483\n",
      "    At iteration 1000 -> loss: 0.09173934943985042\n",
      "    At iteration 1100 -> loss: 0.09218100673638854\n",
      "    At iteration 1200 -> loss: 0.09264675053107893\n",
      "    At iteration 1300 -> loss: 0.09270929232419449\n",
      "    At iteration 1400 -> loss: 0.0924606063441965\n",
      "    At iteration 1500 -> loss: 0.09245839893354542\n",
      "    At iteration 1600 -> loss: 0.09251498369867164\n",
      "    At iteration 1700 -> loss: 0.09278186889110125\n",
      "    At iteration 1800 -> loss: 0.09291198687238084\n",
      "    At iteration 1900 -> loss: 0.09297170112330817\n",
      "    At iteration 2000 -> loss: 0.09278157386071856\n",
      "    At iteration 2100 -> loss: 0.09265915432482116\n",
      "    At iteration 2200 -> loss: 0.09257962116692542\n",
      "    At iteration 2300 -> loss: 0.0928830453951994\n",
      "    At iteration 2400 -> loss: 0.09287279409640208\n",
      "    At iteration 2500 -> loss: 0.09279287111723182\n",
      "    At iteration 2600 -> loss: 0.09287399405131322\n",
      "    At iteration 2700 -> loss: 0.0929697679720912\n",
      "    At iteration 2800 -> loss: 0.09281883193320492\n",
      "    At iteration 2900 -> loss: 0.09284815721235269\n",
      "    At iteration 3000 -> loss: 0.09292085093900115\n",
      "    At iteration 3100 -> loss: 0.09296314566266331\n",
      "    At iteration 3200 -> loss: 0.09300875249654246\n",
      "    At iteration 3300 -> loss: 0.09288462797227008\n",
      "    At iteration 3400 -> loss: 0.09289574532286546\n",
      "    At iteration 3500 -> loss: 0.09280097594734237\n",
      "    At iteration 3600 -> loss: 0.09281696661422534\n",
      "    At iteration 3700 -> loss: 0.09295125767952488\n",
      "    At iteration 3800 -> loss: 0.09302245904465738\n",
      "    At iteration 3900 -> loss: 0.09291215205263983\n",
      "    At iteration 4000 -> loss: 0.09284252408315755\n",
      "    At iteration 4100 -> loss: 0.09275739276595868\n",
      "    At iteration 4200 -> loss: 0.09275830200357228\n",
      "    At iteration 4300 -> loss: 0.09292458087680898\n",
      "    At iteration 4400 -> loss: 0.0935994591123733\n",
      "    At iteration 4500 -> loss: 0.09355001659170574\n",
      "    At iteration 4600 -> loss: 0.09345684407032151\n",
      "    At iteration 4700 -> loss: 0.09339328955500464\n",
      "    At iteration 4800 -> loss: 0.09336810635342929\n",
      "    At iteration 4900 -> loss: 0.09340266423084323\n",
      "    At iteration 5000 -> loss: 0.0933611923226716\n",
      "    At iteration 5100 -> loss: 0.0933207389417743\n",
      "    At iteration 5200 -> loss: 0.09330459100559699\n",
      "    At iteration 5300 -> loss: 0.09324212968987836\n",
      "    At iteration 5400 -> loss: 0.09317577409772194\n",
      "    At iteration 5500 -> loss: 0.09310844332461791\n",
      "    At iteration 5600 -> loss: 0.09317631129619554\n",
      "    At iteration 5700 -> loss: 0.09335324631489532\n",
      "    At iteration 5800 -> loss: 0.0934252208366207\n",
      "    At iteration 5900 -> loss: 0.09349410122470926\n",
      "    At iteration 6000 -> loss: 0.09352119974065853\n",
      "    At iteration 6100 -> loss: 0.09347583389483041\n",
      "    At iteration 6200 -> loss: 0.09352527818792651\n",
      "    At iteration 6300 -> loss: 0.09357089275691524\n",
      "    At iteration 6400 -> loss: 0.0935433375641558\n",
      "    At iteration 6500 -> loss: 0.09348290861117724\n",
      "    At iteration 6600 -> loss: 0.0934335924355747\n",
      "    At iteration 6700 -> loss: 0.09338783342025407\n",
      "    At iteration 6800 -> loss: 0.09341237396234459\n",
      "    At iteration 6900 -> loss: 0.09337588494799835\n",
      "    At iteration 7000 -> loss: 0.09333270051471694\n",
      "    At iteration 7100 -> loss: 0.0932921385880178\n",
      "    At iteration 7200 -> loss: 0.09326945349178824\n",
      "    At iteration 7300 -> loss: 0.09338136898719074\n",
      "    At iteration 7400 -> loss: 0.09335656876851721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 7500 -> loss: 0.09330027404273798\n",
      "    At iteration 7600 -> loss: 0.09325473242659624\n",
      "    At iteration 7700 -> loss: 0.09324857250543342\n",
      "    At iteration 7800 -> loss: 0.09320063320814083\n",
      "    At iteration 7900 -> loss: 0.09316604244340693\n",
      "    At iteration 8000 -> loss: 0.09316363870794474\n",
      "    At iteration 8100 -> loss: 0.09315967418225189\n",
      "    At iteration 8200 -> loss: 0.09319377581117351\n",
      "    At iteration 8300 -> loss: 0.09316828370861661\n",
      "    At iteration 8400 -> loss: 0.09319709439645404\n",
      "    At iteration 8500 -> loss: 0.0931977346455973\n",
      "    At iteration 8600 -> loss: 0.09319159365237753\n",
      "    At iteration 8700 -> loss: 0.0931918051297624\n",
      "    At iteration 8800 -> loss: 0.09313448949945764\n",
      "    At iteration 8900 -> loss: 0.0931091942788527\n",
      "    At iteration 9000 -> loss: 0.09309053894088774\n",
      "    At iteration 9100 -> loss: 0.0930724151707158\n",
      "    At iteration 9200 -> loss: 0.09307931039420286\n",
      "    At iteration 9300 -> loss: 0.09306543392505161\n",
      "    At iteration 9400 -> loss: 0.09317942951290548\n",
      "    At iteration 9500 -> loss: 0.09315998828861848\n",
      "    At iteration 9600 -> loss: 0.09310426067701683\n",
      "    At iteration 9700 -> loss: 0.09311276239727086\n",
      "    At iteration 9800 -> loss: 0.09312048177191579\n",
      "    At iteration 9900 -> loss: 0.09312466885537246\n",
      "    At iteration 10000 -> loss: 0.09308529582339894\n",
      "    At iteration 10100 -> loss: 0.09309984435748508\n",
      "    At iteration 10200 -> loss: 0.09304969200753095\n",
      "    At iteration 10300 -> loss: 0.09307969572795342\n",
      "    At iteration 10400 -> loss: 0.09309200155036658\n",
      "    At iteration 10500 -> loss: 0.0931131879991809\n",
      "    At iteration 10600 -> loss: 0.09320245670881033\n",
      "    At iteration 10700 -> loss: 0.09319273933297044\n",
      "    At iteration 10800 -> loss: 0.09316186362728285\n",
      "    At iteration 10900 -> loss: 0.09315206222479569\n",
      "    At iteration 11000 -> loss: 0.09314937433607438\n",
      "    At iteration 11100 -> loss: 0.09311218886009504\n",
      "    At iteration 11200 -> loss: 0.09309910106189444\n",
      "    At iteration 11300 -> loss: 0.093109171374118\n",
      "    At iteration 11400 -> loss: 0.09311388808184479\n",
      "    At iteration 11500 -> loss: 0.09310779898332404\n",
      "    At iteration 11600 -> loss: 0.09311917218532763\n",
      "    At iteration 11700 -> loss: 0.09312489745919368\n",
      "    At iteration 11800 -> loss: 0.0931108068396521\n",
      "    At iteration 11900 -> loss: 0.09310210448418536\n",
      "    At iteration 12000 -> loss: 0.09308465615221911\n",
      "    At iteration 12100 -> loss: 0.09312568211132081\n",
      "    At iteration 12200 -> loss: 0.09309725599624943\n",
      "    At iteration 12300 -> loss: 0.09307948129290874\n",
      "    At iteration 12400 -> loss: 0.093067790749242\n",
      "    At iteration 12500 -> loss: 0.09304329125994297\n",
      "    At iteration 12600 -> loss: 0.09303127710824619\n",
      "    At iteration 12700 -> loss: 0.09301851634101119\n",
      "    At iteration 12800 -> loss: 0.09304700269613782\n",
      "    At iteration 12900 -> loss: 0.09303762151586388\n",
      "    At iteration 13000 -> loss: 0.09305246115479483\n",
      "    At iteration 13100 -> loss: 0.09304238215881347\n",
      "    At iteration 13200 -> loss: 0.09301700044674313\n",
      "    At iteration 13300 -> loss: 0.0930101267286858\n",
      "    At iteration 13400 -> loss: 0.0930098219348502\n",
      "    At iteration 13500 -> loss: 0.0930103082819313\n",
      "    At iteration 13600 -> loss: 0.09301696794182086\n",
      "Staring Epoch 126\n",
      "    At iteration 0 -> loss: 0.08166155830258504\n",
      "    At iteration 100 -> loss: 0.09355296496995018\n",
      "    At iteration 200 -> loss: 0.09278259389852717\n",
      "    At iteration 300 -> loss: 0.09304921196740076\n",
      "    At iteration 400 -> loss: 0.09345789906172057\n",
      "    At iteration 500 -> loss: 0.09324510276780303\n",
      "    At iteration 600 -> loss: 0.09361283409971906\n",
      "    At iteration 700 -> loss: 0.09380180234737959\n",
      "    At iteration 800 -> loss: 0.09341629889921838\n",
      "    At iteration 900 -> loss: 0.09292124818447828\n",
      "    At iteration 1000 -> loss: 0.09253553098309517\n",
      "    At iteration 1100 -> loss: 0.09235868505408319\n",
      "    At iteration 1200 -> loss: 0.09256581569155595\n",
      "    At iteration 1300 -> loss: 0.09249053070376505\n",
      "    At iteration 1400 -> loss: 0.09250357863611348\n",
      "    At iteration 1500 -> loss: 0.09253504583277702\n",
      "    At iteration 1600 -> loss: 0.0931868894100919\n",
      "    At iteration 1700 -> loss: 0.09304257786839033\n",
      "    At iteration 1800 -> loss: 0.09308143049828912\n",
      "    At iteration 1900 -> loss: 0.09301172829268715\n",
      "    At iteration 2000 -> loss: 0.09321880320450687\n",
      "    At iteration 2100 -> loss: 0.09324382644333558\n",
      "    At iteration 2200 -> loss: 0.09374210385293545\n",
      "    At iteration 2300 -> loss: 0.09359950983085663\n",
      "    At iteration 2400 -> loss: 0.09414184012955813\n",
      "    At iteration 2500 -> loss: 0.09400426150468265\n",
      "    At iteration 2600 -> loss: 0.09401821674759626\n",
      "    At iteration 2700 -> loss: 0.09399186940122602\n",
      "    At iteration 2800 -> loss: 0.09383839308808285\n",
      "    At iteration 2900 -> loss: 0.09373973652275479\n",
      "    At iteration 3000 -> loss: 0.09380380888890874\n",
      "    At iteration 3100 -> loss: 0.09369573198833352\n",
      "    At iteration 3200 -> loss: 0.09381315815655734\n",
      "    At iteration 3300 -> loss: 0.09364839918035776\n",
      "    At iteration 3400 -> loss: 0.09351124397405912\n",
      "    At iteration 3500 -> loss: 0.09344538839030925\n",
      "    At iteration 3600 -> loss: 0.09335145931041708\n",
      "    At iteration 3700 -> loss: 0.09356341318345081\n",
      "    At iteration 3800 -> loss: 0.09358812394601605\n",
      "    At iteration 3900 -> loss: 0.09346104243416112\n",
      "    At iteration 4000 -> loss: 0.09336345311133681\n",
      "    At iteration 4100 -> loss: 0.0933784476951863\n",
      "    At iteration 4200 -> loss: 0.0933602192751617\n",
      "    At iteration 4300 -> loss: 0.09331381295818562\n",
      "    At iteration 4400 -> loss: 0.09328139152288704\n",
      "    At iteration 4500 -> loss: 0.09318385752889453\n",
      "    At iteration 4600 -> loss: 0.09311372494345625\n",
      "    At iteration 4700 -> loss: 0.09310186995149379\n",
      "    At iteration 4800 -> loss: 0.0931021055115728\n",
      "    At iteration 4900 -> loss: 0.0931425421445288\n",
      "    At iteration 5000 -> loss: 0.09309687532965857\n",
      "    At iteration 5100 -> loss: 0.09309214669027928\n",
      "    At iteration 5200 -> loss: 0.092995433142485\n",
      "    At iteration 5300 -> loss: 0.09301569662063824\n",
      "    At iteration 5400 -> loss: 0.09294197336789757\n",
      "    At iteration 5500 -> loss: 0.09300313026248684\n",
      "    At iteration 5600 -> loss: 0.09295022149280872\n",
      "    At iteration 5700 -> loss: 0.09298091729183557\n",
      "    At iteration 5800 -> loss: 0.09292493918004131\n",
      "    At iteration 5900 -> loss: 0.09285405227890726\n",
      "    At iteration 6000 -> loss: 0.09281712721520341\n",
      "    At iteration 6100 -> loss: 0.0928535934950417\n",
      "    At iteration 6200 -> loss: 0.09286268798195112\n",
      "    At iteration 6300 -> loss: 0.09281055139203957\n",
      "    At iteration 6400 -> loss: 0.0928051865620433\n",
      "    At iteration 6500 -> loss: 0.09283948595117276\n",
      "    At iteration 6600 -> loss: 0.09278020931476373\n",
      "    At iteration 6700 -> loss: 0.09277721662714707\n",
      "    At iteration 6800 -> loss: 0.09277218042767704\n",
      "    At iteration 6900 -> loss: 0.09273336604914516\n",
      "    At iteration 7000 -> loss: 0.09287025335210844\n",
      "    At iteration 7100 -> loss: 0.09288407885511436\n",
      "    At iteration 7200 -> loss: 0.09282559417971314\n",
      "    At iteration 7300 -> loss: 0.09280085773540495\n",
      "    At iteration 7400 -> loss: 0.09275044379107067\n",
      "    At iteration 7500 -> loss: 0.09271788430717767\n",
      "    At iteration 7600 -> loss: 0.09267170467930926\n",
      "    At iteration 7700 -> loss: 0.09268599352853275\n",
      "    At iteration 7800 -> loss: 0.0926672563552367\n",
      "    At iteration 7900 -> loss: 0.09263763753726027\n",
      "    At iteration 8000 -> loss: 0.09268677688358715\n",
      "    At iteration 8100 -> loss: 0.09269220233772174\n",
      "    At iteration 8200 -> loss: 0.09283508446607056\n",
      "    At iteration 8300 -> loss: 0.09279983618030299\n",
      "    At iteration 8400 -> loss: 0.09287053423548867\n",
      "    At iteration 8500 -> loss: 0.09284292462518196\n",
      "    At iteration 8600 -> loss: 0.0928031618696094\n",
      "    At iteration 8700 -> loss: 0.09274906753477895\n",
      "    At iteration 8800 -> loss: 0.09274140415714166\n",
      "    At iteration 8900 -> loss: 0.09281658505621423\n",
      "    At iteration 9000 -> loss: 0.09279523148518061\n",
      "    At iteration 9100 -> loss: 0.09278769578052325\n",
      "    At iteration 9200 -> loss: 0.09288833595446803\n",
      "    At iteration 9300 -> loss: 0.09293022639431293\n",
      "    At iteration 9400 -> loss: 0.09287930039630714\n",
      "    At iteration 9500 -> loss: 0.09292087924481938\n",
      "    At iteration 9600 -> loss: 0.09290826962686714\n",
      "    At iteration 9700 -> loss: 0.09287948384966782\n",
      "    At iteration 9800 -> loss: 0.09286189986187549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 9900 -> loss: 0.09292373241691171\n",
      "    At iteration 10000 -> loss: 0.09295351222199474\n",
      "    At iteration 10100 -> loss: 0.09296110347361017\n",
      "    At iteration 10200 -> loss: 0.09294346245419718\n",
      "    At iteration 10300 -> loss: 0.09291606436800605\n",
      "    At iteration 10400 -> loss: 0.09294776943007614\n",
      "    At iteration 10500 -> loss: 0.0929323315778386\n",
      "    At iteration 10600 -> loss: 0.09291856388312218\n",
      "    At iteration 10700 -> loss: 0.09290203050415459\n",
      "    At iteration 10800 -> loss: 0.09288301207731368\n",
      "    At iteration 10900 -> loss: 0.09288108254912997\n",
      "    At iteration 11000 -> loss: 0.09315633113127071\n",
      "    At iteration 11100 -> loss: 0.0931356866242218\n",
      "    At iteration 11200 -> loss: 0.09311758388213805\n",
      "    At iteration 11300 -> loss: 0.09308574019629828\n",
      "    At iteration 11400 -> loss: 0.09306740622775486\n",
      "    At iteration 11500 -> loss: 0.09311502942889882\n",
      "    At iteration 11600 -> loss: 0.09310089046275236\n",
      "    At iteration 11700 -> loss: 0.09308153616154782\n",
      "    At iteration 11800 -> loss: 0.09307079544266066\n",
      "    At iteration 11900 -> loss: 0.09303952168802546\n",
      "    At iteration 12000 -> loss: 0.09306002577627694\n",
      "    At iteration 12100 -> loss: 0.09302872548291163\n",
      "    At iteration 12200 -> loss: 0.09301247810599592\n",
      "    At iteration 12300 -> loss: 0.09302404454546707\n",
      "    At iteration 12400 -> loss: 0.09300744119110284\n",
      "    At iteration 12500 -> loss: 0.09299215169238262\n",
      "    At iteration 12600 -> loss: 0.09300949021304805\n",
      "    At iteration 12700 -> loss: 0.09302637803104122\n",
      "    At iteration 12800 -> loss: 0.09302222796026088\n",
      "    At iteration 12900 -> loss: 0.09303068839041355\n",
      "    At iteration 13000 -> loss: 0.09300394117715763\n",
      "    At iteration 13100 -> loss: 0.09299862585174212\n",
      "    At iteration 13200 -> loss: 0.09299361688776063\n",
      "    At iteration 13300 -> loss: 0.09297544076181358\n",
      "    At iteration 13400 -> loss: 0.0930013609921353\n",
      "    At iteration 13500 -> loss: 0.09299601854144547\n",
      "    At iteration 13600 -> loss: 0.0929947951650974\n",
      "Staring Epoch 127\n",
      "    At iteration 0 -> loss: 0.08055772294756025\n",
      "    At iteration 100 -> loss: 0.09216169658744021\n",
      "    At iteration 200 -> loss: 0.09701792781360816\n",
      "    At iteration 300 -> loss: 0.0972054585158685\n",
      "    At iteration 400 -> loss: 0.10299892547669558\n",
      "    At iteration 500 -> loss: 0.100540266577927\n",
      "    At iteration 600 -> loss: 0.09925033802062184\n",
      "    At iteration 700 -> loss: 0.09793905819651043\n",
      "    At iteration 800 -> loss: 0.09713656479404872\n",
      "    At iteration 900 -> loss: 0.09673883642251742\n",
      "    At iteration 1000 -> loss: 0.09622657946296012\n",
      "    At iteration 1100 -> loss: 0.09574688078980041\n",
      "    At iteration 1200 -> loss: 0.09590031445997932\n",
      "    At iteration 1300 -> loss: 0.09582009577813783\n",
      "    At iteration 1400 -> loss: 0.09538411991638072\n",
      "    At iteration 1500 -> loss: 0.09578529359054017\n",
      "    At iteration 1600 -> loss: 0.0954960838204834\n",
      "    At iteration 1700 -> loss: 0.09507098403221141\n",
      "    At iteration 1800 -> loss: 0.09479217112213177\n",
      "    At iteration 1900 -> loss: 0.0947755272114943\n",
      "    At iteration 2000 -> loss: 0.09482865087476774\n",
      "    At iteration 2100 -> loss: 0.09454113097786772\n",
      "    At iteration 2200 -> loss: 0.09453015557760401\n",
      "    At iteration 2300 -> loss: 0.0947766629934841\n",
      "    At iteration 2400 -> loss: 0.09459821543431635\n",
      "    At iteration 2500 -> loss: 0.0945205838994365\n",
      "    At iteration 2600 -> loss: 0.09442262316164542\n",
      "    At iteration 2700 -> loss: 0.0942835305926902\n",
      "    At iteration 2800 -> loss: 0.09411560128388191\n",
      "    At iteration 2900 -> loss: 0.0940607624709499\n",
      "    At iteration 3000 -> loss: 0.09389725755651071\n",
      "    At iteration 3100 -> loss: 0.0940004177541077\n",
      "    At iteration 3200 -> loss: 0.09388015346667775\n",
      "    At iteration 3300 -> loss: 0.09381808206890864\n",
      "    At iteration 3400 -> loss: 0.09362665048357816\n",
      "    At iteration 3500 -> loss: 0.09353256577349439\n",
      "    At iteration 3600 -> loss: 0.0938253000325426\n",
      "    At iteration 3700 -> loss: 0.09403090460415071\n",
      "    At iteration 3800 -> loss: 0.09400710003870107\n",
      "    At iteration 3900 -> loss: 0.09406672995056345\n",
      "    At iteration 4000 -> loss: 0.09402320397914314\n",
      "    At iteration 4100 -> loss: 0.09404897784785164\n",
      "    At iteration 4200 -> loss: 0.09392550651494612\n",
      "    At iteration 4300 -> loss: 0.09386633655990174\n",
      "    At iteration 4400 -> loss: 0.09387863024158892\n",
      "    At iteration 4500 -> loss: 0.09378286931946297\n",
      "    At iteration 4600 -> loss: 0.09379288651156226\n",
      "    At iteration 4700 -> loss: 0.09374380877583142\n",
      "    At iteration 4800 -> loss: 0.09372915457848027\n",
      "    At iteration 4900 -> loss: 0.09369219953329765\n",
      "    At iteration 5000 -> loss: 0.09362362525002872\n",
      "    At iteration 5100 -> loss: 0.0937215393759718\n",
      "    At iteration 5200 -> loss: 0.09366198031718423\n",
      "    At iteration 5300 -> loss: 0.09364328207040205\n",
      "    At iteration 5400 -> loss: 0.09356003935096555\n",
      "    At iteration 5500 -> loss: 0.09351839422390018\n",
      "    At iteration 5600 -> loss: 0.09350833470645374\n",
      "    At iteration 5700 -> loss: 0.09342304131485141\n",
      "    At iteration 5800 -> loss: 0.09342418980459265\n",
      "    At iteration 5900 -> loss: 0.0933618278705614\n",
      "    At iteration 6000 -> loss: 0.09341591731705208\n",
      "    At iteration 6100 -> loss: 0.09338511543696727\n",
      "    At iteration 6200 -> loss: 0.0933384401863004\n",
      "    At iteration 6300 -> loss: 0.09331394287748752\n",
      "    At iteration 6400 -> loss: 0.09328318776178096\n",
      "    At iteration 6500 -> loss: 0.09337014168852814\n",
      "    At iteration 6600 -> loss: 0.09341686925358927\n",
      "    At iteration 6700 -> loss: 0.09338340417147813\n",
      "    At iteration 6800 -> loss: 0.09336820305581782\n",
      "    At iteration 6900 -> loss: 0.09337055660805585\n",
      "    At iteration 7000 -> loss: 0.09329963665609711\n",
      "    At iteration 7100 -> loss: 0.09329550887811666\n",
      "    At iteration 7200 -> loss: 0.09324970699880217\n",
      "    At iteration 7300 -> loss: 0.09334955172916665\n",
      "    At iteration 7400 -> loss: 0.09339436907914564\n",
      "    At iteration 7500 -> loss: 0.09356687120969646\n",
      "    At iteration 7600 -> loss: 0.09353099820282072\n",
      "    At iteration 7700 -> loss: 0.09348164762599102\n",
      "    At iteration 7800 -> loss: 0.09354669908355293\n",
      "    At iteration 7900 -> loss: 0.09357167913493117\n",
      "    At iteration 8000 -> loss: 0.09352151001674247\n",
      "    At iteration 8100 -> loss: 0.09350463449936629\n",
      "    At iteration 8200 -> loss: 0.09349246392098087\n",
      "    At iteration 8300 -> loss: 0.0934562781059651\n",
      "    At iteration 8400 -> loss: 0.09344428487501616\n",
      "    At iteration 8500 -> loss: 0.09341893197646396\n",
      "    At iteration 8600 -> loss: 0.09337565854445277\n",
      "    At iteration 8700 -> loss: 0.09332679299287239\n",
      "    At iteration 8800 -> loss: 0.09332841796610829\n",
      "    At iteration 8900 -> loss: 0.09333421439126005\n",
      "    At iteration 9000 -> loss: 0.09329969519341678\n",
      "    At iteration 9100 -> loss: 0.09337349896605344\n",
      "    At iteration 9200 -> loss: 0.09334919411654295\n",
      "    At iteration 9300 -> loss: 0.09333455612950661\n",
      "    At iteration 9400 -> loss: 0.09330573336855318\n",
      "    At iteration 9500 -> loss: 0.09327869346690494\n",
      "    At iteration 9600 -> loss: 0.09334413127501749\n",
      "    At iteration 9700 -> loss: 0.09334503687767433\n",
      "    At iteration 9800 -> loss: 0.0933551112905729\n",
      "    At iteration 9900 -> loss: 0.09335638391340438\n",
      "    At iteration 10000 -> loss: 0.09335639675450219\n",
      "    At iteration 10100 -> loss: 0.09332009877387915\n",
      "    At iteration 10200 -> loss: 0.09333200777770785\n",
      "    At iteration 10300 -> loss: 0.09330091899723707\n",
      "    At iteration 10400 -> loss: 0.09331282898245655\n",
      "    At iteration 10500 -> loss: 0.09329250375502779\n",
      "    At iteration 10600 -> loss: 0.09327915885936924\n",
      "    At iteration 10700 -> loss: 0.09327845482965827\n",
      "    At iteration 10800 -> loss: 0.09326273217118196\n",
      "    At iteration 10900 -> loss: 0.09322391848918113\n",
      "    At iteration 11000 -> loss: 0.09323068387722316\n",
      "    At iteration 11100 -> loss: 0.09322442056518315\n",
      "    At iteration 11200 -> loss: 0.09322124471818816\n",
      "    At iteration 11300 -> loss: 0.09321093603535678\n",
      "    At iteration 11400 -> loss: 0.09324924504386496\n",
      "    At iteration 11500 -> loss: 0.0932470573869081\n",
      "    At iteration 11600 -> loss: 0.09320866496756493\n",
      "    At iteration 11700 -> loss: 0.0931931876844331\n",
      "    At iteration 11800 -> loss: 0.09319868840180091\n",
      "    At iteration 11900 -> loss: 0.09320684170085597\n",
      "    At iteration 12000 -> loss: 0.09320145120776467\n",
      "    At iteration 12100 -> loss: 0.09318141424937666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12200 -> loss: 0.09315996281962477\n",
      "    At iteration 12300 -> loss: 0.09312663207841507\n",
      "    At iteration 12400 -> loss: 0.09313785992702694\n",
      "    At iteration 12500 -> loss: 0.09310663166386537\n",
      "    At iteration 12600 -> loss: 0.09308536956715478\n",
      "    At iteration 12700 -> loss: 0.09306773922578321\n",
      "    At iteration 12800 -> loss: 0.09306776666370666\n",
      "    At iteration 12900 -> loss: 0.09308222238308224\n",
      "    At iteration 13000 -> loss: 0.09306537044016525\n",
      "    At iteration 13100 -> loss: 0.09309506252518525\n",
      "    At iteration 13200 -> loss: 0.09306936689047254\n",
      "    At iteration 13300 -> loss: 0.09305122572820261\n",
      "    At iteration 13400 -> loss: 0.09304859072012987\n",
      "    At iteration 13500 -> loss: 0.09304706728614813\n",
      "    At iteration 13600 -> loss: 0.09302373601131923\n",
      "Staring Epoch 128\n",
      "    At iteration 0 -> loss: 0.10429528541862965\n",
      "    At iteration 100 -> loss: 0.09581413073815716\n",
      "    At iteration 200 -> loss: 0.0924920647877787\n",
      "    At iteration 300 -> loss: 0.09173591849172173\n",
      "    At iteration 400 -> loss: 0.09176850563300046\n",
      "    At iteration 500 -> loss: 0.09195859510542785\n",
      "    At iteration 600 -> loss: 0.09247997102167527\n",
      "    At iteration 700 -> loss: 0.09368472000483494\n",
      "    At iteration 800 -> loss: 0.09357794264193359\n",
      "    At iteration 900 -> loss: 0.09331211239919822\n",
      "    At iteration 1000 -> loss: 0.0931402937787663\n",
      "    At iteration 1100 -> loss: 0.09280350480550402\n",
      "    At iteration 1200 -> loss: 0.0925000904501402\n",
      "    At iteration 1300 -> loss: 0.09225554083410208\n",
      "    At iteration 1400 -> loss: 0.09422707755888644\n",
      "    At iteration 1500 -> loss: 0.0939748058364617\n",
      "    At iteration 1600 -> loss: 0.09393293314470301\n",
      "    At iteration 1700 -> loss: 0.09368366099005\n",
      "    At iteration 1800 -> loss: 0.09372739380846275\n",
      "    At iteration 1900 -> loss: 0.09370111216180879\n",
      "    At iteration 2000 -> loss: 0.09345837146024141\n",
      "    At iteration 2100 -> loss: 0.09324365849057537\n",
      "    At iteration 2200 -> loss: 0.09341968882437009\n",
      "    At iteration 2300 -> loss: 0.09347181464204134\n",
      "    At iteration 2400 -> loss: 0.09339009046322742\n",
      "    At iteration 2500 -> loss: 0.09341572020828444\n",
      "    At iteration 2600 -> loss: 0.09333309775220106\n",
      "    At iteration 2700 -> loss: 0.09333922499788154\n",
      "    At iteration 2800 -> loss: 0.09338630525870312\n",
      "    At iteration 2900 -> loss: 0.09321160047608432\n",
      "    At iteration 3000 -> loss: 0.09332491258059625\n",
      "    At iteration 3100 -> loss: 0.09327986452052008\n",
      "    At iteration 3200 -> loss: 0.09332019020060212\n",
      "    At iteration 3300 -> loss: 0.09328534522893557\n",
      "    At iteration 3400 -> loss: 0.09319779902486594\n",
      "    At iteration 3500 -> loss: 0.09316676466055038\n",
      "    At iteration 3600 -> loss: 0.09337205884615568\n",
      "    At iteration 3700 -> loss: 0.09335561503585726\n",
      "    At iteration 3800 -> loss: 0.09348281553442243\n",
      "    At iteration 3900 -> loss: 0.093375110210561\n",
      "    At iteration 4000 -> loss: 0.09325685613193138\n",
      "    At iteration 4100 -> loss: 0.0932351639325315\n",
      "    At iteration 4200 -> loss: 0.09322175227886317\n",
      "    At iteration 4300 -> loss: 0.09333119049156513\n",
      "    At iteration 4400 -> loss: 0.09324445759493129\n",
      "    At iteration 4500 -> loss: 0.09323688606975962\n",
      "    At iteration 4600 -> loss: 0.09317635443287284\n",
      "    At iteration 4700 -> loss: 0.09308785000957395\n",
      "    At iteration 4800 -> loss: 0.09304771905968973\n",
      "    At iteration 4900 -> loss: 0.09314194556608808\n",
      "    At iteration 5000 -> loss: 0.093107354823342\n",
      "    At iteration 5100 -> loss: 0.09310439450328617\n",
      "    At iteration 5200 -> loss: 0.09313419150905196\n",
      "    At iteration 5300 -> loss: 0.09313715354631298\n",
      "    At iteration 5400 -> loss: 0.09307734476099987\n",
      "    At iteration 5500 -> loss: 0.0931203503853784\n",
      "    At iteration 5600 -> loss: 0.0930633176472973\n",
      "    At iteration 5700 -> loss: 0.09304839765690061\n",
      "    At iteration 5800 -> loss: 0.09318341945491192\n",
      "    At iteration 5900 -> loss: 0.09312008252023493\n",
      "    At iteration 6000 -> loss: 0.09313157125137195\n",
      "    At iteration 6100 -> loss: 0.09312710689314795\n",
      "    At iteration 6200 -> loss: 0.09305462788235459\n",
      "    At iteration 6300 -> loss: 0.09306283155412808\n",
      "    At iteration 6400 -> loss: 0.09302420112168078\n",
      "    At iteration 6500 -> loss: 0.09299566276474865\n",
      "    At iteration 6600 -> loss: 0.09298605947750209\n",
      "    At iteration 6700 -> loss: 0.09295564808500636\n",
      "    At iteration 6800 -> loss: 0.0929590301159959\n",
      "    At iteration 6900 -> loss: 0.09291933990291039\n",
      "    At iteration 7000 -> loss: 0.09295623367546592\n",
      "    At iteration 7100 -> loss: 0.0929220865347448\n",
      "    At iteration 7200 -> loss: 0.0928874784354849\n",
      "    At iteration 7300 -> loss: 0.09290589043576987\n",
      "    At iteration 7400 -> loss: 0.09287892489553282\n",
      "    At iteration 7500 -> loss: 0.09287382174666946\n",
      "    At iteration 7600 -> loss: 0.0928689986499478\n",
      "    At iteration 7700 -> loss: 0.09281926046123448\n",
      "    At iteration 7800 -> loss: 0.09276437186559856\n",
      "    At iteration 7900 -> loss: 0.09284792307576714\n",
      "    At iteration 8000 -> loss: 0.09284451986221876\n",
      "    At iteration 8100 -> loss: 0.09282749625695859\n",
      "    At iteration 8200 -> loss: 0.09293486623248999\n",
      "    At iteration 8300 -> loss: 0.09290034391154303\n",
      "    At iteration 8400 -> loss: 0.0928671336267292\n",
      "    At iteration 8500 -> loss: 0.09282179197044586\n",
      "    At iteration 8600 -> loss: 0.09291408632594005\n",
      "    At iteration 8700 -> loss: 0.09291160857468914\n",
      "    At iteration 8800 -> loss: 0.09286023590270957\n",
      "    At iteration 8900 -> loss: 0.09292251699168175\n",
      "    At iteration 9000 -> loss: 0.09306464870866109\n",
      "    At iteration 9100 -> loss: 0.09302631076854286\n",
      "    At iteration 9200 -> loss: 0.09302494550438577\n",
      "    At iteration 9300 -> loss: 0.09315669077032417\n",
      "    At iteration 9400 -> loss: 0.09314888922304865\n",
      "    At iteration 9500 -> loss: 0.09314307532767341\n",
      "    At iteration 9600 -> loss: 0.09327705514201982\n",
      "    At iteration 9700 -> loss: 0.09325899299418254\n",
      "    At iteration 9800 -> loss: 0.09325017386543444\n",
      "    At iteration 9900 -> loss: 0.09323789478726617\n",
      "    At iteration 10000 -> loss: 0.09323470339243008\n",
      "    At iteration 10100 -> loss: 0.0932520519336412\n",
      "    At iteration 10200 -> loss: 0.09322347943064425\n",
      "    At iteration 10300 -> loss: 0.09320516904831976\n",
      "    At iteration 10400 -> loss: 0.09319168130927077\n",
      "    At iteration 10500 -> loss: 0.09318394322104949\n",
      "    At iteration 10600 -> loss: 0.09319148642028113\n",
      "    At iteration 10700 -> loss: 0.093192279237353\n",
      "    At iteration 10800 -> loss: 0.09314558965826288\n",
      "    At iteration 10900 -> loss: 0.09311613388835456\n",
      "    At iteration 11000 -> loss: 0.09309011672771776\n",
      "    At iteration 11100 -> loss: 0.09305261525988164\n",
      "    At iteration 11200 -> loss: 0.09305599761502555\n",
      "    At iteration 11300 -> loss: 0.09303718974070631\n",
      "    At iteration 11400 -> loss: 0.0930155613208207\n",
      "    At iteration 11500 -> loss: 0.09299465582607898\n",
      "    At iteration 11600 -> loss: 0.0929884223115498\n",
      "    At iteration 11700 -> loss: 0.09298769781835522\n",
      "    At iteration 11800 -> loss: 0.09300768146038532\n",
      "    At iteration 11900 -> loss: 0.09305095449329119\n",
      "    At iteration 12000 -> loss: 0.09303442617886325\n",
      "    At iteration 12100 -> loss: 0.09301079988188012\n",
      "    At iteration 12200 -> loss: 0.09299927110164138\n",
      "    At iteration 12300 -> loss: 0.09297986749337997\n",
      "    At iteration 12400 -> loss: 0.09299699632707552\n",
      "    At iteration 12500 -> loss: 0.09297409978957392\n",
      "    At iteration 12600 -> loss: 0.09299366796107263\n",
      "    At iteration 12700 -> loss: 0.09298674152168322\n",
      "    At iteration 12800 -> loss: 0.09295966826364782\n",
      "    At iteration 12900 -> loss: 0.0929443300352836\n",
      "    At iteration 13000 -> loss: 0.09294367448883147\n",
      "    At iteration 13100 -> loss: 0.092962728812423\n",
      "    At iteration 13200 -> loss: 0.09298063646919377\n",
      "    At iteration 13300 -> loss: 0.09295499338267534\n",
      "    At iteration 13400 -> loss: 0.09296618655778931\n",
      "    At iteration 13500 -> loss: 0.09297067553845383\n",
      "    At iteration 13600 -> loss: 0.09298453243992229\n",
      "Staring Epoch 129\n",
      "    At iteration 0 -> loss: 0.0811346605187282\n",
      "    At iteration 100 -> loss: 0.09269537000915824\n",
      "    At iteration 200 -> loss: 0.09327926853970119\n",
      "    At iteration 300 -> loss: 0.09259954384237726\n",
      "    At iteration 400 -> loss: 0.09375142809753143\n",
      "    At iteration 500 -> loss: 0.09290189620935477\n",
      "    At iteration 600 -> loss: 0.09233241126819466\n",
      "    At iteration 700 -> loss: 0.09395096644322032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 800 -> loss: 0.09393424228176621\n",
      "    At iteration 900 -> loss: 0.0940431593808278\n",
      "    At iteration 1000 -> loss: 0.09384653662428487\n",
      "    At iteration 1100 -> loss: 0.09372530819317786\n",
      "    At iteration 1200 -> loss: 0.09340334880184308\n",
      "    At iteration 1300 -> loss: 0.0932601572090183\n",
      "    At iteration 1400 -> loss: 0.09349527859125037\n",
      "    At iteration 1500 -> loss: 0.09360614557515345\n",
      "    At iteration 1600 -> loss: 0.09330852615698865\n",
      "    At iteration 1700 -> loss: 0.09370219844620697\n",
      "    At iteration 1800 -> loss: 0.09360206716609605\n",
      "    At iteration 1900 -> loss: 0.09341631727088527\n",
      "    At iteration 2000 -> loss: 0.09338197748766484\n",
      "    At iteration 2100 -> loss: 0.0932837740804025\n",
      "    At iteration 2200 -> loss: 0.09337970455414926\n",
      "    At iteration 2300 -> loss: 0.09334694818942448\n",
      "    At iteration 2400 -> loss: 0.09323250560802898\n",
      "    At iteration 2500 -> loss: 0.09338532300023639\n",
      "    At iteration 2600 -> loss: 0.09320441738640076\n",
      "    At iteration 2700 -> loss: 0.09324270829354693\n",
      "    At iteration 2800 -> loss: 0.09307801894536351\n",
      "    At iteration 2900 -> loss: 0.0930795804813183\n",
      "    At iteration 3000 -> loss: 0.09323802869329144\n",
      "    At iteration 3100 -> loss: 0.0930940576384927\n",
      "    At iteration 3200 -> loss: 0.09298612720493254\n",
      "    At iteration 3300 -> loss: 0.09293102799039389\n",
      "    At iteration 3400 -> loss: 0.09305715238782067\n",
      "    At iteration 3500 -> loss: 0.09297750063189976\n",
      "    At iteration 3600 -> loss: 0.09297039316849784\n",
      "    At iteration 3700 -> loss: 0.09296626532517482\n",
      "    At iteration 3800 -> loss: 0.09286149026297215\n",
      "    At iteration 3900 -> loss: 0.09295290125042065\n",
      "    At iteration 4000 -> loss: 0.09306838979798863\n",
      "    At iteration 4100 -> loss: 0.09311602743634452\n",
      "    At iteration 4200 -> loss: 0.09311852835016447\n",
      "    At iteration 4300 -> loss: 0.09306637245566061\n",
      "    At iteration 4400 -> loss: 0.0929983206898443\n",
      "    At iteration 4500 -> loss: 0.09295283454892814\n",
      "    At iteration 4600 -> loss: 0.09295749517659523\n",
      "    At iteration 4700 -> loss: 0.09291513204659704\n",
      "    At iteration 4800 -> loss: 0.09294415747932488\n",
      "    At iteration 4900 -> loss: 0.09285530816446948\n",
      "    At iteration 5000 -> loss: 0.09294856455744296\n",
      "    At iteration 5100 -> loss: 0.09290653237789769\n",
      "    At iteration 5200 -> loss: 0.09288316317699792\n",
      "    At iteration 5300 -> loss: 0.09284135105240189\n",
      "    At iteration 5400 -> loss: 0.09278188307344634\n",
      "    At iteration 5500 -> loss: 0.09274807193424983\n",
      "    At iteration 5600 -> loss: 0.09276090410483882\n",
      "    At iteration 5700 -> loss: 0.09282037408758735\n",
      "    At iteration 5800 -> loss: 0.09278966862420333\n",
      "    At iteration 5900 -> loss: 0.09328546102335308\n",
      "    At iteration 6000 -> loss: 0.09325224407474851\n",
      "    At iteration 6100 -> loss: 0.09330685242300095\n",
      "    At iteration 6200 -> loss: 0.09321730518808886\n",
      "    At iteration 6300 -> loss: 0.09347872897087231\n",
      "    At iteration 6400 -> loss: 0.09344796931427647\n",
      "    At iteration 6500 -> loss: 0.09338695140103524\n",
      "    At iteration 6600 -> loss: 0.09334520605317345\n",
      "    At iteration 6700 -> loss: 0.09331008365743504\n",
      "    At iteration 6800 -> loss: 0.09334548277863801\n",
      "    At iteration 6900 -> loss: 0.09338665728694019\n",
      "    At iteration 7000 -> loss: 0.09336964403901211\n",
      "    At iteration 7100 -> loss: 0.09331118725685941\n",
      "    At iteration 7200 -> loss: 0.09327958062283681\n",
      "    At iteration 7300 -> loss: 0.09323891215169947\n",
      "    At iteration 7400 -> loss: 0.0932148135625972\n",
      "    At iteration 7500 -> loss: 0.09317217575938186\n",
      "    At iteration 7600 -> loss: 0.09315737608390244\n",
      "    At iteration 7700 -> loss: 0.0931137229818295\n",
      "    At iteration 7800 -> loss: 0.0930735681587483\n",
      "    At iteration 7900 -> loss: 0.09303430097109977\n",
      "    At iteration 8000 -> loss: 0.09299241015256034\n",
      "    At iteration 8100 -> loss: 0.09296895241703679\n",
      "    At iteration 8200 -> loss: 0.0929813479634848\n",
      "    At iteration 8300 -> loss: 0.0929433509025339\n",
      "    At iteration 8400 -> loss: 0.09292672775978791\n",
      "    At iteration 8500 -> loss: 0.0929311601240663\n",
      "    At iteration 8600 -> loss: 0.09293922750776136\n",
      "    At iteration 8700 -> loss: 0.09290583731262136\n",
      "    At iteration 8800 -> loss: 0.09292079832265357\n",
      "    At iteration 8900 -> loss: 0.09306756668157634\n",
      "    At iteration 9000 -> loss: 0.09303902316599849\n",
      "    At iteration 9100 -> loss: 0.09310941433933959\n",
      "    At iteration 9200 -> loss: 0.09305907105421558\n",
      "    At iteration 9300 -> loss: 0.09305174868578815\n",
      "    At iteration 9400 -> loss: 0.09306690854586167\n",
      "    At iteration 9500 -> loss: 0.0930231523877839\n",
      "    At iteration 9600 -> loss: 0.09310518918738014\n",
      "    At iteration 9700 -> loss: 0.09313442896897965\n",
      "    At iteration 9800 -> loss: 0.09314321327315024\n",
      "    At iteration 9900 -> loss: 0.09311334442565226\n",
      "    At iteration 10000 -> loss: 0.09307215794457196\n",
      "    At iteration 10100 -> loss: 0.0930876408229416\n",
      "    At iteration 10200 -> loss: 0.09311909021774956\n",
      "    At iteration 10300 -> loss: 0.09312236582726606\n",
      "    At iteration 10400 -> loss: 0.0930836243260615\n",
      "    At iteration 10500 -> loss: 0.09306021952945974\n",
      "    At iteration 10600 -> loss: 0.09301946794059832\n",
      "    At iteration 10700 -> loss: 0.09303748768832766\n",
      "    At iteration 10800 -> loss: 0.09306093359033059\n",
      "    At iteration 10900 -> loss: 0.09307257750095257\n",
      "    At iteration 11000 -> loss: 0.0930841566656106\n",
      "    At iteration 11100 -> loss: 0.09307637284417511\n",
      "    At iteration 11200 -> loss: 0.09307483564134787\n",
      "    At iteration 11300 -> loss: 0.09306171325749814\n",
      "    At iteration 11400 -> loss: 0.09309946445747387\n",
      "    At iteration 11500 -> loss: 0.09320408957153578\n",
      "    At iteration 11600 -> loss: 0.09321760572359926\n",
      "    At iteration 11700 -> loss: 0.0931824675436022\n",
      "    At iteration 11800 -> loss: 0.09315626259477841\n",
      "    At iteration 11900 -> loss: 0.09315076523795833\n",
      "    At iteration 12000 -> loss: 0.09314485621828043\n",
      "    At iteration 12100 -> loss: 0.09316440751487\n",
      "    At iteration 12200 -> loss: 0.09314343452086556\n",
      "    At iteration 12300 -> loss: 0.09313342066081767\n",
      "    At iteration 12400 -> loss: 0.09312385014497239\n",
      "    At iteration 12500 -> loss: 0.0931046920598916\n",
      "    At iteration 12600 -> loss: 0.09310272172338276\n",
      "    At iteration 12700 -> loss: 0.09307540686926423\n",
      "    At iteration 12800 -> loss: 0.09306901678048894\n",
      "    At iteration 12900 -> loss: 0.09306415832161753\n",
      "    At iteration 13000 -> loss: 0.09309445281350322\n",
      "    At iteration 13100 -> loss: 0.09307769143071234\n",
      "    At iteration 13200 -> loss: 0.09304585297871235\n",
      "    At iteration 13300 -> loss: 0.0931062566927491\n",
      "    At iteration 13400 -> loss: 0.09309353117755803\n",
      "    At iteration 13500 -> loss: 0.09308102767058407\n",
      "    At iteration 13600 -> loss: 0.09305380591419031\n",
      "Staring Epoch 130\n",
      "    At iteration 0 -> loss: 0.09105719602666795\n",
      "    At iteration 100 -> loss: 0.09443697602375047\n",
      "    At iteration 200 -> loss: 0.09144102831158361\n",
      "    At iteration 300 -> loss: 0.09084048088229069\n",
      "    At iteration 400 -> loss: 0.09338931599890436\n",
      "    At iteration 500 -> loss: 0.094604723258225\n",
      "    At iteration 600 -> loss: 0.09455221951524799\n",
      "    At iteration 700 -> loss: 0.0941801226280969\n",
      "    At iteration 800 -> loss: 0.0939413045096014\n",
      "    At iteration 900 -> loss: 0.09340043491926923\n",
      "    At iteration 1000 -> loss: 0.09283837174102219\n",
      "    At iteration 1100 -> loss: 0.09294388174309168\n",
      "    At iteration 1200 -> loss: 0.09295890252467706\n",
      "    At iteration 1300 -> loss: 0.09283723681030899\n",
      "    At iteration 1400 -> loss: 0.0925323488841854\n",
      "    At iteration 1500 -> loss: 0.09238565821433745\n",
      "    At iteration 1600 -> loss: 0.09236427669470365\n",
      "    At iteration 1700 -> loss: 0.0921472349193008\n",
      "    At iteration 1800 -> loss: 0.09220160381459616\n",
      "    At iteration 1900 -> loss: 0.09249883016219074\n",
      "    At iteration 2000 -> loss: 0.09245306764768477\n",
      "    At iteration 2100 -> loss: 0.09247254677253912\n",
      "    At iteration 2200 -> loss: 0.09264159035687428\n",
      "    At iteration 2300 -> loss: 0.09267875080887886\n",
      "    At iteration 2400 -> loss: 0.09260420292060524\n",
      "    At iteration 2500 -> loss: 0.0926141639825707\n",
      "    At iteration 2600 -> loss: 0.09268585486717752\n",
      "    At iteration 2700 -> loss: 0.09255356751648018\n",
      "    At iteration 2800 -> loss: 0.09249141924783807\n",
      "    At iteration 2900 -> loss: 0.09245631862218864\n",
      "    At iteration 3000 -> loss: 0.09237836052126656\n",
      "    At iteration 3100 -> loss: 0.09233411117671654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3200 -> loss: 0.09262677258579066\n",
      "    At iteration 3300 -> loss: 0.09278340650860738\n",
      "    At iteration 3400 -> loss: 0.09277536445064447\n",
      "    At iteration 3500 -> loss: 0.09275471904286049\n",
      "    At iteration 3600 -> loss: 0.09269797433210902\n",
      "    At iteration 3700 -> loss: 0.09254085000553033\n",
      "    At iteration 3800 -> loss: 0.09257241231986715\n",
      "    At iteration 3900 -> loss: 0.09267332118898218\n",
      "    At iteration 4000 -> loss: 0.09262377220921357\n",
      "    At iteration 4100 -> loss: 0.09274002026044482\n",
      "    At iteration 4200 -> loss: 0.09286300338080818\n",
      "    At iteration 4300 -> loss: 0.09287377487630175\n",
      "    At iteration 4400 -> loss: 0.09287463024501753\n",
      "    At iteration 4500 -> loss: 0.09281471958612325\n",
      "    At iteration 4600 -> loss: 0.0927135856859439\n",
      "    At iteration 4700 -> loss: 0.09271396348215351\n",
      "    At iteration 4800 -> loss: 0.0927434884999107\n",
      "    At iteration 4900 -> loss: 0.09276614013124783\n",
      "    At iteration 5000 -> loss: 0.09268909275969457\n",
      "    At iteration 5100 -> loss: 0.09266574085372128\n",
      "    At iteration 5200 -> loss: 0.09263680635392783\n",
      "    At iteration 5300 -> loss: 0.09281404382194153\n",
      "    At iteration 5400 -> loss: 0.0927386433254662\n",
      "    At iteration 5500 -> loss: 0.09268978592838613\n",
      "    At iteration 5600 -> loss: 0.09263929183544667\n",
      "    At iteration 5700 -> loss: 0.0926237637278436\n",
      "    At iteration 5800 -> loss: 0.09254584853874695\n",
      "    At iteration 5900 -> loss: 0.09255627184217395\n",
      "    At iteration 6000 -> loss: 0.09254313843949132\n",
      "    At iteration 6100 -> loss: 0.09251794263567593\n",
      "    At iteration 6200 -> loss: 0.09250757825652023\n",
      "    At iteration 6300 -> loss: 0.09251029760076619\n",
      "    At iteration 6400 -> loss: 0.09249639108120027\n",
      "    At iteration 6500 -> loss: 0.09254055890444442\n",
      "    At iteration 6600 -> loss: 0.09249373590379088\n",
      "    At iteration 6700 -> loss: 0.09261151406339814\n",
      "    At iteration 6800 -> loss: 0.0925582198878982\n",
      "    At iteration 6900 -> loss: 0.09254515830390934\n",
      "    At iteration 7000 -> loss: 0.09250995606839559\n",
      "    At iteration 7100 -> loss: 0.09251179616568789\n",
      "    At iteration 7200 -> loss: 0.0925231712739846\n",
      "    At iteration 7300 -> loss: 0.09252594444206029\n",
      "    At iteration 7400 -> loss: 0.09248390886826224\n",
      "    At iteration 7500 -> loss: 0.09246927906825901\n",
      "    At iteration 7600 -> loss: 0.09244287311192341\n",
      "    At iteration 7700 -> loss: 0.09252979559462393\n",
      "    At iteration 7800 -> loss: 0.09253438005377267\n",
      "    At iteration 7900 -> loss: 0.09252610730772291\n",
      "    At iteration 8000 -> loss: 0.09248016110050511\n",
      "    At iteration 8100 -> loss: 0.09245684694555778\n",
      "    At iteration 8200 -> loss: 0.09246552695280301\n",
      "    At iteration 8300 -> loss: 0.0924625143848989\n",
      "    At iteration 8400 -> loss: 0.09246072631005886\n",
      "    At iteration 8500 -> loss: 0.09246825378640038\n",
      "    At iteration 8600 -> loss: 0.09245264442159684\n",
      "    At iteration 8700 -> loss: 0.0925192334029975\n",
      "    At iteration 8800 -> loss: 0.09247526942018588\n",
      "    At iteration 8900 -> loss: 0.09251550764894864\n",
      "    At iteration 9000 -> loss: 0.09250021737761843\n",
      "    At iteration 9100 -> loss: 0.09248271281914193\n",
      "    At iteration 9200 -> loss: 0.09248001259895647\n",
      "    At iteration 9300 -> loss: 0.09247061347392482\n",
      "    At iteration 9400 -> loss: 0.09245858996750404\n",
      "    At iteration 9500 -> loss: 0.09243523981233442\n",
      "    At iteration 9600 -> loss: 0.09248830269336282\n",
      "    At iteration 9700 -> loss: 0.09247337028602788\n",
      "    At iteration 9800 -> loss: 0.09247710390728044\n",
      "    At iteration 9900 -> loss: 0.09251178188025805\n",
      "    At iteration 10000 -> loss: 0.09250050752831182\n",
      "    At iteration 10100 -> loss: 0.09257352688219549\n",
      "    At iteration 10200 -> loss: 0.0925805399914843\n",
      "    At iteration 10300 -> loss: 0.09257438091359205\n",
      "    At iteration 10400 -> loss: 0.09256005941330185\n",
      "    At iteration 10500 -> loss: 0.09284787089589448\n",
      "    At iteration 10600 -> loss: 0.09285096131936944\n",
      "    At iteration 10700 -> loss: 0.09283587429462586\n",
      "    At iteration 10800 -> loss: 0.09279407607771194\n",
      "    At iteration 10900 -> loss: 0.09279069024915872\n",
      "    At iteration 11000 -> loss: 0.09279338637676131\n",
      "    At iteration 11100 -> loss: 0.09278616842195\n",
      "    At iteration 11200 -> loss: 0.09277224726087684\n",
      "    At iteration 11300 -> loss: 0.0927825985871826\n",
      "    At iteration 11400 -> loss: 0.0927734299163494\n",
      "    At iteration 11500 -> loss: 0.09276009806885392\n",
      "    At iteration 11600 -> loss: 0.09274860025083324\n",
      "    At iteration 11700 -> loss: 0.09275520141406626\n",
      "    At iteration 11800 -> loss: 0.09276328406792483\n",
      "    At iteration 11900 -> loss: 0.09283763163544417\n",
      "    At iteration 12000 -> loss: 0.09282081830796414\n",
      "    At iteration 12100 -> loss: 0.0928688001667258\n",
      "    At iteration 12200 -> loss: 0.09289236345009602\n",
      "    At iteration 12300 -> loss: 0.09291146737820646\n",
      "    At iteration 12400 -> loss: 0.09289417565818579\n",
      "    At iteration 12500 -> loss: 0.09290528508418187\n",
      "    At iteration 12600 -> loss: 0.092938172487097\n",
      "    At iteration 12700 -> loss: 0.0929091255580257\n",
      "    At iteration 12800 -> loss: 0.09290364064815114\n",
      "    At iteration 12900 -> loss: 0.09288807687629921\n",
      "    At iteration 13000 -> loss: 0.09286732671369553\n",
      "    At iteration 13100 -> loss: 0.09287403310418763\n",
      "    At iteration 13200 -> loss: 0.09296230657291746\n",
      "    At iteration 13300 -> loss: 0.09297994414919998\n",
      "    At iteration 13400 -> loss: 0.09295334029168562\n",
      "    At iteration 13500 -> loss: 0.09294357880136801\n",
      "    At iteration 13600 -> loss: 0.09299009688472877\n",
      "Staring Epoch 131\n",
      "    At iteration 0 -> loss: 0.08010911780729657\n",
      "    At iteration 100 -> loss: 0.09664642236086618\n",
      "    At iteration 200 -> loss: 0.09395456841740928\n",
      "    At iteration 300 -> loss: 0.09367861137224374\n",
      "    At iteration 400 -> loss: 0.09373661686000384\n",
      "    At iteration 500 -> loss: 0.09295020295756164\n",
      "    At iteration 600 -> loss: 0.09230624469088398\n",
      "    At iteration 700 -> loss: 0.09233423999452348\n",
      "    At iteration 800 -> loss: 0.0920982928895579\n",
      "    At iteration 900 -> loss: 0.09198219185699802\n",
      "    At iteration 1000 -> loss: 0.09202760517138872\n",
      "    At iteration 1100 -> loss: 0.09202821628358814\n",
      "    At iteration 1200 -> loss: 0.09184852985993248\n",
      "    At iteration 1300 -> loss: 0.09178578424024347\n",
      "    At iteration 1400 -> loss: 0.09208882736395814\n",
      "    At iteration 1500 -> loss: 0.09205460723015137\n",
      "    At iteration 1600 -> loss: 0.09226134951769775\n",
      "    At iteration 1700 -> loss: 0.09252121261096703\n",
      "    At iteration 1800 -> loss: 0.09262367933654041\n",
      "    At iteration 1900 -> loss: 0.09253834967236806\n",
      "    At iteration 2000 -> loss: 0.09245129804415635\n",
      "    At iteration 2100 -> loss: 0.09255903626903181\n",
      "    At iteration 2200 -> loss: 0.0923709772890719\n",
      "    At iteration 2300 -> loss: 0.0924010255456252\n",
      "    At iteration 2400 -> loss: 0.09225745343044345\n",
      "    At iteration 2500 -> loss: 0.09227015573547358\n",
      "    At iteration 2600 -> loss: 0.092297409389042\n",
      "    At iteration 2700 -> loss: 0.09223334855130211\n",
      "    At iteration 2800 -> loss: 0.09214697875357303\n",
      "    At iteration 2900 -> loss: 0.09213307036885762\n",
      "    At iteration 3000 -> loss: 0.09230057512946314\n",
      "    At iteration 3100 -> loss: 0.09231634960289245\n",
      "    At iteration 3200 -> loss: 0.0925161800876886\n",
      "    At iteration 3300 -> loss: 0.09249014730704307\n",
      "    At iteration 3400 -> loss: 0.09244480201548455\n",
      "    At iteration 3500 -> loss: 0.09264523215822355\n",
      "    At iteration 3600 -> loss: 0.09265571670450544\n",
      "    At iteration 3700 -> loss: 0.0928669225204754\n",
      "    At iteration 3800 -> loss: 0.09283300772548299\n",
      "    At iteration 3900 -> loss: 0.09293432565217893\n",
      "    At iteration 4000 -> loss: 0.0928507814964905\n",
      "    At iteration 4100 -> loss: 0.09283366071877587\n",
      "    At iteration 4200 -> loss: 0.09285086996559244\n",
      "    At iteration 4300 -> loss: 0.09279644661204615\n",
      "    At iteration 4400 -> loss: 0.09276985464869665\n",
      "    At iteration 4500 -> loss: 0.0927386990721248\n",
      "    At iteration 4600 -> loss: 0.09265633333547962\n",
      "    At iteration 4700 -> loss: 0.09261328106362787\n",
      "    At iteration 4800 -> loss: 0.0929096270763923\n",
      "    At iteration 4900 -> loss: 0.09292900076968631\n",
      "    At iteration 5000 -> loss: 0.09285664869371417\n",
      "    At iteration 5100 -> loss: 0.09285437023705115\n",
      "    At iteration 5200 -> loss: 0.0928661263775891\n",
      "    At iteration 5300 -> loss: 0.09285784168918794\n",
      "    At iteration 5400 -> loss: 0.0927984017776686\n",
      "    At iteration 5500 -> loss: 0.09273592812625764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 5600 -> loss: 0.09267309458867047\n",
      "    At iteration 5700 -> loss: 0.0926769454118767\n",
      "    At iteration 5800 -> loss: 0.09266905477328327\n",
      "    At iteration 5900 -> loss: 0.09282244402884289\n",
      "    At iteration 6000 -> loss: 0.09282360670056894\n",
      "    At iteration 6100 -> loss: 0.09276739163778525\n",
      "    At iteration 6200 -> loss: 0.09277437471540463\n",
      "    At iteration 6300 -> loss: 0.0927622386164856\n",
      "    At iteration 6400 -> loss: 0.0927147783897017\n",
      "    At iteration 6500 -> loss: 0.09273649327253158\n",
      "    At iteration 6600 -> loss: 0.09270637108232913\n",
      "    At iteration 6700 -> loss: 0.09289234038743951\n",
      "    At iteration 6800 -> loss: 0.09296513818887353\n",
      "    At iteration 6900 -> loss: 0.09299165397484746\n",
      "    At iteration 7000 -> loss: 0.09297415908019566\n",
      "    At iteration 7100 -> loss: 0.09291713123909276\n",
      "    At iteration 7200 -> loss: 0.09294463541584441\n",
      "    At iteration 7300 -> loss: 0.09290139075500506\n",
      "    At iteration 7400 -> loss: 0.0928619667798946\n",
      "    At iteration 7500 -> loss: 0.09298068987215825\n",
      "    At iteration 7600 -> loss: 0.09296583498136346\n",
      "    At iteration 7700 -> loss: 0.09291683573156882\n",
      "    At iteration 7800 -> loss: 0.09295585047140643\n",
      "    At iteration 7900 -> loss: 0.09291679084256346\n",
      "    At iteration 8000 -> loss: 0.09326821860036812\n",
      "    At iteration 8100 -> loss: 0.09325082129041366\n",
      "    At iteration 8200 -> loss: 0.09333482247511206\n",
      "    At iteration 8300 -> loss: 0.09334892629896469\n",
      "    At iteration 8400 -> loss: 0.09331189328937137\n",
      "    At iteration 8500 -> loss: 0.09328926236784207\n",
      "    At iteration 8600 -> loss: 0.09325817708817288\n",
      "    At iteration 8700 -> loss: 0.09323812911988769\n",
      "    At iteration 8800 -> loss: 0.09324986198296538\n",
      "    At iteration 8900 -> loss: 0.09323127894326595\n",
      "    At iteration 9000 -> loss: 0.09321261393574967\n",
      "    At iteration 9100 -> loss: 0.09318281733131599\n",
      "    At iteration 9200 -> loss: 0.09324695343699337\n",
      "    At iteration 9300 -> loss: 0.09320871985616548\n",
      "    At iteration 9400 -> loss: 0.09318550836225564\n",
      "    At iteration 9500 -> loss: 0.0931656150836171\n",
      "    At iteration 9600 -> loss: 0.09312426499147093\n",
      "    At iteration 9700 -> loss: 0.09311249088364391\n",
      "    At iteration 9800 -> loss: 0.09306053553888538\n",
      "    At iteration 9900 -> loss: 0.09311793060712777\n",
      "    At iteration 10000 -> loss: 0.09308718270403554\n",
      "    At iteration 10100 -> loss: 0.09318897122741965\n",
      "    At iteration 10200 -> loss: 0.09317220521242377\n",
      "    At iteration 10300 -> loss: 0.09319165518363898\n",
      "    At iteration 10400 -> loss: 0.0931703190145529\n",
      "    At iteration 10500 -> loss: 0.09316643553480498\n",
      "    At iteration 10600 -> loss: 0.093156129900874\n",
      "    At iteration 10700 -> loss: 0.09314117869109687\n",
      "    At iteration 10800 -> loss: 0.09312762722545793\n",
      "    At iteration 10900 -> loss: 0.09313975047421713\n",
      "    At iteration 11000 -> loss: 0.09313827500043792\n",
      "    At iteration 11100 -> loss: 0.09318288894794498\n",
      "    At iteration 11200 -> loss: 0.09322266226959544\n",
      "    At iteration 11300 -> loss: 0.09319119580668667\n",
      "    At iteration 11400 -> loss: 0.09315524326107702\n",
      "    At iteration 11500 -> loss: 0.09313447728154857\n",
      "    At iteration 11600 -> loss: 0.09312406135170614\n",
      "    At iteration 11700 -> loss: 0.09311768400864445\n",
      "    At iteration 11800 -> loss: 0.09312711107239664\n",
      "    At iteration 11900 -> loss: 0.09311267398129039\n",
      "    At iteration 12000 -> loss: 0.09310147619422428\n",
      "    At iteration 12100 -> loss: 0.093090549640963\n",
      "    At iteration 12200 -> loss: 0.09309047184888611\n",
      "    At iteration 12300 -> loss: 0.09307458943502679\n",
      "    At iteration 12400 -> loss: 0.09318341537141098\n",
      "    At iteration 12500 -> loss: 0.09314789089114728\n",
      "    At iteration 12600 -> loss: 0.093142817358205\n",
      "    At iteration 12700 -> loss: 0.09315709035337116\n",
      "    At iteration 12800 -> loss: 0.09311935983463322\n",
      "    At iteration 12900 -> loss: 0.09314306824717083\n",
      "    At iteration 13000 -> loss: 0.09313969608543074\n",
      "    At iteration 13100 -> loss: 0.09313600933999994\n",
      "    At iteration 13200 -> loss: 0.09310043705253669\n",
      "    At iteration 13300 -> loss: 0.0930607440618438\n",
      "    At iteration 13400 -> loss: 0.09304236938254076\n",
      "    At iteration 13500 -> loss: 0.09302066107887053\n",
      "    At iteration 13600 -> loss: 0.09299735608487847\n",
      "Staring Epoch 132\n",
      "    At iteration 0 -> loss: 0.08790469518862665\n",
      "    At iteration 100 -> loss: 0.09239695596499929\n",
      "    At iteration 200 -> loss: 0.09173023721420104\n",
      "    At iteration 300 -> loss: 0.09176499110760807\n",
      "    At iteration 400 -> loss: 0.09383897819106443\n",
      "    At iteration 500 -> loss: 0.09516732844002165\n",
      "    At iteration 600 -> loss: 0.0955878521862095\n",
      "    At iteration 700 -> loss: 0.09663236552876181\n",
      "    At iteration 800 -> loss: 0.09608036105288033\n",
      "    At iteration 900 -> loss: 0.09565231008165986\n",
      "    At iteration 1000 -> loss: 0.09558435625339229\n",
      "    At iteration 1100 -> loss: 0.09501529873017579\n",
      "    At iteration 1200 -> loss: 0.09482018276745302\n",
      "    At iteration 1300 -> loss: 0.09453711272301579\n",
      "    At iteration 1400 -> loss: 0.09439824357064149\n",
      "    At iteration 1500 -> loss: 0.09431002335491226\n",
      "    At iteration 1600 -> loss: 0.09414058810522968\n",
      "    At iteration 1700 -> loss: 0.09377209237361418\n",
      "    At iteration 1800 -> loss: 0.09372025761480193\n",
      "    At iteration 1900 -> loss: 0.09371487610168583\n",
      "    At iteration 2000 -> loss: 0.0937471475278403\n",
      "    At iteration 2100 -> loss: 0.09372295514540699\n",
      "    At iteration 2200 -> loss: 0.0935454830054296\n",
      "    At iteration 2300 -> loss: 0.09398258554022947\n",
      "    At iteration 2400 -> loss: 0.09391745488118063\n",
      "    At iteration 2500 -> loss: 0.0937345724547549\n",
      "    At iteration 2600 -> loss: 0.09353891817860134\n",
      "    At iteration 2700 -> loss: 0.09335308850618014\n",
      "    At iteration 2800 -> loss: 0.09356381742692617\n",
      "    At iteration 2900 -> loss: 0.0935777717856275\n",
      "    At iteration 3000 -> loss: 0.0935958342369407\n",
      "    At iteration 3100 -> loss: 0.09350775271979743\n",
      "    At iteration 3200 -> loss: 0.09335128056059934\n",
      "    At iteration 3300 -> loss: 0.09325044753102377\n",
      "    At iteration 3400 -> loss: 0.0931745718758114\n",
      "    At iteration 3500 -> loss: 0.09329928732957787\n",
      "    At iteration 3600 -> loss: 0.09325021626008854\n",
      "    At iteration 3700 -> loss: 0.09317798851508594\n",
      "    At iteration 3800 -> loss: 0.09307551215744618\n",
      "    At iteration 3900 -> loss: 0.09300463429613576\n",
      "    At iteration 4000 -> loss: 0.0932340529736284\n",
      "    At iteration 4100 -> loss: 0.09324953973368592\n",
      "    At iteration 4200 -> loss: 0.09317388707154926\n",
      "    At iteration 4300 -> loss: 0.09311508970803895\n",
      "    At iteration 4400 -> loss: 0.0930171776181961\n",
      "    At iteration 4500 -> loss: 0.0929696505696172\n",
      "    At iteration 4600 -> loss: 0.09301022852986605\n",
      "    At iteration 4700 -> loss: 0.09304436398220822\n",
      "    At iteration 4800 -> loss: 0.0936527867840497\n",
      "    At iteration 4900 -> loss: 0.09361138376248072\n",
      "    At iteration 5000 -> loss: 0.09363504954631645\n",
      "    At iteration 5100 -> loss: 0.09357446241378896\n",
      "    At iteration 5200 -> loss: 0.0935536610314906\n",
      "    At iteration 5300 -> loss: 0.09348395952771375\n",
      "    At iteration 5400 -> loss: 0.09341407717796552\n",
      "    At iteration 5500 -> loss: 0.09338559115921134\n",
      "    At iteration 5600 -> loss: 0.09329696416161208\n",
      "    At iteration 5700 -> loss: 0.09326154295007631\n",
      "    At iteration 5800 -> loss: 0.09325213454070672\n",
      "    At iteration 5900 -> loss: 0.09318871064601153\n",
      "    At iteration 6000 -> loss: 0.09315400337959492\n",
      "    At iteration 6100 -> loss: 0.09314447994661586\n",
      "    At iteration 6200 -> loss: 0.09324169187712382\n",
      "    At iteration 6300 -> loss: 0.09319558376743863\n",
      "    At iteration 6400 -> loss: 0.09312072126114794\n",
      "    At iteration 6500 -> loss: 0.09310664260275259\n",
      "    At iteration 6600 -> loss: 0.09321742964058526\n",
      "    At iteration 6700 -> loss: 0.09322972001158471\n",
      "    At iteration 6800 -> loss: 0.09324720215886681\n",
      "    At iteration 6900 -> loss: 0.09322788648446798\n",
      "    At iteration 7000 -> loss: 0.09328441975454439\n",
      "    At iteration 7100 -> loss: 0.09326035168389188\n",
      "    At iteration 7200 -> loss: 0.09326704426319811\n",
      "    At iteration 7300 -> loss: 0.0932639754854836\n",
      "    At iteration 7400 -> loss: 0.09322885015518018\n",
      "    At iteration 7500 -> loss: 0.09321282247387089\n",
      "    At iteration 7600 -> loss: 0.09321443723210375\n",
      "    At iteration 7700 -> loss: 0.09323217813302556\n",
      "    At iteration 7800 -> loss: 0.09322003003334818\n",
      "    At iteration 7900 -> loss: 0.09317119655674164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 8000 -> loss: 0.09315938111411187\n",
      "    At iteration 8100 -> loss: 0.09322872162661142\n",
      "    At iteration 8200 -> loss: 0.09331894156432904\n",
      "    At iteration 8300 -> loss: 0.09332849181869014\n",
      "    At iteration 8400 -> loss: 0.09331909118745467\n",
      "    At iteration 8500 -> loss: 0.09327126544039104\n",
      "    At iteration 8600 -> loss: 0.09328593988444334\n",
      "    At iteration 8700 -> loss: 0.09326158405217176\n",
      "    At iteration 8800 -> loss: 0.09330341024665177\n",
      "    At iteration 8900 -> loss: 0.09328410859600489\n",
      "    At iteration 9000 -> loss: 0.0932237225974446\n",
      "    At iteration 9100 -> loss: 0.09324472630866809\n",
      "    At iteration 9200 -> loss: 0.09321888373778778\n",
      "    At iteration 9300 -> loss: 0.0932106637654493\n",
      "    At iteration 9400 -> loss: 0.09333513951494166\n",
      "    At iteration 9500 -> loss: 0.09330950586670858\n",
      "    At iteration 9600 -> loss: 0.09331313217023934\n",
      "    At iteration 9700 -> loss: 0.0933744325069944\n",
      "    At iteration 9800 -> loss: 0.09333742695601753\n",
      "    At iteration 9900 -> loss: 0.09334383967372091\n",
      "    At iteration 10000 -> loss: 0.09336412814447745\n",
      "    At iteration 10100 -> loss: 0.09336522973102804\n",
      "    At iteration 10200 -> loss: 0.09333767478908284\n",
      "    At iteration 10300 -> loss: 0.09331995141981081\n",
      "    At iteration 10400 -> loss: 0.09328241120438921\n",
      "    At iteration 10500 -> loss: 0.09325781104305036\n",
      "    At iteration 10600 -> loss: 0.09322709193052817\n",
      "    At iteration 10700 -> loss: 0.09320814065157859\n",
      "    At iteration 10800 -> loss: 0.09318267394155624\n",
      "    At iteration 10900 -> loss: 0.09317356150130478\n",
      "    At iteration 11000 -> loss: 0.09315854718025204\n",
      "    At iteration 11100 -> loss: 0.09317267665115445\n",
      "    At iteration 11200 -> loss: 0.09319400207472897\n",
      "    At iteration 11300 -> loss: 0.09315651146821759\n",
      "    At iteration 11400 -> loss: 0.09313859834962006\n",
      "    At iteration 11500 -> loss: 0.09314100224388754\n",
      "    At iteration 11600 -> loss: 0.09310833473960074\n",
      "    At iteration 11700 -> loss: 0.09309307242031761\n",
      "    At iteration 11800 -> loss: 0.0930992304588665\n",
      "    At iteration 11900 -> loss: 0.09317114688842422\n",
      "    At iteration 12000 -> loss: 0.09312637380076705\n",
      "    At iteration 12100 -> loss: 0.09308978190315097\n",
      "    At iteration 12200 -> loss: 0.09307637859190203\n",
      "    At iteration 12300 -> loss: 0.09307080485702138\n",
      "    At iteration 12400 -> loss: 0.09306047151242429\n",
      "    At iteration 12500 -> loss: 0.09304188674058743\n",
      "    At iteration 12600 -> loss: 0.09305524512757792\n",
      "    At iteration 12700 -> loss: 0.09303416844327371\n",
      "    At iteration 12800 -> loss: 0.09305991675151473\n",
      "    At iteration 12900 -> loss: 0.09305786554234459\n",
      "    At iteration 13000 -> loss: 0.09304034706303459\n",
      "    At iteration 13100 -> loss: 0.09302412593732501\n",
      "    At iteration 13200 -> loss: 0.09300555737712313\n",
      "    At iteration 13300 -> loss: 0.0929743646696586\n",
      "    At iteration 13400 -> loss: 0.09296098859040877\n",
      "    At iteration 13500 -> loss: 0.09294979552374885\n",
      "    At iteration 13600 -> loss: 0.09301788065930348\n",
      "Staring Epoch 133\n",
      "    At iteration 0 -> loss: 0.09343625558540225\n",
      "    At iteration 100 -> loss: 0.08956338780740507\n",
      "    At iteration 200 -> loss: 0.08981688179905029\n",
      "    At iteration 300 -> loss: 0.09009530253154294\n",
      "    At iteration 400 -> loss: 0.08987540283992987\n",
      "    At iteration 500 -> loss: 0.08992871019150649\n",
      "    At iteration 600 -> loss: 0.09037572802468063\n",
      "    At iteration 700 -> loss: 0.09084029922490301\n",
      "    At iteration 800 -> loss: 0.09151424646647047\n",
      "    At iteration 900 -> loss: 0.09172799537080616\n",
      "    At iteration 1000 -> loss: 0.09145187386995658\n",
      "    At iteration 1100 -> loss: 0.09153680095320556\n",
      "    At iteration 1200 -> loss: 0.09171710154383707\n",
      "    At iteration 1300 -> loss: 0.09163713213430676\n",
      "    At iteration 1400 -> loss: 0.09167102929874171\n",
      "    At iteration 1500 -> loss: 0.09148513357067879\n",
      "    At iteration 1600 -> loss: 0.0915585374604103\n",
      "    At iteration 1700 -> loss: 0.09188020287048207\n",
      "    At iteration 1800 -> loss: 0.09224666199937537\n",
      "    At iteration 1900 -> loss: 0.09251091068637562\n",
      "    At iteration 2000 -> loss: 0.092454042425301\n",
      "    At iteration 2100 -> loss: 0.09252117350330434\n",
      "    At iteration 2200 -> loss: 0.09244268204155774\n",
      "    At iteration 2300 -> loss: 0.09266737604997134\n",
      "    At iteration 2400 -> loss: 0.09258259944286189\n",
      "    At iteration 2500 -> loss: 0.09294593139414348\n",
      "    At iteration 2600 -> loss: 0.09306899064649478\n",
      "    At iteration 2700 -> loss: 0.09297214502775586\n",
      "    At iteration 2800 -> loss: 0.09281697483069525\n",
      "    At iteration 2900 -> loss: 0.09317135231528138\n",
      "    At iteration 3000 -> loss: 0.09308402595472659\n",
      "    At iteration 3100 -> loss: 0.09302403719367902\n",
      "    At iteration 3200 -> loss: 0.09299342284133566\n",
      "    At iteration 3300 -> loss: 0.09294824460756407\n",
      "    At iteration 3400 -> loss: 0.09296576658731705\n",
      "    At iteration 3500 -> loss: 0.09294691791773367\n",
      "    At iteration 3600 -> loss: 0.09284335075923975\n",
      "    At iteration 3700 -> loss: 0.09282031575799748\n",
      "    At iteration 3800 -> loss: 0.09276336310410634\n",
      "    At iteration 3900 -> loss: 0.09269067624581606\n",
      "    At iteration 4000 -> loss: 0.0926495468979784\n",
      "    At iteration 4100 -> loss: 0.09276864181970391\n",
      "    At iteration 4200 -> loss: 0.09290560542339052\n",
      "    At iteration 4300 -> loss: 0.09285694127801335\n",
      "    At iteration 4400 -> loss: 0.09286874234454104\n",
      "    At iteration 4500 -> loss: 0.09290681135925237\n",
      "    At iteration 4600 -> loss: 0.09290214932401124\n",
      "    At iteration 4700 -> loss: 0.09288976866110024\n",
      "    At iteration 4800 -> loss: 0.0928250317425092\n",
      "    At iteration 4900 -> loss: 0.09284877749344429\n",
      "    At iteration 5000 -> loss: 0.09285329306197873\n",
      "    At iteration 5100 -> loss: 0.09287098951687171\n",
      "    At iteration 5200 -> loss: 0.09302448057953122\n",
      "    At iteration 5300 -> loss: 0.0930022747970937\n",
      "    At iteration 5400 -> loss: 0.09297866601841068\n",
      "    At iteration 5500 -> loss: 0.09292398206865793\n",
      "    At iteration 5600 -> loss: 0.09304835846666733\n",
      "    At iteration 5700 -> loss: 0.09301292448712664\n",
      "    At iteration 5800 -> loss: 0.09295423420920135\n",
      "    At iteration 5900 -> loss: 0.09292593963253913\n",
      "    At iteration 6000 -> loss: 0.09286171274002375\n",
      "    At iteration 6100 -> loss: 0.09299400059317113\n",
      "    At iteration 6200 -> loss: 0.09300451181276212\n",
      "    At iteration 6300 -> loss: 0.09299992711025497\n",
      "    At iteration 6400 -> loss: 0.09304189013820596\n",
      "    At iteration 6500 -> loss: 0.09304096002521203\n",
      "    At iteration 6600 -> loss: 0.09306816370046433\n",
      "    At iteration 6700 -> loss: 0.09300594145649121\n",
      "    At iteration 6800 -> loss: 0.09299631695137124\n",
      "    At iteration 6900 -> loss: 0.0929994393571586\n",
      "    At iteration 7000 -> loss: 0.0930090209417568\n",
      "    At iteration 7100 -> loss: 0.09299811665268298\n",
      "    At iteration 7200 -> loss: 0.09296698383955627\n",
      "    At iteration 7300 -> loss: 0.09297914588211977\n",
      "    At iteration 7400 -> loss: 0.09294807986531792\n",
      "    At iteration 7500 -> loss: 0.09292758070464076\n",
      "    At iteration 7600 -> loss: 0.09291904322282331\n",
      "    At iteration 7700 -> loss: 0.09299271079391892\n",
      "    At iteration 7800 -> loss: 0.09295733822797297\n",
      "    At iteration 7900 -> loss: 0.09292157399175513\n",
      "    At iteration 8000 -> loss: 0.09288728884677895\n",
      "    At iteration 8100 -> loss: 0.09286523766133988\n",
      "    At iteration 8200 -> loss: 0.09283702075197041\n",
      "    At iteration 8300 -> loss: 0.09283292688416166\n",
      "    At iteration 8400 -> loss: 0.09283983126716543\n",
      "    At iteration 8500 -> loss: 0.09282622588441101\n",
      "    At iteration 8600 -> loss: 0.09278205900473631\n",
      "    At iteration 8700 -> loss: 0.09288676716362616\n",
      "    At iteration 8800 -> loss: 0.09287794238889303\n",
      "    At iteration 8900 -> loss: 0.0928536546400853\n",
      "    At iteration 9000 -> loss: 0.09286629246275359\n",
      "    At iteration 9100 -> loss: 0.09291342016179614\n",
      "    At iteration 9200 -> loss: 0.09288513838610576\n",
      "    At iteration 9300 -> loss: 0.09286691351157578\n",
      "    At iteration 9400 -> loss: 0.09287435559444764\n",
      "    At iteration 9500 -> loss: 0.09288797776952172\n",
      "    At iteration 9600 -> loss: 0.09286772113416472\n",
      "    At iteration 9700 -> loss: 0.0928585535523811\n",
      "    At iteration 9800 -> loss: 0.09287083538446012\n",
      "    At iteration 9900 -> loss: 0.09286045807532059\n",
      "    At iteration 10000 -> loss: 0.09285079816058227\n",
      "    At iteration 10100 -> loss: 0.09284839363763976\n",
      "    At iteration 10200 -> loss: 0.09280327930020565\n",
      "    At iteration 10300 -> loss: 0.09282860894172636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 10400 -> loss: 0.09283066345476779\n",
      "    At iteration 10500 -> loss: 0.09279908799999849\n",
      "    At iteration 10600 -> loss: 0.09280227282498904\n",
      "    At iteration 10700 -> loss: 0.09283947651753721\n",
      "    At iteration 10800 -> loss: 0.09283405866333998\n",
      "    At iteration 10900 -> loss: 0.09280209614390826\n",
      "    At iteration 11000 -> loss: 0.09277807908892291\n",
      "    At iteration 11100 -> loss: 0.0928487432952054\n",
      "    At iteration 11200 -> loss: 0.09282915944131445\n",
      "    At iteration 11300 -> loss: 0.09280341363189093\n",
      "    At iteration 11400 -> loss: 0.09280263479373682\n",
      "    At iteration 11500 -> loss: 0.0928034526320517\n",
      "    At iteration 11600 -> loss: 0.09282615977365205\n",
      "    At iteration 11700 -> loss: 0.09280236630983973\n",
      "    At iteration 11800 -> loss: 0.09277904111836231\n",
      "    At iteration 11900 -> loss: 0.09277270202713533\n",
      "    At iteration 12000 -> loss: 0.09277623593236674\n",
      "    At iteration 12100 -> loss: 0.09281358740119898\n",
      "    At iteration 12200 -> loss: 0.0928137006232842\n",
      "    At iteration 12300 -> loss: 0.09277743767095625\n",
      "    At iteration 12400 -> loss: 0.0927510263721586\n",
      "    At iteration 12500 -> loss: 0.09275731275506274\n",
      "    At iteration 12600 -> loss: 0.09274354577187932\n",
      "    At iteration 12700 -> loss: 0.09272391059639291\n",
      "    At iteration 12800 -> loss: 0.09304071531276875\n",
      "    At iteration 12900 -> loss: 0.0930283735349836\n",
      "    At iteration 13000 -> loss: 0.09302569520417715\n",
      "    At iteration 13100 -> loss: 0.0930038551565571\n",
      "    At iteration 13200 -> loss: 0.09303646239693876\n",
      "    At iteration 13300 -> loss: 0.09305037377827627\n",
      "    At iteration 13400 -> loss: 0.09304409355421503\n",
      "    At iteration 13500 -> loss: 0.09302036976252707\n",
      "    At iteration 13600 -> loss: 0.09302738894325523\n",
      "Staring Epoch 134\n",
      "    At iteration 0 -> loss: 0.10080420691519976\n",
      "    At iteration 100 -> loss: 0.09265217961382853\n",
      "    At iteration 200 -> loss: 0.09488245050159565\n",
      "    At iteration 300 -> loss: 0.09709049788117277\n",
      "    At iteration 400 -> loss: 0.09743364549520832\n",
      "    At iteration 500 -> loss: 0.0958798633401157\n",
      "    At iteration 600 -> loss: 0.0964018572230452\n",
      "    At iteration 700 -> loss: 0.09555383559887769\n",
      "    At iteration 800 -> loss: 0.09541607305834232\n",
      "    At iteration 900 -> loss: 0.09514157307158755\n",
      "    At iteration 1000 -> loss: 0.09504011697186084\n",
      "    At iteration 1100 -> loss: 0.0947341128452994\n",
      "    At iteration 1200 -> loss: 0.09439586144551323\n",
      "    At iteration 1300 -> loss: 0.09434957944211565\n",
      "    At iteration 1400 -> loss: 0.09407649764696305\n",
      "    At iteration 1500 -> loss: 0.09391960005080521\n",
      "    At iteration 1600 -> loss: 0.09370127945112117\n",
      "    At iteration 1700 -> loss: 0.09354273176931979\n",
      "    At iteration 1800 -> loss: 0.09357353141311257\n",
      "    At iteration 1900 -> loss: 0.09343050716937629\n",
      "    At iteration 2000 -> loss: 0.09329838180962732\n",
      "    At iteration 2100 -> loss: 0.09312577067121883\n",
      "    At iteration 2200 -> loss: 0.09359909126136833\n",
      "    At iteration 2300 -> loss: 0.09348930080423055\n",
      "    At iteration 2400 -> loss: 0.09362823269944177\n",
      "    At iteration 2500 -> loss: 0.09362741534157884\n",
      "    At iteration 2600 -> loss: 0.09355529999854516\n",
      "    At iteration 2700 -> loss: 0.0936199146302156\n",
      "    At iteration 2800 -> loss: 0.09355433079292234\n",
      "    At iteration 2900 -> loss: 0.09357126331628368\n",
      "    At iteration 3000 -> loss: 0.09354603327540148\n",
      "    At iteration 3100 -> loss: 0.09354200136636967\n",
      "    At iteration 3200 -> loss: 0.0934932855690074\n",
      "    At iteration 3300 -> loss: 0.09351345648218724\n",
      "    At iteration 3400 -> loss: 0.09341343419915807\n",
      "    At iteration 3500 -> loss: 0.0933922757659809\n",
      "    At iteration 3600 -> loss: 0.09330315886045079\n",
      "    At iteration 3700 -> loss: 0.09327979775166122\n",
      "    At iteration 3800 -> loss: 0.09328986650807375\n",
      "    At iteration 3900 -> loss: 0.0932616693250959\n",
      "    At iteration 4000 -> loss: 0.09314746424775512\n",
      "    At iteration 4100 -> loss: 0.09309260179522823\n",
      "    At iteration 4200 -> loss: 0.0930731664836514\n",
      "    At iteration 4300 -> loss: 0.09302167475473193\n",
      "    At iteration 4400 -> loss: 0.09315976647336156\n",
      "    At iteration 4500 -> loss: 0.0932069804535003\n",
      "    At iteration 4600 -> loss: 0.09314224191598698\n",
      "    At iteration 4700 -> loss: 0.09305081737808528\n",
      "    At iteration 4800 -> loss: 0.09301041401639457\n",
      "    At iteration 4900 -> loss: 0.09296740797953766\n",
      "    At iteration 5000 -> loss: 0.09296115016030596\n",
      "    At iteration 5100 -> loss: 0.09288776368718159\n",
      "    At iteration 5200 -> loss: 0.09287235001898972\n",
      "    At iteration 5300 -> loss: 0.09279607715872833\n",
      "    At iteration 5400 -> loss: 0.0928848997745255\n",
      "    At iteration 5500 -> loss: 0.09291374178455439\n",
      "    At iteration 5600 -> loss: 0.09290914703917597\n",
      "    At iteration 5700 -> loss: 0.09295375619899085\n",
      "    At iteration 5800 -> loss: 0.09288782341818334\n",
      "    At iteration 5900 -> loss: 0.09283168316455065\n",
      "    At iteration 6000 -> loss: 0.09276452104892835\n",
      "    At iteration 6100 -> loss: 0.09272146295093942\n",
      "    At iteration 6200 -> loss: 0.09268753814370687\n",
      "    At iteration 6300 -> loss: 0.09270025042709727\n",
      "    At iteration 6400 -> loss: 0.09268162418803529\n",
      "    At iteration 6500 -> loss: 0.0927644688088855\n",
      "    At iteration 6600 -> loss: 0.09270322576957192\n",
      "    At iteration 6700 -> loss: 0.09277134586717115\n",
      "    At iteration 6800 -> loss: 0.09290141459368992\n",
      "    At iteration 6900 -> loss: 0.09292861107627728\n",
      "    At iteration 7000 -> loss: 0.09293117746122195\n",
      "    At iteration 7100 -> loss: 0.09286983385041375\n",
      "    At iteration 7200 -> loss: 0.0928438508375868\n",
      "    At iteration 7300 -> loss: 0.09291883320238152\n",
      "    At iteration 7400 -> loss: 0.0929280030098814\n",
      "    At iteration 7500 -> loss: 0.09292048618197911\n",
      "    At iteration 7600 -> loss: 0.09294679665644859\n",
      "    At iteration 7700 -> loss: 0.09293380628517002\n",
      "    At iteration 7800 -> loss: 0.09289459954658942\n",
      "    At iteration 7900 -> loss: 0.09284582550865675\n",
      "    At iteration 8000 -> loss: 0.09286834790083068\n",
      "    At iteration 8100 -> loss: 0.0928453272353566\n",
      "    At iteration 8200 -> loss: 0.0928333863775557\n",
      "    At iteration 8300 -> loss: 0.09294068318797546\n",
      "    At iteration 8400 -> loss: 0.09300004138035894\n",
      "    At iteration 8500 -> loss: 0.09296733409495314\n",
      "    At iteration 8600 -> loss: 0.09296930786451468\n",
      "    At iteration 8700 -> loss: 0.09293055876639178\n",
      "    At iteration 8800 -> loss: 0.09288830878285771\n",
      "    At iteration 8900 -> loss: 0.09292444920306309\n",
      "    At iteration 9000 -> loss: 0.09289759294859944\n",
      "    At iteration 9100 -> loss: 0.09284868170926322\n",
      "    At iteration 9200 -> loss: 0.0928297296968632\n",
      "    At iteration 9300 -> loss: 0.09317794712355112\n",
      "    At iteration 9400 -> loss: 0.09314810687547405\n",
      "    At iteration 9500 -> loss: 0.09313768282434406\n",
      "    At iteration 9600 -> loss: 0.09313496940234696\n",
      "    At iteration 9700 -> loss: 0.09312349036729316\n",
      "    At iteration 9800 -> loss: 0.09309634460249569\n",
      "    At iteration 9900 -> loss: 0.09307822389899256\n",
      "    At iteration 10000 -> loss: 0.09304891188964179\n",
      "    At iteration 10100 -> loss: 0.09306657909944081\n",
      "    At iteration 10200 -> loss: 0.09305009118354526\n",
      "    At iteration 10300 -> loss: 0.0930383608293003\n",
      "    At iteration 10400 -> loss: 0.0930005183783816\n",
      "    At iteration 10500 -> loss: 0.0930066217271997\n",
      "    At iteration 10600 -> loss: 0.0930089509413696\n",
      "    At iteration 10700 -> loss: 0.09301942398012797\n",
      "    At iteration 10800 -> loss: 0.093014168442748\n",
      "    At iteration 10900 -> loss: 0.09299910646994622\n",
      "    At iteration 11000 -> loss: 0.09298740992486904\n",
      "    At iteration 11100 -> loss: 0.09298689597398568\n",
      "    At iteration 11200 -> loss: 0.0929620887718383\n",
      "    At iteration 11300 -> loss: 0.092928594444529\n",
      "    At iteration 11400 -> loss: 0.09295865339292138\n",
      "    At iteration 11500 -> loss: 0.09293390406268581\n",
      "    At iteration 11600 -> loss: 0.0929218912487641\n",
      "    At iteration 11700 -> loss: 0.09293722037876073\n",
      "    At iteration 11800 -> loss: 0.09298618569773395\n",
      "    At iteration 11900 -> loss: 0.09302256370287262\n",
      "    At iteration 12000 -> loss: 0.09300983887251396\n",
      "    At iteration 12100 -> loss: 0.09298367572059237\n",
      "    At iteration 12200 -> loss: 0.09298066568311174\n",
      "    At iteration 12300 -> loss: 0.09298387435156297\n",
      "    At iteration 12400 -> loss: 0.09296855889616244\n",
      "    At iteration 12500 -> loss: 0.09300737405130481\n",
      "    At iteration 12600 -> loss: 0.09298952818570619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 12700 -> loss: 0.09299865178973993\n",
      "    At iteration 12800 -> loss: 0.09304780013211782\n",
      "    At iteration 12900 -> loss: 0.09300723950826091\n",
      "    At iteration 13000 -> loss: 0.09298725355413591\n",
      "    At iteration 13100 -> loss: 0.09295862067851349\n",
      "    At iteration 13200 -> loss: 0.09296855805787613\n",
      "    At iteration 13300 -> loss: 0.09298334373266638\n",
      "    At iteration 13400 -> loss: 0.09300757820989698\n",
      "    At iteration 13500 -> loss: 0.09299137521259132\n",
      "    At iteration 13600 -> loss: 0.09297232554109525\n",
      "Staring Epoch 135\n",
      "    At iteration 0 -> loss: 0.08431897882837802\n",
      "    At iteration 100 -> loss: 0.09313837910140857\n",
      "    At iteration 200 -> loss: 0.0937002766229135\n",
      "    At iteration 300 -> loss: 0.09329269649966132\n",
      "    At iteration 400 -> loss: 0.09251347295091243\n",
      "    At iteration 500 -> loss: 0.09206746803007507\n",
      "    At iteration 600 -> loss: 0.09180833558570772\n",
      "    At iteration 700 -> loss: 0.09244488168123036\n",
      "    At iteration 800 -> loss: 0.09238132054569058\n",
      "    At iteration 900 -> loss: 0.09217394344997554\n",
      "    At iteration 1000 -> loss: 0.09209535547596841\n",
      "    At iteration 1100 -> loss: 0.09232535143997414\n",
      "    At iteration 1200 -> loss: 0.09225249363447256\n",
      "    At iteration 1300 -> loss: 0.0923714603032935\n",
      "    At iteration 1400 -> loss: 0.09232965403674673\n",
      "    At iteration 1500 -> loss: 0.09220879669208087\n",
      "    At iteration 1600 -> loss: 0.09243487673986697\n",
      "    At iteration 1700 -> loss: 0.09275503851383685\n",
      "    At iteration 1800 -> loss: 0.092624092967114\n",
      "    At iteration 1900 -> loss: 0.0925373066214063\n",
      "    At iteration 2000 -> loss: 0.09297863960704114\n",
      "    At iteration 2100 -> loss: 0.09307342197338254\n",
      "    At iteration 2200 -> loss: 0.09307150255574682\n",
      "    At iteration 2300 -> loss: 0.09302602797916976\n",
      "    At iteration 2400 -> loss: 0.09313476716202689\n",
      "    At iteration 2500 -> loss: 0.09325556036777449\n",
      "    At iteration 2600 -> loss: 0.09319691543050992\n",
      "    At iteration 2700 -> loss: 0.09316707322812899\n",
      "    At iteration 2800 -> loss: 0.093152953784937\n",
      "    At iteration 2900 -> loss: 0.09319440794468349\n",
      "    At iteration 3000 -> loss: 0.09313211628370942\n",
      "    At iteration 3100 -> loss: 0.09312901980604939\n",
      "    At iteration 3200 -> loss: 0.09301669384868108\n",
      "    At iteration 3300 -> loss: 0.09290356381105991\n",
      "    At iteration 3400 -> loss: 0.09300405194877505\n",
      "    At iteration 3500 -> loss: 0.09296358545995748\n",
      "    At iteration 3600 -> loss: 0.0928884292023284\n",
      "    At iteration 3700 -> loss: 0.09283352028923014\n",
      "    At iteration 3800 -> loss: 0.09285845385919449\n",
      "    At iteration 3900 -> loss: 0.09274907427385848\n",
      "    At iteration 4000 -> loss: 0.09276322310393141\n",
      "    At iteration 4100 -> loss: 0.09300370403239375\n",
      "    At iteration 4200 -> loss: 0.09294470255768732\n",
      "    At iteration 4300 -> loss: 0.09282689861709854\n",
      "    At iteration 4400 -> loss: 0.09280704706011468\n",
      "    At iteration 4500 -> loss: 0.09282410248547236\n",
      "    At iteration 4600 -> loss: 0.09279434443554815\n",
      "    At iteration 4700 -> loss: 0.09279477690970117\n",
      "    At iteration 4800 -> loss: 0.09277241648056463\n",
      "    At iteration 4900 -> loss: 0.0927376752297662\n",
      "    At iteration 5000 -> loss: 0.09329310981171159\n",
      "    At iteration 5100 -> loss: 0.09336523159721206\n",
      "    At iteration 5200 -> loss: 0.09338005038940096\n",
      "    At iteration 5300 -> loss: 0.0932965236991119\n",
      "    At iteration 5400 -> loss: 0.09329214512227213\n",
      "    At iteration 5500 -> loss: 0.09326403266385652\n",
      "    At iteration 5600 -> loss: 0.09321799118775208\n",
      "    At iteration 5700 -> loss: 0.09314248256755137\n",
      "    At iteration 5800 -> loss: 0.09309633938093455\n",
      "    At iteration 5900 -> loss: 0.09304929466526789\n",
      "    At iteration 6000 -> loss: 0.09306148652407592\n",
      "    At iteration 6100 -> loss: 0.09300041414590718\n",
      "    At iteration 6200 -> loss: 0.09301693659294816\n",
      "    At iteration 6300 -> loss: 0.09296451227582142\n",
      "    At iteration 6400 -> loss: 0.0929747753929737\n",
      "    At iteration 6500 -> loss: 0.09310894748896521\n",
      "    At iteration 6600 -> loss: 0.09304594767829086\n",
      "    At iteration 6700 -> loss: 0.09303713957237704\n",
      "    At iteration 6800 -> loss: 0.0930330754407112\n",
      "    At iteration 6900 -> loss: 0.09301364049614769\n",
      "    At iteration 7000 -> loss: 0.09301517610074156\n",
      "    At iteration 7100 -> loss: 0.0930005137905443\n",
      "    At iteration 7200 -> loss: 0.09295010608398774\n",
      "    At iteration 7300 -> loss: 0.09293759113758693\n",
      "    At iteration 7400 -> loss: 0.09288492926883031\n",
      "    At iteration 7500 -> loss: 0.0928371834731539\n",
      "    At iteration 7600 -> loss: 0.09282139398949081\n",
      "    At iteration 7700 -> loss: 0.09286299969242702\n",
      "    At iteration 7800 -> loss: 0.09289937420976808\n",
      "    At iteration 7900 -> loss: 0.09296770690967238\n",
      "    At iteration 8000 -> loss: 0.09297434177774694\n",
      "    At iteration 8100 -> loss: 0.0929482804767469\n",
      "    At iteration 8200 -> loss: 0.09292411370458485\n",
      "    At iteration 8300 -> loss: 0.09292198979630885\n",
      "    At iteration 8400 -> loss: 0.09294226451624343\n",
      "    At iteration 8500 -> loss: 0.0929048243370874\n",
      "    At iteration 8600 -> loss: 0.09288798713072065\n",
      "    At iteration 8700 -> loss: 0.09292169627503448\n",
      "    At iteration 8800 -> loss: 0.09291696972725222\n",
      "    At iteration 8900 -> loss: 0.09291967759886573\n",
      "    At iteration 9000 -> loss: 0.09287689529658658\n",
      "    At iteration 9100 -> loss: 0.09288645957005541\n",
      "    At iteration 9200 -> loss: 0.09300620861198879\n",
      "    At iteration 9300 -> loss: 0.09299620310343962\n",
      "    At iteration 9400 -> loss: 0.09311565527243022\n",
      "    At iteration 9500 -> loss: 0.0930933430508778\n",
      "    At iteration 9600 -> loss: 0.09304789172783107\n",
      "    At iteration 9700 -> loss: 0.09301733151905586\n",
      "    At iteration 9800 -> loss: 0.09301592482754839\n",
      "    At iteration 9900 -> loss: 0.09297550703993533\n",
      "    At iteration 10000 -> loss: 0.09298756692319221\n",
      "    At iteration 10100 -> loss: 0.09294491430055156\n",
      "    At iteration 10200 -> loss: 0.09293553628048462\n",
      "    At iteration 10300 -> loss: 0.09298717461694034\n",
      "    At iteration 10400 -> loss: 0.09297048878804758\n",
      "    At iteration 10500 -> loss: 0.09294213212908406\n",
      "    At iteration 10600 -> loss: 0.09292644895770652\n",
      "    At iteration 10700 -> loss: 0.09292748472080337\n",
      "    At iteration 10800 -> loss: 0.0929864903112974\n",
      "    At iteration 10900 -> loss: 0.0929474255796655\n",
      "    At iteration 11000 -> loss: 0.09293742244124809\n",
      "    At iteration 11100 -> loss: 0.09291490761164918\n",
      "    At iteration 11200 -> loss: 0.09290491270733428\n",
      "    At iteration 11300 -> loss: 0.0928963814070902\n",
      "    At iteration 11400 -> loss: 0.0928707860822615\n",
      "    At iteration 11500 -> loss: 0.09291114233112018\n",
      "    At iteration 11600 -> loss: 0.09289128575973428\n",
      "    At iteration 11700 -> loss: 0.09287555615172911\n",
      "    At iteration 11800 -> loss: 0.09287054292568457\n",
      "    At iteration 11900 -> loss: 0.09295848809871383\n",
      "    At iteration 12000 -> loss: 0.09293352395542319\n",
      "    At iteration 12100 -> loss: 0.09294988838967382\n",
      "    At iteration 12200 -> loss: 0.09296477618195466\n",
      "    At iteration 12300 -> loss: 0.09304833738628081\n",
      "    At iteration 12400 -> loss: 0.09303298675676232\n",
      "    At iteration 12500 -> loss: 0.09302156538138222\n",
      "    At iteration 12600 -> loss: 0.09300685803355714\n",
      "    At iteration 12700 -> loss: 0.09304828667470252\n",
      "    At iteration 12800 -> loss: 0.09301879686641643\n",
      "    At iteration 12900 -> loss: 0.09306115940511439\n",
      "    At iteration 13000 -> loss: 0.09306478245202437\n",
      "    At iteration 13100 -> loss: 0.09303165651330286\n",
      "    At iteration 13200 -> loss: 0.09301048574822952\n",
      "    At iteration 13300 -> loss: 0.09304216872275152\n",
      "    At iteration 13400 -> loss: 0.09304603257784752\n",
      "    At iteration 13500 -> loss: 0.09301781295050421\n",
      "    At iteration 13600 -> loss: 0.09301350421029685\n",
      "Staring Epoch 136\n",
      "    At iteration 0 -> loss: 0.0907569054979831\n",
      "    At iteration 100 -> loss: 0.09129165123121462\n",
      "    At iteration 200 -> loss: 0.09106057891918838\n",
      "    At iteration 300 -> loss: 0.09048255785586531\n",
      "    At iteration 400 -> loss: 0.09100981105411116\n",
      "    At iteration 500 -> loss: 0.0910561619928477\n",
      "    At iteration 600 -> loss: 0.09617645808463857\n",
      "    At iteration 700 -> loss: 0.0953348516980235\n",
      "    At iteration 800 -> loss: 0.09551951493494237\n",
      "    At iteration 900 -> loss: 0.09482824820120238\n",
      "    At iteration 1000 -> loss: 0.09441683800268998\n",
      "    At iteration 1100 -> loss: 0.09410867934887207\n",
      "    At iteration 1200 -> loss: 0.09471105914560139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 1300 -> loss: 0.0944213382985714\n",
      "    At iteration 1400 -> loss: 0.09450588806508362\n",
      "    At iteration 1500 -> loss: 0.09424376821419318\n",
      "    At iteration 1600 -> loss: 0.09411941069989359\n",
      "    At iteration 1700 -> loss: 0.09390742764971581\n",
      "    At iteration 1800 -> loss: 0.09396216870745569\n",
      "    At iteration 1900 -> loss: 0.09389437800209735\n",
      "    At iteration 2000 -> loss: 0.09374861643064428\n",
      "    At iteration 2100 -> loss: 0.09362623298312017\n",
      "    At iteration 2200 -> loss: 0.09377094153058167\n",
      "    At iteration 2300 -> loss: 0.09361427005446629\n",
      "    At iteration 2400 -> loss: 0.09351402185330876\n",
      "    At iteration 2500 -> loss: 0.09346858449242279\n",
      "    At iteration 2600 -> loss: 0.09334023788419589\n",
      "    At iteration 2700 -> loss: 0.09370859104076\n",
      "    At iteration 2800 -> loss: 0.09354522742371091\n",
      "    At iteration 2900 -> loss: 0.09338037623229548\n",
      "    At iteration 3000 -> loss: 0.09323449997279741\n",
      "    At iteration 3100 -> loss: 0.09337999754492446\n",
      "    At iteration 3200 -> loss: 0.09334120918019581\n",
      "    At iteration 3300 -> loss: 0.09332383293536259\n",
      "    At iteration 3400 -> loss: 0.0932020407552278\n",
      "    At iteration 3500 -> loss: 0.0932766664175176\n",
      "    At iteration 3600 -> loss: 0.09337818724283908\n",
      "    At iteration 3700 -> loss: 0.09343497568605942\n",
      "    At iteration 3800 -> loss: 0.09345840546268051\n",
      "    At iteration 3900 -> loss: 0.09354200764819047\n",
      "    At iteration 4000 -> loss: 0.09347605212498575\n",
      "    At iteration 4100 -> loss: 0.093528216180346\n",
      "    At iteration 4200 -> loss: 0.09358467586340873\n",
      "    At iteration 4300 -> loss: 0.09367701534494127\n",
      "    At iteration 4400 -> loss: 0.0936306553966527\n",
      "    At iteration 4500 -> loss: 0.09353666265639227\n",
      "    At iteration 4600 -> loss: 0.0935042218535848\n",
      "    At iteration 4700 -> loss: 0.09344915702981381\n",
      "    At iteration 4800 -> loss: 0.0936214360968293\n",
      "    At iteration 4900 -> loss: 0.09356552658450662\n",
      "    At iteration 5000 -> loss: 0.09358925063332403\n",
      "    At iteration 5100 -> loss: 0.09362158587035514\n",
      "    At iteration 5200 -> loss: 0.09355448787775043\n",
      "    At iteration 5300 -> loss: 0.09353133916657483\n",
      "    At iteration 5400 -> loss: 0.09347186182107846\n",
      "    At iteration 5500 -> loss: 0.0935224053353962\n",
      "    At iteration 5600 -> loss: 0.0936083807689969\n",
      "    At iteration 5700 -> loss: 0.09356897185732822\n",
      "    At iteration 5800 -> loss: 0.09356223729072564\n",
      "    At iteration 5900 -> loss: 0.09350860434288054\n",
      "    At iteration 6000 -> loss: 0.09360609285648884\n",
      "    At iteration 6100 -> loss: 0.0935409297404892\n",
      "    At iteration 6200 -> loss: 0.09351479428245707\n",
      "    At iteration 6300 -> loss: 0.09347688223458817\n",
      "    At iteration 6400 -> loss: 0.09349161152594947\n",
      "    At iteration 6500 -> loss: 0.09344030643437895\n",
      "    At iteration 6600 -> loss: 0.09339877602658476\n",
      "    At iteration 6700 -> loss: 0.09334163021451136\n",
      "    At iteration 6800 -> loss: 0.0933399947089101\n",
      "    At iteration 6900 -> loss: 0.0933816786535268\n",
      "    At iteration 7000 -> loss: 0.09332486514133666\n",
      "    At iteration 7100 -> loss: 0.09337384678469492\n",
      "    At iteration 7200 -> loss: 0.09342293446010885\n",
      "    At iteration 7300 -> loss: 0.09341602653353902\n",
      "    At iteration 7400 -> loss: 0.09338774312570537\n",
      "    At iteration 7500 -> loss: 0.0933351567998565\n",
      "    At iteration 7600 -> loss: 0.09329518168579824\n",
      "    At iteration 7700 -> loss: 0.09325010831109051\n",
      "    At iteration 7800 -> loss: 0.09331513374394412\n",
      "    At iteration 7900 -> loss: 0.09327580305127312\n",
      "    At iteration 8000 -> loss: 0.0933795559576745\n",
      "    At iteration 8100 -> loss: 0.09332180760813144\n",
      "    At iteration 8200 -> loss: 0.09330685061220659\n",
      "    At iteration 8300 -> loss: 0.09330496842764009\n",
      "    At iteration 8400 -> loss: 0.09339175789850719\n",
      "    At iteration 8500 -> loss: 0.09335276122965984\n",
      "    At iteration 8600 -> loss: 0.09333440187818914\n",
      "    At iteration 8700 -> loss: 0.09330492242503317\n",
      "    At iteration 8800 -> loss: 0.09333828894933961\n",
      "    At iteration 8900 -> loss: 0.09332084160655503\n",
      "    At iteration 9000 -> loss: 0.09329500441238844\n",
      "    At iteration 9100 -> loss: 0.09325471618793066\n",
      "    At iteration 9200 -> loss: 0.09324615169410015\n",
      "    At iteration 9300 -> loss: 0.09322871385422825\n",
      "    At iteration 9400 -> loss: 0.09320672401317646\n",
      "    At iteration 9500 -> loss: 0.09321107977863724\n",
      "    At iteration 9600 -> loss: 0.09321998535399417\n",
      "    At iteration 9700 -> loss: 0.09326228180722371\n",
      "    At iteration 9800 -> loss: 0.09329303434811347\n",
      "    At iteration 9900 -> loss: 0.0932545502999838\n",
      "    At iteration 10000 -> loss: 0.09323731256163573\n",
      "    At iteration 10100 -> loss: 0.09324261263182265\n",
      "    At iteration 10200 -> loss: 0.09322091349457005\n",
      "    At iteration 10300 -> loss: 0.0932724603530346\n",
      "    At iteration 10400 -> loss: 0.09324763974462504\n",
      "    At iteration 10500 -> loss: 0.09323990962389739\n",
      "    At iteration 10600 -> loss: 0.09325555529497834\n",
      "    At iteration 10700 -> loss: 0.09324529335870649\n",
      "    At iteration 10800 -> loss: 0.09321041204441953\n",
      "    At iteration 10900 -> loss: 0.09319798932578928\n",
      "    At iteration 11000 -> loss: 0.09315910701581717\n",
      "    At iteration 11100 -> loss: 0.093177496015449\n",
      "    At iteration 11200 -> loss: 0.0931821793666554\n",
      "    At iteration 11300 -> loss: 0.09318576097053671\n",
      "    At iteration 11400 -> loss: 0.09319557849369355\n",
      "    At iteration 11500 -> loss: 0.09318259049342263\n",
      "    At iteration 11600 -> loss: 0.09317294161894166\n",
      "    At iteration 11700 -> loss: 0.09317030407556605\n",
      "    At iteration 11800 -> loss: 0.09317090492933282\n",
      "    At iteration 11900 -> loss: 0.09314565836445576\n",
      "    At iteration 12000 -> loss: 0.09311985457551104\n",
      "    At iteration 12100 -> loss: 0.0931104090236068\n",
      "    At iteration 12200 -> loss: 0.093157532369701\n",
      "    At iteration 12300 -> loss: 0.0931422985910395\n",
      "    At iteration 12400 -> loss: 0.09312582774909799\n",
      "    At iteration 12500 -> loss: 0.09314460946297083\n",
      "    At iteration 12600 -> loss: 0.09316008189533267\n",
      "    At iteration 12700 -> loss: 0.09313570694835546\n",
      "    At iteration 12800 -> loss: 0.09311063974059003\n",
      "    At iteration 12900 -> loss: 0.09309005474377355\n",
      "    At iteration 13000 -> loss: 0.09308123080526028\n",
      "    At iteration 13100 -> loss: 0.09307320737874326\n",
      "    At iteration 13200 -> loss: 0.09307303792530877\n",
      "    At iteration 13300 -> loss: 0.09305723718500561\n",
      "    At iteration 13400 -> loss: 0.09304540793094954\n",
      "    At iteration 13500 -> loss: 0.09303266090096332\n",
      "    At iteration 13600 -> loss: 0.09301461764919239\n",
      "Staring Epoch 137\n",
      "    At iteration 0 -> loss: 0.0949876201339066\n",
      "    At iteration 100 -> loss: 0.0896030351264685\n",
      "    At iteration 200 -> loss: 0.09024834567669818\n",
      "    At iteration 300 -> loss: 0.09180326456827809\n",
      "    At iteration 400 -> loss: 0.09132456105271039\n",
      "    At iteration 500 -> loss: 0.09121138203200713\n",
      "    At iteration 600 -> loss: 0.0986079776182583\n",
      "    At iteration 700 -> loss: 0.09760909027070491\n",
      "    At iteration 800 -> loss: 0.09663861550267204\n",
      "    At iteration 900 -> loss: 0.09600962921644789\n",
      "    At iteration 1000 -> loss: 0.09566182578453387\n",
      "    At iteration 1100 -> loss: 0.09561054960199589\n",
      "    At iteration 1200 -> loss: 0.09577181517892877\n",
      "    At iteration 1300 -> loss: 0.09585735933504644\n",
      "    At iteration 1400 -> loss: 0.09557394089934115\n",
      "    At iteration 1500 -> loss: 0.09513473171203654\n",
      "    At iteration 1600 -> loss: 0.09498219159669527\n",
      "    At iteration 1700 -> loss: 0.09479444204105873\n",
      "    At iteration 1800 -> loss: 0.09458685510037661\n",
      "    At iteration 1900 -> loss: 0.09486941646936288\n",
      "    At iteration 2000 -> loss: 0.09492415006448982\n",
      "    At iteration 2100 -> loss: 0.09485778521754865\n",
      "    At iteration 2200 -> loss: 0.0945963871838369\n",
      "    At iteration 2300 -> loss: 0.09469235476696299\n",
      "    At iteration 2400 -> loss: 0.09448933356093629\n",
      "    At iteration 2500 -> loss: 0.09435586810106891\n",
      "    At iteration 2600 -> loss: 0.09435446507228994\n",
      "    At iteration 2700 -> loss: 0.09428831935045044\n",
      "    At iteration 2800 -> loss: 0.09429354732321177\n",
      "    At iteration 2900 -> loss: 0.09419025686326489\n",
      "    At iteration 3000 -> loss: 0.09406013404247862\n",
      "    At iteration 3100 -> loss: 0.0939946472490245\n",
      "    At iteration 3200 -> loss: 0.09386667410360063\n",
      "    At iteration 3300 -> loss: 0.09372200821539078\n",
      "    At iteration 3400 -> loss: 0.09365087607343872\n",
      "    At iteration 3500 -> loss: 0.0938395397637596\n",
      "    At iteration 3600 -> loss: 0.09374995698158062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    At iteration 3700 -> loss: 0.09360897283567818\n",
      "    At iteration 3800 -> loss: 0.09363617403771281\n",
      "    At iteration 3900 -> loss: 0.09364591276877111\n",
      "    At iteration 4000 -> loss: 0.0938049066478905\n",
      "    At iteration 4100 -> loss: 0.09376844047238113\n",
      "    At iteration 4200 -> loss: 0.09370595029539881\n",
      "    At iteration 4300 -> loss: 0.09372947371637154\n",
      "    At iteration 4400 -> loss: 0.09374633111196691\n",
      "    At iteration 4500 -> loss: 0.09367136546641626\n",
      "    At iteration 4600 -> loss: 0.09388233773207597\n",
      "    At iteration 4700 -> loss: 0.09377470597956984\n",
      "    At iteration 4800 -> loss: 0.09371160538844796\n",
      "    At iteration 4900 -> loss: 0.09367113630411632\n",
      "    At iteration 5000 -> loss: 0.0935711081057671\n",
      "    At iteration 5100 -> loss: 0.093547054332114\n",
      "    At iteration 5200 -> loss: 0.09346200010642942\n",
      "    At iteration 5300 -> loss: 0.09341658724279157\n",
      "    At iteration 5400 -> loss: 0.09339143875333046\n",
      "    At iteration 5500 -> loss: 0.09335311963122658\n",
      "    At iteration 5600 -> loss: 0.09344970778376002\n",
      "    At iteration 5700 -> loss: 0.0934190199293073\n",
      "    At iteration 5800 -> loss: 0.09345935562715903\n",
      "    At iteration 5900 -> loss: 0.09368062529733326\n",
      "    At iteration 6000 -> loss: 0.09360849051833091\n",
      "    At iteration 6100 -> loss: 0.09366863717885128\n",
      "    At iteration 6200 -> loss: 0.09358221450185161\n",
      "    At iteration 6300 -> loss: 0.09354440265588569\n",
      "    At iteration 6400 -> loss: 0.09356228497199508\n",
      "    At iteration 6500 -> loss: 0.0935588588985767\n",
      "    At iteration 6600 -> loss: 0.09355666862796391\n",
      "    At iteration 6700 -> loss: 0.09355885242688176\n",
      "    At iteration 6800 -> loss: 0.09352629724219574\n",
      "    At iteration 6900 -> loss: 0.09347740841914011\n",
      "    At iteration 7000 -> loss: 0.0934624866345274\n",
      "    At iteration 7100 -> loss: 0.09347195446077966\n",
      "    At iteration 7200 -> loss: 0.093417439086912\n",
      "    At iteration 7300 -> loss: 0.09341958686407188\n",
      "    At iteration 7400 -> loss: 0.09343221265423585\n",
      "    At iteration 7500 -> loss: 0.09342273383607028\n",
      "    At iteration 7600 -> loss: 0.0934027669508248\n",
      "    At iteration 7700 -> loss: 0.09334951329699792\n",
      "    At iteration 7800 -> loss: 0.09343319041525434\n",
      "    At iteration 7900 -> loss: 0.09339452915394128\n",
      "    At iteration 8000 -> loss: 0.09338442441689902\n",
      "    At iteration 8100 -> loss: 0.09336159145328349\n",
      "    At iteration 8200 -> loss: 0.09336267480517427\n",
      "    At iteration 8300 -> loss: 0.09331004497350927\n",
      "    At iteration 8400 -> loss: 0.09334215930526528\n",
      "    At iteration 8500 -> loss: 0.09337418938292913\n",
      "    At iteration 8600 -> loss: 0.09335112948856536\n",
      "    At iteration 8700 -> loss: 0.0933028955169353\n",
      "    At iteration 8800 -> loss: 0.09327868172443005\n",
      "    At iteration 8900 -> loss: 0.09325302622634017\n",
      "    At iteration 9000 -> loss: 0.09330368083069182\n",
      "    At iteration 9100 -> loss: 0.0932773254037259\n",
      "    At iteration 9200 -> loss: 0.09325387358409505\n",
      "    At iteration 9300 -> loss: 0.0932652694823143\n",
      "    At iteration 9400 -> loss: 0.09327304445955405\n",
      "    At iteration 9500 -> loss: 0.09325919529948246\n",
      "    At iteration 9600 -> loss: 0.09325170016901545\n",
      "    At iteration 9700 -> loss: 0.09323005762731117\n",
      "    At iteration 9800 -> loss: 0.09321859969081657\n",
      "    At iteration 9900 -> loss: 0.09321083756621792\n",
      "    At iteration 10000 -> loss: 0.0932061816182865\n",
      "    At iteration 10100 -> loss: 0.093169755659655\n",
      "    At iteration 10200 -> loss: 0.09320563948023318\n",
      "    At iteration 10300 -> loss: 0.09320114381725256\n",
      "    At iteration 10400 -> loss: 0.09317681028435919\n",
      "    At iteration 10500 -> loss: 0.09313760231575377\n",
      "    At iteration 10600 -> loss: 0.09309909026384947\n",
      "    At iteration 10700 -> loss: 0.09316981161893602\n",
      "    At iteration 10800 -> loss: 0.09314347442921601\n",
      "    At iteration 10900 -> loss: 0.09310172408487227\n",
      "    At iteration 11000 -> loss: 0.09307452433547685\n",
      "    At iteration 11100 -> loss: 0.09306048811870728\n",
      "    At iteration 11200 -> loss: 0.09308293827446366\n",
      "    At iteration 11300 -> loss: 0.093055910967361\n",
      "    At iteration 11400 -> loss: 0.09304231920089362\n",
      "    At iteration 11500 -> loss: 0.09308683580155015\n",
      "    At iteration 11600 -> loss: 0.09306414114433126\n",
      "    At iteration 11700 -> loss: 0.09304736860889333\n",
      "    At iteration 11800 -> loss: 0.09302482086384027\n",
      "    At iteration 11900 -> loss: 0.09301265894398039\n",
      "    At iteration 12000 -> loss: 0.09305470169187546\n",
      "    At iteration 12100 -> loss: 0.0931333791681129\n",
      "    At iteration 12200 -> loss: 0.09310140008907467\n",
      "    At iteration 12300 -> loss: 0.09307856745241602\n",
      "    At iteration 12400 -> loss: 0.09309368388102905\n",
      "    At iteration 12500 -> loss: 0.0930857780755668\n",
      "    At iteration 12600 -> loss: 0.09314352108355732\n",
      "    At iteration 12700 -> loss: 0.09312268551794138\n",
      "    At iteration 12800 -> loss: 0.09310273356884972\n",
      "    At iteration 12900 -> loss: 0.09308050304262401\n",
      "    At iteration 13000 -> loss: 0.09305865384599012\n",
      "    At iteration 13100 -> loss: 0.09303140179234382\n",
      "    At iteration 13200 -> loss: 0.09302907063441117\n",
      "    At iteration 13300 -> loss: 0.09302278540385982\n",
      "    At iteration 13400 -> loss: 0.09301738594269657\n",
      "    At iteration 13500 -> loss: 0.09298796688720032\n",
      "    At iteration 13600 -> loss: 0.09298374087413722\n",
      "Staring Epoch 138\n",
      "    At iteration 0 -> loss: 0.08055051621340681\n",
      "    At iteration 100 -> loss: 0.08989472926854786\n",
      "    At iteration 200 -> loss: 0.09407726322129667\n",
      "    At iteration 300 -> loss: 0.09360758957648252\n",
      "    At iteration 400 -> loss: 0.09313513738780886\n",
      "    At iteration 500 -> loss: 0.09242891249300009\n",
      "    At iteration 600 -> loss: 0.09364546739958277\n",
      "    At iteration 700 -> loss: 0.09373049657381421\n",
      "    At iteration 800 -> loss: 0.09331126599813715\n",
      "    At iteration 900 -> loss: 0.0928186830649263\n",
      "    At iteration 1000 -> loss: 0.09248799149263605\n",
      "    At iteration 1100 -> loss: 0.0923519979889058\n",
      "    At iteration 1200 -> loss: 0.09214549163889767\n",
      "    At iteration 1300 -> loss: 0.09212247439887142\n",
      "    At iteration 1400 -> loss: 0.09203044022256673\n",
      "    At iteration 1500 -> loss: 0.09194474548123426\n",
      "    At iteration 1600 -> loss: 0.0919162392158834\n",
      "    At iteration 1700 -> loss: 0.09191062453053675\n",
      "    At iteration 1800 -> loss: 0.09272807597053076\n",
      "    At iteration 1900 -> loss: 0.09268838263766545\n",
      "    At iteration 2000 -> loss: 0.09256356039461525\n",
      "    At iteration 2100 -> loss: 0.09244824974782723\n",
      "    At iteration 2200 -> loss: 0.09241824676168296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26843/3628099697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"Staring Epoch {epoch}\")\n",
    "    model.to(device)\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    for i, (centers, lefts, rights) in enumerate(data_generator):\n",
    "        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n",
    "        optimizer.zero_grad()\n",
    "        datas = [centers, lefts, rights]\n",
    "        for data in datas:\n",
    "            imgs, angles = data\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, angles.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.data.item()\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print(f\"    At iteration {i} -> loss: {train_loss / (i+1)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.080 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.089 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.090 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.092 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.095 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.093 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n",
      "Valid Loss: 0.094 \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "valid_loss = 0\n",
    "with torch.set_grad_enabled(False):\n",
    "    for local_batch, (centers, lefts, rights) in enumerate(data_generator):\n",
    "        # Transfer to GPU\n",
    "        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n",
    "\n",
    "        # Model computations\n",
    "        optimizer.zero_grad()\n",
    "        datas = [centers, lefts, rights]        \n",
    "        for data in datas:\n",
    "            imgs, angles = data\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, angles.unsqueeze(1))\n",
    "\n",
    "            valid_loss += loss.data.item()\n",
    "\n",
    "        if local_batch % 10 == 0:\n",
    "            print('Valid Loss: %.3f '\n",
    "                 % (valid_loss/(local_batch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step8: Define state and save the model wrt to state\n",
    "# state = {\n",
    "#         'model': model.module if device == 'cuda' else model,\n",
    "#         }\n",
    "\n",
    "# torch.save(state, 'model.h5')\n",
    "torch.save(model, 'model.h5')\n",
    "# torch.save(model.state_dict(), 'state_dict.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
